{"meta":{"title":"wang's dayliy document","subtitle":null,"description":null,"author":"Kuiq  Wang","url":"http://www.wqkenqing.ren/daydoc","root":"/daydoc/"},"pages":[{"title":"categories","date":"2020-05-22T02:41:30.000Z","updated":"2023-10-27T06:50:27.036Z","comments":true,"path":"categories/index.html","permalink":"http://www.wqkenqing.ren/daydoc/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-05-22T02:42:26.000Z","updated":"2023-10-27T06:50:27.036Z","comments":true,"path":"tags/index.html","permalink":"http://www.wqkenqing.ren/daydoc/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"","slug":"王阁/专题/运维专题/备份/备份脚本","date":"2024-05-10T02:26:50.704Z","updated":"2024-05-10T02:27:10.861Z","comments":true,"path":"2024/05/10/王阁/专题/运维专题/备份/备份脚本/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/05/10/%E7%8E%8B%E9%98%81/%E4%B8%93%E9%A2%98/%E8%BF%90%E7%BB%B4%E4%B8%93%E9%A2%98/%E5%A4%87%E4%BB%BD/%E5%A4%87%E4%BB%BD%E8%84%9A%E6%9C%AC/","excerpt":"","text":"123456789101112131415161718192021:#!/bin/bashdate_suffix=$(date +&quot;%Y%m%d%%h%M%s&quot;)archive_dir=/data2/backuptar -cvf $archive_dir/$1_$date_suffix.tar.gz $2file_count=$(ls -1 &quot;$archive_dir&quot; | wc -l)# 如果文件数量大于7，则删除最旧的文件if [ &quot;$file_count&quot; -gt 7 ]; then # 列出归档目录中的所有文件，并按修改时间升序排序 files_to_delete=($(ls -1t &quot;$archive_dir&quot;)) # 循环删除多余的文件，只保留最新的七个 for ((i=$&#123;#files_to_delete[@]&#125;-1; i&gt;=7; i--)); do file_to_delete=&quot;$archive_dir/$&#123;files_to_delete[i]&#125;&quot; rm &quot;$file_to_delete&quot; echo &quot;Deleted: $file_to_delete&quot; donefi","categories":[],"tags":[]},{"title":"","slug":"王阁/专题/运维专题/备份/日志备份说明","date":"2024-05-09T08:27:03.921Z","updated":"2024-05-09T10:10:11.328Z","comments":true,"path":"2024/05/09/王阁/专题/运维专题/备份/日志备份说明/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/05/09/%E7%8E%8B%E9%98%81/%E4%B8%93%E9%A2%98/%E8%BF%90%E7%BB%B4%E4%B8%93%E9%A2%98/%E5%A4%87%E4%BB%BD/%E6%97%A5%E5%BF%97%E5%A4%87%E4%BB%BD%E8%AF%B4%E6%98%8E/","excerpt":"","text":"按等保要求进行相关日志备份功能处理 1、日志备份功能设计说明本功能按等保要求有以下功能设计 1.1 设有三台日志集中存储节点，用于集中存储所有节点日志信息，存储最大限为最近30天的数据 集中存储节点分别为hostname(ip) datanode1(172.16.9.62) datanode2(172.16.9.64)","categories":[],"tags":[]},{"title":"","slug":"王阁/专题/知识库构建","date":"2024-05-07T09:00:19.781Z","updated":"2024-05-07T09:01:42.623Z","comments":true,"path":"2024/05/07/王阁/专题/知识库构建/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/05/07/%E7%8E%8B%E9%98%81/%E4%B8%93%E9%A2%98/%E7%9F%A5%E8%AF%86%E5%BA%93%E6%9E%84%E5%BB%BA/","excerpt":"","text":"构建知识库思路","categories":[],"tags":[]},{"title":"","slug":"王阁/专题/广告语","date":"2024-05-06T13:13:12.283Z","updated":"2024-05-07T08:49:27.900Z","comments":true,"path":"2024/05/06/王阁/专题/广告语/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/05/06/%E7%8E%8B%E9%98%81/%E4%B8%93%E9%A2%98/%E5%B9%BF%E5%91%8A%E8%AF%AD/","excerpt":"","text":"1姐妹们天气热了，这款冰丝豆豆的靠背赶紧安排起来吧，放在床上追个剧，看着书都特别舒服。这个角度刚好能够支撑整个后背和脖子采用冰丝豆豆面料凉而不冰，而且可拆卸清洗，方便放在哪里都好看。","categories":[],"tags":[]},{"title":"","slug":"王阁/专题/研究生专题/Untitled","date":"2024-04-26T07:51:12.739Z","updated":"2024-04-26T09:15:04.695Z","comments":true,"path":"2024/04/26/王阁/专题/研究生专题/Untitled/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/04/26/%E7%8E%8B%E9%98%81/%E4%B8%93%E9%A2%98/%E7%A0%94%E7%A9%B6%E7%94%9F%E4%B8%93%E9%A2%98/Untitled/","excerpt":"","text":"你好，蒋老师。 在报考农业信息与工程技术这个专业之前，我做过一定的功课了解这个专业，了解到这个专业是农业与信息技术专业的一个交叉学科。旨在运用信息技术在传统农业基础上进行融合，创新，赋能。 我以我老家门口改建的果园进行过相关联想。我擅长的大数据业务领域中主要的内容可以分为四块“采”、“管”、“治”、“用”四个大块，我以此为基础，尝试去构建智慧果园。 采： 1. 各种传感器，收集农作物生物样本信息、空气湿度、温度等信息。2、实时采集天气信息，为后续管理模块提供预警数据源。3、采集农作物国际、国内市场价格走势等信息。 管： 构建丰富的数据与设备等内容的管理模块，实现各种设备的动态调控，实现各种管理预案的自动执行 治：1. 治理收集到的数据中的无效无用数据。2. 分析收集的到各种数据信息，如农作物的生长数据，根据生长数据的监测，进行相应的治理。 用：1. 我理解这里的用就是通过这套多体系联动系统，合理运用，致使相应的农作物，健康茁壮生长，从而产生更好的经济价值。 我个人对物联网硬件与数据信号的联动，这块内容了解较少，我当时有在想通过此种尝试，扩充我这方面的能力。 更长远的考虑中，我有想这套数据与设备的管理与联动，过程中有许多决策与执行，想在这块尝试机器学习介入的可能，这块也是我的盲点，在我目前的工作中难有实质性的突破，更多的是走到这块内容时，由团队内部其它专业人士负责。 以上是我对这个专业的简单思考。","categories":[],"tags":[]},{"title":"","slug":"王阁/专题/研究生专题/选导师2","date":"2024-04-25T11:41:26.757Z","updated":"2024-04-25T11:44:59.224Z","comments":true,"path":"2024/04/25/王阁/专题/研究生专题/选导师2/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/04/25/%E7%8E%8B%E9%98%81/%E4%B8%93%E9%A2%98/%E7%A0%94%E7%A9%B6%E7%94%9F%E4%B8%93%E9%A2%98/%E9%80%89%E5%AF%BC%E5%B8%882/","excerpt":"","text":"魏维导师1231．领域软件工程2．图形图像与可视化3．软件自动生成与智能服务","categories":[],"tags":[]},{"title":"","slug":"王阁/专题/研究生专题/选导师","date":"2024-04-23T06:46:20.704Z","updated":"2024-04-23T07:57:11.300Z","comments":true,"path":"2024/04/23/王阁/专题/研究生专题/选导师/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/04/23/%E7%8E%8B%E9%98%81/%E4%B8%93%E9%A2%98/%E7%A0%94%E7%A9%B6%E7%94%9F%E4%B8%93%E9%A2%98/%E9%80%89%E5%AF%BC%E5%B8%88/","excerpt":"","text":"安老师 何磊老师 赵卓宁老师管的很严 能避就避 https://rjgcxy.cuit.edu.cn/yjsjy/dsjj.htm","categories":[],"tags":[]},{"title":"","slug":"王阁/专题/软考专题/什么是软考","date":"2024-04-11T08:36:05.034Z","updated":"2024-04-11T08:57:12.335Z","comments":true,"path":"2024/04/11/王阁/专题/软考专题/什么是软考/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/04/11/%E7%8E%8B%E9%98%81/%E4%B8%93%E9%A2%98/%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98/%E4%BB%80%E4%B9%88%E6%98%AF%E8%BD%AF%E8%80%83/","excerpt":"","text":"软考相关 高级资格： 网络规划设计师 系统架构设计师 信息系统项目管理师 系统分析师 系统规划与管理师（2017年新增）；","categories":[],"tags":[]},{"title":"","slug":"王阁/path","date":"2024-04-11T06:51:31.915Z","updated":"2024-04-11T06:51:31.988Z","comments":true,"path":"2024/04/11/王阁/path/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/04/11/%E7%8E%8B%E9%98%81/path/","excerpt":"","text":"..&#x2F;自研项目.&#x2F;自研项目&#x2F;古诗词竞赛.&#x2F;自研项目&#x2F;古诗词竞赛&#x2F;诗词竞赛业务架构.md.&#x2F;自研项目&#x2F;古诗词竞赛&#x2F;建设思路.md.&#x2F;自研项目&#x2F;接口测试与性能测试演示说明.md.&#x2F;回顾.&#x2F;回顾&#x2F;技能回顾.&#x2F;回顾&#x2F;技能回顾&#x2F;大数据相关组件.&#x2F;回顾&#x2F;技能回顾&#x2F;大数据相关组件&#x2F;hadoop发展史.md.&#x2F;回顾&#x2F;技能回顾&#x2F;大数据相关组件&#x2F;存储组件.md.&#x2F;回顾&#x2F;技能回顾&#x2F;整体规划思路.md.&#x2F;回顾&#x2F;备战.&#x2F;回顾&#x2F;备战&#x2F;组件-hdfs篇.md.&#x2F;回顾&#x2F;备战&#x2F;组件-hive篇.md.&#x2F;回顾&#x2F;备战&#x2F;Untitled.md.&#x2F;回顾&#x2F;备战&#x2F;组件-hbase篇.md.&#x2F;回顾&#x2F;备战&#x2F;业务问题篇.md.&#x2F;回顾&#x2F;备战&#x2F;HiveSql.md.&#x2F;回顾&#x2F;备战&#x2F;README.md.&#x2F;回顾&#x2F;备战&#x2F;hadoop版本差异说明.md.&#x2F;回顾&#x2F;备战&#x2F;建档流程.md.&#x2F;回顾&#x2F;备战&#x2F;基础说明.md.&#x2F;回顾&#x2F;hive.&#x2F;回顾&#x2F;hive&#x2F;hive梳理2.md.&#x2F;回顾&#x2F;hive&#x2F;hive练习实操记录.md.&#x2F;回顾&#x2F;hive&#x2F;hive梳理.md.&#x2F;回顾&#x2F;hive&#x2F;hive应用案例.md.&#x2F;回顾&#x2F;hive&#x2F;hive回顾.md.&#x2F;.DS_Store.&#x2F;技术.&#x2F;技术&#x2F;运维.&#x2F;技术&#x2F;运维&#x2F;yd_server_config.&#x2F;技术&#x2F;运维&#x2F;yd_server_config&#x2F;README.md.&#x2F;技术&#x2F;运维&#x2F;yd_server_config&#x2F;sjksh_store_info.&#x2F;技术&#x2F;运维&#x2F;yd_server_config&#x2F;sjksh_store_info&#x2F;docker-compose.yaml.&#x2F;技术&#x2F;运维&#x2F;yd_server_config&#x2F;sjksh_store_info&#x2F;common.env.&#x2F;技术&#x2F;运维&#x2F;docker.&#x2F;技术&#x2F;运维&#x2F;docker&#x2F;Dockerfile基础&amp;编写.md.&#x2F;技术&#x2F;运维&#x2F;docker&#x2F;docker镜像.md.&#x2F;技术&#x2F;运维&#x2F;docker&#x2F;docker.md.&#x2F;技术&#x2F;运维&#x2F;docker&#x2F;docker操作.md.&#x2F;技术&#x2F;运维&#x2F;调度.&#x2F;技术&#x2F;运维&#x2F;调度&#x2F;调度组件部署.md.&#x2F;技术&#x2F;运维&#x2F;owncloud.&#x2F;技术&#x2F;运维&#x2F;owncloud&#x2F;账号.md.&#x2F;技术&#x2F;运维&#x2F;hy-sjksh-info-dev.&#x2F;技术&#x2F;运维&#x2F;hy-sjksh-info-dev&#x2F;docker-compose.yaml.&#x2F;技术&#x2F;运维&#x2F;hy-sjksh-info-dev&#x2F;common.env.&#x2F;技术&#x2F;运维&#x2F;ELK.&#x2F;技术&#x2F;运维&#x2F;ELK&#x2F;logstash.&#x2F;技术&#x2F;运维&#x2F;ELK&#x2F;logstash&#x2F;beats.md.&#x2F;技术&#x2F;运维&#x2F;ELK&#x2F;logstash&#x2F;logstash.md.&#x2F;技术&#x2F;运维&#x2F;ELK&#x2F;kibana使用.md.&#x2F;技术&#x2F;运维&#x2F;cdh.&#x2F;技术&#x2F;运维&#x2F;cdh&#x2F;hue.md.&#x2F;技术&#x2F;运维&#x2F;flinkSql.&#x2F;技术&#x2F;运维&#x2F;flinkSql&#x2F;flinksql.md.&#x2F;技术&#x2F;运维&#x2F;ubuntu.&#x2F;技术&#x2F;运维&#x2F;ubuntu&#x2F;desk_gui.md.&#x2F;技术&#x2F;运维&#x2F;ubuntu&#x2F;linux自启服务.md.&#x2F;技术&#x2F;运维&#x2F;front_end.&#x2F;技术&#x2F;运维&#x2F;front_end&#x2F;接口规划.&#x2F;技术&#x2F;运维&#x2F;front_end&#x2F;接口规划&#x2F;data-manager.md.&#x2F;技术&#x2F;运维&#x2F;front_end&#x2F;nginx部署.md.&#x2F;技术&#x2F;运维&#x2F;front_end&#x2F;conf.&#x2F;技术&#x2F;运维&#x2F;front_end&#x2F;conf&#x2F;h-ui.conf.&#x2F;技术&#x2F;运维&#x2F;front_end&#x2F;调研.md.&#x2F;技术&#x2F;operate.md.&#x2F;技术&#x2F;专题.&#x2F;技术&#x2F;专题&#x2F;sql专题.&#x2F;技术&#x2F;专题&#x2F;sql专题&#x2F;sql基础知识.md.&#x2F;技术&#x2F;专题&#x2F;数据建模.&#x2F;技术&#x2F;专题&#x2F;数据建模&#x2F;数据指标体系.md.&#x2F;技术&#x2F;专题&#x2F;数据建模&#x2F;数据仓库建设方案.md.&#x2F;技术&#x2F;专题&#x2F;技术专题.md.&#x2F;技术&#x2F;数据库.&#x2F;技术&#x2F;数据库&#x2F;sql储备&amp;练习.md.&#x2F;技术&#x2F;大数据.&#x2F;技术&#x2F;大数据&#x2F;flume.&#x2F;技术&#x2F;大数据&#x2F;flume&#x2F;config.&#x2F;技术&#x2F;大数据&#x2F;flume&#x2F;config&#x2F;kafka_spool.conf.&#x2F;技术&#x2F;大数据&#x2F;flume&#x2F;Dockerfile.&#x2F;技术&#x2F;大数据&#x2F;flume&#x2F;Flume.md.&#x2F;技术&#x2F;大数据&#x2F;flume&#x2F;new.&#x2F;技术&#x2F;大数据&#x2F;flume&#x2F;docker版flume使用.md.&#x2F;技术&#x2F;大数据&#x2F;flink.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;2023.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;2023&#x2F;Flink 状态编程.md.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;2023&#x2F;容错机制.md.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;2023&#x2F;Flink_Time_Window.md.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;2023&#x2F;Flink学习历程.md.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;2023&#x2F;Flink Connector.md.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;2023&#x2F;flink_study.md.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;2023&#x2F;Flink多流转换.md.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;2023&#x2F;Flink_Process_Function.md.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;2023&#x2F;FlinkTable&amp;SQL.md.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;2023&#x2F;flink_transform.md.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;2023&#x2F;FlinkSource.md.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;flink任务提交.md.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;flink.md.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;flink_book.md.&#x2F;技术&#x2F;大数据&#x2F;flink&#x2F;Flink记录.md.&#x2F;技术&#x2F;大数据&#x2F;atlas.md.&#x2F;技术&#x2F;大数据&#x2F;yarn.&#x2F;技术&#x2F;大数据&#x2F;yarn&#x2F;YarnState.md.&#x2F;技术&#x2F;大数据&#x2F;Mongodb.&#x2F;技术&#x2F;大数据&#x2F;Mongodb&#x2F;常用命令.md.&#x2F;技术&#x2F;大数据&#x2F;集群.&#x2F;技术&#x2F;大数据&#x2F;集群&#x2F;.bashrc.&#x2F;技术&#x2F;大数据&#x2F;集群&#x2F;zookeeper.&#x2F;技术&#x2F;大数据&#x2F;集群&#x2F;zookeeper&#x2F;zoo.cfg.&#x2F;技术&#x2F;大数据&#x2F;集群&#x2F;hdfs.&#x2F;技术&#x2F;大数据&#x2F;集群&#x2F;hdfs&#x2F;yarn-site.xml.&#x2F;技术&#x2F;大数据&#x2F;集群&#x2F;hdfs&#x2F;hdfs-site.xml.&#x2F;技术&#x2F;大数据&#x2F;集群&#x2F;hdfs&#x2F;core-site.xml.&#x2F;技术&#x2F;大数据&#x2F;集群&#x2F;hdfs&#x2F;slaves.&#x2F;技术&#x2F;大数据&#x2F;集群&#x2F;hdfs&#x2F;mapred-site.xml.&#x2F;技术&#x2F;大数据&#x2F;集群&#x2F;hbase.&#x2F;技术&#x2F;大数据&#x2F;集群&#x2F;hbase&#x2F;regionservers.&#x2F;技术&#x2F;大数据&#x2F;集群&#x2F;hbase&#x2F;hbase-site.xml.&#x2F;技术&#x2F;大数据&#x2F;集群&#x2F;私有集群.md.&#x2F;技术&#x2F;大数据&#x2F;kafka.&#x2F;技术&#x2F;大数据&#x2F;kafka&#x2F;kafka极客.md.&#x2F;技术&#x2F;大数据&#x2F;kafka&#x2F;kafka小结.md.&#x2F;技术&#x2F;大数据&#x2F;kafka&#x2F;kafka疑问点.md.&#x2F;技术&#x2F;大数据&#x2F;kafka&#x2F;kafka原理.md.&#x2F;技术&#x2F;大数据&#x2F;hbase.&#x2F;技术&#x2F;大数据&#x2F;hbase&#x2F;hbase20190109.md.&#x2F;技术&#x2F;大数据&#x2F;hbase&#x2F;hbase_api升级.md.&#x2F;技术&#x2F;大数据&#x2F;hive.&#x2F;技术&#x2F;大数据&#x2F;hive&#x2F;2023.&#x2F;技术&#x2F;大数据&#x2F;hive&#x2F;2023&#x2F;hive_learn.md.&#x2F;技术&#x2F;大数据&#x2F;hive&#x2F;hivsql.&#x2F;技术&#x2F;大数据&#x2F;hive&#x2F;hive20190109.md.&#x2F;技术&#x2F;大数据&#x2F;es.&#x2F;技术&#x2F;大数据&#x2F;es&#x2F;积累.md.&#x2F;技术&#x2F;大数据&#x2F;es&#x2F;es的docker安装.md.&#x2F;技术&#x2F;大数据&#x2F;es&#x2F;es常用命令.md.&#x2F;技术&#x2F;大数据&#x2F;es&#x2F;共享数据集合.md.&#x2F;技术&#x2F;大数据&#x2F;es&#x2F;esDSL详解.md.&#x2F;技术&#x2F;大数据&#x2F;es&#x2F;es详细.md.&#x2F;技术&#x2F;大数据&#x2F;es&#x2F;es小结.md.&#x2F;技术&#x2F;大数据&#x2F;es&#x2F;elk使用.md.&#x2F;技术&#x2F;大数据&#x2F;spark.&#x2F;技术&#x2F;大数据&#x2F;spark&#x2F;spark-streaming.md.&#x2F;技术&#x2F;编程.&#x2F;技术&#x2F;编程&#x2F;python.&#x2F;技术&#x2F;编程&#x2F;python&#x2F;learn.&#x2F;技术&#x2F;编程&#x2F;python&#x2F;reptile.&#x2F;技术&#x2F;编程&#x2F;python&#x2F;reptile&#x2F;python爬虫篇.md.&#x2F;技术&#x2F;编程&#x2F;python&#x2F;summary.&#x2F;技术&#x2F;编程&#x2F;python&#x2F;summary&#x2F;python小结4.md.&#x2F;技术&#x2F;编程&#x2F;python&#x2F;summary&#x2F;2020.3.17.md.&#x2F;技术&#x2F;编程&#x2F;python&#x2F;summary&#x2F;python5小结.md.&#x2F;技术&#x2F;编程&#x2F;python&#x2F;summary&#x2F;python小结3.md.&#x2F;技术&#x2F;编程&#x2F;java.&#x2F;技术&#x2F;编程&#x2F;java&#x2F;计算处理.md.&#x2F;技术&#x2F;编程&#x2F;java&#x2F;并发编程.md.&#x2F;技术&#x2F;编程&#x2F;java&#x2F;日期处理.md.&#x2F;技术&#x2F;编程&#x2F;bash.&#x2F;技术&#x2F;编程&#x2F;bash&#x2F;yd.&#x2F;技术&#x2F;编程&#x2F;bash&#x2F;yd&#x2F;manager.sh.&#x2F;技术&#x2F;编程&#x2F;bash&#x2F;awk.&#x2F;技术&#x2F;编程&#x2F;bash&#x2F;awk&#x2F;script.&#x2F;技术&#x2F;编程&#x2F;bash&#x2F;awk&#x2F;script&#x2F;number.awk.&#x2F;技术&#x2F;编程&#x2F;bash&#x2F;awk&#x2F;script&#x2F;add.awk.&#x2F;技术&#x2F;编程&#x2F;bash&#x2F;awk&#x2F;script&#x2F;cal.awk.&#x2F;技术&#x2F;编程&#x2F;bash&#x2F;awk&#x2F;script&#x2F;example.txt.&#x2F;技术&#x2F;编程&#x2F;bash&#x2F;awk&#x2F;script&#x2F;demo.html.&#x2F;技术&#x2F;编程&#x2F;bash&#x2F;awk&#x2F;awk.md.&#x2F;技术&#x2F;编程&#x2F;bash&#x2F;python_learn.md.&#x2F;技术&#x2F;编程&#x2F;bash&#x2F;shell积累.md.&#x2F;技术&#x2F;编程&#x2F;bash&#x2F;order.sh.&#x2F;技术&#x2F;编程&#x2F;datastructure.&#x2F;技术&#x2F;编程&#x2F;datastructure&#x2F;ds.md.&#x2F;技术&#x2F;编程&#x2F;datastructure&#x2F;未命名.md.&#x2F;技术&#x2F;编程&#x2F;datastructure&#x2F;线性表、栈、队列.md.&#x2F;技术&#x2F;编程&#x2F;datastructure&#x2F;测试计划.pdf.&#x2F;技术&#x2F;编程&#x2F;datastructure&#x2F;归档.zip.&#x2F;技术&#x2F;编程&#x2F;datastructure&#x2F;测试报告.pdf.&#x2F;技术&#x2F;编程&#x2F;datastructure&#x2F;tree.md.&#x2F;技术&#x2F;编程&#x2F;datastructure&#x2F;树.md.&#x2F;技术&#x2F;编程&#x2F;scala.&#x2F;技术&#x2F;编程&#x2F;scala&#x2F;scala学习.md.&#x2F;技术&#x2F;编程&#x2F;c.&#x2F;技术&#x2F;编程&#x2F;c&#x2F;python.md.&#x2F;技术&#x2F;编程&#x2F;c&#x2F;c_learn.md.&#x2F;技术&#x2F;编程&#x2F;c&#x2F;c_ algorithm.md.&#x2F;技术&#x2F;编程&#x2F;c&#x2F;程序设计与数据结构.md.&#x2F;技术&#x2F;编程&#x2F;c&#x2F;C复试.md.&#x2F;技术&#x2F;课程记录.&#x2F;技术&#x2F;课程记录&#x2F;es专项.&#x2F;技术&#x2F;课程记录&#x2F;es专项&#x2F;es记录.md.&#x2F;技术&#x2F;课程记录&#x2F;Flink专项.&#x2F;技术&#x2F;课程记录&#x2F;Flink专项&#x2F;flink记录.md.&#x2F;技术&#x2F;考研.&#x2F;技术&#x2F;考研&#x2F;C学习笔记.md.&#x2F;技术&#x2F;考研&#x2F;代码.&#x2F;技术&#x2F;考研&#x2F;代码&#x2F;demo.&#x2F;技术&#x2F;考研&#x2F;代码&#x2F;demo2.c.&#x2F;技术&#x2F;考研&#x2F;代码&#x2F;demo2.&#x2F;技术&#x2F;考研&#x2F;代码&#x2F;demo.c.&#x2F;技术&#x2F;项目.&#x2F;技术&#x2F;项目&#x2F;README.md.&#x2F;技术&#x2F;项目&#x2F;登陆与sso.&#x2F;技术&#x2F;项目&#x2F;登陆与sso&#x2F;后端功能实现.md.&#x2F;技术&#x2F;项目&#x2F;登陆与sso&#x2F;login&amp;sso.md.&#x2F;技术&#x2F;项目&#x2F;开发过程.&#x2F;技术&#x2F;项目&#x2F;开发过程&#x2F;测试报告修改意见.md.&#x2F;技术&#x2F;项目&#x2F;开发过程&#x2F;川藏会议.md.&#x2F;技术&#x2F;项目&#x2F;开发过程&#x2F;川藏会议.pdf.&#x2F;技术&#x2F;项目&#x2F;开发过程&#x2F;2023.08.9安全问题处理.pdf.&#x2F;技术&#x2F;项目&#x2F;开发过程&#x2F;10.17发版内容.pdf.&#x2F;技术&#x2F;项目&#x2F;开发过程&#x2F;片段.md.&#x2F;技术&#x2F;项目&#x2F;开发过程&#x2F;王奎清的发言.md.&#x2F;技术&#x2F;项目&#x2F;开发过程&#x2F;数据基础组件安全升级方案.md.&#x2F;技术&#x2F;项目&#x2F;开发过程&#x2F;测试计划.md.&#x2F;技术&#x2F;项目&#x2F;开发过程&#x2F;curl ‘httpsapi.md.&#x2F;技术&#x2F;hexo.&#x2F;技术&#x2F;hexo&#x2F;flink.&#x2F;技术&#x2F;hexo&#x2F;flink&#x2F;Flink.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog2.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog18.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog6.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog7.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog3.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog19.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog8.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog26.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog12.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog16.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog22.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog17.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog23.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog9.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog13.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog14.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog20.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog24.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog10.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog25.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog11.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog15.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog21.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog4.md.&#x2F;技术&#x2F;hexo&#x2F;oldblog&#x2F;blog1.md.&#x2F;技术&#x2F;hexo&#x2F;old.&#x2F;技术&#x2F;hexo&#x2F;old&#x2F;kafka.md.&#x2F;技术&#x2F;hexo&#x2F;old&#x2F;sqoop记录.md.&#x2F;技术&#x2F;hexo&#x2F;old&#x2F;hbase积累.md.&#x2F;技术&#x2F;hexo&#x2F;old&#x2F;spark学习.md.&#x2F;技术&#x2F;hexo&#x2F;old&#x2F;flume记录.md.&#x2F;技术&#x2F;hexo&#x2F;old&#x2F;准备小结.md.&#x2F;技术&#x2F;hexo&#x2F;old&#x2F;hive总结.md.&#x2F;技术&#x2F;hexo&#x2F;old&#x2F;spark算子.md.&#x2F;技术&#x2F;hexo&#x2F;old&#x2F;sqoop实现方案.md.&#x2F;技术&#x2F;hexo&#x2F;old&#x2F;Yarn配置.md.&#x2F;技术&#x2F;hexo&#x2F;old&#x2F;mapreduce组件总结.md.&#x2F;技术&#x2F;hexo&#x2F;old&#x2F;spark学习3.md.&#x2F;技术&#x2F;hexo&#x2F;old&#x2F;大数据相关分享.md.&#x2F;技术&#x2F;hexo&#x2F;old&#x2F;Lamda积累.md.&#x2F;技术&#x2F;hexo&#x2F;old&#x2F;spark学习2.md.&#x2F;技术&#x2F;hexo&#x2F;CDH.&#x2F;技术&#x2F;hexo&#x2F;CDH&#x2F;cdh集成phoenix.md.&#x2F;技术&#x2F;hexo&#x2F;CDH&#x2F;搭建.md.&#x2F;技术&#x2F;hexo&#x2F;kafka.&#x2F;技术&#x2F;hexo&#x2F;kafka&#x2F;kafka实现.md.&#x2F;技术&#x2F;hexo&#x2F;hadoop.&#x2F;技术&#x2F;hexo&#x2F;hadoop&#x2F;hadoopHA搭建.md.&#x2F;技术&#x2F;hexo&#x2F;hadoop&#x2F;hdfs.&#x2F;技术&#x2F;hexo&#x2F;hadoop&#x2F;hdfs&#x2F;hdfs操作.md.&#x2F;技术&#x2F;hexo&#x2F;hadoop&#x2F;hdfs&#x2F;hdfs命令.md.&#x2F;技术&#x2F;hexo&#x2F;hadoop&#x2F;hbase.&#x2F;技术&#x2F;hexo&#x2F;hadoop&#x2F;hbase&#x2F;hbase操作.md.&#x2F;技术&#x2F;hexo&#x2F;Users.&#x2F;技术&#x2F;hexo&#x2F;Users&#x2F;wqkenqing.&#x2F;技术&#x2F;hexo&#x2F;Users&#x2F;wqkenqing&#x2F;Desktop.&#x2F;技术&#x2F;hexo&#x2F;Users&#x2F;wqkenqing&#x2F;Desktop&#x2F;qiniu_idea.&#x2F;技术&#x2F;hexo&#x2F;Users&#x2F;wqkenqing&#x2F;Desktop&#x2F;qiniu_idea&#x2F;WSZpdf.png.&#x2F;技术&#x2F;hexo&#x2F;Users&#x2F;wqkenqing&#x2F;Desktop&#x2F;qiniu_idea&#x2F;GKW7SA.png.&#x2F;技术&#x2F;hexo&#x2F;Thread.&#x2F;技术&#x2F;hexo&#x2F;Thread&#x2F;多线程.md.&#x2F;技术&#x2F;hexo&#x2F;spark.&#x2F;技术&#x2F;hexo&#x2F;spark&#x2F;spark操作.md.&#x2F;技术&#x2F;hexo&#x2F;spark&#x2F;算子.&#x2F;技术&#x2F;hexo&#x2F;spark&#x2F;算子&#x2F;Spark-Windos函数.md.&#x2F;技术&#x2F;hexo&#x2F;spark&#x2F;算子&#x2F;RDD算子.md.&#x2F;技术&#x2F;hexo&#x2F;spark&#x2F;宽窄依赖.md.&#x2F;技术&#x2F;hexo&#x2F;spark&#x2F;stream.md.&#x2F;技术&#x2F;hexo&#x2F;spark&#x2F;stream2.md.&#x2F;技术&#x2F;hexo&#x2F;spark&#x2F;sql.&#x2F;技术&#x2F;hexo&#x2F;spark&#x2F;sql&#x2F;SparkSql.md.&#x2F;技术&#x2F;开发.&#x2F;技术&#x2F;开发&#x2F;前端.&#x2F;专题.&#x2F;专题&#x2F;demo.&#x2F;专题&#x2F;数据湖.md.&#x2F;专题&#x2F;Flinkcdc.md.&#x2F;专题&#x2F;字贴.md.&#x2F;专题&#x2F;mysql-binlog.md.&#x2F;专题&#x2F;运营手册.md.&#x2F;专题&#x2F;回顾.md.&#x2F;专题&#x2F;cdc调研.md.&#x2F;专题&#x2F;cdc实战.md.&#x2F;sqltools_20221206170549_58159.log.&#x2F;path.md.&#x2F;日常.&#x2F;日常&#x2F;运维.&#x2F;日常&#x2F;运维&#x2F;docker启动mysql.md.&#x2F;日常&#x2F;运维&#x2F;大数据组件升级改造.md.&#x2F;日常&#x2F;运维&#x2F;tmux上手.md.&#x2F;日常&#x2F;运维&#x2F;运维操作.md.&#x2F;日常&#x2F;运维&#x2F;about云账号密码.md.&#x2F;日常&#x2F;运维&#x2F;mac python版本替换.md.&#x2F;日常&#x2F;运维&#x2F;操作记录.md.&#x2F;日常&#x2F;运维&#x2F;博客不同主题切换.md.&#x2F;日常&#x2F;运维&#x2F;hexo改造记录.md.&#x2F;日常&#x2F;运维&#x2F;京东云.md.&#x2F;日常&#x2F;运维&#x2F;jumpserver.md.&#x2F;日常&#x2F;map.html.&#x2F;日常&#x2F;配置X-Pack.md.&#x2F;日常&#x2F;prepare.&#x2F;日常&#x2F;prepare&#x2F;collect_es.sh.&#x2F;日常&#x2F;prepare&#x2F;数据储备.md.&#x2F;日常&#x2F;es密码添加.md.&#x2F;日常&#x2F;blog_sync_project.&#x2F;日常&#x2F;blog_sync_project&#x2F;bash.&#x2F;日常&#x2F;blog_sync_project&#x2F;bash&#x2F;git_pull.sh.&#x2F;日常&#x2F;blog_sync_project&#x2F;plan.md.&#x2F;README.md.&#x2F;工作.&#x2F;工作&#x2F;杂记.&#x2F;工作&#x2F;杂记&#x2F;测试报告.md.&#x2F;工作&#x2F;杂记&#x2F;汉口源点项目相关信息.md.&#x2F;工作&#x2F;杂记&#x2F;四院数据处理等改造.md.&#x2F;工作&#x2F;杂记&#x2F;汉口项目公司服务操作.md.&#x2F;工作&#x2F;杂记&#x2F;测试计划.md.&#x2F;工作&#x2F;项目.&#x2F;工作&#x2F;项目&#x2F;钢管场.&#x2F;工作&#x2F;项目&#x2F;钢管场&#x2F;验收会.md.&#x2F;工作&#x2F;项目&#x2F;数据基础平台.&#x2F;工作&#x2F;项目&#x2F;数据基础平台&#x2F;数据集成.&#x2F;工作&#x2F;项目&#x2F;数据基础平台&#x2F;数据集成&#x2F;数据采集构建方案.md.&#x2F;工作&#x2F;项目&#x2F;数据基础平台&#x2F;数据运维.&#x2F;工作&#x2F;项目&#x2F;数据基础平台&#x2F;数据运维&#x2F;数据运维-采集任务模块.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;数仓.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;数仓&#x2F;数仓架构设计图.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;归档.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;部署.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;部署&#x2F;ck地址.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;部署&#x2F;2023年3月.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版计划6.21.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版计划6.15.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版计划10.17.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版计划908.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版计划908_2.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;demo1.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;6.8.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;725更新内容清单.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版计划621.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版计划912.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版计划913.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版计划725.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;demo.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版计划721.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版计划707.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版计划915.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版计划.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版计划718.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;6.5发版计划.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版计划629.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;验收相关.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;10.17发版内容.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;发版.md.&#x2F;工作&#x2F;项目&#x2F;铁四院数据平台&#x2F;发版&#x2F;开发环境更新.md.&#x2F;工作&#x2F;项目&#x2F;洪雅.&#x2F;工作&#x2F;项目&#x2F;洪雅&#x2F;大数据验证.md.&#x2F;工作&#x2F;项目&#x2F;济泺路隧道.&#x2F;工作&#x2F;项目&#x2F;济泺路隧道&#x2F;flink服务.&#x2F;工作&#x2F;项目&#x2F;济泺路隧道&#x2F;flink服务&#x2F;flink公司环境部署记录.md.&#x2F;工作&#x2F;项目&#x2F;济泺路隧道&#x2F;出差.&#x2F;工作&#x2F;项目&#x2F;济泺路隧道&#x2F;出差&#x2F;flink运行参数.md.&#x2F;工作&#x2F;项目&#x2F;济泺路隧道&#x2F;出差&#x2F;相关配置地址.md.&#x2F;工作&#x2F;项目&#x2F;绥延.&#x2F;工作&#x2F;项目&#x2F;绥延&#x2F;kafka测试环境.md.&#x2F;工作&#x2F;项目&#x2F;绥延&#x2F;采集地址.md.&#x2F;工作&#x2F;项目&#x2F;和燕路.&#x2F;工作&#x2F;旸谷数据中心.&#x2F;工作&#x2F;旸谷数据中心&#x2F;数据基础平台.&#x2F;工作&#x2F;旸谷数据中心&#x2F;数据基础平台&#x2F;dataworks.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;数据基础平台&#x2F;数据应用的构建方案.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;数据基础平台&#x2F;数据集成的构建方案.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;数据基础平台&#x2F;数据治理的构建方案.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;数据基础平台&#x2F;数据基础平台建设思路.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;数据基础平台&#x2F;数据管理的构建方案.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;数据基础平台&#x2F;数据安全的构建方案.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;数据基础平台&#x2F;数据运维的构建方案.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;基础.&#x2F;工作&#x2F;旸谷数据中心&#x2F;基础&#x2F;数据中心组件管理服务封装.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;基础&#x2F;数据采集服务封装.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;基础&#x2F;元数据.&#x2F;工作&#x2F;旸谷数据中心&#x2F;基础&#x2F;元数据&#x2F;Atlas切换搜索组件.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;基础&#x2F;元数据&#x2F;数据血缘组件.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;基础&#x2F;数据中心服务资源.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;基础&#x2F;Flink集成CDH.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;基础&#x2F;CDH基础环境部署.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;基础&#x2F;Logstash 部署.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;安全模块.&#x2F;工作&#x2F;旸谷数据中心&#x2F;安全模块&#x2F;数据中心安全模块设计大纲.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;安全模块&#x2F;kerberos部署与使用.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;组件.&#x2F;工作&#x2F;旸谷数据中心&#x2F;组件&#x2F;clickhouse集群版实录.md.&#x2F;工作&#x2F;旸谷数据中心&#x2F;组件&#x2F;azkaban的使用.md.&#x2F;运营.&#x2F;运营&#x2F;运营工具.&#x2F;运营&#x2F;运营工具&#x2F;看板.md.&#x2F;运营&#x2F;.DS_Store.&#x2F;运营&#x2F;钉钉的课程.&#x2F;运营&#x2F;钉钉的课程&#x2F;钉钉课程3.md.&#x2F;运营&#x2F;模型分析.&#x2F;运营&#x2F;模型分析&#x2F;秋衣模型后投流计划.md.&#x2F;运营&#x2F;模型分析&#x2F;模型分析.md.&#x2F;运营&#x2F;Untitled.md.&#x2F;运营&#x2F;小黄车.&#x2F;运营&#x2F;小黄车&#x2F;绵拖鞋.md.&#x2F;运营&#x2F;小黄车&#x2F;每日任务.&#x2F;运营&#x2F;小黄车&#x2F;每日任务&#x2F;1118.md.&#x2F;运营&#x2F;小黄车&#x2F;每日任务&#x2F;1113.md.&#x2F;运营&#x2F;未分类.&#x2F;运营&#x2F;未分类&#x2F;待产包.md.&#x2F;运营&#x2F;未分类&#x2F;阿里云盘资源.md.&#x2F;运营&#x2F;未分类&#x2F;账号1000粉后.md.&#x2F;运营&#x2F;未分类&#x2F;两人从_品目资询.png.&#x2F;运营&#x2F;未分类&#x2F;李海莉检讨书.md.&#x2F;运营&#x2F;未分类&#x2F;Untitled.md.&#x2F;运营&#x2F;未分类&#x2F;自动化涨粉脚本.md.&#x2F;运营&#x2F;未分类&#x2F;去水印记录.md.&#x2F;运营&#x2F;未分类&#x2F;1000粉记录.md.&#x2F;运营&#x2F;未分类&#x2F;素材提取功能单.md.&#x2F;运营&#x2F;未分类&#x2F;女装长裤.md.&#x2F;运营&#x2F;未分类&#x2F;运作方案调研.md.&#x2F;运营&#x2F;未分类&#x2F;demo.md.&#x2F;运营&#x2F;未分类&#x2F;莉儿的文档2.md.&#x2F;运营&#x2F;未分类&#x2F;莉儿3.md.&#x2F;运营&#x2F;未分类&#x2F;自动化思路.md.&#x2F;运营&#x2F;未分类&#x2F;视频生成流程.md.&#x2F;运营&#x2F;未分类&#x2F;选品功能设计.md.&#x2F;运营&#x2F;未分类&#x2F;hyl_现场数据环境.md.&#x2F;运营&#x2F;未分类&#x2F;1025脚本.md.&#x2F;运营&#x2F;未分类&#x2F;自动化运营思路.md.&#x2F;运营&#x2F;未分类&#x2F;运营工具.md.&#x2F;运营&#x2F;未分类&#x2F;莉儿的文档.md.&#x2F;运营&#x2F;未分类&#x2F;整体总结.md.&#x2F;运营&#x2F;未分类&#x2F;私教课相关.md.&#x2F;运营&#x2F;未分类&#x2F;如何念脚本.md.&#x2F;运营&#x2F;未分类&#x2F;听课笔记.md.&#x2F;运营&#x2F;日常选品.&#x2F;运营&#x2F;日常选品&#x2F;操作记录.md.&#x2F;运营&#x2F;process.svg.&#x2F;运营&#x2F;Untitled 1.md.&#x2F;运营&#x2F;视频更新记录.&#x2F;运营&#x2F;视频更新记录&#x2F;1107.md.&#x2F;运营&#x2F;process.drawio.&#x2F;运营&#x2F;运营话术.&#x2F;运营&#x2F;运营话术&#x2F;312.md.&#x2F;运营&#x2F;运营话术&#x2F;ddd.&#x2F;分享.&#x2F;分享&#x2F;demo.drawio.&#x2F;分享&#x2F;Pulsar部署.md.&#x2F;分享&#x2F;数据湖分享.md.&#x2F;分享&#x2F;数仓分享.md.&#x2F;分享&#x2F;Pulsar分享2.md.&#x2F;分享&#x2F;Pulsar分享.md","categories":[],"tags":[]},{"title":"","slug":"王阁/专题/业务专题/cdc专题/cdc实战","date":"2024-04-02T06:23:46.148Z","updated":"2024-05-07T08:13:26.233Z","comments":true,"path":"2024/04/02/王阁/专题/业务专题/cdc专题/cdc实战/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/04/02/%E7%8E%8B%E9%98%81/%E4%B8%93%E9%A2%98/%E4%B8%9A%E5%8A%A1%E4%B8%93%E9%A2%98/cdc%E4%B8%93%E9%A2%98/cdc%E5%AE%9E%E6%88%98/","excerpt":"","text":"canal使用 cdc使用步骤： 下载了canal.deployer-1.1.5.tar.gz 解压 配置conf中的canal.properties文件 server.mode=kafka canal.id=122 # 这里主要是标记唯一id 122. ``` bootstrap.server=kafka01:9092 这里在example&#x2F; 路径下配置了instance.properties 1将内容配置了master.address、dbuserName、password等信息 bin&#x2F;startup.sh 启动了命令 查看日志文件，通过日志信息进行相应的处理","categories":[],"tags":[]},{"title":"","slug":"王阁/专题/运营专题/运营手册","date":"2024-03-14T08:40:58.879Z","updated":"2024-05-07T08:53:40.047Z","comments":true,"path":"2024/03/14/王阁/专题/运营专题/运营手册/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/03/14/%E7%8E%8B%E9%98%81/%E4%B8%93%E9%A2%98/%E8%BF%90%E8%90%A5%E4%B8%93%E9%A2%98/%E8%BF%90%E8%90%A5%E6%89%8B%E5%86%8C/","excerpt":"","text":"姐妹们，春款百搭好裤上新啦。这款时尚微喇叭裤，它阔腿设计 个性时尚，还能从视觉上拉长腿型，还是立体剪裁，高腰设计，隐藏肉肉，凸显身材曲线，是日常穿搭好物，真心墙裂推荐！ 家长们千万要注意，学前宝宝可以不练字但要练控笔，这样即可以锻炼宝宝的手部力量，还能培养他们的专注力，为以后正式上学打下坚实的基础 有没有那种就是连笔，但是看得清还有点笔锋赞不多，然后还给人一种很出挑的感觉，很潇洒，写作业、考试不会扣分，那就对着这套字贴练习吧。 1234千万不要觉得剪刀危险，就不让孩子碰，让小朋友多玩手工，肯定要比玩手机好很，要给他准备合适的剪纸，一两岁以后多玩剪纸。对宝宝以后控笔写字，画画都非常有好处。这一套蒙式剪纸教具，还搭配了一把安全的儿童剪刀。宝宝可以从基础的直线剪过渡到曲线，连续剪再剪复杂的图形，剪下来的纸片，还可以贴成有趣的图案，培养宝宝的想象力和创造力。剪粘贴的过程，还特别锻炼孩子的手眼，协调和专注力。 姐妹们，快看，春款好裤上新啦，微喇叭开口设计，时尚不漏风，高腰设计，遮肉显瘦，不论你穿什么上装，搭我们这个裤子，都很时尚大气，赶紧入手试试看吧","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/运营话术/312","date":"2024-03-12T09:43:26.164Z","updated":"2024-03-16T11:34:31.396Z","comments":true,"path":"2024/03/12/王阁/运营/运营话术/312/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/03/12/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E8%BF%90%E8%90%A5%E8%AF%9D%E6%9C%AF/312/","excerpt":"","text":"剪纸1宝爸宝妈们，一定要让宝贝少玩手机，要让孩子们在闲暇之余，拿起彩色剪纸，用小手一点点地剪出一个个生动的图案，不仅能锻炼他们的动手能力，还能培养他们的耐心和专注力。更重要的是，剪纸的过程还能激发孩子们的创造力和想象力，让他们在玩耍中不断成长。快来选购吧，让我们的孩子在快乐中学习！在玩耍中成长！ 家长们，要让少宝宝们少玩手机，趁着她们对世界充满了好奇，还活泼好动，试试这套剪纸套装，培养宝宝们的认知与动手能力！#儿童彩纸手工 #一起玩手工吧 家长们，想宝宝们少玩手机，趁着他们对世界充满了好奇，还活泼好动试试这套剪纸套装#让孩子远离手机 #一起玩手工吧 姐妹们，塑造优美身材，看看这款男女通用的矫姿开肩美背提升气质背部矫正带,柔软透气,大人小孩都可以穿，赶紧入手吧 姐妹们，时尚#微喇裤 #春季新款谁穿谁好看 #面料柔软舒适胖瘦都能穿 #穿出长腿美感","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/日常选品/操作记录","date":"2024-03-04T02:41:07.380Z","updated":"2024-03-07T15:39:18.380Z","comments":true,"path":"2024/03/04/王阁/运营/日常选品/操作记录/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/03/04/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%97%A5%E5%B8%B8%E9%80%89%E5%93%81/%E6%93%8D%E4%BD%9C%E8%AE%B0%E5%BD%95/","excerpt":"","text":"有没有那种就是连笔，但是看得清还有点笔锋但不多，然后还给人一种张扬个性的感觉，很潇洒，考试不会扣分，写作业，有种出挑灵动的感觉的字体，就在这里对着这本字贴练习吧，大人学生都适用！ 抢！【9.9元7件套】成人学生行书速成字帖零基础行书控笔常用3000字","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/待产包","date":"2024-02-14T12:18:14.053Z","updated":"2024-02-14T12:18:14.053Z","comments":true,"path":"2024/02/14/王阁/运营/未分类/待产包/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/02/14/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/%E5%BE%85%E4%BA%A7%E5%8C%85/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/demo","date":"2024-02-02T03:13:10.534Z","updated":"2024-02-02T03:13:10.534Z","comments":true,"path":"2024/02/02/王阁/运营/未分类/demo/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/02/02/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/demo/","excerpt":"","text":"家里有初中生的家长们，孩子是不是背单词比较痛苦的，可以看看这本书。初中生，你得这样背单词，这是一本不需要家长和老师辅导的书，有新颖的导读，导背方法，它里面涵盖了初中三年所有的单词用不同的颜色对单词进行拆解，让孩子学会自然拼读，更符合记忆曲线，这样记住就不容易忘。后面还有例句以及拓展。如果孩子不会读，可以扫码进行跟读，这样背单词，再也不用家长操心。 我在大学专业是古典文学，本职是一名语文老师，深入在教育第一线。 为各位朋友，家长分享一些实用，有趣，有意义的读物和教辅材料等 欢迎各位选购 MiiOW&#x2F;猫人无痕内衣无钢圈软支撑女士睡眠胸罩聚拢收副乳文胸新款 2024新版【初中生你得这样背单词】人教版7-9年级必备词汇秒记单词，假期也不要忘了提升自己#单词速记 1家长请收藏！只要孩子吃透这两三百个知识点，初中小四门就跟玩一样，我们可以给孩子准备这套小四门睡前5分钟考点暗记！小四门包含了地理、历史、生物、道德与法治。不要小看小小的一本，他把初中三年的知识都囊括了，走到哪都能随时背诵。每一科的考点、知识点都给孩子罗列的非常清楚，并且用漫画图解的方式呈现给孩子，内容丰富有趣不枯燥，孩子爱看，就像读课外书一样，每晚睡前5分钟，需要记什么罗列的非常详细，趁着假期，给孩子准备起来吧！","categories":[],"tags":[]},{"title":"","slug":"王阁/分享/Pulsar分享2","date":"2024-01-22T08:19:37.680Z","updated":"2024-01-23T05:08:18.276Z","comments":true,"path":"2024/01/22/王阁/分享/Pulsar分享2/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/22/%E7%8E%8B%E9%98%81/%E5%88%86%E4%BA%AB/Pulsar%E5%88%86%E4%BA%AB2/","excerpt":"","text":"Pulsar分享大家好，我今天想给大家分享的是 Pulsar。一款云原生分布式消息流平台，集消息、存储、轻量化函数式计算为一体，采用计算与存储分离架构设计，支持多租户、持久化存储、多机房跨区域数据复制，具有强一致性、高吞吐、低延时及高可扩展性等流数据存储特性。 它有以下关键特性： 是下一代云原生分布式消息流平台。 Pulsar 的单个实例原生支持多个集群，可跨机房在集群间无缝地完成消息复制。 极低的发布延迟和端到端延迟。 可无缝扩展到超过一百万个 topic。 简单的客户端 API，支持 Java、Go、Python 和 C++。 主题的多种订阅模式（独占、共享和故障转移）。 通过 Apache BookKeeper 提供的持久化消息存储机制保证消息传递 。 由轻量级的 serverless 计算框架 Pulsar Functions 实现流原生的数据处理。 基于 Pulsar Functions 的 serverless connector 框架 Pulsar IO 使得数据更易移入、移出 Apache Pulsar。 分层式存储可在数据陈旧时，将数据从热存储卸载到冷&#x2F;长期存储（如 S3、GCS）中。 今天主要是想对Pulsar这款组件进行一个介绍，为了让大家更直观的了解到Pulsar的一些特性，我的分享将会更直接的将它与kafka对比来开展。 kafka VS pulsar 架构图对比 由上图可知 Pulsar的组成 bookie（bookkeeper底层数据存储，在kafka中是由broker节点承担） BookKeeper 节点（bookie）存储消息和游标，ZooKeeper 则只用于为 broker 和 bookie 存储元数据。另外，BookKeeper 使用 RocksDB 作为内嵌数据库，用于存储内部索引，但不能独立于 BookKeeper 单独管理 RocksDB。 采用了 BookKeeper，因此伸缩性更灵活，速度更快，性能更一致，运维开销更小 broker（这里的broker更像hbase-client、hive-thrift-server等节点与kafka的broker有所区别） Broker 是无状态服务，客户端需要连接到 broker 进行核心消息传递。而 BookKeeper 和 ZooKeeper 是有状态服务。 zookeeper（元数据存储，这倒是跟kafka中的zookeeper雷同） kafka VS Pulsar的一些关键名词 Pulsar Kafka Topic Topic Partition Partition Ledger(Segment)&#x2F;Fragment Fragment&#x2F;Segment Bookie Broker Broker Client SDK Ensemble Size metadata.broker.list Write Quorum Size (Qw) Replica Number Ack Quorum Size (Qa) request.required.acks 上面我们看到了，接下来我们按正常层级进行分析 存储层 (bookie vs kafka broker) Pulsar 的多层架构影响了存储数据的方式，即Pulsar也是分层存储 第一层是Topic、Subscription、CursorsTopic下面是ledger层，保存了分片(Segment)，分片里面保存更小粒度的ertries，entries存储一条条的Message。 1消息存储在Topic中。逻辑上一个Topic是日志结构，每个消息都在这个日志结构中有一个偏移量。Apache Pulsar使用游标来跟踪偏移量。生产者将消息发送到一个指定的Topic，Apache Pulsar保证消息一旦被确认就不会丢失(正确的配置和非整个集群故障的情况下)。 消费者通过订阅来消费Topic中的消息。订阅是游标(跟踪偏移量)的逻辑实体，并且还根据不同的订阅类型提供一些额外的保证 ​ 1. Exclusive(独享) - 一个订阅只能有一个消息者消费消息 ​ 2. Shared(共享) - 一个订阅中同时可以有多个消费者，多个消费者共享Topic中的消息 3. Fail-Over(灾备) - 一个订阅同时只有一个消费者，可以有多个备份消费者。一旦主消费者故障则备份消费者接管。不会出现同时有两个活跃的消费者。 一个Topic可以添加多个订阅。订阅不包含消息的数据，只包含元数据和游标。 Apache Pulsar通过允许消费者将Topic看做在消费者消费确认后删除消息的队列，或者消费者可以根据游标的回放来提供队列和日志的语义。在底层都使用日志作为存储模型。 Pulsar的简介 Pulsar 组件架构 Pulsar 的一条数据的读写之旅 Pulsar的运维特点 Pulsar与kafka的区别","categories":[],"tags":[]},{"title":"","slug":"王阁/分享/Pulsar部署","date":"2024-01-22T01:54:00.223Z","updated":"2024-01-22T07:02:43.401Z","comments":true,"path":"2024/01/22/王阁/分享/Pulsar部署/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/22/%E7%8E%8B%E9%98%81/%E5%88%86%E4%BA%AB/Pulsar%E9%83%A8%E7%BD%B2/","excerpt":"","text":"Pulsar安装部署123456pulsar initialize-cluster-metadata \\ --cluster yanggu-pulsar \\ --zookeeper 192.168.10.217:2181 \\ --configuration-store 192.168.10.217:2181 \\ --web-service-url http://192.168.10.217:8080,192.168.10.218:8080,192.168.10.219:8080 \\ --broker-service-url pulsar://192.168.10.217:6650,192.168.10.218:6650,192.168.10.219:6650","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/fetch-blob/README","date":"2024-01-06T06:44:19.000Z","updated":"2024-01-06T06:44:19.000Z","comments":true,"path":"2024/01/06/王阁/test/node_modules/fetch-blob/README/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/06/%E7%8E%8B%E9%98%81/test/node_modules/fetch-blob/README/","excerpt":"","text":"fetch-blob A Blob implementation in Node.js, originally from node-fetch. Installation1npm install fetch-blob Upgrading from 2x to 3x Updating from 2 to 3 should be a breeze since there is not many changes to the blob specification. The major cause of a major release is coding standards. - internal WeakMaps was replaced with private fields - internal Buffer.from was replaced with TextEncoder&#x2F;Decoder - internal buffers was replaced with Uint8Arrays - CommonJS was replaced with ESM - The node stream returned by calling blob.stream() was replaced with whatwg streams - (Read “Differences from other blobs” for more info.) Differences from other Blobs Unlike NodeJS buffer.Blob (Added in: v15.7.0) and browser native Blob this polyfilled version can’t be sent via PostMessage This blob version is more arbitrary, it can be constructed with blob parts that isn’t a instance of itself it has to look and behave as a blob to be accepted as a blob part. The benefit of this is that you can create other types of blobs that don’t contain any internal data that has to be read in other ways, such as the BlobDataItem created in from.js that wraps a file path into a blob-like item and read lazily (nodejs plans to implement this as well) The blob.stream() is the most noticeable differences. It returns a WHATWG stream now. to keep it as a node stream you would have to do: 12import &#123;Readable&#125; from &#x27;stream&#x27;const stream = Readable.from(blob.stream()) Usage12345678910111213141516171819// Ways to import// (PS it&#x27;s dependency free ESM package so regular http-import from CDN works too)import Blob from &#x27;fetch-blob&#x27;import File from &#x27;fetch-blob/file.js&#x27;import &#123;Blob&#125; from &#x27;fetch-blob&#x27;import &#123;File&#125; from &#x27;fetch-blob/file.js&#x27;const &#123;Blob&#125; = await import(&#x27;fetch-blob&#x27;)// Ways to read the blob:const blob = new Blob([&#x27;hello, world&#x27;])await blob.text()await blob.arrayBuffer()for await (let chunk of blob.stream()) &#123; ... &#125;blob.stream().getReader().read()blob.stream().getReader(&#123;mode: &#x27;byob&#x27;&#125;).read(view) Blob part backed up by filesystemfetch-blob/from.js comes packed with tools to convert any filepath into either a Blob or a FileIt will not read the content into memory. It will only stat the file for last modified date and file size. 1234567891011// The default export is sync and use fs.stat to retrieve size &amp; last modified as a blobimport blobFromSync from &#x27;fetch-blob/from.js&#x27;import &#123;File, Blob, blobFrom, blobFromSync, fileFrom, fileFromSync&#125; from &#x27;fetch-blob/from.js&#x27;const fsFile = fileFromSync(&#x27;./2-GiB-file.bin&#x27;, &#x27;application/octet-stream&#x27;)const fsBlob = await blobFrom(&#x27;./2-GiB-file.mp4&#x27;)// Not a 4 GiB memory snapshot, just holds references// points to where data is located on the diskconst blob = new Blob([fsFile, fsBlob, &#x27;memory&#x27;, new Uint8Array(10)])console.log(blob.size) // ~4 GiB blobFrom|blobFromSync|fileFrom|fileFromSync(path, [mimetype]) Creating Blobs backed up by other async sourcesOur Blob &amp; File class are more generic then any other polyfills in the way that it can accept any blob look-a-like itemAn example of this is that our blob implementation can be constructed with parts coming from BlobDataItem (aka a filepath) or from buffer.Blob, It dose not have to implement all the methods - just enough that it can be read&#x2F;understood by our Blob implementation. The minium requirements is that it has Symbol.toStringTag, size, slice() and either a stream() or a arrayBuffer() method. If you then wrap it in our Blob or File new Blob([blobDataItem]) then you get all of the other methods that should be implemented in a blob or file An example of this could be to create a file or blob like item coming from a remote HTTP request. Or from a DataBase See the MDN documentation and tests for more details of how to use the Blob.","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/node-domexception/README","date":"2024-01-06T06:44:19.000Z","updated":"2024-01-06T06:44:19.000Z","comments":true,"path":"2024/01/06/王阁/test/node_modules/node-domexception/README/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/06/%E7%8E%8B%E9%98%81/test/node_modules/node-domexception/README/","excerpt":"","text":"DOMExceptionAn implementation of the DOMException class from NodeJS NodeJS has DOMException built in, but it’s not globally available, and you can’t require&#x2F;import it from somewhere. This package exposes the DOMException class that comes from NodeJS itself. (including all of the legacy codes) (plz don’t depend on this package in any other environment other than node &gt;&#x3D;10.5) 123456789101112131415161718192021222324import DOMException from &#x27;node-domexception&#x27;import &#123; MessageChannel &#125; from &#x27;worker_threads&#x27;async function hello() &#123; const port = new MessageChannel().port1 const ab = new ArrayBuffer() port.postMessage(ab, [ab, ab])&#125;hello().catch(err =&gt; &#123; console.assert(err.name === &#x27;DataCloneError&#x27;) console.assert(err.code === 25) console.assert(err instanceof DOMException)&#125;)const e1 = new DOMException(&#x27;Something went wrong&#x27;, &#x27;BadThingsError&#x27;)console.assert(e1.name === &#x27;BadThingsError&#x27;)console.assert(e1.code === 0)const e2 = new DOMException(&#x27;Another exciting error message&#x27;, &#x27;NoModificationAllowedError&#x27;)console.assert(e2.name === &#x27;NoModificationAllowedError&#x27;)console.assert(e2.code === 7)console.assert(DOMException.INUSE_ATTRIBUTE_ERR === 10) BackgroundThe only possible way is to use some web-ish tools that have been introduced into NodeJS that throws a DOMException and catch the constructor. This is exactly what this package dose for you and exposes it.This way you will have the same class that NodeJS has and you can check if the error is a instance of DOMException.The instanceof check would not have worked with a custom class such as the DOMException provided by domenic which also is much larger in size since it has to re-construct the hole class from the ground up. The DOMException is used in many places such as the Fetch API, File &amp; Blobs, PostMessaging and more. Why they decided to call it DOM, I don’t know Please consider sponsoring if you find this helpful","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/node-fetch/LICENSE","date":"2024-01-06T06:44:19.000Z","updated":"2024-01-06T06:44:19.000Z","comments":true,"path":"2024/01/06/王阁/test/node_modules/node-fetch/LICENSE/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/06/%E7%8E%8B%E9%98%81/test/node_modules/node-fetch/LICENSE/","excerpt":"","text":"The MIT License (MIT) Copyright (c) 2016 - 2020 Node Fetch Team Permission is hereby granted, free of charge, to any person obtaining a copyof this software and associated documentation files (the “Software”), to dealin the Software without restriction, including without limitation the rightsto use, copy, modify, merge, publish, distribute, sublicense, and&#x2F;or sellcopies of the Software, and to permit persons to whom the Software isfurnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in allcopies or substantial portions of the Software. THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS ORIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THEAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHERLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THESOFTWARE.","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/node-fetch/README","date":"2024-01-06T06:44:19.000Z","updated":"2024-01-06T06:44:19.000Z","comments":true,"path":"2024/01/06/王阁/test/node_modules/node-fetch/README/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/06/%E7%8E%8B%E9%98%81/test/node_modules/node-fetch/README/","excerpt":"","text":"A light-weight module that brings Fetch API to Node.js. Consider supporting us on our Open Collective: You might be looking for the v2 docs Motivation Features Difference from client-side fetch Installation Loading and configuring the module Upgrading Common Usage Plain text or HTML JSON Simple Post Post with JSON Post with form parameters Handling exceptions Handling client and server errors Handling cookies Advanced Usage Streams Accessing Headers and other Metadata Extract Set-Cookie Header Post data using a file Request cancellation with AbortSignal API fetch(url[, options]) Options Default Headers Custom Agent Custom highWaterMark Insecure HTTP Parser Class: Request new Request(input[, options]) Class: Response [new Response([body[, options]])](#new-responsebody-options) response.ok response.redirected response.type Class: Headers new Headers([init]) Interface: Body body.body body.bodyUsed body.arrayBuffer() body.blob() body.formData() body.json() body.text() Class: FetchError Class: AbortError TypeScript Acknowledgement Team - Former License MotivationInstead of implementing XMLHttpRequest in Node.js to run browser-specific Fetch polyfill, why not go from native http to fetch API directly? Hence, node-fetch, minimal code for a window.fetch compatible API on Node.js runtime. See Jason Miller’s isomorphic-unfetch or Leonardo Quixada’s cross-fetch for isomorphic usage (exports node-fetch for server-side, whatwg-fetch for client-side). Features Stay consistent with window.fetch API. Make conscious trade-off when following WHATWG fetch spec and stream spec implementation details, document known differences. Use native promise and async functions. Use native Node streams for body, on both request and response. Decode content encoding (gzip&#x2F;deflate&#x2F;brotli) properly, and convert string output (such as res.text() and res.json()) to UTF-8 automatically. Useful extensions such as redirect limit, response size limit, explicit errors for troubleshooting. Difference from client-side fetch See known differences: As of v3.x As of v2.x If you happen to use a missing feature that window.fetch offers, feel free to open an issue. Pull requests are welcomed too! InstallationCurrent stable release (3.x) requires at least Node.js 12.20.0. 1npm install node-fetch Loading and configuring the moduleES Modules (ESM)1import fetch from &#x27;node-fetch&#x27;; CommonJSnode-fetch from v3 is an ESM-only module - you are not able to import it with require(). If you cannot switch to ESM, please use v2 which remains compatible with CommonJS. Critical bug fixes will continue to be published for v2. 1npm install node-fetch@2 Alternatively, you can use the async import() function from CommonJS to load node-fetch asynchronously: 12// mod.cjsconst fetch = (...args) =&gt; import(&#x27;node-fetch&#x27;).then((&#123;default: fetch&#125;) =&gt; fetch(...args)); Providing global accessTo use fetch() without importing it, you can patch the global object in node: 12345678910111213141516171819202122232425// fetch-polyfill.jsimport fetch, &#123; Blob, blobFrom, blobFromSync, File, fileFrom, fileFromSync, FormData, Headers, Request, Response,&#125; from &#x27;node-fetch&#x27;if (!globalThis.fetch) &#123; globalThis.fetch = fetch globalThis.Headers = Headers globalThis.Request = Request globalThis.Response = Response&#125;// index.jsimport &#x27;./fetch-polyfill&#x27;// ... UpgradingUsing an old version of node-fetch? Check out the following files: 2.x to 3.x upgrade guide 1.x to 2.x upgrade guide Changelog Common UsageNOTE: The documentation below is up-to-date with 3.x releases, if you are using an older version, please check how to upgrade. Plain text or HTML123456import fetch from &#x27;node-fetch&#x27;;const response = await fetch(&#x27;https://github.com/&#x27;);const body = await response.text();console.log(body); JSON123456import fetch from &#x27;node-fetch&#x27;;const response = await fetch(&#x27;https://api.github.com/users/github&#x27;);const data = await response.json();console.log(data); Simple Post123456import fetch from &#x27;node-fetch&#x27;;const response = await fetch(&#x27;https://httpbin.org/post&#x27;, &#123;method: &#x27;POST&#x27;, body: &#x27;a=1&#x27;&#125;);const data = await response.json();console.log(data); Post with JSON123456789101112import fetch from &#x27;node-fetch&#x27;;const body = &#123;a: 1&#125;;const response = await fetch(&#x27;https://httpbin.org/post&#x27;, &#123; method: &#x27;post&#x27;, body: JSON.stringify(body), headers: &#123;&#x27;Content-Type&#x27;: &#x27;application/json&#x27;&#125;&#125;);const data = await response.json();console.log(data); Post with form parametersURLSearchParams is available on the global object in Node.js as of v10.0.0. See official documentation for more usage methods. NOTE: The Content-Type header is only set automatically to x-www-form-urlencoded when an instance of URLSearchParams is given as such: 123456789import fetch from &#x27;node-fetch&#x27;;const params = new URLSearchParams();params.append(&#x27;a&#x27;, 1);const response = await fetch(&#x27;https://httpbin.org/post&#x27;, &#123;method: &#x27;POST&#x27;, body: params&#125;);const data = await response.json();console.log(data); Handling exceptionsNOTE: 3xx-5xx responses are NOT exceptions, and should be handled in then(), see the next section. Wrapping the fetch function into a try/catch block will catch all exceptions, such as errors originating from node core libraries, like network errors, and operational errors which are instances of FetchError. See the error handling document for more details. 1234567import fetch from &#x27;node-fetch&#x27;;try &#123; await fetch(&#x27;https://domain.invalid/&#x27;);&#125; catch (error) &#123; console.log(error);&#125; Handling client and server errorsIt is common to create a helper function to check that the response contains no client (4xx) or server (5xx) error responses: 12345678910111213141516171819202122232425262728import fetch from &#x27;node-fetch&#x27;;class HTTPResponseError extends Error &#123; constructor(response) &#123; super(`HTTP Error Response: $&#123;response.status&#125; $&#123;response.statusText&#125;`); this.response = response; &#125;&#125;const checkStatus = response =&gt; &#123; if (response.ok) &#123; // response.status &gt;= 200 &amp;&amp; response.status &lt; 300 return response; &#125; else &#123; throw new HTTPResponseError(response); &#125;&#125;const response = await fetch(&#x27;https://httpbin.org/status/400&#x27;);try &#123; checkStatus(response);&#125; catch (error) &#123; console.error(error); const errorBody = await error.response.text(); console.error(`Error body: $&#123;errorBody&#125;`);&#125; Handling cookiesCookies are not stored by default. However, cookies can be extracted and passed by manipulating request and response headers. See Extract Set-Cookie Header for details. Advanced UsageStreamsThe “Node.js way” is to use streams when possible. You can pipe res.body to another stream. This example uses stream.pipeline to attach stream error handlers and wait for the download to complete. 123456789101112import &#123;createWriteStream&#125; from &#x27;node:fs&#x27;;import &#123;pipeline&#125; from &#x27;node:stream&#x27;;import &#123;promisify&#125; from &#x27;node:util&#x27;import fetch from &#x27;node-fetch&#x27;;const streamPipeline = promisify(pipeline);const response = await fetch(&#x27;https://github.githubassets.com/images/modules/logos_page/Octocat.png&#x27;);if (!response.ok) throw new Error(`unexpected response $&#123;response.statusText&#125;`);await streamPipeline(response.body, createWriteStream(&#x27;./octocat.png&#x27;)); In Node.js 14 you can also use async iterators to read body; however, be careful to catcherrors – the longer a response runs, the more likely it is to encounter an error. 1234567891011import fetch from &#x27;node-fetch&#x27;;const response = await fetch(&#x27;https://httpbin.org/stream/3&#x27;);try &#123; for await (const chunk of response.body) &#123; console.dir(JSON.parse(chunk.toString())); &#125;&#125; catch (err) &#123; console.error(err.stack);&#125; In Node.js 12 you can also use async iterators to read body; however, async iterators with streamsdid not mature until Node.js 14, so you need to do some extra work to ensure you handle errorsdirectly from the stream and wait on it response to fully close. 12345678910111213141516171819202122232425import fetch from &#x27;node-fetch&#x27;;const read = async body =&gt; &#123; let error; body.on(&#x27;error&#x27;, err =&gt; &#123; error = err; &#125;); for await (const chunk of body) &#123; console.dir(JSON.parse(chunk.toString())); &#125; return new Promise((resolve, reject) =&gt; &#123; body.on(&#x27;close&#x27;, () =&gt; &#123; error ? reject(error) : resolve(); &#125;); &#125;);&#125;;try &#123; const response = await fetch(&#x27;https://httpbin.org/stream/3&#x27;); await read(response.body);&#125; catch (err) &#123; console.error(err.stack);&#125; Accessing Headers and other Metadata123456789import fetch from &#x27;node-fetch&#x27;;const response = await fetch(&#x27;https://github.com/&#x27;);console.log(response.ok);console.log(response.status);console.log(response.statusText);console.log(response.headers.raw());console.log(response.headers.get(&#x27;content-type&#x27;)); Extract Set-Cookie HeaderUnlike browsers, you can access raw Set-Cookie headers manually using Headers.raw(). This is a node-fetch only API. 123456import fetch from &#x27;node-fetch&#x27;;const response = await fetch(&#x27;https://example.com&#x27;);// Returns an array of values, instead of a string of comma-separated valuesconsole.log(response.headers.raw()[&#x27;set-cookie&#x27;]); Post data using a file1234567891011121314151617import fetch, &#123; Blob, blobFrom, blobFromSync, File, fileFrom, fileFromSync,&#125; from &#x27;node-fetch&#x27;const mimetype = &#x27;text/plain&#x27;const blob = fileFromSync(&#x27;./input.txt&#x27;, mimetype)const url = &#x27;https://httpbin.org/post&#x27;const response = await fetch(url, &#123; method: &#x27;POST&#x27;, body: blob &#125;)const data = await response.json()console.log(data) node-fetch comes with a spec-compliant FormData implementations for postingmultipart&#x2F;form-data payloads 1234567891011121314import fetch, &#123; FormData, File, fileFrom &#125; from &#x27;node-fetch&#x27;const httpbin = &#x27;https://httpbin.org/post&#x27;const formData = new FormData()const binary = new Uint8Array([ 97, 98, 99 ])const abc = new File([binary], &#x27;abc.txt&#x27;, &#123; type: &#x27;text/plain&#x27; &#125;)formData.set(&#x27;greeting&#x27;, &#x27;Hello, world!&#x27;)formData.set(&#x27;file-upload&#x27;, abc, &#x27;new name.txt&#x27;)const response = await fetch(httpbin, &#123; method: &#x27;POST&#x27;, body: formData &#125;)const data = await response.json()console.log(data) If you for some reason need to post a stream coming from any arbitrary place,then you can append a Blob or a File look-a-like item. The minimum requirement is that it has: A Symbol.toStringTag getter or property that is either Blob or File A known size. And either a stream() method or a arrayBuffer() method that returns a ArrayBuffer. The stream() must return any async iterable object as long as it yields Uint8Array (or Buffer)so Node.Readable streams and whatwg streams works just fine. 12345678910formData.append(&#x27;upload&#x27;, &#123; [Symbol.toStringTag]: &#x27;Blob&#x27;, size: 3, *stream() &#123; yield new Uint8Array([97, 98, 99]) &#125;, arrayBuffer() &#123; return new Uint8Array([97, 98, 99]).buffer &#125;&#125;, &#x27;abc.txt&#x27;) Request cancellation with AbortSignalYou may cancel requests with AbortController. A suggested implementation is abort-controller. An example of timing out a request after 150ms could be achieved as the following: 1234567891011121314151617181920import fetch, &#123; AbortError &#125; from &#x27;node-fetch&#x27;;// AbortController was added in node v14.17.0 globallyconst AbortController = globalThis.AbortController || await import(&#x27;abort-controller&#x27;)const controller = new AbortController();const timeout = setTimeout(() =&gt; &#123; controller.abort();&#125;, 150);try &#123; const response = await fetch(&#x27;https://example.com&#x27;, &#123;signal: controller.signal&#125;); const data = await response.json();&#125; catch (error) &#123; if (error instanceof AbortError) &#123; console.log(&#x27;request was aborted&#x27;); &#125;&#125; finally &#123; clearTimeout(timeout);&#125; See test cases for more examples. APIfetch(url[, options]) url A string representing the URL for fetching options Options for the HTTP(S) request Returns: Promise&lt;Response&gt; Perform an HTTP(S) fetch. url should be an absolute URL, such as https://example.com/. A path-relative URL (/file/under/root) or protocol-relative URL (//can-be-http-or-https.com/) will result in a rejected Promise. OptionsThe default values are shown after each option key. 12345678910111213141516&#123; // These properties are part of the Fetch Standard method: &#x27;GET&#x27;, headers: &#123;&#125;, // Request headers. format is the identical to that accepted by the Headers constructor (see below) body: null, // Request body. can be null, or a Node.js Readable stream redirect: &#x27;follow&#x27;, // Set to `manual` to extract redirect headers, `error` to reject redirect signal: null, // Pass an instance of AbortSignal to optionally abort requests // The following properties are node-fetch extensions follow: 20, // maximum redirect count. 0 to not follow redirect compress: true, // support gzip/deflate content encoding. false to disable size: 0, // maximum response body size in bytes. 0 to disable agent: null, // http(s).Agent instance or function that returns an instance (see below) highWaterMark: 16384, // the maximum number of bytes to store in the internal buffer before ceasing to read from the underlying resource. insecureHTTPParser: false // Use an insecure HTTP parser that accepts invalid HTTP headers when `true`.&#125; Default HeadersIf no values are set, the following request headers will be sent automatically: Header Value Accept-Encoding gzip, deflate, br (when options.compress === true) Accept */* Content-Length (automatically calculated, if possible) Host (host and port information from the target URI) Transfer-Encoding chunked (when req.body is a stream) User-Agent node-fetch Note: when body is a Stream, Content-Length is not set automatically. Custom AgentThe agent option allows you to specify networking related options which are out of the scope of Fetch, including and not limited to the following: Support self-signed certificate Use only IPv4 or IPv6 Custom DNS Lookup See http.Agent for more information. If no agent is specified, the default agent provided by Node.js is used. Note that this changed in Node.js 19 to have keepalive true by default. If you wish to enable keepalive in an earlier version of Node.js, you can override the agent as per the following code sample. In addition, the agent option accepts a function that returns http(s).Agent instance given current URL, this is useful during a redirection chain across HTTP and HTTPS protocol. 12345678910111213141516171819import http from &#x27;node:http&#x27;;import https from &#x27;node:https&#x27;;const httpAgent = new http.Agent(&#123; keepAlive: true&#125;);const httpsAgent = new https.Agent(&#123; keepAlive: true&#125;);const options = &#123; agent: function(_parsedURL) &#123; if (_parsedURL.protocol == &#x27;http:&#x27;) &#123; return httpAgent; &#125; else &#123; return httpsAgent; &#125; &#125;&#125;; Custom highWaterMarkStream on Node.js have a smaller internal buffer size (16kB, aka highWaterMark) from client-side browsers (&gt;1MB, not consistent across browsers). Because of that, when you are writing an isomorphic app and using res.clone(), it will hang with large response in Node. The recommended way to fix this problem is to resolve cloned response in parallel: 123456789import fetch from &#x27;node-fetch&#x27;;const response = await fetch(&#x27;https://example.com&#x27;);const r1 = response.clone();const results = await Promise.all([response.json(), r1.text()]);console.log(results[0]);console.log(results[1]); If for some reason you don’t like the solution above, since 3.x you are able to modify the highWaterMark option: 123456789import fetch from &#x27;node-fetch&#x27;;const response = await fetch(&#x27;https://example.com&#x27;, &#123; // About 1MB highWaterMark: 1024 * 1024&#125;);const result = await res.clone().arrayBuffer();console.dir(result); Insecure HTTP ParserPassed through to the insecureHTTPParser option on http(s).request. See http.request for more information. Manual RedirectThe redirect: &#39;manual&#39; option for node-fetch is different from the browser &amp; specification, whichresults in an opaque-redirect filtered response.node-fetch gives you the typical basic filtered response instead. 123456789import fetch from &#x27;node-fetch&#x27;;const response = await fetch(&#x27;https://httpbin.org/status/301&#x27;, &#123; redirect: &#x27;manual&#x27; &#125;);if (response.status === 301 || response.status === 302) &#123; const locationURL = new URL(response.headers.get(&#x27;location&#x27;), response.url); const response2 = await fetch(locationURL, &#123; redirect: &#x27;manual&#x27; &#125;); console.dir(response2);&#125; Class: RequestAn HTTP(S) request containing information about URL, method, headers, and the body. This class implements the Body interface. Due to the nature of Node.js, the following properties are not implemented at this moment: type destination mode credentials cache integrity keepalive The following node-fetch extension properties are provided: follow compress counter agent highWaterMark See options for exact meaning of these extensions. new Request(input[, options])(spec-compliant) input A string representing a URL, or another Request (which will be cloned) options Options for the HTTP(S) request Constructs a new Request object. The constructor is identical to that in the browser. In most cases, directly fetch(url, options) is simpler than creating a Request object. Class: ResponseAn HTTP(S) response. This class implements the Body interface. The following properties are not implemented in node-fetch at this moment: trailer new Response([body[, options]])(spec-compliant) body A String or Readable stream options A ResponseInit options dictionary Constructs a new Response object. The constructor is identical to that in the browser. Because Node.js does not implement service workers (for which this class was designed), one rarely has to construct a Response directly. response.ok(spec-compliant) Convenience property representing if the request ended normally. Will evaluate to true if the response status was greater than or equal to 200 but smaller than 300. response.redirected(spec-compliant) Convenience property representing if the request has been redirected at least once. Will evaluate to true if the internal redirect counter is greater than 0. response.type(deviation from spec) Convenience property representing the response’s type. node-fetch only supports &#39;default&#39; and &#39;error&#39; and does not make use of filtered responses. Class: HeadersThis class allows manipulating and iterating over a set of HTTP headers. All methods specified in the Fetch Standard are implemented. new Headers([init])(spec-compliant) init Optional argument to pre-fill the Headers object Construct a new Headers object. init can be either null, a Headers object, an key-value map object or any iterable object. 1234567891011121314151617// Example adapted from https://fetch.spec.whatwg.org/#example-headers-classimport &#123;Headers&#125; from &#x27;node-fetch&#x27;;const meta = &#123; &#x27;Content-Type&#x27;: &#x27;text/xml&#x27;&#125;;const headers = new Headers(meta);// The above is equivalent toconst meta = [[&#x27;Content-Type&#x27;, &#x27;text/xml&#x27;]];const headers = new Headers(meta);// You can in fact use any iterable objects, like a Map or even another Headersconst meta = new Map();meta.set(&#x27;Content-Type&#x27;, &#x27;text/xml&#x27;);const headers = new Headers(meta);const copyOfHeaders = new Headers(headers); Interface: BodyBody is an abstract interface with methods that are applicable to both Request and Response classes. body.body(deviation from spec) Node.js Readable stream Data are encapsulated in the Body object. Note that while the Fetch Standard requires the property to always be a WHATWG ReadableStream, in node-fetch it is a Node.js Readable stream. body.bodyUsed(spec-compliant) Boolean A boolean property for if this body has been consumed. Per the specs, a consumed body cannot be used again. body.arrayBuffer()body.formData()body.blob()body.json()body.text()fetch comes with methods to parse multipart/form-data payloads as well asx-www-form-urlencoded bodies using .formData() this comes from the idea thatService Worker can intercept such messages before it’s sent to the server toalter them. This is useful for anybody building a server so you can use it toparse &amp; consume payloads. Code example 12345678910111213141516171819202122import http from &#x27;node:http&#x27;import &#123; Response &#125; from &#x27;node-fetch&#x27;http.createServer(async function (req, res) &#123; const formData = await new Response(req, &#123; headers: req.headers // Pass along the boundary value &#125;).formData() const allFields = [...formData] const file = formData.get(&#x27;uploaded-files&#x27;) const arrayBuffer = await file.arrayBuffer() const text = await file.text() const whatwgReadableStream = file.stream() // other was to consume the request could be to do: const json = await new Response(req).json() const text = await new Response(req).text() const arrayBuffer = await new Response(req).arrayBuffer() const blob = await new Response(req, &#123; headers: req.headers // So that `type` inherits `Content-Type` &#125;.blob()&#125;) Class: FetchError(node-fetch extension) An operational error in the fetching process. See ERROR-HANDLING.md for more info. Class: AbortError(node-fetch extension) An Error thrown when the request is aborted in response to an AbortSignal‘s abort event. It has a name property of AbortError. See ERROR-HANDLING.MD for more info. TypeScriptSince 3.x types are bundled with node-fetch, so you don’t need to install any additional packages. For older versions please use the type definitions from DefinitelyTyped: 1npm install --save-dev @types/node-fetch@2.x AcknowledgementThanks to github&#x2F;fetch for providing a solid implementation reference. Team David Frank Jimmy Wärting Antoni Kepinski Richie Bendall Gregor Martynus Former Timothy Gu Jared Kantrowitz LicenseMIT","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/web-streams-polyfill/README","date":"2024-01-06T06:44:19.000Z","updated":"2024-01-06T06:44:19.000Z","comments":true,"path":"2024/01/06/王阁/test/node_modules/web-streams-polyfill/README/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/06/%E7%8E%8B%E9%98%81/test/node_modules/web-streams-polyfill/README/","excerpt":"","text":"web-streams-polyfillWeb Streams, based on the WHATWG spec reference implementation. Links Official spec Reference implementation UsageThis library comes in multiple variants: web-streams-polyfill: a polyfill that replaces the native stream implementations.Recommended for use in web apps supporting older browsers through a &lt;script&gt; tag. web-streams-polyfill/es6: a polyfill targeting ES2015+ environments.Recommended for use in web apps supporting modern browsers through a &lt;script&gt; tag. web-streams-polyfill/es2018: a polyfill targeting ES2018+ environments. web-streams-polyfill/ponyfill: a ponyfill that providesthe stream implementations without replacing any globals.Recommended for use in legacy Node applications, or in web libraries supporting older browsers. web-streams-polyfill/ponyfill/es6: a ponyfill targeting ES2015+ environments.Recommended for use in Node 6+ applications, or in web libraries supporting modern browsers. web-streams-polyfill/ponyfill/es2018: a ponyfill targeting ES2018+ environments.Recommended for use in Node 10+ applications. Each variant also includes TypeScript type definitions, compatible with the DOM type definitions for streams included in TypeScript. Usage as a polyfill: 1234567&lt;!-- option 1: hosted by unpkg CDN --&gt;&lt;script src=&quot;https://unpkg.com/web-streams-polyfill/dist/polyfill.min.js&quot;&gt;&lt;/script&gt;&lt;!-- option 2: self hosted --&gt;&lt;script src=&quot;/path/to/web-streams-polyfill/dist/polyfill.min.js&quot;&gt;&lt;/script&gt;&lt;script&gt;var readable = new ReadableStream();&lt;/script&gt; Usage as a Node module: 12var streams = require(&quot;web-streams-polyfill/ponyfill&quot;);var readable = new streams.ReadableStream(); Usage as a ES2015 module: 12import &#123; ReadableStream &#125; from &quot;web-streams-polyfill/ponyfill&quot;;const readable = new ReadableStream(); CompatibilityThe polyfill and ponyfill variants work in any ES5-compatible environment that has a global Promise.If you need to support older browsers or Node versions that do not have a native Promise implementation(check the support table), you must first include a Promise polyfill(e.g. promise-polyfill). The polyfill/es6 and ponyfill/es6 variants work in any ES2015-compatible environment. The polyfill/es2018 and ponyfill/es2018 variants work in any ES2018-compatible environment. Async iterable support for ReadableStream is available in all variants, but requires an ES2018-compatible environment or a polyfill for Symbol.asyncIterator. WritableStreamDefaultController.signal is available in all variants, but requires a global AbortController constructor. If necessary, consider using a polyfill such as abortcontroller-polyfill. Reading with a BYOB reader is available in all variants, but requires ArrayBuffer.prototype.transfer() or structuredClone() to exist in order to correctly transfer the given view’s buffer. If not available, then the buffer won’t be transferred during the read. ComplianceThe polyfill implements version 4dc123a (13 Nov 2023) of the streams specification. The polyfill is tested against the same web platform tests that are used by browsers to test their native implementations.The polyfill aims to pass all tests, although it allows some exceptions for practical reasons: The es2018 variant passes all of the tests. The es6 variant passes the same tests as the es2018 variant, except for the test for the prototype of ReadableStream‘s async iterator.Retrieving the correct %AsyncIteratorPrototype% requires using an async generator (async function* () &#123;&#125;), which is invalid syntax before ES2018.Instead, the polyfill creates its own version which is functionally equivalent to the real prototype. The es5 variant passes the same tests as the es6 variant, except for various tests about specific characteristics of the constructors, properties and methods.These test failures do not affect the run-time behavior of the polyfill.For example: The name property of down-leveled constructors is incorrect. The length property of down-leveled constructors and methods with optional arguments is incorrect. Not all properties and methods are correctly marked as non-enumerable. Down-leveled class methods are not correctly marked as non-constructable. The type definitions are compatible with the built-in stream types of TypeScript 3.3. ContributorsThanks to these people for their work on the original polyfill: Diwank Singh Tomer (creatorrr) Anders Riutta (ariutta)","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/axios/MIGRATION_GUIDE","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/axios/MIGRATION_GUIDE/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/axios/MIGRATION_GUIDE/","excerpt":"","text":"Migration Guide0.x.x -&gt; 1.1.0","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/axios/SECURITY","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/axios/SECURITY/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/axios/SECURITY/","excerpt":"","text":"Reporting a VulnerabilityIf you discover a security vulnerability in axios please disclose it via our huntr page. Bounty eligibility, CVE assignment, response times and past reports are all there. Thank you for improving the security of axios.","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/axios/CHANGELOG","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/axios/CHANGELOG/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/axios/CHANGELOG/","excerpt":"","text":"Changelog1.6.4 (2024-01-03)Bug Fixes security: fixed formToJSON prototype pollution vulnerability; (#6167) (3c0c11c) security: fixed security vulnerability in follow-redirects (#6163) (75af1cd) Contributors to this release Jay Dmitriy Mozgovoy Guy Nesher 1.6.3 (2023-12-26)Bug Fixes Regular Expression Denial of Service (ReDoS) (#6132) (5e7ad38) Contributors to this release Jay Willian Agostini Dmitriy Mozgovoy 1.6.2 (2023-11-14)Features withXSRFToken: added withXSRFToken option as a workaround to achieve the old withCredentials behavior; (#6046) (cff9967) PRs feat(withXSRFToken): added withXSRFToken option as a workaround to achieve the old &#x60;withCredentials&#x60; behavior; ( #6046 )1234📢 This PR added &amp;#x27;withXSRFToken&amp;#x27; option as a replacement for old withCredentials behaviour. You should now use withXSRFToken along with withCredential to get the old behavior.This functionality is considered as a fix. Contributors to this release Dmitriy Mozgovoy Ng Choon Khon (CK) Muhammad Noman 1.6.1 (2023-11-08)Bug Fixes formdata: fixed content-type header normalization for non-standard browser environments; (#6056) (dd465ab) platform: fixed emulated browser detection in node.js environment; (#6055) (3dc8369) Contributors to this release Dmitriy Mozgovoy Fabian Meyer PRs feat(withXSRFToken): added withXSRFToken option as a workaround to achieve the old &#x60;withCredentials&#x60; behavior; ( #6046 )1234📢 This PR added &amp;#x27;withXSRFToken&amp;#x27; option as a replacement for old withCredentials behaviour. You should now use withXSRFToken along with withCredential to get the old behavior.This functionality is considered as a fix. 1.6.0 (2023-10-26)Bug Fixes CSRF: fixed CSRF vulnerability CVE-2023-45857 (#6028) (96ee232) dns: fixed lookup function decorator to work properly in node v20; (#6011) (5aaff53) types: fix AxiosHeaders types; (#5931) (a1c8ad0) PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 Contributors to this release Dmitriy Mozgovoy Valentin Panov Rinku Chaudhari 1.5.1 (2023-09-26)Bug Fixes adapters: improved adapters loading logic to have clear error messages; (#5919) (e410779) formdata: fixed automatic addition of the Content-Type header for FormData in non-browser environments; (#5917) (bc9af51) headers: allow content-encoding header to handle case-insensitive values (#5890) (#5892) (4c89f25) types: removed duplicated code (9e62056) Contributors to this release Dmitriy Mozgovoy David Dallas Sean Sattler Mustafa Ateş Uzun Przemyslaw Motacki Michael Di Prisco PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 1.5.0 (2023-08-26)Bug Fixes adapter: make adapter loading error more clear by using platform-specific adapters explicitly (#5837) (9a414bb) dns: fixed cacheable-lookup integration; (#5836) (b3e327d) headers: added support for setting header names that overlap with class methods; (#5831) (d8b4ca0) headers: fixed common Content-Type header merging; (#5832) (8fda276) Features export getAdapter function (#5324) (ca73eb8) export: export adapters without unsafe prefix (#5839) (1601f4a) Contributors to this release Dmitriy Mozgovoy 夜葬 Jonathan Budiman Michael Di Prisco PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 1.4.0 (2023-04-27)Bug Fixes formdata: add multipart/form-data content type for FormData payload on custom client environments; (#5678) (bbb61e7) package: export package internals with unsafe path prefix; (#5677) (df38c94) Features dns: added support for a custom lookup function; (#5339) (2701911) types: export AxiosHeaderValue type. (#5525) (726f1c8) Performance Improvements merge-config: optimize mergeConfig performance by avoiding duplicate key visits; (#5679) (e6f7053) Contributors to this release Dmitriy Mozgovoy Arthur Fiorette PIYUSH NEGI PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 1.3.6 (2023-04-19)Bug Fixes types: added transport to RawAxiosRequestConfig (#5445) (6f360a2) utils: make isFormData detection logic stricter to avoid unnecessary calling of the toString method on the target; (#5661) (aa372f7) Contributors to this release Dmitriy Mozgovoy Michael Di Prisco PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 1.3.5 (2023-04-05)Bug Fixes headers: fixed isValidHeaderName to support full list of allowed characters; (#5584) (e7decef) params: re-added the ability to set the function as paramsSerializer config; (#5633) (a56c866) Contributors to this release Dmitriy Mozgovoy PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 1.3.4 (2023-02-22)Bug Fixes blob: added a check to make sure the Blob class is available in the browser’s global scope; (#5548) (3772c8f) http: fixed regression bug when handling synchronous errors inside the adapter; (#5564) (a3b246c) Contributors to this release Dmitriy Mozgovoy lcysgsg Michael Di Prisco PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 1.3.3 (2023-02-13)Bug Fixes formdata: added a check to make sure the FormData class is available in the browser’s global scope; (#5545) (a6dfa72) formdata: fixed setting NaN as Content-Length for form payload in some cases; (#5535) (c19f7bf) headers: fixed the filtering logic of the clear method; (#5542) (ea87ebf) Contributors to this release Dmitriy Mozgovoy 陈若枫 PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 1.3.2 (2023-02-03)Bug Fixes http: treat http://localhost as base URL for relative paths to avoid ERR_INVALID_URL error; (#5528) (128d56f) http: use explicit import instead of TextEncoder global; (#5530) (6b3c305) Contributors to this release Dmitriy Mozgovoy PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 1.3.1 (2023-02-01)Bug Fixes formdata: add hotfix to use the asynchronous API to compute the content-length header value; (#5521) (96d336f) serializer: fixed serialization of array-like objects; (#5518) (08104c0) Contributors to this release Dmitriy Mozgovoy PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 1.3.0 (2023-01-31)Bug Fixes headers: fixed &amp; optimized clear method; (#5507) (9915635) http: add zlib headers if missing (#5497) (65e8d1e) Features fomdata: added support for spec-compliant FormData &amp; Blob types; (#5316) (6ac574e) Contributors to this release Dmitriy Mozgovoy ItsNotGoodName PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 1.2.6 (2023-01-28)Bug Fixes headers: added missed Authorization accessor; (#5502) (342c0ba) types: fixed CommonRequestHeadersList &amp; CommonResponseHeadersList types to be private in commonJS; (#5503) (5a3d0a3) Contributors to this release Dmitriy Mozgovoy PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 1.2.5 (2023-01-26)Bug Fixes types: fixed AxiosHeaders to handle spread syntax by making all methods non-enumerable; (#5499) (580f1e8) Contributors to this release Dmitriy Mozgovoy Elliot Ford PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 1.2.4 (2023-01-22)Bug Fixes types: renamed RawAxiosRequestConfig back to AxiosRequestConfig; (#5486) (2a71f49) types: fix AxiosRequestConfig generic; (#5478) (9bce81b) Contributors to this release Dmitriy Mozgovoy Daniel Hillmann PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 1.2.3 (2023-01-10)Bug Fixes types: fixed AxiosRequestConfig header interface by refactoring it to RawAxiosRequestConfig; (#5420) (0811963) Contributors to this release Dmitriy Mozgovoy PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 [1.2.2] - 2022-12-29Fixed fix(ci): fix release script inputs #5392 fix(ci): prerelease scipts #5377 fix(ci): release scripts #5376 fix(ci): typescript tests #5375 fix: Brotli decompression #5353 fix: add missing HttpStatusCode #5345 Chores chore(ci): set conventional-changelog header config #5406 chore(ci): fix automatic contributors resolving #5403 chore(ci): improved logging for the contributors list generator #5398 chore(ci): fix release action #5397 chore(ci): fix version bump script by adding bump argument for target version #5393 chore(deps): bump decode-uri-component from 0.2.0 to 0.2.2 #5342 chore(ci): GitHub Actions Release script #5384 chore(ci): release scripts #5364 Contributors to this release Dmitriy Mozgovoy Winnie [1.2.1] - 2022-12-05Changed feat(exports): export mergeConfig #5151 Fixed fix(CancelledError): include config #4922 fix(general): removing multiple&#x2F;trailing&#x2F;leading whitespace #5022 fix(headers): decompression for responses without Content-Length header #5306 fix(webWorker): exception to sending form data in web worker #5139 Refactors refactor(types): AxiosProgressEvent.event type to any #5308 refactor(types): add missing types for static AxiosError.from method #4956 Chores chore(docs): remove README link to non-existent upgrade guide #5307 chore(docs): typo in issue template name #5159 Contributors to this release Dmitriy Mozgovoy Zachary Lysobey Kevin Ennis Philipp Loose secondl1ght wenzheng Ivan Barsukov Arthur Fiorette PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 [1.2.0] - 2022-11-10Changed changed: refactored module exports #5162 change: re-added support for loading Axios with require(‘axios’).default #5225 Fixed fix: improve AxiosHeaders class #5224 fix: TypeScript type definitions for commonjs #5196 fix: type definition of use method on AxiosInterceptorManager to match the the README #5071 fix: __dirname is not defined in the sandbox #5269 fix: AxiosError.toJSON method to avoid circular references #5247 fix: Z_BUF_ERROR when content-encoding is set but the response body is empty #5250 Refactors refactor: allowing adapters to be loaded by name #5277 Chores chore: force CI restart #5243 chore: update ECOSYSTEM.md #5077 chore: update get&#x2F;index.html #5116 chore: update Sandbox UI&#x2F;UX #5205 chore:(actions): remove git credentials after checkout #5235 chore(actions): bump actions&#x2F;dependency-review-action from 2 to 3 #5266 chore(packages): bump loader-utils from 1.4.1 to 1.4.2 #5295 chore(packages): bump engine.io from 6.2.0 to 6.2.1 #5294 chore(packages): bump socket.io-parser from 4.0.4 to 4.0.5 #5241 chore(packages): bump loader-utils from 1.4.0 to 1.4.1 #5245 chore(docs): update Resources links in README #5119 chore(docs): update the link for JSON url #5265 chore(docs): fix broken links #5218 chore(docs): update and rename UPGRADE_GUIDE.md to MIGRATION_GUIDE.md #5170 chore(docs): typo fix line #856 and #920 #5194 chore(docs): typo fix #800 #5193 chore(docs): fix typos #5184 chore(docs): fix punctuation in README.md #5197 chore(docs): update readme in the Handling Errors section - issue reference #5260 #5261 chore: remove \\b from filename #5207 chore(docs): update CHANGELOG.md #5137 chore: add sideEffects false to package.json #5025 Contributors to this release Maddy Miller Amit Saini ecyrbe Ikko Ashimine Geeth Gunnampalli Shreem Asati Frieder Bluemle 윤세영 Claudio Busatto Remco Haszing Dmitriy Mozgovoy Csaba Maulis MoPaMo Daniel Fjeldstad Adrien Brunet Frazer Smith HaiTao AZM relbns PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 [1.1.3] - 2022-10-15Added Added custom params serializer support #5113 Fixed Fixed top-level export to keep them in-line with static properties #5109 Stopped including null values to query string. #5108 Restored proxy config backwards compatibility with 0.x #5097 Added back AxiosHeaders in AxiosHeaderValue #5103 Pin CDN install instructions to a specific version #5060 Handling of array values fixed for AxiosHeaders #5085 Chores docs: match badge style, add link to them #5046 chore: fixing comments typo #5054 chore: update issue template #5061 chore: added progress capturing section to the docs; #5084 Contributors to this release Jason Saayman scarf Lenz Weber-Tronic Arvindh Félix Legrelle Patrick Petrovic Dmitriy Mozgovoy littledian ChronosMasterOfAllTime PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 [1.1.2] - 2022-10-07Fixed Fixed broken exports for UMD builds. Contributors to this release Jason Saayman PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 [1.1.1] - 2022-10-07Fixed Fixed broken exports for common js. This fix breaks a prior fix, I will fix both issues ASAP but the commonJS use is more impactful. Contributors to this release Jason Saayman PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 [1.1.0] - 2022-10-06Fixed Fixed missing exports in type definition index.d.ts #5003 Fixed query params composing #5018 Fixed GenericAbortSignal interface by making it more generic #5021 Fixed adding “clear” to AxiosInterceptorManager #5010 Fixed commonjs &amp; umd exports #5030 Fixed inability to access response headers when using axios 1.x with Jest #5036 Contributors to this release Trim21 Dmitriy Mozgovoy shingo.sasaki Ivan Pepelko Richard Kořínek PRs CVE 2023 45857 ( #6028 )12⚠️ Critical vulnerability fix. See https://security.snyk.io/vuln/SNYK-JS-AXIOS-6032459 [1.0.0] - 2022-10-04Added Added stack trace to AxiosError #4624 Add AxiosError to AxiosStatic #4654 Replaced Rollup as our build runner #4596 Added generic TS types for the exposed toFormData helper #4668 Added listen callback function #4096 Added instructions for installing using PNPM #4207 Added generic AxiosAbortSignal TS interface to avoid importing AbortController polyfill #4229 Added axios-url-template in ECOSYSTEM.md #4238 Added a clear() function to the request and response interceptors object so a user can ensure that all interceptors have been removed from an axios instance #4248 Added react hook plugin #4319 Adding HTTP status code for transformResponse #4580 Added blob to the list of protocols supported by the browser #4678 Resolving proxy from env on redirect #4436 Added enhanced toFormData implementation with additional options 4704 Adding Canceler parameters config and request #4711 Added automatic payload serialization to application&#x2F;x-www-form-urlencoded #4714 Added the ability for webpack users to overwrite built-ins #4715 Added string[] to AxiosRequestHeaders type #4322 Added the ability for the url-encoded-form serializer to respect the formSerializer config #4721 Added isCancel type assert #4293 Added data URL support for node.js #4725 Adding types for progress event callbacks #4675 URL params serializer #4734 Added axios.formToJSON method #4735 Bower platform add data protocol #4804 Use WHATWG URL API instead of url.parse() #4852 Add ENUM containing Http Status Codes to typings #4903 Improve typing of timeout in index.d.ts #4934 Changed Updated AxiosError.config to be optional in the type definition #4665 Updated README emphasizing the URLSearchParam built-in interface over other solutions #4590 Include request and config when creating a CanceledError instance #4659 Changed func-names eslint rule to as-needed #4492 Replacing deprecated substr() with slice() as substr() is deprecated #4468 Updating HTTP links in README.md to use HTTPS #4387 Updated to a better trim() polyfill #4072 Updated types to allow specifying partial default headers on instance create #4185 Expanded isAxiosError types #4344 Updated type definition for axios instance methods #4224 Updated eslint config #4722 Updated Docs #4742 Refactored Axios to use ES2017 #4787 Deprecated There are multiple deprecations, refactors and fixes provided in this release. Please read through the full release notes to see how this may impact your project and use case. Removed Removed incorrect argument for NetworkError constructor #4656 Removed Webpack #4596 Removed function that transform arguments to array #4544 Fixed Fixed grammar in README #4649 Fixed code error in README #4599 Optimized the code that checks cancellation #4587 Fix url pointing to defaults.js in README #4532 Use type alias instead of interface for AxiosPromise #4505 Fix some word spelling and lint style in code comments #4500 Edited readme with 3 updated browser icons of Chrome, FireFox and Safari #4414 Bump follow-redirects from 1.14.9 to 1.15.0 #4673 Fixing http tests to avoid hanging when assertions fail #4435 Fix TS definition for AxiosRequestTransformer #4201 Fix grammatical issues in README #4232 Fixing instance.defaults.headers type #4557 Fixed race condition on immediate requests cancellation #4261 Fixing Z_BUF_ERROR when no content #4701 Fixing proxy beforeRedirect regression #4708 Fixed AxiosError status code type #4717 Fixed AxiosError stack capturing #4718 Fixing AxiosRequestHeaders typings #4334 Fixed max body length defaults #4731 Fixed toFormData Blob issue on node&gt;v17 #4728 Bump grunt from 1.5.2 to 1.5.3 #4743 Fixing content-type header repeated #4745 Fixed timeout error message for http 4738 Request ignores false, 0 and empty string as body values #4785 Added back missing minified builds #4805 Fixed a type error #4815 Fixed a regression bug with unsubscribing from cancel token; #4819 Remove repeated compression algorithm #4820 The error of calling extend to pass parameters #4857 SerializerOptions.indexes allows boolean | null | undefined #4862 Require interceptors to return values #4874 Removed unused imports #4949 Allow null indexes on formSerializer and paramsSerializer #4960 Chores Set permissions for GitHub actions #4765 Included githubactions in the dependabot config #4770 Included dependency review #4771 Update security.md #4784 Remove unnecessary spaces #4854 Simplify the import path of AxiosError #4875 Fix Gitpod dead link #4941 Enable syntax highlighting for a code block #4970 Using Logo Axios in Readme.md #4993 Fix markup for note in README #4825 Fix typo and formatting, add colons #4853 Fix typo in readme #4942 Security Update SECURITY.md #4687 Contributors to this release Bertrand Marron Dmitriy Mozgovoy Dan Mooney Michael Li aong Des Preston Ted Robertson zhoulixiang Arthur Fiorette Kumar Shanu JALAL Jingyi Lin Philipp Loose Alexander Shchukin Dave Cardwell Cat Scarlet Luca Pizzini Kai Maxime Bargiel Brian Helba reslear Jamie Slome Landro3 rafw87 Afzal Sayed Koki Oyatsu Dave 暴走老七 Spencer Adrian Wieprzkowicz Jamie Telin 毛呆 Kirill Shakirov Rraji Abdelbari Jelle Schutter Tom Ceuppens Johann Cooper Dimitris Halatsis chenjigeng João Gabriel Quaresma Victor Augusto neilnaveen Pavlos Kiryl Valkovich Naveen wenzheng hcwhan Bassel Rachid Grégoire Pineau felipedamin Karl Horky Yue JIN Usman Ali Siddiqui WD Günther Foidl Stephen Jennings C.T.Lin mia-z Parth Banathia parth0105pluang Marco Weber Luca Pizzini Willian Agostini Huyen Nguyen","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/follow-redirects/README","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/follow-redirects/README/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/follow-redirects/README/","excerpt":"","text":"Follow RedirectsDrop-in replacement for Node’s http and https modules that automatically follows redirects. follow-redirects provides request and get methods that behave identically to those found on the native http and https modules, with the exception that they will seamlessly follow redirects. 123456789const &#123; http, https &#125; = require(&#x27;follow-redirects&#x27;);http.get(&#x27;http://bit.ly/900913&#x27;, response =&gt; &#123; response.on(&#x27;data&#x27;, chunk =&gt; &#123; console.log(chunk); &#125;);&#125;).on(&#x27;error&#x27;, err =&gt; &#123; console.error(err);&#125;); You can inspect the final redirected URL through the responseUrl property on the response.If no redirection happened, responseUrl is the original request URL. 12345678const request = https.request(&#123; host: &#x27;bitly.com&#x27;, path: &#x27;/UHfDGO&#x27;,&#125;, response =&gt; &#123; console.log(response.responseUrl); // &#x27;http://duckduckgo.com/robots.txt&#x27;&#125;);request.end(); OptionsGlobal optionsGlobal options are set directly on the follow-redirects module: 123const followRedirects = require(&#x27;follow-redirects&#x27;);followRedirects.maxRedirects = 10;followRedirects.maxBodyLength = 20 * 1024 * 1024; // 20 MB The following global options are supported: maxRedirects (default: 21) – sets the maximum number of allowed redirects; if exceeded, an error will be emitted. maxBodyLength (default: 10MB) – sets the maximum size of the request body; if exceeded, an error will be emitted. Per-request optionsPer-request options are set by passing an options object: 123456789101112131415161718192021const url = require(&#x27;url&#x27;);const &#123; http, https &#125; = require(&#x27;follow-redirects&#x27;);const options = url.parse(&#x27;http://bit.ly/900913&#x27;);options.maxRedirects = 10;options.beforeRedirect = (options, response, request) =&gt; &#123; // Use this to adjust the request options upon redirecting, // to inspect the latest response headers, // or to cancel the request by throwing an error // response.headers = the redirect response headers // response.statusCode = the redirect response code (eg. 301, 307, etc.) // request.url = the requested URL that resulted in a redirect // request.headers = the headers in the request that resulted in a redirect // request.method = the method of the request that resulted in a redirect if (options.hostname === &quot;example.com&quot;) &#123; options.auth = &quot;user:password&quot;; &#125;&#125;;http.request(options); In addition to the standard HTTP and HTTPS options,the following per-request options are supported: followRedirects (default: true) – whether redirects should be followed. maxRedirects (default: 21) – sets the maximum number of allowed redirects; if exceeded, an error will be emitted. maxBodyLength (default: 10MB) – sets the maximum size of the request body; if exceeded, an error will be emitted. beforeRedirect (default: undefined) – optionally change the request options on redirects, or abort the request by throwing an error. agents (default: undefined) – sets the agent option per protocol, since HTTP and HTTPS use different agents. Example value: &#123; http: new http.Agent(), https: new https.Agent() &#125; trackRedirects (default: false) – whether to store the redirected response details into the redirects array on the response object. Advanced usageBy default, follow-redirects will use the Node.js default implementationsof httpand https.To enable features such as caching and&#x2F;or intermediate request tracking,you might instead want to wrap follow-redirects around custom protocol implementations: 1234const &#123; http, https &#125; = require(&#x27;follow-redirects&#x27;).wrap(&#123; http: require(&#x27;your-custom-http&#x27;), https: require(&#x27;your-custom-https&#x27;),&#125;); Such custom protocols only need an implementation of the request method. Browser UsageDue to the way the browser works,the http and https browser equivalents perform redirects by default. By requiring follow-redirects this way: 12const http = require(&#x27;follow-redirects/http&#x27;);const https = require(&#x27;follow-redirects/https&#x27;); you can easily tell webpack and friends to replacefollow-redirect by the built-in versions: 1234&#123; &quot;follow-redirects/http&quot; : &quot;http&quot;, &quot;follow-redirects/https&quot; : &quot;https&quot;&#125; ContributingPull Requests are always welcome. Please file an issue detailing your proposal before you invest your valuable time. Additional features and bug fixes should be accompanied by tests. You can run the test suite locally with a simple npm test command. Debug Loggingfollow-redirects uses the excellent debug for logging. To turn on logging set the environment variable DEBUG=follow-redirects for debug output from just this module. When running the test suite it is sometimes advantageous to set DEBUG=* to see output from the express server as well. Authors Ruben Verborgh Olivier Lalonde James Talmage LicenseMIT License","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/delayed-stream/Readme","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/delayed-stream/Readme/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/delayed-stream/Readme/","excerpt":"","text":"delayed-streamBuffers events from a stream until you are ready to handle them. Installation1npm install delayed-stream UsageThe following example shows how to write a http echo server that delays itsresponse by 1000 ms. 1234567891011var DelayedStream = require(&#x27;delayed-stream&#x27;);var http = require(&#x27;http&#x27;);http.createServer(function(req, res) &#123; var delayed = DelayedStream.create(req); setTimeout(function() &#123; res.writeHead(200); delayed.pipe(res); &#125;, 1000);&#125;); If you are not using Stream#pipe, you can also manually release the bufferedevents by calling delayedStream.resume(): 123456var delayed = DelayedStream.create(req);setTimeout(function() &#123; // Emit all buffered events and resume underlaying source delayed.resume();&#125;, 1000); ImplementationIn order to use this meta stream properly, here are a few things you shouldknow about the implementation. Event Buffering &#x2F; ProxyingAll events of the source stream are hijacked by overwriting the source.emitmethod. Until node implements a catch-all event listener, this is the only way. However, delayed-stream still continues to emit all events it captures on thesource, regardless of whether you have released the delayed stream yet ornot. Upon creation, delayed-stream captures all source events and stores them inan internal event buffer. Once delayedStream.release() is called, allbuffered events are emitted on the delayedStream, and the event buffer iscleared. After that, delayed-stream merely acts as a proxy for the underlayingsource. Error handlingError events on source are buffered &#x2F; proxied just like any other events.However, delayedStream.create attaches a no-op &#39;error&#39; listener to thesource. This way you only have to handle errors on the delayedStreamobject, rather than in two places. Buffer limitsdelayed-stream provides a maxDataSize property that can be used to limitthe amount of data being buffered. In order to protect you from bad sourcestreams that don’t react to source.pause(), this feature is enabled bydefault. APIDelayedStream.create(source, [options])Returns a new delayedStream. Available options are: pauseStream maxDataSize The description for those properties can be found below. delayedStream.sourceThe source stream managed by this object. This is useful if you arepassing your delayedStream around, and you still want to access propertieson the source object. delayedStream.pauseStream &#x3D; trueWhether to pause the underlaying source when callingDelayedStream.create(). Modifying this property afterwards has no effect. delayedStream.maxDataSize &#x3D; 1024 * 1024The amount of data to buffer before emitting an error. If the underlaying source is emitting Buffer objects, the maxDataSizerefers to bytes. If the underlaying source is emitting JavaScript strings, the size refers tocharacters. If you know what you are doing, you can set this property to Infinity todisable this feature. You can also modify this property during runtime. delayedStream.dataSize &#x3D; 0The amount of data buffered so far. delayedStream.readableAn ECMA5 getter that returns the value of source.readable. delayedStream.resume()If the delayedStream has not been released so far, delayedStream.release()is called. In either case, source.resume() is called. delayedStream.pause()Calls source.pause(). delayedStream.pipe(dest)Calls delayedStream.resume() and then proxies the arguments to source.pipe. delayedStream.release()Emits and clears all events that have been buffered up so far. This does notresume the underlaying source, use delayedStream.resume() instead. Licensedelayed-stream is licensed under the MIT license.","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/asynckit/README","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/asynckit/README/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/asynckit/README/","excerpt":"","text":"asynckit Minimal async jobs utility library, with streams support. AsyncKit provides harness for parallel and serial iterators over list of items represented by arrays or objects.Optionally it accepts abort function (should be synchronously return by iterator for each item), and terminates left over jobs upon an error event. For specific iteration order built-in (ascending and descending) and custom sort helpers also supported, via asynckit.serialOrdered method. It ensures async operations to keep behavior more stable and prevent Maximum call stack size exceeded errors, from sync iterators. compression size asynckit.js 12.34 kB asynckit.min.js 4.11 kB asynckit.min.js.gz 1.47 kB Install1$ npm install --save asynckit ExamplesParallel JobsRuns iterator over provided array in parallel. Stores output in the result array,on the matching positions. In unlikely event of an error from one of the jobs,will terminate rest of the active jobs (if abort function is provided)and return error along with salvaged data to the main callback function. Input Array12345678910111213141516171819202122232425262728293031323334var parallel = require(&#x27;asynckit&#x27;).parallel , assert = require(&#x27;assert&#x27;) ;var source = [ 1, 1, 4, 16, 64, 32, 8, 2 ] , expectedResult = [ 2, 2, 8, 32, 128, 64, 16, 4 ] , expectedTarget = [ 1, 1, 2, 4, 8, 16, 32, 64 ] , target = [] ;parallel(source, asyncJob, function(err, result)&#123; assert.deepEqual(result, expectedResult); assert.deepEqual(target, expectedTarget);&#125;);// async job accepts one element from the array// and a callback functionfunction asyncJob(item, cb)&#123; // different delays (in ms) per item var delay = item * 25; // pretend different jobs take different time to finish // and not in consequential order var timeoutId = setTimeout(function() &#123; target.push(item); cb(null, item * 2); &#125;, delay); // allow to cancel &quot;leftover&quot; jobs upon error // return function, invoking of which will abort this job return clearTimeout.bind(null, timeoutId);&#125; More examples could be found in test&#x2F;test-parallel-array.js. Input ObjectAlso it supports named jobs, listed via object. 12345678910111213141516171819202122232425262728293031323334353637var parallel = require(&#x27;asynckit/parallel&#x27;) , assert = require(&#x27;assert&#x27;) ;var source = &#123; first: 1, one: 1, four: 4, sixteen: 16, sixtyFour: 64, thirtyTwo: 32, eight: 8, two: 2 &#125; , expectedResult = &#123; first: 2, one: 2, four: 8, sixteen: 32, sixtyFour: 128, thirtyTwo: 64, eight: 16, two: 4 &#125; , expectedTarget = [ 1, 1, 2, 4, 8, 16, 32, 64 ] , expectedKeys = [ &#x27;first&#x27;, &#x27;one&#x27;, &#x27;two&#x27;, &#x27;four&#x27;, &#x27;eight&#x27;, &#x27;sixteen&#x27;, &#x27;thirtyTwo&#x27;, &#x27;sixtyFour&#x27; ] , target = [] , keys = [] ;parallel(source, asyncJob, function(err, result)&#123; assert.deepEqual(result, expectedResult); assert.deepEqual(target, expectedTarget); assert.deepEqual(keys, expectedKeys);&#125;);// supports full value, key, callback (shortcut) interfacefunction asyncJob(item, key, cb)&#123; // different delays (in ms) per item var delay = item * 25; // pretend different jobs take different time to finish // and not in consequential order var timeoutId = setTimeout(function() &#123; keys.push(key); target.push(item); cb(null, item * 2); &#125;, delay); // allow to cancel &quot;leftover&quot; jobs upon error // return function, invoking of which will abort this job return clearTimeout.bind(null, timeoutId);&#125; More examples could be found in test&#x2F;test-parallel-object.js. Serial JobsRuns iterator over provided array sequentially. Stores output in the result array,on the matching positions. In unlikely event of an error from one of the jobs,will not proceed to the rest of the items in the listand return error along with salvaged data to the main callback function. Input Array1234567891011121314151617181920212223242526var serial = require(&#x27;asynckit/serial&#x27;) , assert = require(&#x27;assert&#x27;) ;var source = [ 1, 1, 4, 16, 64, 32, 8, 2 ] , expectedResult = [ 2, 2, 8, 32, 128, 64, 16, 4 ] , expectedTarget = [ 0, 1, 2, 3, 4, 5, 6, 7 ] , target = [] ;serial(source, asyncJob, function(err, result)&#123; assert.deepEqual(result, expectedResult); assert.deepEqual(target, expectedTarget);&#125;);// extended interface (item, key, callback)// also supported for arraysfunction asyncJob(item, key, cb)&#123; target.push(key); // it will be automatically made async // even it iterator &quot;returns&quot; in the same event loop cb(null, item * 2);&#125; More examples could be found in test&#x2F;test-serial-array.js. Input ObjectAlso it supports named jobs, listed via object. 123456789101112131415161718192021222324252627282930313233var serial = require(&#x27;asynckit&#x27;).serial , assert = require(&#x27;assert&#x27;) ;var source = [ 1, 1, 4, 16, 64, 32, 8, 2 ] , expectedResult = [ 2, 2, 8, 32, 128, 64, 16, 4 ] , expectedTarget = [ 0, 1, 2, 3, 4, 5, 6, 7 ] , target = [] ;var source = &#123; first: 1, one: 1, four: 4, sixteen: 16, sixtyFour: 64, thirtyTwo: 32, eight: 8, two: 2 &#125; , expectedResult = &#123; first: 2, one: 2, four: 8, sixteen: 32, sixtyFour: 128, thirtyTwo: 64, eight: 16, two: 4 &#125; , expectedTarget = [ 1, 1, 4, 16, 64, 32, 8, 2 ] , target = [] ;serial(source, asyncJob, function(err, result)&#123; assert.deepEqual(result, expectedResult); assert.deepEqual(target, expectedTarget);&#125;);// shortcut interface (item, callback)// works for object as well as for the arraysfunction asyncJob(item, cb)&#123; target.push(item); // it will be automatically made async // even it iterator &quot;returns&quot; in the same event loop cb(null, item * 2);&#125; More examples could be found in test&#x2F;test-serial-object.js. Note: Since object is an unordered collection of properties,it may produce unexpected results with sequential iterations.Whenever order of the jobs’ execution is important please use serialOrdered method. Ordered Serial IterationsTBD For example compare-property package. Streaming interfaceTBD Want to Know More?More examples can be found in test folder. Or open an issue with questions and&#x2F;or suggestions. LicenseAsyncKit is licensed under the MIT license.","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/mime-types/HISTORY","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/mime-types/HISTORY/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/mime-types/HISTORY/","excerpt":"","text":"2.1.35 &#x2F; 2022-03-12 deps: &#109;&#x69;&#x6d;&#101;&#45;&#x64;&#x62;&#64;&#49;&#46;&#x35;&#50;&#46;&#48; Add extensions from IANA for more image/* types Add extension .asc to application/pgp-keys Add extensions to various XML types Add new upstream MIME types 2.1.34 &#x2F; 2021-11-08 deps: &#x6d;&#105;&#109;&#x65;&#45;&#100;&#x62;&#x40;&#49;&#46;&#53;&#49;&#x2e;&#x30; Add new upstream MIME types 2.1.33 &#x2F; 2021-10-01 deps: &#x6d;&#x69;&#x6d;&#101;&#45;&#100;&#98;&#64;&#x31;&#x2e;&#53;&#48;&#46;&#x30; Add deprecated iWorks mime types and extensions Add new upstream MIME types 2.1.32 &#x2F; 2021-07-27 deps: &#109;&#x69;&#109;&#x65;&#x2d;&#x64;&#x62;&#64;&#x31;&#46;&#52;&#57;&#46;&#x30; Add extension .trig to application/trig Add new upstream MIME types 2.1.31 &#x2F; 2021-06-01 deps: &#109;&#x69;&#109;&#101;&#x2d;&#100;&#x62;&#64;&#49;&#46;&#52;&#x38;&#46;&#x30; Add extension .mvt to application/vnd.mapbox-vector-tile Add new upstream MIME types 2.1.30 &#x2F; 2021-04-02 deps: &#x6d;&#x69;&#109;&#101;&#45;&#100;&#x62;&#64;&#49;&#x2e;&#x34;&#55;&#46;&#x30; Add extension .amr to audio/amr Remove ambigious extensions from IANA for application/*+xml types Update primary extension to .es for application/ecmascript 2.1.29 &#x2F; 2021-02-17 deps: &#x6d;&#x69;&#109;&#101;&#45;&#x64;&#x62;&#64;&#x31;&#x2e;&#52;&#54;&#x2e;&#48; Add extension .amr to audio/amr Add extension .m4s to video/iso.segment Add extension .opus to audio/ogg Add new upstream MIME types 2.1.28 &#x2F; 2021-01-01 deps: &#x6d;&#x69;&#109;&#101;&#45;&#100;&#x62;&#64;&#x31;&#46;&#52;&#53;&#x2e;&#48; Add application/ubjson with extension .ubj Add image/avif with extension .avif Add image/ktx2 with extension .ktx2 Add extension .dbf to application/vnd.dbf Add extension .rar to application/vnd.rar Add extension .td to application/urc-targetdesc+xml Add new upstream MIME types Fix extension of application/vnd.apple.keynote to be .key 2.1.27 &#x2F; 2020-04-23 deps: &#109;&#x69;&#109;&#101;&#45;&#100;&#98;&#x40;&#x31;&#46;&#x34;&#52;&#x2e;&#x30; Add charsets from IANA Add extension .cjs to application/node Add new upstream MIME types 2.1.26 &#x2F; 2020-01-05 deps: &#x6d;&#105;&#x6d;&#x65;&#x2d;&#100;&#98;&#64;&#49;&#x2e;&#52;&#51;&#x2e;&#48; Add application/x-keepass2 with extension .kdbx Add extension .mxmf to audio/mobile-xmf Add extensions from IANA for application/*+xml types Add new upstream MIME types 2.1.25 &#x2F; 2019-11-12 deps: &#x6d;&#x69;&#109;&#101;&#45;&#100;&#x62;&#64;&#49;&#x2e;&#x34;&#x32;&#x2e;&#x30; Add new upstream MIME types Add application/toml with extension .toml Add image/vnd.ms-dds with extension .dds 2.1.24 &#x2F; 2019-04-20 deps: &#x6d;&#105;&#109;&#x65;&#x2d;&#x64;&#98;&#x40;&#49;&#46;&#52;&#48;&#x2e;&#48; Add extensions from IANA for model/* types Add text/mdx with extension .mdx 2.1.23 &#x2F; 2019-04-17 deps: mime-db@~1.39.0 Add extensions .siv and .sieve to application/sieve Add new upstream MIME types 2.1.22 &#x2F; 2019-02-14 deps: mime-db@~1.38.0 Add extension .nq to application/n-quads Add extension .nt to application/n-triples Add new upstream MIME types 2.1.21 &#x2F; 2018-10-19 deps: mime-db@~1.37.0 Add extensions to HEIC image types Add new upstream MIME types 2.1.20 &#x2F; 2018-08-26 deps: mime-db@~1.36.0 Add Apple file extensions from IANA Add extensions from IANA for image/* types Add new upstream MIME types 2.1.19 &#x2F; 2018-07-17 deps: mime-db@~1.35.0 Add extension .csl to application/vnd.citationstyles.style+xml Add extension .es to application/ecmascript Add extension .owl to application/rdf+xml Add new upstream MIME types Add UTF-8 as default charset for text/turtle 2.1.18 &#x2F; 2018-02-16 deps: mime-db@~1.33.0 Add application/raml+yaml with extension .raml Add application/wasm with extension .wasm Add text/shex with extension .shex Add extensions for JPEG-2000 images Add extensions from IANA for message/* types Add new upstream MIME types Update font MIME types Update text/hjson to registered application/hjson 2.1.17 &#x2F; 2017-09-01 deps: mime-db@~1.30.0 Add application/vnd.ms-outlook Add application/x-arj Add extension .mjs to application/javascript Add glTF types and extensions Add new upstream MIME types Add text/x-org Add VirtualBox MIME types Fix source records for video/* types that are IANA Update font/opentype to registered font/otf 2.1.16 &#x2F; 2017-07-24 deps: mime-db@~1.29.0 Add application/fido.trusted-apps+json Add extension .wadl to application/vnd.sun.wadl+xml Add extension .gz to application/gzip Add new upstream MIME types Update extensions .md and .markdown to be text/markdown 2.1.15 &#x2F; 2017-03-23 deps: mime-db@~1.27.0 Add new mime types Add image/apng 2.1.14 &#x2F; 2017-01-14 deps: mime-db@~1.26.0 Add new mime types 2.1.13 &#x2F; 2016-11-18 deps: mime-db@~1.25.0 Add new mime types 2.1.12 &#x2F; 2016-09-18 deps: mime-db@~1.24.0 Add new mime types Add audio/mp3 2.1.11 &#x2F; 2016-05-01 deps: mime-db@~1.23.0 Add new mime types 2.1.10 &#x2F; 2016-02-15 deps: mime-db@~1.22.0 Add new mime types Fix extension of application/dash+xml Update primary extension for audio/mp4 2.1.9 &#x2F; 2016-01-06 deps: mime-db@~1.21.0 Add new mime types 2.1.8 &#x2F; 2015-11-30 deps: mime-db@~1.20.0 Add new mime types 2.1.7 &#x2F; 2015-09-20 deps: mime-db@~1.19.0 Add new mime types 2.1.6 &#x2F; 2015-09-03 deps: mime-db@~1.18.0 Add new mime types 2.1.5 &#x2F; 2015-08-20 deps: mime-db@~1.17.0 Add new mime types 2.1.4 &#x2F; 2015-07-30 deps: mime-db@~1.16.0 Add new mime types 2.1.3 &#x2F; 2015-07-13 deps: mime-db@~1.15.0 Add new mime types 2.1.2 &#x2F; 2015-06-25 deps: mime-db@~1.14.0 Add new mime types 2.1.1 &#x2F; 2015-06-08 perf: fix deopt during mapping 2.1.0 &#x2F; 2015-06-07 Fix incorrectly treating extension-less file name as extension i.e. &#39;path/to/json&#39; will no longer return application/json Fix .charset(type) to accept parameters Fix .charset(type) to match case-insensitive Improve generation of extension to MIME mapping Refactor internals for readability and no argument reassignment Prefer application/* MIME types from the same source Prefer any type over application/octet-stream deps: mime-db@~1.13.0 Add nginx as a source Add new mime types 2.0.14 &#x2F; 2015-06-06 deps: mime-db@~1.12.0 Add new mime types 2.0.13 &#x2F; 2015-05-31 deps: mime-db@~1.11.0 Add new mime types 2.0.12 &#x2F; 2015-05-19 deps: mime-db@~1.10.0 Add new mime types 2.0.11 &#x2F; 2015-05-05 deps: mime-db@~1.9.1 Add new mime types 2.0.10 &#x2F; 2015-03-13 deps: mime-db@~1.8.0 Add new mime types 2.0.9 &#x2F; 2015-02-09 deps: mime-db@~1.7.0 Add new mime types Community extensions ownership transferred from node-mime 2.0.8 &#x2F; 2015-01-29 deps: mime-db@~1.6.0 Add new mime types 2.0.7 &#x2F; 2014-12-30 deps: mime-db@~1.5.0 Add new mime types Fix various invalid MIME type entries 2.0.6 &#x2F; 2014-12-30 deps: mime-db@~1.4.0 Add new mime types Fix various invalid MIME type entries Remove example template MIME types 2.0.5 &#x2F; 2014-12-29 deps: mime-db@~1.3.1 Fix missing extensions 2.0.4 &#x2F; 2014-12-10 deps: mime-db@~1.3.0 Add new mime types 2.0.3 &#x2F; 2014-11-09 deps: mime-db@~1.2.0 Add new mime types 2.0.2 &#x2F; 2014-09-28 deps: mime-db@~1.1.0 Add new mime types Update charsets 2.0.1 &#x2F; 2014-09-07 Support Node.js 0.6 2.0.0 &#x2F; 2014-09-02 Use mime-db Remove .define() 1.0.2 &#x2F; 2014-08-04 Set charset&#x3D;utf-8 for text/javascript 1.0.1 &#x2F; 2014-06-24 Add text/jsx type 1.0.0 &#x2F; 2014-05-12 Return false for unknown types Set charset&#x3D;utf-8 for application/json 0.1.0 &#x2F; 2014-05-02 Initial release","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/mime-types/README","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/mime-types/README/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/mime-types/README/","excerpt":"","text":"mime-types The ultimate javascript content-type utility. Similar to the mime@1.x module, except: No fallbacks. Instead of naively returning the first available type,mime-types simply returns false, so dovar type = mime.lookup(&#39;unrecognized&#39;) || &#39;application/octet-stream&#39;. No new Mime() business, so you could do var lookup = require(&#39;mime-types&#39;).lookup. No .define() functionality Bug fixes for .lookup(path) Otherwise, the API is compatible with mime 1.x. InstallThis is a Node.js module available through thenpm registry. Installation is done using thenpm install command: 1$ npm install mime-types Adding TypesAll mime types are based on mime-db,so open a PR there if you’d like to add mime types. API1var mime = require(&#x27;mime-types&#x27;) All functions return false if input is invalid or not found. mime.lookup(path)Lookup the content-type associated with a file. 1234567mime.lookup(&#x27;json&#x27;) // &#x27;application/json&#x27;mime.lookup(&#x27;.md&#x27;) // &#x27;text/markdown&#x27;mime.lookup(&#x27;file.html&#x27;) // &#x27;text/html&#x27;mime.lookup(&#x27;folder/file.js&#x27;) // &#x27;application/javascript&#x27;mime.lookup(&#x27;folder/.htaccess&#x27;) // falsemime.lookup(&#x27;cats&#x27;) // false mime.contentType(type)Create a full content-type header given a content-type or extension.When given an extension, mime.lookup is used to get the matchingcontent-type, otherwise the given content-type is used. Then if thecontent-type does not already have a charset parameter, mime.charsetis used to get the default charset and add to the returned content-type. 1234567mime.contentType(&#x27;markdown&#x27;) // &#x27;text/x-markdown; charset=utf-8&#x27;mime.contentType(&#x27;file.json&#x27;) // &#x27;application/json; charset=utf-8&#x27;mime.contentType(&#x27;text/html&#x27;) // &#x27;text/html; charset=utf-8&#x27;mime.contentType(&#x27;text/html; charset=iso-8859-1&#x27;) // &#x27;text/html; charset=iso-8859-1&#x27;// from a full pathmime.contentType(path.extname(&#x27;/path/to/file.json&#x27;)) // &#x27;application/json; charset=utf-8&#x27; mime.extension(type)Get the default extension for a content-type. 1mime.extension(&#x27;application/octet-stream&#x27;) // &#x27;bin&#x27; mime.charset(type)Lookup the implied default charset of a content-type. 1mime.charset(&#x27;text/markdown&#x27;) // &#x27;UTF-8&#x27; var type &#x3D; mime.types[extension]A map of content-types by extension. [extensions…] &#x3D; mime.extensions[type]A map of extensions by content-type. LicenseMIT","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/combined-stream/Readme","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/combined-stream/Readme/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/combined-stream/Readme/","excerpt":"","text":"combined-streamA stream that emits multiple other streams one after another. NB Currently combined-stream works with streams version 1 only. There is ongoing effort to switch this library to streams version 2. Any help is welcome. :) Meanwhile you can explore other libraries that provide streams2 support with more or less compatibility with combined-stream. combined-stream2: A drop-in streams2-compatible replacement for the combined-stream module. multistream: A stream that emits multiple other streams one after another. Installation1npm install combined-stream UsageHere is a simple example that shows how you can use combined-stream to combinetwo files into one: 12345678var CombinedStream = require(&#x27;combined-stream&#x27;);var fs = require(&#x27;fs&#x27;);var combinedStream = CombinedStream.create();combinedStream.append(fs.createReadStream(&#x27;file1.txt&#x27;));combinedStream.append(fs.createReadStream(&#x27;file2.txt&#x27;));combinedStream.pipe(fs.createWriteStream(&#x27;combined.txt&#x27;)); While the example above works great, it will pause all source streams untilthey are needed. If you don’t want that to happen, you can set pauseStreamsto false: 12345678var CombinedStream = require(&#x27;combined-stream&#x27;);var fs = require(&#x27;fs&#x27;);var combinedStream = CombinedStream.create(&#123;pauseStreams: false&#125;);combinedStream.append(fs.createReadStream(&#x27;file1.txt&#x27;));combinedStream.append(fs.createReadStream(&#x27;file2.txt&#x27;));combinedStream.pipe(fs.createWriteStream(&#x27;combined.txt&#x27;)); However, what if you don’t have all the source streams yet, or you don’t wantto allocate the resources (file descriptors, memory, etc.) for them right away?Well, in that case you can simply provide a callback that supplies the streamby calling a next() function: 123456789101112var CombinedStream = require(&#x27;combined-stream&#x27;);var fs = require(&#x27;fs&#x27;);var combinedStream = CombinedStream.create();combinedStream.append(function(next) &#123; next(fs.createReadStream(&#x27;file1.txt&#x27;));&#125;);combinedStream.append(function(next) &#123; next(fs.createReadStream(&#x27;file2.txt&#x27;));&#125;);combinedStream.pipe(fs.createWriteStream(&#x27;combined.txt&#x27;)); APICombinedStream.create([options])Returns a new combined stream object. Available options are: maxDataSize pauseStreams The effect of those options is described below. combinedStream.pauseStreams &#x3D; trueWhether to apply back pressure to the underlaying streams. If set to false,the underlaying streams will never be paused. If set to true, theunderlaying streams will be paused right after being appended, as well as whendelayedStream.pipe() wants to throttle. combinedStream.maxDataSize &#x3D; 2 * 1024 * 1024The maximum amount of bytes (or characters) to buffer for all source streams.If this value is exceeded, combinedStream emits an &#39;error&#39; event. combinedStream.dataSize &#x3D; 0The amount of bytes (or characters) currently buffered by combinedStream. combinedStream.append(stream)Appends the given stream to the combinedStream object. If pauseStreams isset to &#96;true, this stream will also be paused right away. streams can also be a function that takes one parameter called next. nextis a function that must be invoked in order to provide the next stream, seeexample above. Regardless of how the stream is appended, combined-stream always attaches an&#39;error&#39; listener to it, so you don’t have to do that manually. Special case: stream can also be a String or Buffer. combinedStream.write(data)You should not call this, combinedStream takes care of piping the appendedstreams into itself for you. combinedStream.resume()Causes combinedStream to start drain the streams it manages. The function isidempotent, and also emits a &#39;resume&#39; event each time which usually goes tothe stream that is currently being drained. combinedStream.pause();If combinedStream.pauseStreams is set to false, this does nothing.Otherwise a &#39;pause&#39; event is emitted, this goes to the stream that iscurrently being drained, so you can use it to apply back pressure. combinedStream.end();Sets combinedStream.writable to false, emits an &#39;end&#39; event, and removesall streams from the queue. combinedStream.destroy();Same as combinedStream.end(), except it emits a &#39;close&#39; event instead of&#39;end&#39;. Licensecombined-stream is licensed under the MIT license.","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/mime-db/HISTORY","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/mime-db/HISTORY/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/mime-db/HISTORY/","excerpt":"","text":"1.52.0 &#x2F; 2022-02-21 Add extensions from IANA for more image/* types Add extension .asc to application/pgp-keys Add extensions to various XML types Add new upstream MIME types 1.51.0 &#x2F; 2021-11-08 Add new upstream MIME types Mark image/vnd.microsoft.icon as compressible Mark image/vnd.ms-dds as compressible 1.50.0 &#x2F; 2021-09-15 Add deprecated iWorks mime types and extensions Add new upstream MIME types 1.49.0 &#x2F; 2021-07-26 Add extension .trig to application/trig Add new upstream MIME types 1.48.0 &#x2F; 2021-05-30 Add extension .mvt to application/vnd.mapbox-vector-tile Add new upstream MIME types Mark text/yaml as compressible 1.47.0 &#x2F; 2021-04-01 Add new upstream MIME types Remove ambigious extensions from IANA for application/*+xml types Update primary extension to .es for application/ecmascript 1.46.0 &#x2F; 2021-02-13 Add extension .amr to audio/amr Add extension .m4s to video/iso.segment Add extension .opus to audio/ogg Add new upstream MIME types 1.45.0 &#x2F; 2020-09-22 Add application/ubjson with extension .ubj Add image/avif with extension .avif Add image/ktx2 with extension .ktx2 Add extension .dbf to application/vnd.dbf Add extension .rar to application/vnd.rar Add extension .td to application/urc-targetdesc+xml Add new upstream MIME types Fix extension of application/vnd.apple.keynote to be .key 1.44.0 &#x2F; 2020-04-22 Add charsets from IANA Add extension .cjs to application/node Add new upstream MIME types 1.43.0 &#x2F; 2020-01-05 Add application/x-keepass2 with extension .kdbx Add extension .mxmf to audio/mobile-xmf Add extensions from IANA for application/*+xml types Add new upstream MIME types 1.42.0 &#x2F; 2019-09-25 Add image/vnd.ms-dds with extension .dds Add new upstream MIME types Remove compressible from multipart/mixed 1.41.0 &#x2F; 2019-08-30 Add new upstream MIME types Add application/toml with extension .toml Mark font/ttf as compressible 1.40.0 &#x2F; 2019-04-20 Add extensions from IANA for model/* types Add text/mdx with extension .mdx 1.39.0 &#x2F; 2019-04-04 Add extensions .siv and .sieve to application/sieve Add new upstream MIME types 1.38.0 &#x2F; 2019-02-04 Add extension .nq to application/n-quads Add extension .nt to application/n-triples Add new upstream MIME types Mark text/less as compressible 1.37.0 &#x2F; 2018-10-19 Add extensions to HEIC image types Add new upstream MIME types 1.36.0 &#x2F; 2018-08-20 Add Apple file extensions from IANA Add extensions from IANA for image/* types Add new upstream MIME types 1.35.0 &#x2F; 2018-07-15 Add extension .owl to application/rdf+xml Add new upstream MIME types Removes extension .woff from application/font-woff 1.34.0 &#x2F; 2018-06-03 Add extension .csl to application/vnd.citationstyles.style+xml Add extension .es to application/ecmascript Add new upstream MIME types Add UTF-8 as default charset for text/turtle Mark all XML-derived types as compressible 1.33.0 &#x2F; 2018-02-15 Add extensions from IANA for message/* types Add new upstream MIME types Fix some incorrect OOXML types Remove application/font-woff2 1.32.0 &#x2F; 2017-11-29 Add new upstream MIME types Update text/hjson to registered application/hjson Add text/shex with extension .shex 1.31.0 &#x2F; 2017-10-25 Add application/raml+yaml with extension .raml Add application/wasm with extension .wasm Add new font type from IANA Add new upstream font extensions Add new upstream MIME types Add extensions for JPEG-2000 images 1.30.0 &#x2F; 2017-08-27 Add application/vnd.ms-outlook Add application/x-arj Add extension .mjs to application/javascript Add glTF types and extensions Add new upstream MIME types Add text/x-org Add VirtualBox MIME types Fix source records for video/* types that are IANA Update font/opentype to registered font/otf 1.29.0 &#x2F; 2017-07-10 Add application/fido.trusted-apps+json Add extension .wadl to application/vnd.sun.wadl+xml Add new upstream MIME types Add UTF-8 as default charset for text/css 1.28.0 &#x2F; 2017-05-14 Add new upstream MIME types Add extension .gz to application/gzip Update extensions .md and .markdown to be text/markdown 1.27.0 &#x2F; 2017-03-16 Add new upstream MIME types Add image/apng with extension .apng 1.26.0 &#x2F; 2017-01-14 Add new upstream MIME types Add extension .geojson to application/geo+json 1.25.0 &#x2F; 2016-11-11 Add new upstream MIME types 1.24.0 &#x2F; 2016-09-18 Add audio/mp3 Add new upstream MIME types 1.23.0 &#x2F; 2016-05-01 Add new upstream MIME types Add extension .3gpp to audio/3gpp 1.22.0 &#x2F; 2016-02-15 Add text/slim Add extension .rng to application/xml Add new upstream MIME types Fix extension of application/dash+xml to be .mpd Update primary extension to .m4a for audio/mp4 1.21.0 &#x2F; 2016-01-06 Add Google document types Add new upstream MIME types 1.20.0 &#x2F; 2015-11-10 Add text/x-suse-ymp Add new upstream MIME types 1.19.0 &#x2F; 2015-09-17 Add application/vnd.apple.pkpass Add new upstream MIME types 1.18.0 &#x2F; 2015-09-03 Add new upstream MIME types 1.17.0 &#x2F; 2015-08-13 Add application/x-msdos-program Add audio/g711-0 Add image/vnd.mozilla.apng Add extension .exe to application/x-msdos-program 1.16.0 &#x2F; 2015-07-29 Add application/vnd.uri-map 1.15.0 &#x2F; 2015-07-13 Add application/x-httpd-php 1.14.0 &#x2F; 2015-06-25 Add application/scim+json Add application/vnd.3gpp.ussd+xml Add application/vnd.biopax.rdf+xml Add text/x-processing 1.13.0 &#x2F; 2015-06-07 Add nginx as a source Add application/x-cocoa Add application/x-java-archive-diff Add application/x-makeself Add application/x-perl Add application/x-pilot Add application/x-redhat-package-manager Add application/x-sea Add audio/x-m4a Add audio/x-realaudio Add image/x-jng Add text/mathml 1.12.0 &#x2F; 2015-06-05 Add application/bdoc Add application/vnd.hyperdrive+json Add application/x-bdoc Add extension .rtf to text/rtf 1.11.0 &#x2F; 2015-05-31 Add audio/wav Add audio/wave Add extension .litcoffee to text/coffeescript Add extension .sfd-hdstx to application/vnd.hydrostatix.sof-data Add extension .n-gage to application/vnd.nokia.n-gage.symbian.install 1.10.0 &#x2F; 2015-05-19 Add application/vnd.balsamiq.bmpr Add application/vnd.microsoft.portable-executable Add application/x-ns-proxy-autoconfig 1.9.1 &#x2F; 2015-04-19 Remove .json extension from application/manifest+json This is causing bugs downstream 1.9.0 &#x2F; 2015-04-19 Add application/manifest+json Add application/vnd.micro+json Add image/vnd.zbrush.pcx Add image/x-ms-bmp 1.8.0 &#x2F; 2015-03-13 Add application/vnd.citationstyles.style+xml Add application/vnd.fastcopy-disk-image Add application/vnd.gov.sk.xmldatacontainer+xml Add extension .jsonld to application/ld+json 1.7.0 &#x2F; 2015-02-08 Add application/vnd.gerber Add application/vnd.msa-disk-image 1.6.1 &#x2F; 2015-02-05 Community extensions ownership transferred from node-mime 1.6.0 &#x2F; 2015-01-29 Add application/jose Add application/jose+json Add application/json-seq Add application/jwk+json Add application/jwk-set+json Add application/jwt Add application/rdap+json Add application/vnd.gov.sk.e-form+xml Add application/vnd.ims.imsccv1p3 1.5.0 &#x2F; 2014-12-30 Add application/vnd.oracle.resource+json Fix various invalid MIME type entries application/mbox+xml application/oscp-response application/vwg-multiplexed audio/g721 1.4.0 &#x2F; 2014-12-21 Add application/vnd.ims.imsccv1p2 Fix various invalid MIME type entries application/vnd-acucobol application/vnd-curl application/vnd-dart application/vnd-dxr application/vnd-fdf application/vnd-mif application/vnd-sema application/vnd-wap-wmlc application/vnd.adobe.flash-movie application/vnd.dece-zip application/vnd.dvb_service application/vnd.micrografx-igx application/vnd.sealed-doc application/vnd.sealed-eml application/vnd.sealed-mht application/vnd.sealed-ppt application/vnd.sealed-tiff application/vnd.sealed-xls application/vnd.sealedmedia.softseal-html application/vnd.sealedmedia.softseal-pdf application/vnd.wap-slc application/vnd.wap-wbxml audio/vnd.sealedmedia.softseal-mpeg image/vnd-djvu image/vnd-svf image/vnd-wap-wbmp image/vnd.sealed-png image/vnd.sealedmedia.softseal-gif image/vnd.sealedmedia.softseal-jpg model/vnd-dwf model/vnd.parasolid.transmit-binary model/vnd.parasolid.transmit-text text/vnd-a text/vnd-curl text/vnd.wap-wml Remove example template MIME types application/example audio/example image/example message/example model/example multipart/example text/example video/example 1.3.1 &#x2F; 2014-12-16 Fix missing extensions application/json5 text/hjson 1.3.0 &#x2F; 2014-12-07 Add application/a2l Add application/aml Add application/atfx Add application/atxml Add application/cdfx+xml Add application/dii Add application/json5 Add application/lxf Add application/mf4 Add application/vnd.apache.thrift.compact Add application/vnd.apache.thrift.json Add application/vnd.coffeescript Add application/vnd.enphase.envoy Add application/vnd.ims.imsccv1p1 Add text/csv-schema Add text/hjson Add text/markdown Add text/yaml 1.2.0 &#x2F; 2014-11-09 Add application/cea Add application/dit Add application/vnd.gov.sk.e-form+zip Add application/vnd.tmd.mediaflex.api+xml Type application/epub+zip is now IANA-registered 1.1.2 &#x2F; 2014-10-23 Rebuild database for application/x-www-form-urlencoded change 1.1.1 &#x2F; 2014-10-20 Mark application/x-www-form-urlencoded as compressible. 1.1.0 &#x2F; 2014-09-28 Add application/font-woff2 1.0.3 &#x2F; 2014-09-25 Fix engine requirement in package 1.0.2 &#x2F; 2014-09-25 Add application/coap-group+json Add application/dcd Add application/vnd.apache.thrift.binary Add image/vnd.tencent.tap Mark all JSON-derived types as compressible Update text/vtt data 1.0.1 &#x2F; 2014-08-30 Fix extension ordering 1.0.0 &#x2F; 2014-08-30 Add application/atf Add application/merge-patch+json Add multipart/x-mixed-replace Add source: &#39;apache&#39; metadata Add source: &#39;iana&#39; metadata Remove badly-assumed charset data","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/mime-db/README","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/mime-db/README/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/mime-db/README/","excerpt":"","text":"mime-db This is a large database of mime types and information about them.It consists of a single, public JSON file and does not include any logic,allowing it to remain as un-opinionated as possible with an API.It aggregates data from the following sources: http://www.iana.org/assignments/media-types/media-types.xhtml http://svn.apache.org/repos/asf/httpd/httpd/trunk/docs/conf/mime.types http://hg.nginx.org/nginx/raw-file/default/conf/mime.types Installation1npm install mime-db Database DownloadIf you’re crazy enough to use this in the browser, you can just grab theJSON file using jsDelivr. It is recommended toreplace master with a release tagas the JSON format may change in the future. 1https://cdn.jsdelivr.net/gh/jshttp/mime-db@master/db.json Usage1234var db = require(&#x27;mime-db&#x27;)// grab data on .js filesvar data = db[&#x27;application/javascript&#x27;] Data StructureThe JSON file is a map lookup for lowercased mime types.Each mime type has the following properties: .source - where the mime type is defined. If not set, it’s probably a custom media type. apache - Apache common media types iana - IANA-defined media types nginx - nginx media types .extensions[] - known extensions associated with this mime type. .compressible - whether a file of this type can be gzipped. .charset - the default charset associated with this type, if any. If unknown, every property could be undefined. ContributingTo edit the database, only make PRs against src/custom-types.json orsrc/custom-suffix.json. The src/custom-types.json file is a JSON object with the MIME type as thekeys and the values being an object with the following keys: compressible - leave out if you don’t know, otherwise true&#x2F;false toindicate whether the data represented by the type is typically compressible. extensions - include an array of file extensions that are associated withthe type. notes - human-readable notes about the type, typically what the type is. sources - include an array of URLs of where the MIME type and the associatedextensions are sourced from. This needs to be a primary source;links to type aggregating sites and Wikipedia are not acceptable. To update the build, run npm run build. Adding Custom Media TypesThe best way to get new media types included in this library is to registerthem with the IANA. The community registration procedure is outlined inRFC 6838 section 5. Typesregistered with the IANA are automatically pulled into this library. If that is not possible &#x2F; feasible, they can be added directly here as a“custom” type. To do this, it is required to have a primary source thatdefinitively lists the media type. If an extension is going to be listed asassociateed with this media type, the source must definitively link themedia type and extension as well.","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/proxy-from-env/README","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/proxy-from-env/README/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/proxy-from-env/README/","excerpt":"","text":"proxy-from-env proxy-from-env is a Node.js package that exports a function (getProxyForUrl)that takes an input URL (a string orurl.parse‘sreturn value) and returns the desired proxy URL (also a string) based onstandard proxy environment variables. If no proxy is set, an empty string isreturned. It is your responsibility to actually proxy the request using the given URL. Installation: 1npm install proxy-from-env ExampleThis example shows how the data for a URL can be fetched via thehttp module, in a proxy-aware way. 1234567891011121314151617181920212223242526272829303132333435363738394041var http = require(&#x27;http&#x27;);var parseUrl = require(&#x27;url&#x27;).parse;var getProxyForUrl = require(&#x27;proxy-from-env&#x27;).getProxyForUrl;var some_url = &#x27;http://example.com/something&#x27;;// // Example, if there is a proxy server at 10.0.0.1:1234, then setting the// // http_proxy environment variable causes the request to go through a proxy.// process.env.http_proxy = &#x27;http://10.0.0.1:1234&#x27;;// // // But if the host to be proxied is listed in NO_PROXY, then the request is// // not proxied (but a direct request is made).// process.env.no_proxy = &#x27;example.com&#x27;;var proxy_url = getProxyForUrl(some_url); // &lt;-- Our magic.if (proxy_url) &#123; // Should be proxied through proxy_url. var parsed_some_url = parseUrl(some_url); var parsed_proxy_url = parseUrl(proxy_url); // A HTTP proxy is quite simple. It is similar to a normal request, except the // path is an absolute URL, and the proxied URL&#x27;s host is put in the header // instead of the server&#x27;s actual host. httpOptions = &#123; protocol: parsed_proxy_url.protocol, hostname: parsed_proxy_url.hostname, port: parsed_proxy_url.port, path: parsed_some_url.href, headers: &#123; Host: parsed_some_url.host, // = host name + optional port. &#125;, &#125;;&#125; else &#123; // Direct request. httpOptions = some_url;&#125;http.get(httpOptions, function(res) &#123; var responses = []; res.on(&#x27;data&#x27;, function(chunk) &#123; responses.push(chunk); &#125;); res.on(&#x27;end&#x27;, function() &#123; console.log(responses.join(&#x27;&#x27;)); &#125;);&#125;); Environment variablesThe environment variables can be specified in lowercase or uppercase, with thelowercase name having precedence over the uppercase variant. A variable that isnot set has the same meaning as a variable that is set but has no value. NO_PROXYNO_PROXY is a list of host names (optionally with a port). If the input URLmatches any of the entries in NO_PROXY, then the input URL should be fetchedby a direct request (i.e. without a proxy). Matching follows the following rules: NO_PROXY=* disables all proxies. Space and commas may be used to separate the entries in the NO_PROXY list. If NO_PROXY does not contain any entries, then proxies are never disabled. If a port is added after the host name, then the ports must match. If the URLdoes not have an explicit port name, the protocol’s default port is used. Generally, the proxy is only disabled if the host name is an exact match foran entry in the NO_PROXY list. The only exceptions are entries that startwith a dot or with a wildcard; then the proxy is disabled if the host nameends with the entry. See test.js for examples of what should match and what does not. *_PROXYThe environment variable used for the proxy depends on the protocol of the URL.For example, https://example.com uses the “https” protocol, and therefore theproxy to be used is HTTPS_PROXY (NOT HTTP_PROXY, which is only used forhttp:-URLs). The library is not limited to http(s), other schemes such asFTP_PROXY (ftp:),WSS_PROXY (wss:),WS_PROXY (ws:)are also supported. If present, ALL_PROXY is used as fallback if there is no other match. External resourcesThe exact way of parsing the environment variables is not codified in anystandard. This library is designed to be compatible with formats as expected byexisting software.The following resources were used to determine the desired behavior: cURL:https://curl.haxx.se/docs/manpage.html#ENVIRONMENThttps://github.com/curl/curl/blob/4af40b3646d3b09f68e419f7ca866ff395d1f897/lib/url.c#L4446-L4514https://github.com/curl/curl/blob/4af40b3646d3b09f68e419f7ca866ff395d1f897/lib/url.c#L4608-L4638 wget:https://www.gnu.org/software/wget/manual/wget.html#Proxieshttp://git.savannah.gnu.org/cgit/wget.git/tree/src/init.c?id=636a5f9a1c508aa39e35a3a8e9e54520a284d93d#n383http://git.savannah.gnu.org/cgit/wget.git/tree/src/retr.c?id=93c1517c4071c4288ba5a4b038e7634e4c6b5482#n1278 W3:https://www.w3.org/Daemon/User/Proxies/ProxyClients.html Python’s urllib:https://github.com/python/cpython/blob/936135bb97fe04223aa30ca6e98eac8f3ed6b349/Lib/urllib/request.py#L755-L782https://github.com/python/cpython/blob/936135bb97fe04223aa30ca6e98eac8f3ed6b349/Lib/urllib/request.py#L2444-L2479","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/axios/lib/adapters/README","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/axios/lib/adapters/README/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/axios/lib/adapters/README/","excerpt":"","text":"axios &#x2F;&#x2F; adaptersThe modules under adapters/ are modules that handle dispatching a request and settling a returned Promise once a response is received. Example1234567891011121314151617181920212223242526272829var settle = require(&#x27;./../core/settle&#x27;);module.exports = function myAdapter(config) &#123; // At this point: // - config has been merged with defaults // - request transformers have already run // - request interceptors have already run // Make the request using config provided // Upon response settle the Promise return new Promise(function(resolve, reject) &#123; var response = &#123; data: responseData, status: request.status, statusText: request.statusText, headers: responseHeaders, config: config, request: request &#125;; settle(resolve, reject, response); // From here: // - response transformers will run // - response interceptors will run &#125;);&#125;","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/axios/lib/core/README","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/axios/lib/core/README/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/axios/lib/core/README/","excerpt":"","text":"axios &#x2F;&#x2F; coreThe modules found in core/ should be modules that are specific to the domain logic of axios. These modules would most likely not make sense to be consumed outside of the axios module, as their logic is too specific. Some examples of core modules are: Dispatching requests Requests sent via adapters/ (see lib&#x2F;adapters&#x2F;README.md) Managing interceptors Handling config","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/axios/lib/env/README","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/axios/lib/env/README/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/axios/lib/env/README/","excerpt":"","text":"axios &#x2F;&#x2F; envThe data.js file is updated automatically when the package version is upgrading. Please do not edit it manually.","categories":[],"tags":[]},{"title":"","slug":"王阁/test/node_modules/axios/lib/helpers/README","date":"2024-01-05T14:24:23.000Z","updated":"2024-01-05T14:24:23.000Z","comments":true,"path":"2024/01/05/王阁/test/node_modules/axios/lib/helpers/README/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/test/node_modules/axios/lib/helpers/README/","excerpt":"","text":"axios &#x2F;&#x2F; helpersThe modules found in helpers/ should be generic modules that are not specific to the domain logic of axios. These modules could theoretically be published to npm on their own and consumed by other modules or apps. Some examples of generic modules are things like: Browser polyfills Managing cookies Parsing HTTP headers","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/女装长裤","date":"2024-01-05T01:19:06.186Z","updated":"2024-01-05T01:37:34.860Z","comments":true,"path":"2024/01/05/王阁/运营/未分类/女装长裤/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/05/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/%E5%A5%B3%E8%A3%85%E9%95%BF%E8%A3%A4/","excerpt":"","text":"女装长裤一周成交件数12件，总成交31件(两周多 ) 近期计划","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/听课笔记","date":"2024-01-03T14:32:21.000Z","updated":"2024-01-03T15:27:46.000Z","comments":true,"path":"2024/01/03/王阁/运营/未分类/听课笔记/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/03/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/","excerpt":"","text":"20看点击， 30 转化 50 止损，二是为账号流量考虑。亏损的标准是一半。 80 追一单 90 追一单 平1、平2、翻2 请教下问题，看得断断续续的，一个品前期要准备多个素材，这个品的测试多个素材，是挨个挂车测(前一个素材测试完了，下架，再添加到新视频上)。还找这个商品的不同推广链接来给素材，同时挂车测。当时看的时候就想问，不想打断。","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/私教课相关","date":"2024-01-02T08:04:53.369Z","updated":"2024-01-02T14:18:08.410Z","comments":true,"path":"2024/01/02/王阁/运营/未分类/私教课相关/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2024/01/02/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/%E7%A7%81%E6%95%99%E8%AF%BE%E7%9B%B8%E5%85%B3/","excerpt":"","text":"QList 如何测品","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/李海莉检讨书","date":"2023-12-24T13:43:00.000Z","updated":"2023-12-24T14:25:58.000Z","comments":true,"path":"2023/12/24/王阁/运营/未分类/李海莉检讨书/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/12/24/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/%E6%9D%8E%E6%B5%B7%E8%8E%89%E6%A3%80%E8%AE%A8%E4%B9%A6/","excerpt":"","text":"## 关于让学生回家反省和家校沟通技巧的检讨反思 各位老师大家好，今天我站在这里，针对我们班上周发生的两件事情进行检讨反思。 两件事情分别如下。 事件一 12023年12月20日，吴雨轩与李奕江同学在教室发生打架事件。我在了解事件后，选择让两位学生独自回家冷静反省，引发家长质疑。 事件过程 12023年12月20日 周三 我们班级两位同学在教室发生打架事件。我接到学生反映，了解了相关情况后，立即从办公室前往教室处理此事情。到现场后，我找两位同学尝试了解事件经过，两位同学有意逃避问题，抵触交流，闭口不谈。考虑两位同学经常生了打架事件，且在昨天下午刚处理了两位同学发生的一起打架事件。我跟双方家长进行了电话沟通，征得家长同意后，拟订了让两位同学暂时回家冷静反省，反省后直接返校的处理措施。我当时电话中询问了家长是否安排家人接孩子回家，双方家长反映孩子一直是独自返家，让学生自行回家就行，我跟家长强调了自行回家存在安全风险，让家长在学生到家后的第一时间，给我打电话或发消息的方式直接反馈学生到家情况。后续学生家长都反馈了学生安全到家的情况。在下午的时候,其中的一位学生家长，跟我进行了电话沟通，起初情绪比较激动，对我提出两点质疑，一是学生独自回家，可能存安全问题。二是认为我区别对待两名同学，认为另一位同学是成绩优秀，所以更早返校。我跟该学生家长逐一进行了沟通，回应了的她的疑问，并商讨了对娃娃后续的教育措施，达成一致意见。 事件二 1要求学生提醒家长处理群内通知，但部分学生未按要求积极传达信息，为了了解相关情况，我请他们向我说明情况，其中有几名同学以书面检讨的形式向我提交了说明材料，上面事件的当事人之一因为跟家长传有误，致使家长误解为因她们不回群消息而处罚他们娃娃写检讨。 事件反思12针对这两起事件处理，我充分反思后，发现我确实有做的不到位的地方。 学生独自回家存在安全风险：即使是在与家长沟通后，也不应该让上课期间让学生独自回家，何况当时两名学生还带有一定的负面情绪，娃娃的安全问题大于天，尽管我在电话沟通中提醒了家长存在安全风险，但在学生离校后，吴雨轩的妈妈仍表达了对此的担忧。我深感自己在学生安全问题上的考虑不够周全，以后在类似情况中应当更加谨慎，避免将学生置于潜在的风险之中。 校内问题应在校内解决：将学生遣返回家处理，给学生家长增加了不必要的负担。在这一点上，我应该更加明智地选择请两位学生的家长到校当面解决问题，通过学校内部的教育和引导，家长的配合督促确保学生能够更好地反思和改正错误。 注意家校沟通的技巧 这次事件是我工作中的一次挑战，通过深刻反思，我将认真吸取教训，不断学习，提升自己的业务水平和处理问题的能力，以更好地履行教育工作者的职责，不再犯类似的错误，也请其他老师引以为戒。","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/莉儿3","date":"2023-12-20T14:52:07.000Z","updated":"2023-12-24T13:19:25.000Z","comments":true,"path":"2023/12/20/王阁/运营/未分类/莉儿3/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/12/20/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/%E8%8E%89%E5%84%BF3/","excerpt":"","text":"关于因为家长不回群消息让娃娃写检讨书的情况说明事情情况说明2023年12月18号，因为班级日常工作，我在群内需要了解学生家长《12月伙食费》交费情况和《致家长及监护人的一封信》的阅读情况。我在班群里内发了通知，但鉴于家长平常工作繁忙，可能无法及时查看与处理信息，所以我在班上多次，反复的跟娃娃们说，让她们返家后多提醒家长处理群内通知事宜。但12月19号返校后，还有部份娃娃的家长尚未接龙。于是，中午我在班上跟这部份娃娃说让他们打电话提醒家长处理一下这些事宜，但我后续跟进时，了解到部份同学还是并未跟家长反应这件事，为了了解情况，我是让这部娃娃跟我说明一下为什么没跟家长反馈信息。我并非因为家长未回群消息而对学生采取处罚措施。我的关注点是学生在我多次提醒的情况下，未能按照要求积极地向家长传达相关通知。我的初衷是通过学生的解释了解情况，以便更好地改进通知的传达方式，提高信息处理的及时性。如果在整个过程中有造成任何误解，我深感抱歉。我将认真吸取教训，未来会更加注重与学生的有效沟通，确保信息更好地传达到家长。","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/莉儿的文档2","date":"2023-12-20T14:31:25.000Z","updated":"2023-12-20T14:31:34.000Z","comments":true,"path":"2023/12/20/王阁/运营/未分类/莉儿的文档2/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/12/20/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/%E8%8E%89%E5%84%BF%E7%9A%84%E6%96%87%E6%A1%A32/","excerpt":"","text":"事情起因2023年12月20日 周三 八点十分左右时。吴雨轩与李奕江两位同学在教室发生打架事件。八点十五分左右，我接到学生反映中，了解了相关情况后，立即从办公室前往教室处理此事情。到现场后，我找两位同学尝试了解事件经过，两位同学有意逃避问题，抵触交流，闭口不谈。考虑两位同学经常生了打架事件，且在昨天下午刚处理了两位同学发生的一起打架事件。我跟双方家长进行了电话沟通，与家长（李奕江的妈妈、吴雨轩的爸爸）协商，征得家长同意后，拟订了让两位同学暂时回家冷静反省，反省后直接返校的处理措施。我当时电话中询问了家长是否安排家人接孩子回家，双方家长反映孩子一直是独自返家，让学生自行回家就行，我跟家长强调了自行回家存在安全风险，让家长在学生到家后的第一时间，给我打电话或发消息的方式直接反馈学生到家情况。在八点四十左右，两位同学离校。吴雨轩家长九点二十四分反馈学生到家。李奕江同学到家后，家长与学生进行沟通教育后，家长第一时间跟我取得联系，并拟订中午家长到校当面再处理该同学的事情，并于九点二十七分将学生送返学校，并在中午与我已经进行了当面沟通与解决。吴雨轩妈妈下午14点16分跟我进行了电话沟通，家长开始情绪比较激动，对我提出两点质疑，一是学生独自回家，可能存安全问题。二是认为我区别对待两名同学，认为另一位同学是成绩优秀，所以更早返校，而吴雨轩却不能返校。我跟吴雨轩妈妈逐一回应了她的疑问，并商讨了对娃娃后续的教育措施，双方达成一致意见。在下午16点26分家长在qq上主动反馈了对孩子的教育结果，并与我达成了一起督促教育娃娃进步的共识。 事件反思针对此事件处理，我充分反思后，发现我确实有做的不到位的地方。一、即使是在与家长沟通后，也不应该让上课期间让学生独自回家，娃娃的安全问题大于天。二、学校发生的事情，应该在校内解决，而不是把学生遣返回家，给学生家长增加负担 。作为一名老师，在学生发生问题题后。 这次打架事件的处理让我深感责任重大，对于问题的发生，我进行了全面的反思，发现了自己在处理过程中存在的不足之处。以下是我对事件的反思： 1. 未及时掌握学生状况： 在接到学生反映后，我迅速前往现场，但在了解事件经过时，发现两位同学有意逃避问题，抵触交流。我应该更加灵活运用沟通技巧，努力获得更准确的事件经过，以更好地制定后续处理方案。 2. 学生独自回家存在安全风险： 尽管我在电话沟通中提醒了家长存在安全风险，但在学生离校后，吴雨轩的妈妈仍表达了对此的担忧。我深感自己在学生安全问题上的考虑不够周全，以后在类似情况中应当更加谨慎，避免将学生置于潜在的风险之中。 3. 校内问题应在校内解决： 将学生遣返回家处理，给学生家长增加了不必要的负担。在这一点上，我应该更加明智地选择在校内解决问题，通过学校内部的教育和引导，确保学生能够在校园环境中更好地反思和改正。 4. 区别对待的误解： 吴雨轩的家长认为我对待两位同学存在区别，我在电话沟通中应该更加清晰地向家长解释，确保家长能够理解我处理问题的公正性和公平性。未来在类似情况中，我将更注重与家长的沟通，防范类似的误解。 5. 学生问题的及时跟进： 学生问题的解决不仅仅是一时的处理，还需要进行后续的教育和辅导。我在这方面可以更主动地与学生、家长进行联系，确保问题得到及时的跟进和解决，防止问题再次发生。 这次事件是我工作中的一次挑战，通过深刻反思，我将认真吸取教训，不断提升自己的业务水平和处理问题的能力，以更好地履行教育工作者的职责。感谢学校领导和家长的理解与支持，我将以更加负责的态度，更加努力地投入到学生教育工作中。","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/莉儿的文档","date":"2023-12-20T12:45:44.000Z","updated":"2023-12-20T14:52:29.000Z","comments":true,"path":"2023/12/20/王阁/运营/未分类/莉儿的文档/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/12/20/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/%E8%8E%89%E5%84%BF%E7%9A%84%E6%96%87%E6%A1%A3/","excerpt":"","text":"关于让吴雨轩与李奕江同学回家反省的情况说明事情起因2023年12月20日 周三 八点十分左右时。吴雨轩与李奕江两位同学在教室发生打架事件。八点十五分左右，我接到学生反映中，了解了相关情况后，立即从办公室前往教室处理此事情。到现场后，我找两位同学尝试了解事件经过，两位同学有意逃避问题，抵触交流，闭口不谈。考虑两位同学经常生了打架事件，且在昨天下午刚处理了两位同学发生的一起打架事件。我跟双方家长进行了电话沟通，与家长（李奕江的妈妈、吴雨轩的爸爸）协商，征得家长同意后，拟订了让两位同学暂时回家冷静反省，反省后直接返校的处理措施。我当时电话中询问了家长是否安排家人接孩子回家，双方家长反映孩子一直是独自返家，让学生自行回家就行，我跟家长强调了自行回家存在安全风险，让家长在学生到家后的第一时间，给我打电话或发消息的方式直接反馈学生到家情况。在八点四十左右，两位同学离校。吴雨轩家长九点二十四分反馈学生到家。李奕江同学到家后，家长与学生进行沟通教育后，家长第一时间跟我取得联系，并拟订中午家长到校当面再处理该同学的事情，并于九点二十七分将学生送返学校，并在中午与我已经进行了当面沟通与解决。吴雨轩妈妈下午14点16分跟我进行了电话沟通，家长开始情绪比较激动，对我提出两点质疑，一是学生独自回家，可能存安全问题。二是认为我区别对待两名同学，认为另一位同学是成绩优秀，所以更早返校，而吴雨轩却不能返校。我跟吴雨轩妈妈逐一回应了她的疑问，并商讨了对娃娃后续的教育措施，双方达成一致意见。在下午16点26分家长在qq上主动反馈了对孩子的教育结果，并与我达成了一起督促教育娃娃进步的共识。 事件反思针对此事件处理，我充分反思后，发现我确实有做的不到位的地方。一、即使是在与家长沟通后，也不应该让上课期间让学生独自回家，娃娃的安全问题大于天。二、学校发生的事情，应该在校内解决，而不是把学生遣返回家，给学生家长增加负担 。作为一名老师，在学生发生问题题后。 这次打架事件的处理让我深感责任重大，对于问题的发生，我进行了全面的反思，发现了自己在处理过程中存在的不足之处。以下是我对事件的反思： 1. 未及时掌握学生状况： 在接到学生反映后，我迅速前往现场，但在了解事件经过时，发现两位同学有意逃避问题，抵触交流。我应该更加灵活运用沟通技巧，努力获得更准确的事件经过，以更好地制定后续处理方案。 2. 学生独自回家存在安全风险： 尽管我在电话沟通中提醒了家长存在安全风险，但在学生离校后，吴雨轩的妈妈仍表达了对此的担忧。我深感自己在学生安全问题上的考虑不够周全，以后在类似情况中应当更加谨慎，避免将学生置于潜在的风险之中。 3. 校内问题应在校内解决： 将学生遣返回家处理，给学生家长增加了不必要的负担。在这一点上，我应该更加明智地选择在校内解决问题，通过学校内部的教育和引导，确保学生能够在校园环境中更好地反思和改正。 4. 区别对待的误解： 吴雨轩的家长认为我对待两位同学存在区别，我在电话沟通中应该更加清晰地向家长解释，确保家长能够理解我处理问题的公正性和公平性。未来在类似情况中，我将更注重与家长的沟通，防范类似的误解。 5. 学生问题的及时跟进： 学生问题的解决不仅仅是一时的处理，还需要进行后续的教育和辅导。我在这方面可以更主动地与学生、家长进行联系，确保问题得到及时的跟进和解决，防止问题再次发生。 这次事件是我工作中的一次挑战，通过深刻反思，我将认真吸取教训，不断提升自己的业务水平和处理问题的能力，以更好地履行教育工作者的职责。感谢学校领导和家长的理解与支持，我将以更加负责的态度，更加努力地投入到学生教育工作中。","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/运营工具","date":"2023-12-18T12:11:52.000Z","updated":"2023-12-18T12:14:10.000Z","comments":true,"path":"2023/12/18/王阁/运营/未分类/运营工具/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/12/18/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/%E8%BF%90%E8%90%A5%E5%B7%A5%E5%85%B7/","excerpt":"","text":"选品随心推计划","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/Untitled","date":"2023-12-05T02:54:15.120Z","updated":"2023-12-05T02:54:15.120Z","comments":true,"path":"2023/12/05/王阁/运营/未分类/Untitled/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/12/05/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/Untitled/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/运营工具/看板","date":"2023-12-01T02:52:35.119Z","updated":"2023-12-01T02:52:37.473Z","comments":true,"path":"2023/12/01/王阁/运营/运营工具/看板/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/12/01/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E8%BF%90%E8%90%A5%E5%B7%A5%E5%85%B7/%E7%9C%8B%E6%9D%BF/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/模型分析/秋衣模型后投流计划","date":"2023-11-30T06:45:52.727Z","updated":"2023-11-30T07:04:16.290Z","comments":true,"path":"2023/11/30/王阁/运营/模型分析/秋衣模型后投流计划/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/11/30/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90/%E7%A7%8B%E8%A1%A3%E6%A8%A1%E5%9E%8B%E5%90%8E%E6%8A%95%E6%B5%81%E8%AE%A1%E5%88%92/","excerpt":"","text":"14:46分投了一笔。 15:04","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/Untitled 1","date":"2023-11-30T05:46:22.740Z","updated":"2023-11-30T05:46:22.740Z","comments":true,"path":"2023/11/30/王阁/运营/Untitled 1/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/11/30/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/Untitled%201/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/hyl_现场数据环境","date":"2023-11-29T02:50:36.743Z","updated":"2023-11-29T02:54:06.960Z","comments":true,"path":"2023/11/29/王阁/运营/未分类/hyl_现场数据环境/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/11/29/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/hyl_%E7%8E%B0%E5%9C%BA%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83/","excerpt":"","text":"clickhouse:地址: 192.168.10.208:8123 账号： yanggu 密码: yg123456 elasticsearch:192.168.10.101:9200 账号: elastic 密码: 3FuMQXOdPTdFDHrKrVeJ kibana192.168.10.101:5601 账号: elastic 密码: 3FuMQXOdPTdFDHrKrVeJ","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/素材提取功能单","date":"2023-11-28T08:54:14.924Z","updated":"2023-11-28T09:27:14.292Z","comments":true,"path":"2023/11/28/王阁/运营/未分类/素材提取功能单/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/11/28/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/%E7%B4%A0%E6%9D%90%E6%8F%90%E5%8F%96%E5%8A%9F%E8%83%BD%E5%8D%95/","excerpt":"","text":"接下来我要实现一个素材提取单的功能，这个功能本质是一个任务提交表单的页面 需要提交的字段信息有 关键词 cookie串 点赞量","categories":[],"tags":[]},{"title":"","slug":"王阁/日常/配置X-Pack","date":"2023-11-27T03:42:00.795Z","updated":"2023-11-27T03:42:00.795Z","comments":true,"path":"2023/11/27/王阁/日常/配置X-Pack/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/11/27/%E7%8E%8B%E9%98%81/%E6%97%A5%E5%B8%B8/%E9%85%8D%E7%BD%AEX-Pack/","excerpt":"","text":"配置X-Packhttp.cors.enabled: truehttp.cors.allow-origin: “*”http.cors.allow-headers: Authorizationxpack.security.enabled: truexpack.security.transport.ssl.enabled: true 1234cluster.initial_master_nodes: [&quot;es01&quot;,&quot;es02&quot;,&quot;es03&quot;]http.cors.allow-headers: Authorizationxpack.security.enabled: truexpack.security.transport.ssl.enabled: true 123xpack.security.transport.ssl.verification_mode: certificatexpack.security.transport.ssl.keystore.path: /opt/cloudera/parcels/ELASTICSEARCH-0.0.5.elasticsearch.p0.5/config/elastic-certificates.p12xpack.security.transport.ssl.truststore.path: /opt/cloudera/parcels/ELASTICSEARCH-0.0.5.elasticsearch.p0.5/config/elastic-certificates.p12","categories":[],"tags":[]},{"title":"","slug":"王阁/日常/es密码添加","date":"2023-11-27T01:29:58.259Z","updated":"2023-11-27T01:30:13.152Z","comments":true,"path":"2023/11/27/王阁/日常/es密码添加/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/11/27/%E7%8E%8B%E9%98%81/%E6%97%A5%E5%B8%B8/es%E5%AF%86%E7%A0%81%E6%B7%BB%E5%8A%A0/","excerpt":"","text":"1","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/Untitled","date":"2023-11-27T01:29:32.368Z","updated":"2023-11-27T08:09:23.952Z","comments":true,"path":"2023/11/27/王阁/运营/Untitled/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/11/27/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/Untitled/","excerpt":"","text":"12345678910111213141516171819curl &#x27;https://buyin.jinritemai.com/pc/decision/promotion/data?promotion_id=3485871718663345311&amp;verifyFp=verify_lpe109kc_lFhvbwkh_IivT_4Zsr_9QsM_odDIEzN6iQMO&amp;fp=verify_lpe109kc_lFhvbwkh_IivT_4Zsr_9QsM_odDIEzN6iQMO&amp;msToken=PyqwSfRmHbs8-rIdM-uUeadIJxcGMr1wukQIEtHHkAcHwJ750v7W3PBsCtQr4nEjWOTm_Q8Ie9P81-EF5leEL4xn_Ov9tHYJPVSjKyuKS0OFvrT1VvjM-Q%3D%3D&amp;a_bogus=xXUDkc21Msm1ZjVYywkz9CUmzVE0YW4NgZEPrilPdtqM&#x27; \\-H &#x27;Authority: buyin.jinritemai.com&#x27; \\-H &#x27;Method: GET&#x27; \\-H &#x27;Path: /pc/decision/promotion/data?promotion_id=3485871718663345311&amp;verifyFp=verify_lpe109kc_lFhvbwkh_IivT_4Zsr_9QsM_odDIEzN6iQMO&amp;fp=verify_lpe109kc_lFhvbwkh_IivT_4Zsr_9QsM_odDIEzN6iQMO&amp;msToken=PyqwSfRmHbs8-rIdM-uUeadIJxcGMr1wukQIEtHHkAcHwJ750v7W3PBsCtQr4nEjWOTm_Q8Ie9P81-EF5leEL4xn_Ov9tHYJPVSjKyuKS0OFvrT1VvjM-Q%3D%3D&amp;a_bogus=xXUDkc21Msm1ZjVYywkz9CUmzVE0YW4NgZEPrilPdtqM&#x27; \\-H &#x27;Scheme: https&#x27; \\-H &#x27;Accept: application/json, text/plain, */*&#x27; \\-H &#x27;Accept-Encoding: gzip, deflate, br&#x27; \\-H &#x27;Accept-Language: zh-CN,zh;q=0.9,en;q=0.8&#x27; \\-H &#x27;Cookie: business-account-center-csrf-secret=K2YNUrdLdE0_mp5g4X9EUYrn; business-account-center-csrf-token=CYNbYeKc-SJZjREnxj1hvitDnC3_jqSm1ShM; passport_csrf_token=b16181c2dffe0db6a3645310ec9198e7; passport_csrf_token_default=b16181c2dffe0db6a3645310ec9198e7; store-region=cn-sc; store-region-src=uid; manage_relation_type=1; csrf_session_id=d00809ede6e54c1754e8c48c0062fea5; qc_tt_tag=0; d_ticket=21851729bc8ac9d26564710de966355d6d51e; sso_auth_status=9430cb104dcf360299ca3f7d06f0c4ff; sso_auth_status_ss=9430cb104dcf360299ca3f7d06f0c4ff; buyin_account_child_type=1128; n_mh=pW7vJKKUKQODX2j0VuPn11q4MVl5hx2ROic9AbTNf8o; sso_uid_tt=dd77124fb8316bb4b1a5c1ef76ef19da; sso_uid_tt_ss=dd77124fb8316bb4b1a5c1ef76ef19da; toutiao_sso_user=c13ec8e6ff5947f69ee232f75fac4211; toutiao_sso_user_ss=c13ec8e6ff5947f69ee232f75fac4211; uid_tt=59d420f5c963531503ed49bb038cb449; uid_tt_ss=59d420f5c963531503ed49bb038cb449; sid_tt=6112669afb1c1fc7e7d6f3bfe005433e; sessionid=6112669afb1c1fc7e7d6f3bfe005433e; sessionid_ss=6112669afb1c1fc7e7d6f3bfe005433e; buyin_account_child_type_v2=1128; buyin_app_id_v2=1128; tt_scid=5Qty1cVt50JeXjL.rH7pc5l2YFRYmpYFddt75Zh9MJfDBcoe-fniO6UXsBCROpFb5312; passport_auth_status=fa1ccf2e80bba47c03f30b4f79950b5f%2Ca7cf06421781701b6df2c733fec2829b; passport_auth_status_ss=fa1ccf2e80bba47c03f30b4f79950b5f%2Ca7cf06421781701b6df2c733fec2829b; sid_ucp_sso_v1=1.0.0-KGExOTEzMDdjNGQxNjUxZDI1OWQ1NTg1ZWUzMzkwZDRiMTc3NDFhNWYKHgism5Chz8ziBhD_vvWqBhimDCAMMLmEvaoGOAhAJhoCbHEiIGMxM2VjOGU2ZmY1OTQ3ZjY5ZWUyMzJmNzVmYWM0MjEx; ssid_ucp_sso_v1=1.0.0-KGExOTEzMDdjNGQxNjUxZDI1OWQ1NTg1ZWUzMzkwZDRiMTc3NDFhNWYKHgism5Chz8ziBhD_vvWqBhimDCAMMLmEvaoGOAhAJhoCbHEiIGMxM2VjOGU2ZmY1OTQ3ZjY5ZWUyMzJmNzVmYWM0MjEx; buyin_shop_type=24; buyin_app_id=1128; ucas_c0_buyin=CkEKBTEuMC4wEICIh4za79euZRi9LyDw4fCgz8z-BCiPETCsm5Chz8ziBkCBv_WqBkiB87GtBlCTvKCIjfK4q2VYfhIUqGXFCc6Cq97ktiicJsHEt9JNLFo; ucas_c0_ss_buyin=CkEKBTEuMC4wEICIh4za79euZRi9LyDw4fCgz8z-BCiPETCsm5Chz8ziBkCBv_WqBkiB87GtBlCTvKCIjfK4q2VYfhIUqGXFCc6Cq97ktiicJsHEt9JNLFo; sid_guard=6112669afb1c1fc7e7d6f3bfe005433e%7C1700618113%7C5184000%7CSun%2C+21-Jan-2024+01%3A55%3A13+GMT; sid_ucp_v1=1.0.0-KGU1ZDYyMTI1YWE3NjliZjhmM2NiMzQ5NjlhMTk1OWE1MTUzM2E3MzkKGAism5Chz8ziBhCBv_WqBhiPESAMOAhAJhoCbHEiIDYxMTI2NjlhZmIxYzFmYzdlN2Q2ZjNiZmUwMDU0MzNl; ssid_ucp_v1=1.0.0-KGU1ZDYyMTI1YWE3NjliZjhmM2NiMzQ5NjlhMTk1OWE1MTUzM2E3MzkKGAism5Chz8ziBhCBv_WqBhiPESAMOAhAJhoCbHEiIDYxMTI2NjlhZmIxYzFmYzdlN2Q2ZjNiZmUwMDU0MzNl; buyin_shop_type_v2=24; odin_tt=fb67d44d36aaba29cf604cffc01123d8d62f7af42092a2612e68d67f6336852566bd3b9cbc4b8d070bb571468c0d92e3; gf_part_1057642=94; gf_part_1057645=36; gf_part_1057644=62; _tea_utm_cache_3813=undefined; x-jupiter-uuid=17008108619949644; gf_part_1060750=61; gf_part_1060747=22; gf_part_1060748=56; _tea_utm_cache_2631=undefined; uidpasaddaehruigqreajf=0; scmVer=1.0.1.5366; MONITOR_WEB_ID=f53e7163-d1f5-4654-97cb-df13d2729510; s_v_web_id=verify_lpe109kc_lFhvbwkh_IivT_4Zsr_9QsM_odDIEzN6iQMO; ttwid=1%7Ca-lHFWe4DBl8bzwfRuguZjfPZK8VtJuVz42gDUkUGOo%7C1701070278%7C83bc455756d36f7fa0e4d914a2ee0460e9043aa9f983d9c972f551ebd4b6c1da; SASID=SID2_7306040622354350362; BUYIN_SASID=SID2_7306040622354350362; gf_part_1063753=60; gf_part_1063754=80; msToken=qP90voxdpuB5TH6DplmRWQV7mKSmU1hPaz2DKcoKoihn9Lxjrwj_0BRwUge4CgQdcLX4FQwghRpDxTP2miaFYigDABVq_9TC-9VX4vqFtfJWqAghc-hrMw==&#x27; \\-H &#x27;Dnt: 1&#x27; \\-H &#x27;Referer: https://buyin.jinritemai.com/dashboard/merch-picking-hall/merch_promoting?id=3485871718663345311&amp;enter_from=%7B%22pick_first_source%22%3A%22%E7%99%BE%E5%BA%94%22%2C%22pick_second_source%22%3A%22select_product_topic%22%2C%22third_source_from%22%3A%22%22%2C%22pick_third_source%22%3A%22center_pay_author_cnt_toplist%22%2C%22pick_source_id%22%3A%22center_pay_author_cnt_toplist_0%22%2C%22page_name%22%3A%22center_pay_author_cnt_toplist%22%2C%22top_list_type%22%3A%22center_pay_author_cnt_toplist%22%2C%22top_list_id%22%3A0%2C%22product_tab_name%22%3A%22%E5%85%A8%E9%83%A8%E5%AE%9E%E6%97%B6%E7%83%AD%E6%8E%A8%E6%A6%9C%22%2C%22product_label%22%3A%22%7B%5C%22in_toplist%5C%22%3A%5C%221%5C%22%2C%5C%22top_list_id%5C%22%3A0%2C%5C%22top_list_type%5C%22%3A%5C%22center_pay_author_cnt_toplist%5C%22%2C%5C%22top_list_name%5C%22%3A%5C%22%E5%85%A8%E9%83%A8%E5%AE%9E%E6%97%B6%E7%83%AD%E6%8E%A8%E6%A6%9C%5C%22%2C%5C%22pick_source_id%5C%22%3A%5C%22center_pay_author_cnt_toplist%5C%22%2C%5C%22button%5C%22%3A%5C%22%7B%7D%5C%22%7D%22%2C%22enter_from%22%3A%22%E9%80%89%E5%93%81%E5%B9%BF%E5%9C%BA%E9%A6%96%E9%A1%B5%22%2C%22source%22%3A%22%E9%80%89%E5%93%81%E5%B9%BF%E5%9C%BA%E9%A6%96%E9%A1%B5%22%2C%22log_pb%22%3A%22202311271531272691D5337DF332FDD7F4%22%2C%22product_id%22%3A%223485828470490151013%22%2C%22commodity_id%22%3A%223485871718663345311%22%2C%22previous_page_name%22%3A%22center_pay_author_cnt_toplist%22%2C%22is_has_same_item%22%3A0%7D&amp;rank_log_params=%7B%22pick_first_source%22%3A%22%E7%99%BE%E5%BA%94%22%2C%22pick_second_source%22%3A%22select_product_topic%22%2C%22third_source_from%22%3A%22%22%2C%22pick_third_source%22%3A%22center_pay_author_cnt_toplist%22%2C%22pick_source_id%22%3A%22center_pay_author_cnt_toplist_0%22%2C%22page_name%22%3A%22center_pay_author_cnt_toplist%22%2C%22top_list_type%22%3A%22center_pay_author_cnt_toplist%22%2C%22top_list_id%22%3A0%2C%22product_tab_name%22%3A%22%E5%85%A8%E9%83%A8%E5%AE%9E%E6%97%B6%E7%83%AD%E6%8E%A8%E6%A6%9C%22%2C%22product_label%22%3A%22%7B%5C%22in_toplist%5C%22%3A%5C%221%5C%22%2C%5C%22top_list_id%5C%22%3A0%2C%5C%22top_list_type%5C%22%3A%5C%22center_pay_author_cnt_toplist%5C%22%2C%5C%22top_list_name%5C%22%3A%5C%22%E5%85%A8%E9%83%A8%E5%AE%9E%E6%97%B6%E7%83%AD%E6%8E%A8%E6%A6%9C%5C%22%2C%5C%22pick_source_id%5C%22%3A%5C%22center_pay_author_cnt_toplist%5C%22%2C%5C%22button%5C%22%3A%5C%22%7B%7D%5C%22%7D%22%2C%22enter_from%22%3A%22%E9%80%89%E5%93%81%E5%B9%BF%E5%9C%BA%E9%A6%96%E9%A1%B5%22%2C%22source%22%3A%22%E9%80%89%E5%93%81%E5%B9%BF%E5%9C%BA%E9%A6%96%E9%A1%B5%22%2C%22log_pb%22%3A%22202311271531272691D5337DF332FDD7F4%22%2C%22product_id%22%3A%223485828470490151013%22%2C%22commodity_id%22%3A%223485871718663345311%22%7D&amp;btm_ppre=a10091.b30986.c72928.d0&amp;btm_pre=a10091.b977842.c0.d0&amp;btm_show_id=dcc00003-0861-4b2e-989d-efbb760085e1&#x27; \\-H &#x27;Sec-Ch-Ua: &quot;Google Chrome&quot;;v=&quot;119&quot;, &quot;Chromium&quot;;v=&quot;119&quot;, &quot;Not?A_Brand&quot;;v=&quot;24&quot;&#x27; \\-H &#x27;Sec-Ch-Ua-Mobile: ?0&#x27; \\-H &#x27;Sec-Ch-Ua-Platform: &quot;macOS&quot;&#x27; \\-H &#x27;Sec-Fetch-Dest: empty&#x27; \\-H &#x27;Sec-Fetch-Mode: cors&#x27; \\-H &#x27;Sec-Fetch-Site: same-origin&#x27; \\-H &#x27;User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36&#x27; 12https://buyin.jinritemai.com/dashboard/merch-picking-hall/merch_promoting?id=3649684545135094775&amp;enter_from=%7B%22pick_first_source%22%3A%22%E7%99%BE%E5%BA%94%22%2C%22pick_second_source%22%3A%22select_product_topic%22%2C%22third_source_from%22%3A%22%22%2C%22pick_third_source%22%3A%22center_pay_prod_qty_cnt_toplist%22%2C%22pick_source_id%22%3A%22center_pay_prod_qty_cnt_toplist_0%22%2C%22page_name%22%3A%22center_pay_prod_qty_cnt_toplist%22%2C%22top_list_type%22%3A%22center_pay_prod_qty_cnt_toplist%22%2C%22top_list_id%22%3A0%2C%22product_tab_name%22%3A%22%E5%85%A8%E9%83%A8%E5%AE%9E%E6%97%B6%E7%88%86%E6%AC%BE%E6%A6%9C%22%2C%22product_label%22%3A%22%7B%5C%22in_toplist%5C%22%3A%5C%221%5C%22%2C%5C%22top_list_id%5C%22%3A0%2C%5C%22top_list_type%5C%22%3A%5C%22center_pay_prod_qty_cnt_toplist%5C%22%2C%5C%22top_list_name%5C%22%3A%5C%22%E5%85%A8%E9%83%A8%E5%AE%9E%E6%97%B6%E7%88%86%E6%AC%BE%E6%A6%9C%5C%22%2C%5C%22pick_source_id%5C%22%3A%5C%22center_pay_prod_qty_cnt_toplist%5C%22%2C%5C%22button%5C%22%3A%5C%22%7B%7D%5C%22%7D%22%2C%22enter_from%22%3A%22%E9%80%89%E5%93%81%E5%B9%BF%E5%9C%BA%E9%A6%96%E9%A1%B5%22%2C%22source%22%3A%22%E9%80%89%E5%93%81%E5%B9%BF%E5%9C%BA%E9%A6%96%E9%A1%B5%22%2C%22log_pb%22%3A%2220231127160807A548B2CDF96F0030B831%22%2C%22product_id%22%3A%223649684542970802008%22%2C%22commodity_id%22%3A%223649684545135094775%22%2C%22previous_page_name%22%3A%22center_pay_prod_qty_cnt_toplist%22%2C%22is_has_same_item%22%3A0%7D&amp;rank_log_params=%7B%22pick_first_source%22%3A%22%E7%99%BE%E5%BA%94%22%2C%22pick_second_source%22%3A%22select_product_topic%22%2C%22third_source_from%22%3A%22%22%2C%22pick_third_source%22%3A%22center_pay_prod_qty_cnt_toplist%22%2C%22pick_source_id%22%3A%22center_pay_prod_qty_cnt_toplist_0%22%2C%22page_name%22%3A%22center_pay_prod_qty_cnt_toplist%22%2C%22top_list_type%22%3A%22center_pay_prod_qty_cnt_toplist%22%2C%22top_list_id%22%3A0%2C%22product_tab_name%22%3A%22%E5%85%A8%E9%83%A8%E5%AE%9E%E6%97%B6%E7%88%86%E6%AC%BE%E6%A6%9C%22%2C%22product_label%22%3A%22%7B%5C%22in_toplist%5C%22%3A%5C%221%5C%22%2C%5C%22top_list_id%5C%22%3A0%2C%5C%22top_list_type%5C%22%3A%5C%22center_pay_prod_qty_cnt_toplist%5C%22%2C%5C%22top_list_name%5C%22%3A%5C%22%E5%85%A8%E9%83%A8%E5%AE%9E%E6%97%B6%E7%88%86%E6%AC%BE%E6%A6%9C%5C%22%2C%5C%22pick_source_id%5C%22%3A%5C%22center_pay_prod_qty_cnt_toplist%5C%22%2C%5C%22button%5C%22%3A%5C%22%7B%7D%5C%22%7D%22%2C%22enter_from%22%3A%22%E9%80%89%E5%93%81%E5%B9%BF%E5%9C%BA%E9%A6%96%E9%A1%B5%22%2C%22source%22%3A%22%E9%80%89%E5%93%81%E5%B9%BF%E5%9C%BA%E9%A6%96%E9%A1%B5%22%2C%22log_pb%22%3A%2220231127160807A548B2CDF96F0030B831%22%2C%22product_id%22%3A%223649684542970802008%22%2C%22commodity_id%22%3A%223649684545135094775%22%7D&amp;btm_ppre=a10091.b30986.c72928.d0&amp;btm_pre=a10091.b977842.c0.d0&amp;btm_show_id=e9fffa20-07da-4d49-a16d-36ed94f07aef","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/选品功能设计","date":"2023-11-23T07:00:29.131Z","updated":"2023-11-24T10:01:17.577Z","comments":true,"path":"2023/11/23/王阁/运营/未分类/选品功能设计/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/11/23/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/%E9%80%89%E5%93%81%E5%8A%9F%E8%83%BD%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"想实现在一个选品的操作功能服务。主要需要的功能有 更新cookie等参数 执行采集任务 数据sql执行框 数据加载表格 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455CREATE TABLE douyin_products ( promotion_id VARCHAR(255), product_id VARCHAR(255), title TEXT, cover JSON, detail_url TEXT, promotion_source INT, price DECIMAL(10, 2), market_price DECIMAL(10, 2), cos_fee DECIMAL(10, 2), cos_ratio DECIMAL(5, 2), month_sales BIGINT, shop_id BIGINT, shop_name VARCHAR(255), exp_score VARCHAR(10), in_promotion BOOLEAN, sales BIGINT, tag_list JSON, text_product_tag JSON, has_same_type BOOLEAN, is_show_score BOOLEAN, board_data JSON, price_text VARCHAR(50), rank_type VARCHAR(255), -- 新增字段 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP -- 新增创建时间字段);CREATE TABLE douyin_products_month ( promotion_id VARCHAR(255), product_id VARCHAR(255), title TEXT, cover JSON, detail_url TEXT, promotion_source INT, price DECIMAL(10, 2), market_price DECIMAL(10, 2), cos_fee DECIMAL(10, 2), cos_ratio DECIMAL(5, 2), month_sales BIGINT, shop_id BIGINT, shop_name VARCHAR(255), exp_score VARCHAR(10), in_promotion BOOLEAN, sales BIGINT, tag_list JSON, text_product_tag JSON, has_same_type BOOLEAN, is_show_score BOOLEAN, board_data JSON, price_text VARCHAR(50), rank_type VARCHAR(255), -- 新增字段 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP -- 新增创建时间字段);","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/operate","date":"2023-11-21T06:23:56.438Z","updated":"2023-11-21T06:25:40.961Z","comments":true,"path":"2023/11/21/王阁/技术/operate/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/11/21/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/operate/","excerpt":"","text":"12curl -X GET &quot;http://localhost:9200/hyl-camera-five-minute-data/_count&quot; -H &#x27;Content-Type: application/json&#x27; -u elastic:3FuMQXOdPTdFDHrKrVeJ 12curl -X GET &quot;http://es01:9200/your_index_name/_count&quot; -H &#x27;Content-Type: application/json&#x27; -u username:password","categories":[],"tags":[]},{"title":"","slug":"王阁/工作/项目/铁四院数据平台/发版/demo1","date":"2023-11-20T03:28:40.391Z","updated":"2023-11-20T06:52:29.924Z","comments":true,"path":"2023/11/20/王阁/工作/项目/铁四院数据平台/发版/demo1/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/11/20/%E7%8E%8B%E9%98%81/%E5%B7%A5%E4%BD%9C/%E9%A1%B9%E7%9B%AE/%E9%93%81%E5%9B%9B%E9%99%A2%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0/%E5%8F%91%E7%89%88/demo1/","excerpt":"","text":"123456clickhouse125.93 GiB,13475500163elasticsearch82864 8M消息队列7天数据 3.3T clickhouse 10.20.10.21:9000 elasticsearch 10.20.10.21:9200 kibana 10.20.10.21:5601 kafka 10.20.10.21:9092","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/视频更新记录/1107","date":"2023-11-08T09:11:34.000Z","updated":"2023-11-20T13:29:14.000Z","comments":true,"path":"2023/11/08/王阁/运营/视频更新记录/1107/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/11/08/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E8%A7%86%E9%A2%91%E6%9B%B4%E6%96%B0%E8%AE%B0%E5%BD%95/1107/","excerpt":"","text":"《新版课程标准解析与教学指导 小学语文》是一本有关小学语文专业知识的书，主编都是高校的教育专家。对于一线小学语文教师而言，可读性很强。此书理论性强，专业术语多，真是需要“硬着头皮”去读，才能读下去。 ​ 本书大致可分为两部分，第一部分是对新版课程标准的解析，第二部分是列举教学案例，具体阐释“学习任务群”这一新概念和当前倡导的学习内容组织样式。 ​ 第一部分的对于新课标的解析，我结合《新课标》，参照着读，感觉比较通彻，现分享几点，如下： ​ 一、新课标的三变化 ​ 相较于《义务教育语文课程标准（2011年版）》，《义务教育语文课程标准（2022年版）》（简称新课标）主要有三个方面的变化： ​ 其一，确定了素养性的学习目标。“2011年版课标”中，学习目标是“三维”表达的，即“知识与技能，过程与方法，情感态度与价值观”，相信大家都不陌生，因为近十年，我们在写教学设计教学目标时，都是依照此三点写的。那么，新课标的学习目标，设定为“培养学生的语文核心素养”。 ​ 其二，用“学习任务群”的形式，明确了义务教育阶段的语文课程内容。“学习任务群”是新出现的一个概念，需要细细解读。 ​ 其三，研制了学业质量标准。让学业质量评测有标可循，有目可参，减少了漫无目的，错杂无序的盲目性。 ​ 二、语文核心素养为何物 ​ 书中是这样界定的：“义务教育语文课程培养的核心素养，是学生在积极的语文实践活动中积累建构，并在真实的语言运用情境中表现出来的，是文化自信和语言运用、思维能力、审美创造的综合体现。” ​ 用词比较专业，严谨。通俗理解一下吧。 ​ 首先，是“学生在积极的语文实践活动中积累建构”，有的老师一看就懵了。其实，我们一线教师，平时就是这么做的，只是不自知而已。比如上课时，让学生读课文了吧，背诵片段了吧，创设情境理解课文了吧，指导写字了吧，讲小故事了吧，做相关练习了吧，等等，这些都是“积极的语文实践活动”，学生从中得到了知识的积累，提升了能力，培养了良好的情感和正确的价值观等，并与已有的建立了联系，达成了“新旧”的建构。 ​ 如此分析，是不是明白了一些？ ​ 其次，是学生“在真实的语言应用情境中表现出来的”。学生学到了什么，怎么表现出来呢？只有通过“真实的语言运用情境”。基础知识掌握，好考核，笔测即可。学生还有一些收获，不是通过考试就能测出来的。比如“解决真实问题的能力，学生的价值观”等隐性的东西，如何测呢？就需要借助“真实的语言运用情境”，可以是现实中的实际情况，也可以是老师设置的“真实情境”的模拟，让学生积极参与，从而表现出来。比如班级开展“我是小雷锋”活动，学生解决问题的能力，情感态度，以及三观，就能借助活动体现出来。 再次，“文化自信和语言应用、思维能力、审美创造的综合体现。”“综合体现”，即表明四者之间不可割裂，而是相互联系，是一个整体。《新课标》中有对这四个概念的明确界定。作为一线老师，用咱朴实的话说一说。 ​ “文化自信”，就是学生对中国文化认同，热爱，并为之自豪，有自信。这很重要。如果身为一个中国人，对自己祖国文化不认同，不热爱，崇洋媚外，那是如何也学不好语文的，因为“根不正，心不纯”。 ​ “语言应用”，就是学生能合适、恰当地运用语言文字，能听懂，会说话，能准确流畅有感情地读，会写表达中心意愿的文章。这也是语文“工具性”的体现。 ​ “思维能力”，是隐性的，一切语言，都是“思维”的外显。在学习课文时，我们是不是让学生展开联想想象画面了？是不是类文分析比较了？是不是归纳文意，判断中心了？等等，这些都是在训练“思维能力”。它并不是什么高深不可触摸的，而是与学生的学习过程一直相伴的。 ​ “审美创造”，即学生在长期的精华语言的咀嚼涵泳中，审美力提升，并激发出对“更美”的创造动力。比如学习了诗歌，学生发自内心地喜欢，极其投入地诵读，并情不自禁地创作诗歌，这不就是接地气的“审美创造”吗？ ​ 在具体活动中，不是只关注其中一个，而是几个，或全面四个同时存在的。“语文核心素养”是以上四方面的综合体现，由“三维”变成“四面”，不是本质上的断层，而是继承中的发展。 ​ 最后，我想说，“核心素养”是舶来品，听着好像深不可测。其实，我们之前提倡的“培养高素质人才”中的“高素质”，和“核心素养”差不多，只不过一个是国产的，一个是国际的而已，。和我们熟知的“德、智、体、美、劳”全面发展也有很大相通之处。 不要，也不用被其“华丽的外衣”所迷惑，要认清其本质。 ​ 三、解读学习任务群 ​ “学习任务群”是《新课标》里的最亮之星。相信大多数老师对其是比较陌生的。那么，我们就先来说一说，什么是“学习任务群”。 《新课标》中如此界定：“学习任务群以任务为导向，以学习项目为载体，整合学习情境，学习内容，学习方法和学习资源，引导学生在运用语言的过程中提升语言素养。若干学习项目组成学习任务群。” 是不是头又大了？其实是被术语蒙住了。一线教师，每节课上都有设计“学习任务群”呀。 ​ 举个例子，你想通过本课的学习，提升学生的语文素养，这就是“任务导向”；你想让学生认识生字，读通课文，理解文意吧，这就是“学习项目”；利用设置的情境，课本材料，恰当的学习方法和拓展的学习资源，引导学生在运用语言的过程中，提升语言素养。这不就是以“学习任务群”的方式学习吗？我们平时就是这么做的，只是给起了一个新的雅观名字——“学习任务群”而已。 ​ “学习任务群”有三个特征：情境性，实践性，综合性。即要设置相关情境，依托具体实践活动，并综合利用各种学习资源，达到提高“语文素养”的目的。 ​ 《新课标》从三个层面，设置了六个“学习任务群”。 ​ 第一层设“语言文字积累与梳理”1个基础型学习任务群，第二层设“实用性任务与阅读与交流”“文学阅读与创意表达”“思辨性阅读与表达”3个发展型学习任务群，第三层设“整本书阅读”“跨学科学习”2个拓展型学习任务群。 看着新名词不少，其实我们一直在用。认字，写字，对字词归类梳理积累，我们都不陌生吧，这就是“语言文字积累与梳理”；对于介绍类的说明性文章的学习，可以设置“实用性阅读与交流任务群”；对于情境画面感强的文学类文章，可以制定“文学阅读与创意表达”；那些议论性强的含哲理的文章，则可用“思辨性阅读与表达任务群”；“整本书阅读”，我们提了好多年了；“跨学科学习”，与之前提倡的“综合性学习”有相通之处。 ​ 这样一梳理，是不是心中明朗了许多呢？拨开迷雾，方见本质。 ​ 需要注意的是，这些“学习任务群”，并不是独立分割，而是彼此之间有联系，可包容，相互交叉、渗透的，体现出语文课程内容的综合性。比如，“语言文字积累与梳理”，不但要集中学习，还要在各个学习任务群中分散设置学习任务；三个发展型学习任务群也会交叉进行，比如实用性阅读、文学阅读中也可以融入思辨性阅读的学习内容；两个拓展型学习任务群更是综合运用，体现了各个学习任务群的融合，互动，互促。 ​ 四、四个方面转变学习观 其一，从关注碎片化学科知识技能的习得，到关注复杂的、不确定性的现实问题的解决。 其意为，学习不仅仅要获取知识，还要获取解决问题的思想方法。正如古语：“授人之鱼，不如授人之渔”。我们要从关注的“死知识”，转变为关注“活方法”，这才是真正有用，受益一生的。比如之前，多是单纯的“认字”，现在提倡“用多种方法认字”，讲出认字的过程方法，偏旁归类找规律，进行积累，梳理，由“重知识”到“重方法”。 ​ 其二，从关注对前人知识的理解和反应，到关注综合运用和主动创造知识。 ​ 这是从关注“传承已知”，到关注“运用创新”。以学习“比喻”为例，一贯的做法是三问：“这句运用了什么修辞手法？把什么比喻成了什么？它们有什么相似点？”而现在要求在“三问”的基础上，关注“运用创新”，可以设置具体的情境，试着用上比喻描述出来，探究比喻的妙处，激发未来面对新的语言现象时能够自主地运用“比喻”，创作出有新意的句子，甚或建立新的修辞方法。这是综合运用和主动创造知识的表现。 ​ 其三，从关注“学什么”，到关注“如何学习”和“学会学习”。 “学什么”是内容，“如何学习”是方法，“学会学习”是效果。内容确定，合适的、良好的方法才能达到学习效果最优化。从单向关注改变到多项的、整体关注。 ​ 其四，从关注自我学习到关注团队合作和沟通。 ​ 将来的社会，很多工作都需要团队合作才能完。因此，在学习中关注其合作与沟通能力，尤为重要。设置小组，自主，合作，探究，纠正“单打独斗”之思，关注“团队合作”之势。 本文第二部分是结合教学案例，具体阐释三类“学习任务群”，先是“理念解读”，再是“案例呈现”，最后“教学讨论”。 ​ 按说，这是我最想看的部分，我想走个捷径，看看“任务群”如何合理设计。可浏览过之后，有些失望。不是人家设计的不好，而是“太好”，“高大上”，不接地气，实用性，操作性，可移植性不强。许是我的“道行”尚浅，理解不了人家的“深刻含义，良苦用心”。 罢了罢了，看了两遍，仍觉受益不大，暂且搁置吧。 ​ 综上，是本人读书后的一些粗陋之见，不当之处，敬请批评指正。","categories":[],"tags":[]},{"title":"","slug":"王阁/工作/项目/铁四院数据平台/发版/开发环境更新","date":"2023-11-07T01:27:59.700Z","updated":"2023-11-07T01:54:20.706Z","comments":true,"path":"2023/11/07/王阁/工作/项目/铁四院数据平台/发版/开发环境更新/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/11/07/%E7%8E%8B%E9%98%81/%E5%B7%A5%E4%BD%9C/%E9%A1%B9%E7%9B%AE/%E9%93%81%E5%9B%9B%E9%99%A2%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0/%E5%8F%91%E7%89%88/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%9B%B4%E6%96%B0/","excerpt":"","text":"1kubectl set image deployments/pre-sxsddsj-web webapp=registry.lisong.pub:28500/sxsddsj-fe/sxsddsj_web:v12353","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/阿里云盘资源","date":"2023-11-06T13:04:22.000Z","updated":"2023-11-07T06:29:18.033Z","comments":true,"path":"2023/11/06/王阁/运营/未分类/阿里云盘资源/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/11/06/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/%E9%98%BF%E9%87%8C%E4%BA%91%E7%9B%98%E8%B5%84%E6%BA%90/","excerpt":"","text":"https://www.ahhhhfs.com/https://www.kdocs.cn/l/cfzhxRXkaxx3 https://panyoubbs.xyz/www.alypw.comyunpan1.cc","categories":[],"tags":[]},{"title":"hive梳理2","slug":"王阁/回顾/hive/hive梳理2","date":"2023-10-31T01:49:59.000Z","updated":"2023-11-01T05:52:32.913Z","comments":true,"path":"2023/10/31/王阁/回顾/hive/hive梳理2/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/31/%E7%8E%8B%E9%98%81/%E5%9B%9E%E9%A1%BE/hive/hive%E6%A2%B3%E7%90%862/","excerpt":"","text":"前面中梳理了hive的DML、DQL等基础用法。本文主要掌握运算符与函数等相关内容。 内置运算符关系运算符关系运算符是二元运算符，执行的是两个操作数的比较运算。每个关系运算符都返回boolean类型结果（TRUE或FALSE）。 1234567891011等值比较: = 、==•不等值比较: &lt;&gt; 、!=•小于比较: &lt;•小于等于比较: &lt;=•大于比较: &gt;•大于等于比较: &gt;=•空值判断: IS NULL •非空判断: IS NOT NULL•LIKE比较: LIKE•JAVA的LIKE操作: RLIKE•REGEXP操作: REGEXP 算术运算符算术运算符操作数必须是数值类型。 分为一元运算符和二元运算符; 一元运算符,只有一个操作数; 二元运算符有两个操作数,运算符在两个操作数之间。 12345678910•加法操作: +•减法操作: -•乘法操作: *•除法操作: /•取整操作: div•取余操作: %•位与操作: &amp;•位或操作: |•位异或操作: ^•位取反操作: ~ 1234567891011121314151617--取整操作: div 给出将A除以B所得的整数部分。例如17 div 3得出5。select 17 div 3;--取余操作: % 也叫做取模 A除以B所得的余数部分select 17 % 3;--位与操作: &amp; A和B按位进行与操作的结果。 与表示两个都为1则结果为1select 4 &amp; 8 from dual; --4转换二进制：0100 8转换二进制：1000select 6 &amp; 4 from dual; --4转换二进制：0100 6转换二进制：0110--位或操作: | A和B按位进行或操作的结果 或表示有一个为1则结果为1select 4 | 8 from dual;select 6 | 4 from dual;--位异或操作: ^ A和B按位进行异或操作的结果 异或表示两个不同则结果为1select 4 ^ 8 from dual;select 6 ^ 4 from dual; 逻辑运算符123456•与操作: A AND B•或操作: A OR B•非操作: NOT A 、!A•在:A IN (val1, val2, ...)•不在:A NOT IN (val1, val2, ...) •逻辑是否存在: [NOT] EXISTS (subquery) Hive函数内置函数内置函数主要有八大类 String Functions 字符串函数123456789101112131415161718192021字符串长度函数：length•字符串反转函数：reverse•字符串连接函数：concat•带分隔符字符串连接函数：concat_ws•字符串截取函数：substr,substring•字符串转大写函数：upper,ucase•字符串转小写函数：lower,lcase•去空格函数：trim•左边去空格函数：ltrim•右边去空格函数：rtrim•正则表达式替换函数：regexp_replace•正则表达式解析函数：regexp_extract•URL解析函数：parse_url•json解析函数：get_json_object•空格字符串函数：space•重复字符串函数：repeat•首字符ascii函数：ascii•左补足函数：lpad•右补足函数：rpad•分割字符串函数: split•集合查找函数: find_in_set 示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667------------String Functions 字符串函数------------describe function extended find_in_set;--字符串长度函数：length(str | binary)select length(&quot;angelababy&quot;);--字符串反转函数：reverseselect reverse(&quot;angelababy&quot;);--字符串连接函数：concat(str1, str2, ... strN)select concat(&quot;angela&quot;,&quot;baby&quot;);--带分隔符字符串连接函数：concat_ws(separator, [string | array(string)]+)select concat_ws(&#x27;.&#x27;, &#x27;www&#x27;, array(&#x27;itcast&#x27;, &#x27;cn&#x27;));--字符串截取函数：substr(str, pos[, len]) 或者 substring(str, pos[, len])select substr(&quot;angelababy&quot;,-2); --pos是从1开始的索引，如果为负数则倒着数select substr(&quot;angelababy&quot;,2,2);--字符串转大写函数：upper,ucaseselect upper(&quot;angelababy&quot;);select ucase(&quot;angelababy&quot;);--字符串转小写函数：lower,lcaseselect lower(&quot;ANGELABABY&quot;);select lcase(&quot;ANGELABABY&quot;);--去空格函数：trim 去除左右两边的空格select trim(&quot; angelababy &quot;);--左边去空格函数：ltrimselect ltrim(&quot; angelababy &quot;);--右边去空格函数：rtrimselect rtrim(&quot; angelababy &quot;);--正则表达式替换函数：regexp_replace(str, regexp, rep)select regexp_replace(&#x27;100-200&#x27;, &#x27;(\\\\d+)&#x27;, &#x27;num&#x27;);--正则表达式解析函数：regexp_extract(str, regexp[, idx]) 提取正则匹配到的指定组内容select regexp_extract(&#x27;100-200&#x27;, &#x27;(\\\\d+)-(\\\\d+)&#x27;, 2);--URL解析函数：parse_url 注意要想一次解析出多个 可以使用parse_url_tuple这个UDTF函数select parse_url(&#x27;http://www.itcast.cn/path/p1.php?query=1&#x27;, &#x27;HOST&#x27;);--json解析函数：get_json_object--空格字符串函数：space(n) 返回指定个数空格select space(4);--重复字符串函数：repeat(str, n) 重复str字符串n次select repeat(&quot;angela&quot;,2);--首字符ascii函数：asciiselect ascii(&quot;angela&quot;); --a对应ASCII 97--左补足函数：lpadselect lpad(&#x27;hi&#x27;, 5, &#x27;??&#x27;); --???hiselect lpad(&#x27;hi&#x27;, 1, &#x27;??&#x27;); --h--右补足函数：rpadselect rpad(&#x27;hi&#x27;, 5, &#x27;??&#x27;);--分割字符串函数: split(str, regex)select split(&#x27;apache hive&#x27;, &#x27;\\\\s+&#x27;);--集合查找函数: find_in_set(str,str_array)select find_in_set(&#x27;a&#x27;,&#x27;abc,b,ab,c,def&#x27;); Date Functions 日期函数1234567891011121314151617获取当前日期: current_date•获取当前时间戳: current_timestamp•UNIX时间戳转日期函数: from_unixtime•获取当前UNIX时间戳函数: unix_timestamp•日期转UNIX时间戳函数: unix_timestamp•指定格式日期转UNIX时间戳函数: unix_timestamp•抽取日期函数: to_date•日期转年函数: year•日期转月函数: month•日期转天函数: day•日期转小时函数: hour•日期转分钟函数: minute•日期转秒函数: second•日期转周函数: weekofyear•日期比较函数: datediff•日期增加函数: date_add•日期减少函数: date_sub 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152--获取当前日期: current_dateselect current_date();--获取当前时间戳: current_timestamp--同一查询中对current_timestamp的所有调用均返回相同的值。select current_timestamp();--获取当前UNIX时间戳函数: unix_timestampselect unix_timestamp();--UNIX时间戳转日期函数: from_unixtimeselect from_unixtime(1618238391);select from_unixtime(0, &#x27;yyyy-MM-dd HH:mm:ss&#x27;);--日期转UNIX时间戳函数: unix_timestampselect unix_timestamp(&quot;2011-12-07 13:01:03&quot;);--指定格式日期转UNIX时间戳函数: unix_timestampselect unix_timestamp(&#x27;20111207 13:01:03&#x27;,&#x27;yyyyMMdd HH:mm:ss&#x27;);--抽取日期函数: to_dateselect to_date(&#x27;2009-07-30 04:17:52&#x27;);--日期转年函数: yearselect year(&#x27;2009-07-30 04:17:52&#x27;);--日期转月函数: monthselect month(&#x27;2009-07-30 04:17:52&#x27;);--日期转天函数: dayselect day(&#x27;2009-07-30 04:17:52&#x27;);--日期转小时函数: hourselect hour(&#x27;2009-07-30 04:17:52&#x27;);--日期转分钟函数: minuteselect minute(&#x27;2009-07-30 04:17:52&#x27;);--日期转秒函数: secondselect second(&#x27;2009-07-30 04:17:52&#x27;);--日期转周函数: weekofyear 返回指定日期所示年份第几周select weekofyear(&#x27;2009-07-30 04:17:52&#x27;);--日期比较函数: datediff 日期格式要求&#x27;yyyy-MM-dd HH:mm:ss&#x27; or &#x27;yyyy-MM-dd&#x27;select datediff(&#x27;2012-12-08&#x27;,&#x27;2012-05-09&#x27;);--日期增加函数: date_addselect date_add(&#x27;2012-02-28&#x27;,10);--日期减少函数: date_subselect date_sub(&#x27;2012-01-1&#x27;,10); Mathematical Functions 数学函数12345678取整函数: round•指定精度取整函数: round•向下取整函数: floor•向上取整函数: ceil•取随机数函数: rand•二进制函数: bin•进制转换函数: conv•绝对值函数: abs 12345678910111213141516171819202122232425262728--取整函数: round 返回double类型的整数值部分 （遵循四舍五入）select round(3.1415926);--指定精度取整函数: round(double a, int d) 返回指定精度d的double类型select round(3.1415926,4);--向下取整函数: floorselect floor(3.1415926);select floor(-3.1415926);--向上取整函数: ceilselect ceil(3.1415926);select ceil(-3.1415926);--取随机数函数: rand 每次执行都不一样 返回一个0到1范围内的随机数select rand();--指定种子取随机数函数: rand(int seed) 得到一个稳定的随机数序列select rand(2);--二进制函数: bin(BIGINT a)select bin(18);--进制转换函数: conv(BIGINT num, int from_base, int to_base)select conv(17,10,16);--绝对值函数: absselect abs(-3.9); Collection Functions 集合函数12345•集合元素size函数: size(Map&lt;K.V&gt;) size(Array&lt;T&gt;)•取map集合keys函数: map_keys(Map&lt;K.V&gt;)•取map集合values函数: map_values(Map&lt;K.V&gt;)•判断数组是否包含指定元素: array_contains(Array&lt;T&gt;, value)•数组排序函数:sort_array(Array&lt;T&gt;) 12345678910111213141516--集合元素size函数: size(Map&lt;K.V&gt;) size(Array&lt;T&gt;)select size(`array`(11,22,33));select size(`map`(&quot;id&quot;,10086,&quot;name&quot;,&quot;zhangsan&quot;,&quot;age&quot;,18));--取map集合keys函数: map_keys(Map&lt;K.V&gt;)select map_keys(`map`(&quot;id&quot;,10086,&quot;name&quot;,&quot;zhangsan&quot;,&quot;age&quot;,18));--取map集合values函数: map_values(Map&lt;K.V&gt;)select map_values(`map`(&quot;id&quot;,10086,&quot;name&quot;,&quot;zhangsan&quot;,&quot;age&quot;,18));--判断数组是否包含指定元素: array_contains(Array&lt;T&gt;, value)select array_contains(`array`(11,22,33),11);select array_contains(`array`(11,22,33),66);--数组排序函数:sort_array(Array&lt;T&gt;)select sort_array(`array`(12,2,32)); Conditional Functions 条件函数12345678if条件判断: if(boolean testCondition, T valueTrue, T valueFalseOrNull)•空判断函数: isnull( a )•非空判断函数: isnotnull ( a )•空值转换函数: nvl(T value, T default_value)•非空查找函数: COALESCE(T v1, T v2, ...)•条件转换函数: CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END•nullif( a, b ): 如果a = b，则返回NULL；否则返回NULL。否则返回一个•assert_true: 如果&#x27;condition&#x27;不为真，则引发异常，否则返回null 1234567891011121314151617181920212223242526272829303132333435363738--使用之前课程创建好的student表数据select * from student limit 3;--if条件判断: if(boolean testCondition, T valueTrue, T valueFalseOrNull)select if(1=2,100,200);select if(sex =&#x27;男&#x27;,&#x27;M&#x27;,&#x27;W&#x27;) from student limit 3;--空判断函数: isnull( a )select isnull(&quot;allen&quot;);select isnull(null);--非空判断函数: isnotnull ( a )select isnotnull(&quot;allen&quot;);select isnotnull(null);--空值转换函数: nvl(T value, T default_value)select nvl(&quot;allen&quot;,&quot;itcast&quot;);select nvl(null,&quot;itcast&quot;);--非空查找函数: COALESCE(T v1, T v2, ...)--返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULLselect COALESCE(null,11,22,33);select COALESCE(null,null,null,33);select COALESCE(null,null,null);--条件转换函数: CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] ENDselect case 100 when 50 then &#x27;tom&#x27; when 100 then &#x27;mary&#x27; else &#x27;tim&#x27; end;select case sex when &#x27;男&#x27; then &#x27;man&#x27; else &#x27;women&#x27; end from student limit 3;--nullif( a, b ):-- 果a = b，则返回NULL；否则返回NULL。否则返回一个select nullif(11,11);select nullif(11,12);--assert_true(condition)--如果&#x27;condition&#x27;不为真，则引发异常，否则返回nullSELECT assert_true(11 &gt;= 0);SELECT assert_true(-1 &gt;= 0); Type Conversion Functions 类型转换函数1234任意数据类型之间转换:cast--任意数据类型之间转换:castselect cast(12.14 as bigint);select cast(12.14 as string); Data Masking Functions 数据脱敏函数123456mask•mask_first_n(string str[, int n]•mask_last_n(string str[, int n])•mask_show_first_n(string str[, int n])•mask_show_last_n(string str[, int n])•mask_hash(string|char|varchar str) 12345678910111213141516171819202122--mask--将查询回的数据，大写字母转换为X，小写字母转换为x，数字转换为n。select mask(&quot;abc123DEF&quot;);select mask(&quot;abc123DEF&quot;,&#x27;-&#x27;,&#x27;.&#x27;,&#x27;^&#x27;); --自定义替换的字母--mask_first_n(string str[, int n]--对前n个进行脱敏替换select mask_first_n(&quot;abc123DEF&quot;,4);--mask_last_n(string str[, int n])select mask_last_n(&quot;abc123DEF&quot;,4);--mask_show_first_n(string str[, int n])--除了前n个字符，其余进行掩码处理select mask_show_first_n(&quot;abc123DEF&quot;,4);--mask_show_last_n(string str[, int n])select mask_show_last_n(&quot;abc123DEF&quot;,4);--mask_hash(string|char|varchar str)--返回字符串的hash编码。select mask_hash(&quot;abc123DEF&quot;); Misc. Functions 其他杂项函数12345678hive调用java方法: java_method(class, method[, arg1[, arg2..]])•反射函数: reflect(class, method[, arg1[, arg2..]])•取哈希值函数:hash•current_user()、logged_in_user()、current_database()、version()•SHA-1加密: sha1(string/binary)•SHA-2家族算法加密：sha2(string/binary, int) (SHA-224, SHA-256, SHA-384, SHA-512)•crc32加密:•MD5加密: md5(string/binary) 1234567891011121314151617181920212223--hive调用java方法: java_method(class, method[, arg1[, arg2..]])select java_method(&quot;java.lang.Math&quot;,&quot;max&quot;,11,22);--反射函数: reflect(class, method[, arg1[, arg2..]])select reflect(&quot;java.lang.Math&quot;,&quot;max&quot;,11,22);--取哈希值函数:hashselect hash(&quot;allen&quot;);--current_user()、logged_in_user()、current_database()、version()--SHA-1加密: sha1(string/binary)select sha1(&quot;allen&quot;);--SHA-2家族算法加密：sha2(string/binary, int) (SHA-224, SHA-256, SHA-384, SHA-512)select sha2(&quot;allen&quot;,224);select sha2(&quot;allen&quot;,512);--crc32加密:select crc32(&quot;allen&quot;);--MD5加密: md5(string/binary)select md5(&quot;allen&quot;); 用户自定义函数用户自定义函数简称UDF，源自于英文user-defined function。自定义函数总共有3类，是根据函数输入输出的行数来区分的，分别是： UDF（User-Defined-Function）普通函数，一进一出 UDAF（User-Defined Aggregation Function）聚合函数，多进一出 UDTF（User-Defined Table-Generating Functions）表生成函数，一进多出 UDF分类标准可以扩大到Hive的所有函数中：包括内置函数和自定义函数。因为不管是什么类型的行数，一定满足于输入输出的要求，那么从输入几行和输出几行上来划分没有任何毛病。千万不要被UD（User-Defined）这两个字母所迷惑，照成视野的狭隘。 UDF 普通函数UDF函数通常把它叫做普通函数，最大的特点是一进一出，也就是输入一行输出一行。比如round这样的取整函数，接收一行数据，输出的还是一行数据。 UDAF 聚合函数UDAF函数通常把它叫做聚合函数，A所代表的单词就是Aggregation聚合的意思。最大的特点是多进一出，也就是输入多行输出一行。比如count、sum这样的函数。 1234567•count:统计检索到的总行数。•sum:求和•avg:求平均•min:最小值•max:最大值•数据收集函数（去重）: collect_set(col)•数据收集函数（不去重）: collect_list(col) UDTF 表生成函数UDTF函数通常把它叫做表生成函数，T所代表的单词是Table-Generating表生成的意思。最大的特点是一进多出，也就是输入一行输出多行。 之所以叫做表生成函数，原因在于这类型的函数作用返回的结果类似于表（多行数据嘛），同时，UDTF函数也是我们接触比较少的函数，陌生。比如explode函数。 高阶函数UDTF****之explode函数12345于UDTF表生成函数，很多人难以理解什么叫做输入一行，输出多行。为什么叫做表生成？能够产生表吗？下面我们就来学习Hive当做内置的一个非常著名的UDTF函数，名字叫做explode函数，中文戏称之为“爆炸函数”，可以炸开数据。explode函数接收map或者array类型的数据作为参数，然后把参数中的每个元素炸开变成一行数据。一个元素一行。这样的效果正好满足于输入一行输出多行。explode函数在关系型数据库中本身是不该出现的。因为他的出现本身就是在操作不满足第一范式的数据（每个属性都不可再分）。本身已经违背了数据库的设计原理，但是在面向分析的数据库或者数据仓库中，这些规范可以发生改变。 123select explode(`array`(11,22,33)) as item;select explode(`map`(&quot;id&quot;,10086,&quot;name&quot;,&quot;zhangsan&quot;,&quot;age&quot;,18)); Lateral View侧视图Lateral View是一种特殊的语法，主要用于搭配UDTF类型功能的函数一起使用，用于解决UDTF函数的一些查询限制的问题。 侧视图的原理是将UDTF的结果构建成一个类似于视图的表，然后将原表中的每一行和UDTF函数输出的每一行进行连接，生成一张新的虚拟表。这样就避免了UDTF的使用限制问题。使用lateral view时也可以对UDTF产生的记录设置字段名称，产生的字段可以用于group by、order by 、limit等语句中，不需要再单独嵌套一层子查询。 一般只要使用UDTF，就会固定搭配lateral view使用。 12345678910--lateral view侧视图基本语法如下select …… from tabelA lateral view UDTF(xxx) 别名 as col1,col2,col3……;select a.team_name ,b.yearfrom the_nba_championship a lateral view explode(champion_year) b as year--根据年份倒序排序select a.team_name ,b.yearfrom the_nba_championship a lateral view explode(champion_year) b as yearorder by b.year desc; Aggregation 聚合函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475--------------基础聚合函数---------------------1、测试数据准备drop table if exists student;create table student( num int, name string, sex string, age int, dept string)row format delimitedfields terminated by &#x27;,&#x27;;--加载数据load data local inpath &#x27;/root/hivedata/students.txt&#x27; into table student;--验证select * from student;--场景1：没有group by子句的聚合操作select count(*) as cnt1,count(1) as cnt2 from student; --两个一样--场景2：带有group by子句的聚合操作 注意group by语法限制select sex,count(*) as cnt from student group by sex;--场景3：select时多个聚合函数一起使用select count(*) as cnt1,avg(age) as cnt2 from student;--场景4：聚合函数和case when条件转换函数、coalesce函数、if函数使用select sum(CASE WHEN sex = &#x27;男&#x27;THEN 1 ELSE 0 END)from student;select sum(if(sex = &#x27;男&#x27;,1,0))from student;--场景5：聚合参数不支持嵌套聚合函数select avg(count(*)) from student;--聚合参数针对null的处理方式--null null 0select max(null), min(null), count(null);--下面这两个不支持nullselect sum(null), avg(null);--场景5：聚合操作时针对null的处理CREATE TABLE tmp_1 (val1 int, val2 int);INSERT INTO TABLE tmp_1 VALUES (1, 2),(null,2),(2,3);select * from tmp_1;--第二行数据(NULL, 2) 在进行sum(val1 + val2)的时候会被忽略select sum(val1), sum(val1 + val2) from tmp_1;--可以使用coalesce函数解决select sum(coalesce(val1,0)), sum(coalesce(val1,0) + val2)from tmp_1;--场景6：配合distinct关键字去重聚合--此场景下，会编译期间会自动设置只启动一个reduce task处理数据 性能可能会不会 造成数据拥堵select count(distinct sex) as cnt1 from student;--可以先去重 在聚合 通过子查询完成--因为先执行distinct的时候 可以使用多个reducetask来跑数据select count(*) as gender_uni_cntfrom (select distinct sex from student) a;--案例需求：找出student中男女学生年龄最大的及其名字--这里使用了struct来构造数据 然后针对struct应用max找出最大元素 然后取值select sex,max(struct(age, name)).col1 as age,max(struct(age, name)).col2 as namefrom studentgroup by sex;select struct(age, name) from student;select struct(age, name).col1 from student;select max(struct(age, name)) from student; Grouping setsgrouping sets是一种将多个group by逻辑写在一个sql语句中的便利写法。 等价于将不同维度的GROUP BY结果集进行UNION ALL。 GROUPING__ID表示结果属于哪一个分组集合。 1234567891011121314151617181920212223242526272829303132333435---group sets---------SELECT month, day, COUNT(DISTINCT cookieid) AS nums, GROUPING__IDFROM cookie_infoGROUP BY month,dayGROUPING SETS (month,day)ORDER BY GROUPING__ID;--grouping_id表示这一组结果属于哪个分组集合，--根据grouping sets中的分组条件month，day，1是代表month，2是代表day--等价于SELECT month,NULL,COUNT(DISTINCT cookieid) AS nums,1 AS GROUPING__ID FROM cookie_info GROUP BY monthUNION ALLSELECT NULL as month,day,COUNT(DISTINCT cookieid) AS nums,2 AS GROUPING__ID FROM cookie_info GROUP BY day;--再比如SELECT month, day, COUNT(DISTINCT cookieid) AS nums, GROUPING__IDFROM cookie_infoGROUP BY month,dayGROUPING SETS (month,day,(month,day))ORDER BY GROUPING__ID;--等价于SELECT month,NULL,COUNT(DISTINCT cookieid) AS nums,1 AS GROUPING__ID FROM cookie_info GROUP BY monthUNION ALLSELECT NULL,day,COUNT(DISTINCT cookieid) AS nums,2 AS GROUPING__ID FROM cookie_info GROUP BY dayUNION ALLSELECT month,day,COUNT(DISTINCT cookieid) AS nums,3 AS GROUPING__ID FROM cookie_info GROUP BY month,day; Window functions 窗口函数窗口函数（Window functions）是一种SQL函数，非常适合于数据分析，因此也叫做OLAP函数，其最大特点是：输入值是从SELECT语句的结果集中的一行或多行的“窗口”中获取的。你也可以理解为窗口有大有小（行有多有少）。 通过OVER子句，窗口函数与其他SQL函数有所区别。如果函数具有OVER子句，则它是窗口函数。如果它缺少OVER子句，则它是一个普通的聚合函数。 窗口函数可以简单地解释为类似于聚合函数的计算函数，但是通过GROUP BY子句组合的常规聚合会隐藏正在聚合的各个行，最终输出一行，窗口函数聚合后还可以访问当中的各个行，并且可以将这些行中的某些属性添加到结果集中。 12345678910111213Function(arg1,..., argn) OVER ([PARTITION BY &lt;...&gt;] [ORDER BY &lt;....&gt;] [&lt;window_expression&gt;])--其中Function(arg1,..., argn) 可以是下面分类中的任意一个 --聚合函数：比如sum max avg等 --排序函数：比如rank row_number等 --分析函数：比如lead lag first_value等--OVER [PARTITION BY &lt;...&gt;] 类似于group by 用于指定分组 每个分组你可以把它叫做窗口--如果没有PARTITION BY 那么整张表的所有行就是一组--[ORDER BY &lt;....&gt;] 用于指定每个分组内的数据排序规则 支持ASC、DESC--[&lt;window_expression&gt;] 用于指定每个窗口中 操作的数据范围 默认是窗口中所有行 窗口聚合函数从Hive v2.2.0开始，支持DISTINCT与窗口函数中的聚合函数一起使用。 123456789101112131415161718192021222324252627-----窗口聚合函数的使用-------------1、求出每个用户总pv数 sum+group by普通常规聚合操作select cookieid,sum(pv) as total_pv from website_pv_info group by cookieid;--2、sum+窗口函数 总共有四种用法 注意是整体聚合 还是累积聚合--sum(...) over( )对表所有行求和--sum(...) over( order by ... ) 连续累积求和--sum(...) over( partition by... ) 同组内所有行求和--sum(...) over( partition by... order by ... ) 在每个分组内，连续累积求和--需求：求出网站总的pv数 所有用户所有访问加起来--sum(...) over( )对表所有行求和select cookieid,createtime,pv, sum(pv) over() as total_pvfrom website_pv_info;--需求：求出每个用户总pv数--sum(...) over( partition by... )，同组内所行求和select cookieid,createtime,pv, sum(pv) over(partition by cookieid) as total_pvfrom website_pv_info;--需求：求出每个用户截止到当天，累积的总pv数--sum(...) over( partition by... order by ... )，在每个分组内，连续累积求和select cookieid,createtime,pv, sum(pv) over(partition by cookieid order by createtime) as current_total_pvfrom website_pv_info; 窗口表达式我们知道，在sum(…) over( partition by… order by … )语法完整的情况下，进行的累积聚合操作，默认累积聚合行为是：从第一行聚合到当前行。 Window expression窗口表达式给我们提供了一种控制行范围的能力，比如向前2行，向后3行。 1234567关键字是rows between，包括下面这几个选项- preceding：往前- following：往后- current row：当前行- unbounded：边界- unbounded preceding 表示从前面的起点- unbounded following：表示到后面的终点 12345678910111213141516171819202122232425---窗口表达式--第一行到当前行select cookieid,createtime,pv, sum(pv) over(partition by cookieid order by createtime rows between unbounded preceding and current row) as pv2from website_pv_info;--向前3行至当前行select cookieid,createtime,pv, sum(pv) over(partition by cookieid order by createtime rows between 3 preceding and current row) as pv4from website_pv_info;--向前3行 向后1行select cookieid,createtime,pv, sum(pv) over(partition by cookieid order by createtime rows between 3 preceding and 1 following) as pv5from website_pv_info;--当前行至最后一行select cookieid,createtime,pv, sum(pv) over(partition by cookieid order by createtime rows between current row and unbounded following) as pv6from website_pv_info;--第一行到最后一行 也就是分组内的所有行select cookieid,createtime,pv, sum(pv) over(partition by cookieid order by createtime rows between unbounded preceding and unbounded following) as pv6from website_pv_info; 窗口分析函数LAG(col,n,DEFAULT) 用于统计窗口内往上第n行值 第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL）； LEAD(col,n,DEFAULT) 用于统计窗口内往下第n行值 第一个参数为列名，第二个参数为往下第n行（可选，默认为1），第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL）； FIRST_VALUE 取分组内排序后，截止到当前行，第一个值； LAST_VALUE 取分组内排序后，截止到当前行，最后一个值； 1234567891011121314151617181920212223242526272829303132333435-----------窗口分析函数------------LAGSELECT cookieid, createtime, url, ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY createtime) AS rn, LAG(createtime,1,&#x27;1970-01-01 00:00:00&#x27;) OVER(PARTITION BY cookieid ORDER BY createtime) AS last_1_time, LAG(createtime,2) OVER(PARTITION BY cookieid ORDER BY createtime) AS last_2_timeFROM website_url_info;--LEADSELECT cookieid, createtime, url, ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY createtime) AS rn, LEAD(createtime,1,&#x27;1970-01-01 00:00:00&#x27;) OVER(PARTITION BY cookieid ORDER BY createtime) AS next_1_time, LEAD(createtime,2) OVER(PARTITION BY cookieid ORDER BY createtime) AS next_2_timeFROM website_url_info;--FIRST_VALUESELECT cookieid, createtime, url, ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY createtime) AS rn, FIRST_VALUE(url) OVER(PARTITION BY cookieid ORDER BY createtime) AS first1FROM website_url_info;--LAST_VALUESELECT cookieid, createtime, url, ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY createtime) AS rn, LAST_VALUE(url) OVER(PARTITION BY cookieid ORDER BY createtime) AS last1FROM website_url_info;","categories":[{"name":"回顾","slug":"回顾","permalink":"http://www.wqkenqing.ren/daydoc/categories/%E5%9B%9E%E9%A1%BE/"}],"tags":[]},{"title":"","slug":"王阁/自研项目/接口测试与性能测试演示说明","date":"2023-10-27T06:50:27.036Z","updated":"2024-05-06T06:56:01.581Z","comments":true,"path":"2023/10/27/王阁/自研项目/接口测试与性能测试演示说明/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E8%87%AA%E7%A0%94%E9%A1%B9%E7%9B%AE/%E6%8E%A5%E5%8F%A3%E6%B5%8B%E8%AF%95%E4%B8%8E%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E6%BC%94%E7%A4%BA%E8%AF%B4%E6%98%8E/","excerpt":"","text":"接口测试在postman中配置了系统中相关接口信息，通过传入相应的参数进行测试。 测试步骤! 先通过接口登录，测试工具加载出相应的token。 随机选择一些接口进行测试。 因为系统已经投入使用，所以相应的接口已经与对应的功能进行绑定。这里的接口测试主要是会选择一些GET请求来方便演示为主。 性能测试通过jmeter进行性能测试，已经选取了一些系统中有代表性的数据接口编写了对应的测试计划。 模拟场景 100人同时对测试计划中的请求发送请求 检测请求成功通过率 响应时间 并呈现如TPS、记录条数&#x2F;秒、数据量&#x2F;秒、通过概等 图表化呈现测试结果","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/1025脚本","date":"2023-10-27T06:50:27.036Z","updated":"2023-11-07T02:05:56.362Z","comments":true,"path":"2023/10/27/王阁/运营/未分类/1025脚本/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/1025%E8%84%9A%E6%9C%AC/","excerpt":"","text":"主题思路 可爱动物集锦","categories":[],"tags":[]},{"title":"","slug":"王阁/运营/未分类/如何念脚本","date":"2023-10-27T06:50:27.036Z","updated":"2023-10-27T06:50:27.036Z","comments":true,"path":"2023/10/27/王阁/运营/未分类/如何念脚本/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/%E5%A6%82%E4%BD%95%E5%BF%B5%E8%84%9A%E6%9C%AC/","excerpt":"","text":"在视频中编辑，抖音自带这个功能。 将文本改成音频，并读出来 2.1 调研了google的 voice功能，效果不错，但不好申请。 示列文档 * 12345为落实生态环境部关于组织开展VOCs治理技术评估并推荐适用技术的相关要求，在生态环境部科技与财务司的指导与带领下，国家大气污染防治攻关联合中心（以下简称“攻关中心”）组织专家编制完成《我国VOCs治理技术适用性分析报告》（以下简称《报告》）。9月26日，攻关中心举办第二十三期细颗粒物和臭氧污染协同防控“一市一策”驻点跟踪研究专题技术培训，分享并解读《报告》相关内容。本次专题技术培训会邀请清华大学马永亮副研究员、同济大学羌宁副教授、山东科技大学梁鹏教授作为讲座专家分别就工业源VOCs减排途径与治理技术总体进展、VOCs治理溶剂回收工艺及适用场景、VOCs治理吸附浓缩与热氧化工艺及适用场景三方面内容做专题报告。 2.2 找一个不错的网站 free text to speech","categories":[],"tags":[]},{"title":"","slug":"王阁/自研项目/古诗词竞赛/诗词竞赛业务架构","date":"2023-10-27T06:50:27.036Z","updated":"2023-10-27T06:50:27.036Z","comments":true,"path":"2023/10/27/王阁/自研项目/古诗词竞赛/诗词竞赛业务架构/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E8%87%AA%E7%A0%94%E9%A1%B9%E7%9B%AE/%E5%8F%A4%E8%AF%97%E8%AF%8D%E7%AB%9E%E8%B5%9B/%E8%AF%97%E8%AF%8D%E7%AB%9E%E8%B5%9B%E4%B8%9A%E5%8A%A1%E6%9E%B6%E6%9E%84/","excerpt":"","text":"诗词竞赛业务架构现有业务模块梳理 消费从而生成诗词源 任务生成，并绑定对应的诗词 生成试题 测验 批改 判胜负","categories":[],"tags":[]},{"title":"","slug":"王阁/自研项目/古诗词竞赛/建设思路","date":"2023-10-27T06:50:27.036Z","updated":"2023-10-27T06:50:27.036Z","comments":true,"path":"2023/10/27/王阁/自研项目/古诗词竞赛/建设思路/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E8%87%AA%E7%A0%94%E9%A1%B9%E7%9B%AE/%E5%8F%A4%E8%AF%97%E8%AF%8D%E7%AB%9E%E8%B5%9B/%E5%BB%BA%E8%AE%BE%E6%80%9D%E8%B7%AF/","excerpt":"","text":"古诗词竞赛1一个趣味小游戏，以诗词竞赛为主题，引入竞赛机制。 一、技术架构图示： 涉及技术栈: java spring全家桶 elasticsearch mybaits-plus flink web jsoup kafka 以上主要是一期 二期会主要围绕小端等来进行开发 本应用采用了三层结构：采集层、治理层、应用层。 为此服务体系也采用微服务架构 二、实现思路2.1 采集层通过爬虫服务，抓取相应的诗词数据，并放入暂存层中 2.2 清洗转换层将暂存层中的数据，清洗后、分发至相应的数据库中 2.3 应用层基于抓取的数据进行设计开发web页面 三、开发记录解析步骤1.获取到总页数2.获取a列表3.抓取页面内容 1. class zhuti yuanjiao 获取名称 2. class jjzz 获取年代与作者名 3. class shicineirong 获取诗词内容 生成全局唯一id 数据库自增长序列或字段生成id UUID Redis生成ID zookeeper生成ID Twitter的snowflake算法 诗歌全局唯一id生成策略","categories":[],"tags":[]},{"title":"","slug":"王阁/日常/prepare/数据储备","date":"2023-10-27T06:50:27.035Z","updated":"2023-10-27T06:50:27.035Z","comments":true,"path":"2023/10/27/王阁/日常/prepare/数据储备/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%97%A5%E5%B8%B8/prepare/%E6%95%B0%E6%8D%AE%E5%82%A8%E5%A4%87/","excerpt":"","text":"12采集数据,留作备用 所涉组件 采集 flume logstash beats elasticsearch-dump 传输 kafka flume 存储 elasticsearch hbase hdfs step one : 采集数据到elasticsearch","categories":[],"tags":[]},{"title":"","slug":"王阁/日常/运维/tmux上手","date":"2023-10-27T06:50:27.035Z","updated":"2023-10-27T06:50:27.035Z","comments":true,"path":"2023/10/27/王阁/日常/运维/tmux上手/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%97%A5%E5%B8%B8/%E8%BF%90%E7%BB%B4/tmux%E4%B8%8A%E6%89%8B/","excerpt":"","text":"使用tmuxmac os 通过 brew install tmux 1tmux 接收快捷键的指令是 ^ + B 即mac中的control + B 窗格的操作 12这些操作都是通过 ^+b 来接收 操作符 作用 % 左右创建两个窗格 ‘’ 左右创建两个窗格 x 关闭当前窗格 { 前移当前窗格 } 后移当前窗格 ; 选择上次用的窗格 o 选择下一个窗格 space 切换窗格布局 z 放大窗格 q 显示序号","categories":[],"tags":[]},{"title":"","slug":"王阁/日常/运维/about云账号密码","date":"2023-10-27T06:50:27.035Z","updated":"2023-10-27T06:50:27.035Z","comments":true,"path":"2023/10/27/王阁/日常/运维/about云账号密码/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%97%A5%E5%B8%B8/%E8%BF%90%E7%BB%B4/about%E4%BA%91%E8%B4%A6%E5%8F%B7%E5%AF%86%E7%A0%81/","excerpt":"","text":"account: wqkenqingpassword:125323wkQ$","categories":[],"tags":[]},{"title":"","slug":"王阁/日常/运维/京东云","date":"2023-10-27T06:50:27.035Z","updated":"2023-10-27T06:50:27.035Z","comments":true,"path":"2023/10/27/王阁/日常/运维/京东云/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%97%A5%E5%B8%B8/%E8%BF%90%E7%BB%B4/%E4%BA%AC%E4%B8%9C%E4%BA%91/","excerpt":"","text":"114.67.107.246wangkuiqingeExfeTmKkgRJ255SRRZq","categories":[],"tags":[]},{"title":"","slug":"王阁/日常/运维/博客不同主题切换","date":"2023-10-27T06:50:27.035Z","updated":"2023-11-18T07:18:49.960Z","comments":true,"path":"2023/10/27/王阁/日常/运维/博客不同主题切换/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%97%A5%E5%B8%B8/%E8%BF%90%E7%BB%B4/%E5%8D%9A%E5%AE%A2%E4%B8%8D%E5%90%8C%E4%B8%BB%E9%A2%98%E5%88%87%E6%8D%A2/","excerpt":"","text":"12这里的背景是针对博客不同项目生成不同主题类目的方案 step one 更换wqkenqing.github.io下的github项目","categories":[],"tags":[]},{"title":"","slug":"王阁/日常/运维/jumpserver","date":"2023-10-27T06:50:27.035Z","updated":"2023-10-27T06:50:27.035Z","comments":true,"path":"2023/10/27/王阁/日常/运维/jumpserver/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%97%A5%E5%B8%B8/%E8%BF%90%E7%BB%B4/jumpserver/","excerpt":"title: jumpserver体验 date: 2020-07-28 tags: [jumpserver,资产管理]","text":"title: jumpserver体验 date: 2020-07-28 tags: [jumpserver,资产管理] # jumpserver体验 ## 一体化安装 mysql -uroot -e “create database jumpserver default charset ‘utf8’; grant all on jumpserver.* to ‘jumpserver‘@’127.0.0.1’ identified by ‘$DB_PASSWORD’; flush privileges;” m6GgXgemmBw5B7om9uYqFSz3erVo0JCLO7Trxpo8S2bxazXk2Y Vv42qLmxsEPUO4Kf xLnAOS5Mc8cqRZ8QDxUzVQvZgxIRnJl73c0rE56WCSq8tYZQFG ETNhIk9zu3fgN2pG","categories":[],"tags":[]},{"title":"操作记录","slug":"王阁/日常/运维/操作记录","date":"2023-10-27T06:50:27.035Z","updated":"2023-10-27T06:50:27.035Z","comments":true,"path":"2023/10/27/王阁/日常/运维/操作记录/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%97%A5%E5%B8%B8/%E8%BF%90%E7%BB%B4/%E6%93%8D%E4%BD%9C%E8%AE%B0%E5%BD%95/","excerpt":"","text":"docker run –name fid_state_new -d -v &#x2F;data&#x2F;query_sql:&#x2F;query –add-host data1:10.20.10.136 –add-host data2:10.20.10.137 –add-host data3:10.20.10.138 registry.lisong.pub:28500&#x2F;toll_job_tmp &#x2F;query&#x2F;fid_state_tmp.sql 30docker run –name fid_state_new -d -v &#x2F;data&#x2F;query_sql:&#x2F;query –add-host data1:10.20.10.136 –add-host data2:10.20.10.137 –add-host data3:10.20.10.138 registry.lisong.pub:28500&#x2F;toll_state_new &#x2F;query&#x2F;fid_state_tmp.sql 30 docker run -v &#x2F;data&#x2F;query_sql:&#x2F;query –add-host data1:10.20.10.136 –add-host data2:10.20.10.137 –add-host data3:10.20.10.138 registry.lisong.pub:28500&#x2F;toll_job_state_tmp &#x2F;query&#x2F;fid_state_tmp.sql 69641199 kafka-console-consumer.sh –bootstrap-server data1:9092 –property print.key&#x3D;true –topic zhsd_real –partition&#x3D;0 –offset 69640199 &gt;real.log kafka-consumer-groups.sh –bootstrap-server data1:9092 –group jzwsd-camera-real-time-forecast-a –describe c9656300807b docker run -d -v &#x2F;data&#x2F;query_sql:&#x2F;query –add-host data1:10.20.10.136 –add-host data2:10.20.10.137 –add-host data3:10.20.10.138 registry.lisong.pub:28500&#x2F;toll_job_tmp &#x2F;query&#x2F;fid_info.sql 30 select * from Biz_Roadstae where FID &gt; 186681323 ssh -o ServerAliveInterval&#x3D;59 -NfL 0.0.0.0:9092:data1:9092 jzw ssh -o ServerAliveInterval&#x3D;59 -NfL 0.0.0.0:2181:data1:2181 jzw kafka-run-class.sh kafka.tools.GetOaffsetShell –broker-list localhost:9092 –partitions 0 –topic zhsd_real –time -1 71429512 kafka-console-consumer.sh –bootstrap-server data1:9092 –property print.key&#x3D;true –topic zhsd_real –partition&#x3D;0 –offset 71409512 &gt;real.log kafka-console-consumer.sh –bootstrap-server data1:9092 –from-beginning –topic jzw_toll_island_state &gt;state.txt kafka-run-class.sh kafka.tools.GetOaffsetShell –broker-list data1:9092 –topic ws_hy_mock_data_five_minute –time -1 PUT &#x2F;_snapshot&#x2F;my_backup{ “type” : “fs”, “settings”: { “location”: “&#x2F;data&#x2F;snapshot&#x2F;backup1” “max_snapshot_bytes_per_sec” : “50mb”, “max_restore_bytes_per_sec” :”50mb”, “compress”:true} }} curl -XPUT ‘http://192.168.5.185:9200/_snapshot/el_bak‘ -d ‘{“type”: “fs”,”settings”: {“location”:”&#x2F;usr&#x2F;hadoop&#x2F;application&#x2F;el_bak”,”max_snapshot_bytes_per_sec” : “50mb”, “max_restore_bytes_per_sec” :”50mb”,”compress”:true}}’ ————————————————版权声明：本文为CSDN博主「warrah」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/warrah/article/details/78978244 plugin install elasticsearch&#x2F;elasticsearch-repository-hdfs&#x2F;6.7.2 POST &#x2F;hy_mock_data_five_minute&#x2F;_delete_by_query{ “query”: { “bool”: { “must”: [ { “range”: { “rangeBegin”: { “gte”: “2020-04-18 00:00:00”, “lt”: “2020-04-22 00:00:00” } } } ] } }} 同步hy_mock_data elasticdump –input&#x3D;http://namenode2:9200/hy_mock_data_five_minute –output&#x3D;http://yd_data1:9200/hy_mock_data_five_minute_new –type&#x3D;settingselasticdump –input&#x3D;http://namenode2:9200/hy_mock_data_five_minute –output&#x3D;http://yd_data1:9200/hy_mock_data_five_minute_new –type&#x3D;mappingelasticdump –input&#x3D;http://namenode2:9200/hy_mock_data_five_minute –output&#x3D;http://yd_data1:9200/hy_mock_data_five_minute_new –type&#x3D;data hy_traffic_baidu_five_minute elasticdump –input&#x3D;http://namenode2:9200/hy_traffic_baidu_five_minute –output&#x3D;http://yd_data1:9200/hy_traffic_baidu_five_minute_new1 –type&#x3D;settingselasticdump –input&#x3D;http://namenode2:9200/hy_traffic_baidu_five_minute –output&#x3D;http://yd_data1:9200/hy_traffic_baidu_five_minute_new1 –type&#x3D;mappingelasticdump –input&#x3D;http://namenode2:9200/hy_traffic_baidu_five_minute –output&#x3D;http://yd_data1:9200/hy_traffic_baidu_five_minute_new1 –type&#x3D;data county_consumption_city hotel_tourist_source 不一致 tourist_local_datatourist_source_provtourist_source_countrycounty_consumptioncounty_consumption_provincehotel_checkin_record tourist_passenger_tickettourist_minute_local_data industry_real-weather_data 26866environment_bureau-data 12749 ddns docker run -d –restart&#x3D;always –name ddns-aliyun -e “AKID&#x3D;LTAI4GBKPp17u31LThucvsab” -e “AKSCT&#x3D;mmR4XXXKC1QG9KeiHCAuWktMq50UyF” -e “DOMAIN&#x3D;home.kuiq.wang” -e “REDO&#x3D;600” chenhw2&#x2F;aliyun-ddns-cli curl -X GET http://jd_cloud:9200/_search?v county_consumption_citycounty_industry_consumptioncounty_consumption_provincecounty_consumptioncounty_business_circle_consumption tourist_local_datatourist_source_provtourist_source_countrytourist_source_city returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String.class, Integer.class)).groupBy(0).sum(1) 2020-07-29 flink run -c org.example.stream.SocketTextStreamWordCount &#x2F;Users&#x2F;wqkenqing&#x2F;Desktop&#x2F;gitfiles&#x2F;flink-demo&#x2F;target&#x2F;original-flink-demo-1.0-SNAPSHOT.jar 127.0.0.1 9000flink run -c org.example.stream.SocketTextStreamWordCount original-flink-demo-1.0-SNAPSHOT.jar 127.0.0.1 9000 &#x2F;Users&#x2F;wqkenqing&#x2F;Desktop&#x2F;gitfiles&#x2F;flink-demo&#x2F;target&#x2F;original-flink-demo-1.0-SNAPSHOT.jar docker run -d –restart&#x3D;always –name ddns-aliyun -e “AKID&#x3D;LTAI4GBKPp17u31LThucvsab” -e “AKSCT&#x3D;mmR4XXXKC1QG9KeiHCAuWktMq50UyF” -e “DOMAIN&#x3D;home.kuiq.wang” -e “REDO&#x3D;600” chenhw2&#x2F;aliyun-ddns-cli LTAI4GBKPp17u31LThucvsab","categories":[],"tags":[]},{"title":"","slug":"王阁/日常/运维/运维操作","date":"2023-10-27T06:50:27.035Z","updated":"2023-10-27T06:50:27.036Z","comments":true,"path":"2023/10/27/王阁/日常/运维/运维操作/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%97%A5%E5%B8%B8/%E8%BF%90%E7%BB%B4/%E8%BF%90%E7%BB%B4%E6%93%8D%E4%BD%9C/","excerpt":"","text":"Q: 更换ubuntu安装源cp &#x2F;source&#x2F;apt&#x2F;source.plist &#x2F;source&#x2F;apt&#x2F;source.list.bak vim &#x2F;source&#x2F;apt&#x2F;source.plist 添加国内源. Q: mac 查看目前哪些进程占用哪些端口 lsof -nP | grep TCP | grep LISTEN lsof -i :TCP","categories":[],"tags":[]},{"title":"","slug":"王阁/日常/blog_sync_project/plan","date":"2023-10-27T06:50:27.034Z","updated":"2023-10-27T06:50:27.035Z","comments":true,"path":"2023/10/27/王阁/日常/blog_sync_project/plan/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%97%A5%E5%B8%B8/blog_sync_project/plan/","excerpt":"","text":"about two git project one hexo .this project is used to deploy the files; two : dayliy_doc so I just divide the project about these parts pull files from github list the files; choose the files to sync. push and deploy;","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/项目/登陆与sso/login&sso","date":"2023-10-27T06:50:27.034Z","updated":"2023-10-27T06:50:27.034Z","comments":true,"path":"2023/10/27/王阁/技术/项目/登陆与sso/login&sso/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E9%A1%B9%E7%9B%AE/%E7%99%BB%E9%99%86%E4%B8%8Esso/login&sso/","excerpt":"","text":"结合现有的服务，需要设计一个login与sso页面的功能","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/项目/开发过程/curl 'httpsapi","date":"2023-10-27T06:50:27.034Z","updated":"2023-10-27T06:50:27.034Z","comments":true,"path":"2023/10/27/王阁/技术/项目/开发过程/curl 'httpsapi/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E9%A1%B9%E7%9B%AE/%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B/curl%20'httpsapi/","excerpt":"","text":"123curl &#x27;https://api.notion.com/v1/users&#x27; \\ -H &#x27;Authorization: Bearer &quot;secret_zfm2jh7s9oLGLGPDaZikI3SkN1xxV9TtHVbp1trM4nj&quot;&#x27; \\ -H &quot;Notion-Version: 2022-06-28&quot; 123curl &#x27;https://api.notion.com/v1/users&#x27; \\ -H &#x27;Authorization: Bearer &#x27;&quot;secret_zfm2jh7s9oLGLGPDaZikI3SkN1xxV9TtHVbp1trM4nj&quot;&#x27;&#x27; \\ -H &quot;Notion-Version: 2022-06-28&quot; 123curl &#x27;https://api.notion.com/v1/pages/Python-655a71827c5d47c787b160ddef220eaf&#x27; \\ -H &#x27;Notion-Version: 2022-06-28&#x27; \\ -H &#x27;Authorization: Bearer &#x27;&quot;secret_zfm2jh7s9oLGLGPDaZikI3SkN1xxV9TtHVbp1trM4nj&quot;&#x27;&#x27; 123curl &#x27;https://api.notion.com/v1/users/me&#x27; \\ -H &#x27;Authorization: Bearer &#x27;&quot;secret_zfm2jh7s9oLGLGPDaZikI3SkN1xxV9TtHVbp1trM4nj&quot;&#x27;&#x27; \\ -H &quot;Notion-Version: 2022-06-28&quot; \\ 123curl &#x27;https://api.notion.com/v1/pages/0c1e8114feb246a6946d357f9f2073e4&#x27; \\ -H &#x27;Notion-Version: 2022-06-28&#x27; \\ -H &#x27;Authorization: Bearer &#x27;&quot;secret_zfm2jh7s9oLGLGPDaZikI3SkN1xxV9TtHVbp1trM4nj&quot;&#x27;&#x27; 现有流程是： 业主提供文件 我们收到文件进行分析 摘取文件内容，选择我们需要的字段。 将摘取出的字段与我们大屏用应层的字段信息进行匹配 完成导入 期望流程是： 我们提供导入标准模版 业主按标准模版摘录导入文件 将整理好的标准数据文件导入 接口采集现有流程: 业主提供采集接口协议文档 分析采集文档或与对方开发人员进行沟通 开发与调试 发布服务，开始采集 接口采集期望流程: 我们提供标准的采集文档，约定相关规则 对方按规定供相应的采集服务接口 对接与调度 发布服务，开始采集","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/项目/开发过程/川藏会议","date":"2023-10-27T06:50:27.034Z","updated":"2023-10-27T06:50:27.034Z","comments":true,"path":"2023/10/27/王阁/技术/项目/开发过程/川藏会议/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E9%A1%B9%E7%9B%AE/%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B/%E5%B7%9D%E8%97%8F%E4%BC%9A%E8%AE%AE/","excerpt":"","text":"起始里程、截止里程、公里数是保密信息 风险源有类型，涉及操作人可能不同 风险等级判定，根据标准来判定 标段里有多少高中低风险源，要显示出来 核心功能：风险源的追踪（产生、消失） 风险巡查: 多少产生、多少消失 只说明问题，不解决问题 屏蔽项目新增功能，只专注川藏 做好大屏，涉及脸面 做好培训 保险模块投保管理&#x3D;&gt;投保信息管理 理赔是由保险公司来实现，本系统主要展示结果。 收集资料，上传给保险公司 追踪进度与结果 现在与保险公司的交互方式 现有的是通过VPN的方式来实现的 保险统计分析功能 什么时候发生、什么时候结案 对应的资料（尽能否跟保险公司打通材料上传） 投保信息要有 服务器 我司服务器 上云 自建","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/项目/开发过程/数据基础组件安全升级方案","date":"2023-10-27T06:50:27.034Z","updated":"2023-10-27T06:50:27.034Z","comments":true,"path":"2023/10/27/王阁/技术/项目/开发过程/数据基础组件安全升级方案/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E9%A1%B9%E7%9B%AE/%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B/%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6%E5%AE%89%E5%85%A8%E5%8D%87%E7%BA%A7%E6%96%B9%E6%A1%88/","excerpt":"","text":"组件安全升级 洪雅项目发生了elaticsearch被黑客入侵事故，事后分析，主要跟我们内部安全策略不强也有关系，接下来对相关组件进行安全升级 1 问题分析1.1 因为elasticsearch默认是9200、9300的端口开放","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/项目/开发过程/测试计划","date":"2023-10-27T06:50:27.034Z","updated":"2023-10-27T06:50:27.034Z","comments":true,"path":"2023/10/27/王阁/技术/项目/开发过程/测试计划/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E9%A1%B9%E7%9B%AE/%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B/%E6%B5%8B%E8%AF%95%E8%AE%A1%E5%88%92/","excerpt":"","text":"按模版调整","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/项目/开发过程/片段","date":"2023-10-27T06:50:27.034Z","updated":"2023-10-27T06:50:27.034Z","comments":true,"path":"2023/10/27/王阁/技术/项目/开发过程/片段/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E9%A1%B9%E7%9B%AE/%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B/%E7%89%87%E6%AE%B5/","excerpt":"","text":"成都旸谷 12345678项目内容: 数据基础服务平台、数据运维平台本项目是结合我司众多项目的研发过程中，归纳相关需求，内部孵化的自研产品，能快速的为项目提供基础服务，加快项目的推进进程，从而更快产生价值。主要的功能包含数据地图、数据集成(数据采集、数据同步)、元数据管理、数据治理、调度系统、数据应用（数据服务、数据图表、报表）等核心功能。该项目较完整的包含了数据流过程中的功能，每部份功能都能在项目中提供具体的功能。如数据采集、数据同步等，通过相关配置，我们即可快速的完成主流的采集协议的采集通道构建。数据同步我们已经支持较多组件之前的同步和多种策略的同步，并在不断的完善中。元数据管理，我们基于atlas进行二次开发，在此基础上完成了我们主要的组件的元数据的采集，粒度细致到字段级，并查相关血缘信息。数据治理，基础海豚调度加flink、spark、hive、clickhouse等组件，完成了一些常见内置治理规则的治理，同时还支持上传udf包，进行自定义规则处理。我的职责1. 担任本项目的团队sm，主导项目的推进与孵化2. 主力架构与拆分项目的主体的功能。3. 管理数据运维平台，保障各基础组件正常运行。 浩海通达 123456项目内容:通过jt807、jt808等协议对平台多个客户方的近千量车辆的轨迹采集，业务数据量级日均百G级，并进行清洗,处理.存储.然后再结合业务数据,进行相同起始点的耗时对比，线路运费对比等业务分析，线路故障信息分析等,从而进行流程优化，线路推荐等。针对积累的数据，进行聚合分析，反馈给司机端，以供司机选择最优路径。以报表形式反馈给B端，以供企业做成本分析，与商业决策。我的职责1. 主导数据中心的建设，为团队提供大数据存储、处理的能力2. 主导JT807、JT808协议解析的数据采集服务的构建、提供稳定，高效的数据采集服务。3. 主力开发相关数据处理任务，如轨迹数据的异常点清洗，轨迹纠偏，轨迹稀疏，轨迹对比等任务开发。 海航 12345项目内容:海航hiapp (MTS app)项目，是一款涉及航旅、住宿、美食等ToC应用服务，承载了海航千万级注册用户，日活过百万。我们的工作职责是以app用户登录为入口产生的行为数据流进行存储、处理、分析、应用等数据流处理。涉及到较大数据量的实时数据流的处理，和丰富的业务应用，如面向运营部门的报表业务、面向AI对话机器人的模型治理数据、推荐中心的推荐模型数据等。我的职责:1. 部份模块的任务开发，如用户埋点日志信息流处理、对话数据的NLP切词流任务、主流网站的出行旅游游记爬虫任务等2. 部份数据报表服务的开发实现. 摩森特 12345针对公司的数百T的存量文本数据反复进行切词、聚类、分析等处理与挖掘，在此基础开发出了公司的DMP平台，主要的核心功能有多标签的用户画像、多主题的数据仓库、脱敏的数据服务等。为公司带来可观的经济增值。主要职责:1.数据集群各组件搭建2.批处理任务的开发，如文本信息的切词、聚类任务等，数据仓库入库数据的清洗等和一些Hive的UDF包开发3.DMP的部份模块开发如数据仓库即席查询服务，用户画像功能等。","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/项目/开发过程/王奎清的发言","date":"2023-10-27T06:50:27.034Z","updated":"2023-10-27T06:50:27.034Z","comments":true,"path":"2023/10/27/王阁/技术/项目/开发过程/王奎清的发言/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E9%A1%B9%E7%9B%AE/%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B/%E7%8E%8B%E5%A5%8E%E6%B8%85%E7%9A%84%E5%8F%91%E8%A8%80/","excerpt":"","text":"大家好，我是王奎清。入职旸谷四年多时间，司职研发部，从事大数据开发，运维等工作。此时此刻想说的话，有很多，但汇集起来，出现的是这四年多来的点点片段，如青岛胶州湾半岛上满山的癞蛤蟆，洪雅办事处因雨季湿润的床铺，汉口漏水的机房、济泺路现场路边的露天厕所，同时当然还有数不清楚的需求迭代、多个项目的紧急响应的夜晚，一幕幕深刻的回忆也都随之回显在我的脑海里。 回顾这些年，我对旸谷精神“危机与奋斗、梦想与坚持、读书与锻炼” ，在经历了这些年作为旸谷人实际的工作与生活，深以为然，感到深深的认同感。在此祝旸谷六周年生日快乐、未来朝朝日上 6.17号之后没有更新的数据 IPGB UPS 车检器数据 (topic2) 情报板数据()","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/项目/开发过程/测试报告修改意见","date":"2023-10-27T06:50:27.034Z","updated":"2023-10-27T06:50:27.034Z","comments":true,"path":"2023/10/27/王阁/技术/项目/开发过程/测试报告修改意见/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E9%A1%B9%E7%9B%AE/%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B/%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A%E4%BF%AE%E6%94%B9%E6%84%8F%E8%A7%81/","excerpt":"","text":"测试结果 增加对软件的评估 4、详细测试结果把测试用例按结果分类编写 测试用例 按模块进行分类 用例名称-&gt;更改成一个测试方法类型 测试记录评价缺陷与限制细分一下 测试建议修改测试建议 结论","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/项目/README","date":"2023-10-27T06:50:27.033Z","updated":"2023-10-27T06:50:27.033Z","comments":true,"path":"2023/10/27/王阁/技术/项目/README/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E9%A1%B9%E7%9B%AE/README/","excerpt":"","text":"项目说明文档","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/运维/docker/docker操作","date":"2023-10-27T06:50:27.032Z","updated":"2023-10-27T06:50:27.032Z","comments":true,"path":"2023/10/27/王阁/技术/运维/docker/docker操作/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/docker/docker%E6%93%8D%E4%BD%9C/","excerpt":"","text":"docker run -d –restart&#x3D;unless-stopped -v &#x2F;data&#x2F;upload&#x2F;newInfo.txt:&#x2F;newInfo.txt registry.lisong.pub:5000&#x2F;kafka_tmpdocker run -d –restart&#x3D;unless-stopped -v &#x2F;Users&#x2F;wqkenqing&#x2F;Desktop&#x2F;newInfo.txt:&#x2F;newInfo.txt kafka_tmpdocker run -d –restart&#x3D;unless-stopped -v &#x2F;data&#x2F;upload&#x2F;newInfo.txt:&#x2F;newInfo.txt registry.lisong.pub:5000&#x2F;kafka_tmpdocker run -d –restart&#x3D;unless-stopped -v &#x2F;data&#x2F;upload&#x2F;state_info.txt:&#x2F;state_info.txt registry.lisong.pub:5000&#x2F;kafka_tmpndocker run -d –restart&#x3D;unless-stopped -v &#x2F;data&#x2F;upload&#x2F;newInfo.txt:&#x2F;newInfo.txt registry.lisong.pub:5000&#x2F;kafka_tmpdocker run -d –restart&#x3D;unless-stopped -v &#x2F;data&#x2F;upload&#x2F;newInfo.txt:&#x2F;info.txt registry.lisong.pub:5000&#x2F;kafka_all &#x2F;info.txt jzw_toll_island_infonn docker run -d –restart&#x3D;unless-stopped -v &#x2F;Users&#x2F;wqkenqing&#x2F;Desktop&#x2F;newInfo.txt:&#x2F;info.txt kafka_all &#x2F;info.txt jzw_toll_island_infonndocker run -v &#x2F;Users&#x2F;wqkenqing&#x2F;Desktop&#x2F;newInfo.txt:&#x2F;info.txt kafka_all &#x2F;info.txt jzw_toll_island_infonndocker run -d –restart&#x3D;unless-stopped -v &#x2F;data&#x2F;upload&#x2F;newInfo.txt:&#x2F;info.txt registry.lisong.pub:5000&#x2F;kafka_all &#x2F;info.txt jzw_toll_island_infonndocker run -d –restart&#x3D;unless-stopped -v &#x2F;data&#x2F;upload&#x2F;state_info.txt:&#x2F;info.txt registry.lisong.pub:5000&#x2F;kafka_all &#x2F;info.txt jzw_toll_island_statedocker run -d –restart&#x3D;unless-stopped -v &#x2F;data&#x2F;upload&#x2F;newInfo.txt:&#x2F;info.txt registry.lisong.pub:5000&#x2F;kafka214 &#x2F;info.txt jzw_toll_island_infonndocker run -d -v &#x2F;Users&#x2F;wqkenqing&#x2F;Desktop&#x2F;newInfo.txt:&#x2F;info.txt kafk214 &#x2F;info.txt jzw_toll_island_infonn 214 docker run -d –name info –restart&#x3D;unless-stopped -v &#x2F;data&#x2F;upload&#x2F;newInfo.txt:&#x2F;info.txt registry.lisong.pub:5000&#x2F;kafka214 &#x2F;info.txt jzw_toll_island_infonn docker run -d –name info –restart&#x3D;unless-stopped -v &#x2F;data&#x2F;upload&#x2F;newInfo.txt:&#x2F;info.txt registry.lisong.pub:5000&#x2F;kafka214 &#x2F;info.txt jzw_toll_island_info docker run -d –name state –restart&#x3D;unless-stopped -v &#x2F;data&#x2F;upload&#x2F;state_info.txt:&#x2F;info.txt registry.lisong.pub:5000&#x2F;kafka214 &#x2F;info.txt jzw_toll_island_state","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/运维/docker/docker","date":"2023-10-27T06:50:27.032Z","updated":"2023-10-27T06:50:27.032Z","comments":true,"path":"2023/10/27/王阁/技术/运维/docker/docker/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/docker/docker/","excerpt":"","text":"docker 积累常用操作命令docker pull XXXX docker search nginx 搜索镜像 docker pull nginx 拉取镜像 docker images 查看镜像 docker save XXXX 导出镜象 Dockerfile1Dockerfile 的组成 与常用编辑形式","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/运维/flinkSql/flinksql","date":"2023-10-27T06:50:27.032Z","updated":"2023-10-27T06:50:27.032Z","comments":true,"path":"2023/10/27/王阁/技术/运维/flinkSql/flinksql/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/flinkSql/flinksql/","excerpt":"","text":"12345678910111213141516171819202122CREATE TABLE mqtt_cmd ( `value` INT, `uuid` STRING, `username` STRING, `uid` STRING, `topic` STRING, `timestamp` BIGINT, `priority_2` INT, `priority_1` INT, `name` STRING, `dataType` STRING) WITH ( &#x27;connector&#x27; = &#x27;kafka&#x27;, &#x27;topic&#x27; = &#x27;mqtt_cmd&#x27;, &#x27;properties.bootstrap.servers&#x27; = &#x27;kafka01:9092,kafka02:9092,kafka03:9092&#x27;, &#x27;properties.group.id&#x27; = &#x27;cmd01&#x27;, &#x27;format&#x27; = &#x27;json&#x27;, &#x27;json.fail-on-missing-field&#x27; = &#x27;false&#x27;, &#x27;scan.startup.mode&#x27; = &#x27;earliest-offset&#x27;, &#x27;json.ignore-parse-errors&#x27; = &#x27;true&#x27;); 12345678910111213141516CREATE TABLE mqtt_resp ( uuid STRING, username STRING, uid STRING, topic_req STRING, error INT) WITH ( &#x27;connector&#x27; = &#x27;kafka&#x27;, &#x27;topic&#x27; = &#x27;mqtt_resp&#x27;, &#x27;properties.bootstrap.servers&#x27; = &#x27;kafka01:9092,kafka02:9092,kafka03:9092&#x27;, &#x27;properties.group.id&#x27; = &#x27;cmd01&#x27;, &#x27;format&#x27; = &#x27;json&#x27;, &#x27;json.fail-on-missing-field&#x27; = &#x27;false&#x27;, &#x27;scan.startup.mode&#x27; = &#x27;earliest-offset&#x27;, &#x27;json.ignore-parse-errors&#x27; = &#x27;true&#x27;);","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/运维/docker/docker镜像","date":"2023-10-27T06:50:27.032Z","updated":"2023-10-27T06:50:27.032Z","comments":true,"path":"2023/10/27/王阁/技术/运维/docker/docker镜像/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/docker/docker%E9%95%9C%E5%83%8F/","excerpt":"","text":"docker 镜像http://hub-mirror.c.163.com https://reg-mirror.qiniu.com https://dockerhub.azk8s.cn my mirror on aliyunregistry.cn-chengdu.aliyuncs.com&#x2F;kuiq_wang&#x2F;daily companyregistry.lisong.pub:5000registry.lisong.pub:28500 { “registry-mirrors”: [“","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/运维/front_end/nginx部署","date":"2023-10-27T06:50:27.032Z","updated":"2023-10-27T06:50:27.032Z","comments":true,"path":"2023/10/27/王阁/技术/运维/front_end/nginx部署/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/front_end/nginx%E9%83%A8%E7%BD%B2/","excerpt":"","text":"server { listen 8089; server_name ; &#x2F;&#x2F; 你的域名或者 ip root &#x2F;root&#x2F;H-ui.admin; &#x2F;&#x2F; 你的克隆到的项目路径 index index.html; &#x2F;&#x2F; 显示首页 location ~* ^.+.(jpg|jpeg|gif|png|ico|css|js|pdf|txt){ root &#x2F;&#x2F;root&#x2F;H-ui.admin; } &#x2F;&#x2F; 静态文件访问} server { listen 9001; server_name localhost; location &#x2F; { root &#x2F;root&#x2F;node-project&#x2F;pm2test; #index index.html index.htm; }} For more information on configuration, see:* Official English Documentation: http://nginx.org/en/docs/* Official Russian Documentation: http://nginx.org/ru/docs/user root;worker_processes auto;error_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;error.log;pid &#x2F;run&#x2F;nginx.pid; Load dynamic modules. See &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;README.dynamic.include &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;modules&#x2F;*.conf; events { worker_connections 1024;} http { log_format main ‘$remote_addr - $remote_user [$time_local] “$request” ‘ ‘$status $body_bytes_sent “$http_referer” ‘ ‘“$http_user_agent” “$http_x_forwarded_for”‘; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; # Load modular configuration files from the /etc/nginx/conf.d directory. # See http://nginx.org/en/docs/ngx_core_module.html#include # for more information. #include /etc/nginx/conf.d/*.conf; server &#123; listen 8089; server_name 116.196.81.123; root &#x2F;root&#x2F;H-ui.admin; index index.html; location ~* ^.+.(jpg|jpeg|gif|png|ico|css|js|pdf|txt){ root &#x2F;&#x2F;root&#x2F;H-ui.admin; }} include &#x2F;etc&#x2F;nginx&#x2F;default.d&#x2F;*.conf; Settings for a TLS enabled server.server {listen 443 ssl http2 default_server;listen [::]:443 ssl http2 default_server;server_name _;root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html;ssl_certificate “&#x2F;etc&#x2F;pki&#x2F;nginx&#x2F;server.crt”;ssl_certificate_key “&#x2F;etc&#x2F;pki&#x2F;nginx&#x2F;private&#x2F;server.key”;ssl_session_cache shared:SSL:1m;ssl_session_timeout 10m;ssl_ciphers HIGH:!aNULL:!MD5;ssl_prefer_server_ciphers on;# Load configuration files for the default server block.include &#x2F;etc&#x2F;nginx&#x2F;default.d&#x2F;*.conf;location &#x2F; {}error_page 404 &#x2F;404.html;location &#x3D; &#x2F;40x.html {}error_page 500 502 503 504 &#x2F;50x.html;location &#x3D; &#x2F;50x.html {}}}","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/运维/owncloud/账号","date":"2023-10-27T06:50:27.032Z","updated":"2023-10-27T06:50:27.032Z","comments":true,"path":"2023/10/27/王阁/技术/运维/owncloud/账号/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/owncloud/%E8%B4%A6%E5%8F%B7/","excerpt":"","text":"账号管理 账号 密码 home home123 kuiq.wang","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/运维/yd_server_config/README","date":"2023-10-27T06:50:27.032Z","updated":"2023-10-27T06:50:27.033Z","comments":true,"path":"2023/10/27/王阁/技术/运维/yd_server_config/README/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/yd_server_config/README/","excerpt":"","text":"记录移动云服务器上部署的服务 服务名 说明 sjksh_store_info 数据可视化模块的存储信息查询服务","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/运维/front_end/调研","date":"2023-10-27T06:50:27.032Z","updated":"2023-10-27T06:50:27.032Z","comments":true,"path":"2023/10/27/王阁/技术/运维/front_end/调研/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/front_end/%E8%B0%83%E7%A0%94/","excerpt":"","text":"调研几款好用的前端框架与架构 H-ui 早期接触过 kafka管理","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/运维/ubuntu/linux自启服务","date":"2023-10-27T06:50:27.032Z","updated":"2023-10-27T06:50:27.032Z","comments":true,"path":"2023/10/27/王阁/技术/运维/ubuntu/linux自启服务/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/ubuntu/linux%E8%87%AA%E5%90%AF%E6%9C%8D%E5%8A%A1/","excerpt":"","text":"12linux自启服务 ubuntu","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/运维/ubuntu/desk_gui","date":"2023-10-27T06:50:27.032Z","updated":"2023-10-27T06:50:27.032Z","comments":true,"path":"2023/10/27/王阁/技术/运维/ubuntu/desk_gui/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/ubuntu/desk_gui/","excerpt":"","text":"安装： sudo apt-get install vnc4server xfce4 vncserver -geometry 1280x800 -alwaysshared :1 vncserver -kill :1 #!&#x2F;bin&#x2F;sh Uncomment the following two lines for normal desktop:#unset SESSION_MANAGER#exec &#x2F;etc&#x2F;X11&#x2F;xinit&#x2F;xinitrc #[ -x &#x2F;etc&#x2F;vnc&#x2F;xstartup ] &amp;&amp; exec &#x2F;etc&#x2F;vnc&#x2F;xstartup#[ -r $HOME&#x2F;.Xresources ] &amp;&amp; xrdb $HOME&#x2F;.Xresources#xsetroot -solid grey#vncconfig -iconic &amp;#x-terminal-emulator -geometry 80x24+10+10 -ls -title “$VNCDESKTOP Desktop” &amp;#x-window-manager &amp;unset SESSION_MANAGERunset DBUS_SESSION_BUS_ADDRESS[ -x &#x2F;etc&#x2F;vnc&#x2F;xstartup ] &amp;&amp; exec &#x2F;etc&#x2F;vnc&#x2F;xstartup[ -r $HOME&#x2F;.Xresources ] &amp;&amp; xrdb $HOME&#x2F;.Xresourcesvncconfig -iconic &amp;-session &amp; sudo x11vnc -display :2 -auth ~&#x2F;.vnc&#x2F;passwd -forever -bg -o &#x2F;var&#x2F;log&#x2F;x11vnc.log -rfbauth &#x2F;etc&#x2F;x11vnc.pass -shared -noxdamage -xrandr “resize” -rfbport 5900 sudo x11vnc -forever -bg -usepw -cursor arrow -display :2","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/运维/front_end/接口规划/data-manager","date":"2023-10-27T06:50:27.032Z","updated":"2023-10-27T06:50:27.032Z","comments":true,"path":"2023/10/27/王阁/技术/运维/front_end/接口规划/data-manager/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/front_end/%E6%8E%A5%E5%8F%A3%E8%A7%84%E5%88%92/data-manager/","excerpt":"","text":"kafka添加topic&#x2F;data&#x2F;kafka&#x2F;add_topic 查询toipic &#x2F;data&#x2F;kafka&#x2F;select_topic 删除topic(待定) &#x2F;data&#x2F;kafka&#x2F;delete_topic 工具类 删除 topic List; redis","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/考研/C学习笔记","date":"2023-10-27T06:50:27.031Z","updated":"2023-10-27T06:50:27.031Z","comments":true,"path":"2023/10/27/王阁/技术/考研/C学习笔记/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%80%83%E7%A0%94/C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/运维/cdh/hue","date":"2023-10-27T06:50:27.031Z","updated":"2023-10-27T06:50:27.031Z","comments":true,"path":"2023/10/27/王阁/技术/运维/cdh/hue/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/cdh/hue/","excerpt":"","text":"hue 使用hive 集成1234create table theme_click_log (user_id string , theme_id string,item_id string,leaf_cate_id string,cate_leve1_id string,clk_cnt int,reach_time date) row format delimited fields terminated by &#x27;,&#x27;; 123alter table user_item_purchase_log set serdeproperties(&#x27;field.delim&#x27;=&#x27;,&#x27;); 12345678910create table theme_click_log (user_id string , theme_id string,item_id string,leaf_cate_id string,cate_leve1_id string,clk_cnt int,reach_time date) row format delimited fields terminated by &#x27;,&#x27;;create table theme_item_pool (theme_id string,item_id string) row format delimited fields terminated by &#x27;,&#x27;;create table user_item_purchase_log (user_id string,item_id string,paytime string);create table item_embedding (item_id string,emb string)row format delimited fields terminated by &#x27;,&#x27;;create table user_embedding (user_id string,emb string) row format delimited fields terminated by &#x27;,&#x27;; 12345678910111213create table theme_click_log (user_id string , theme_id string,item_id string,leaf_cate_id string,cate_leve1_id string,clk_cnt int,reach_time date) row format delimited fields terminated by &#x27;,&#x27;;create table theme_item_pool (theme_id string,item_id string) row format delimited fields terminated by &#x27;,&#x27;;create table user_item_purchase_log (user_id string,item_id string,paytime string);create table item_embedding (item_id string,emb string)row format delimited fields terminated by &#x27;,&#x27;;create table user_embedding (user_id string,emb string) row format delimited fields terminated by &#x27;,&#x27;;drop table test;load data local inpath &#x27;/data/upload/item_embedding.csv&#x27; overwrite into table item_embedding;select count(*) from theme_click_log; 12345678910111213141516create table theme_click_log (user_id string , theme_id string,item_id string,leaf_cate_id string,cate_leve1_id string,clk_cnt int,reach_time date) row format delimited fields terminated by &#x27;,&#x27;;create table theme_item_pool (theme_id string,item_id string) row format delimited fields terminated by &#x27;,&#x27;;create table user_item_purchase_log (user_id string,item_id string,paytime string);create table item_embedding (item_id string,emb string)row format delimited fields terminated by &#x27;,&#x27;;create table user_embedding (user_id string,emb string) row format delimited fields terminated by &#x27;,&#x27;;drop table test;load data local inpath &#x27;/data/upload/item_embedding.csv&#x27; overwrite into table item_embedding;select count(*) from item_embedding;load data local inpath &#x27;/data/upload/theme_item_pool.csv&#x27; overwrite into table theme_item_pool;load data local inpath &#x27;/data/upload/user_item_purchase_log.csv&#x27; overwrite into table user_item_purchase_log; 12345create table flight_info (flight_id int,flight_time string,area string,flight_number int,rise_up_airport int,arrive_in_airport int,rise_up_time string,arrive_in_time string,airplane_id int,airplane_type int,importance double)row format delimitedfields terminated by &#x27;\\t&#x27;;","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/运维/docker/Dockerfile基础&编写","date":"2023-10-27T06:50:27.031Z","updated":"2023-10-27T06:50:27.032Z","comments":true,"path":"2023/10/27/王阁/技术/运维/docker/Dockerfile基础&编写/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/docker/Dockerfile%E5%9F%BA%E7%A1%80&%E7%BC%96%E5%86%99/","excerpt":"","text":"Dockerfile基础12Dockerfile用于指定docker build 指令时执行时的动作与资源划分等,如指定分配内存大小,cpu个数等.从而构成相应的镜像 详细指令 如上图即docker主要的指令与作用 Dockerfile由多条指令组成,每条指令在编译镜像时执行相应的程序完成某些功能,指令+参数组成，以逗号分隔，#作为注释起始符，虽说指令不区分大小写，但是一般指令使用大些，参数使用小写. 指令：FROM功能描述：设置基础镜像语法：FROM &lt; image&gt;[:&lt; tag&gt; | @&lt; digest&gt;]提示：镜像都是从一个基础镜像（操作系统或其他镜像）生成，可以在一个Dockerfile中添加多条FROM指令，一次生成多个镜像注意：如果忽略tag选项，会使用latest镜像 指令：MAINTAINER功能描述：设置镜像作者语法：MAINTAINER &lt; name&gt; 指令：RUN功能描述：语法：RUN &lt; command&gt; RUN [“executable”,”param1”,”param2”]提示：RUN指令会生成容器，在容器中执行脚本，容器使用当前镜像，脚本指令完成后，Docker Daemon会将该容器提交为一个中间镜像，供后面的指令使用补充：RUN指令第一种方式为shell方式，使用&#x2F;bin&#x2F;sh -c &lt; command&gt;运行脚本，可以在其中使用\\将脚本分为多行 RUN指令第二种方式为exec方式，镜像中没有&#x2F;bin&#x2F;sh或者要使用其他shell时使用该方式，其不会调用shell命令例子：RUN source $HOME&#x2F;.bashrc; echo $HOME RUN [“/bin/bash”,”-c”,”echo hello”] RUN [“sh”,”-c”,”echo”,”$HOME”] 使用第二种方式调用shell读取环境变量 指令：CMD功能描述：设置容器的启动命令语法：CMD [“executable”,”param1”,”param2”] CMD [“param1”,”param2”] CMD &lt; command&gt;提示：CMD第一种、第三种方式和RUN类似，第二种方式为ENTRYPOINT参数方式，为entrypoint提供参数列表注意：Dockerfile中只能有一条CMD命令，如果写了多条则最后一条生效 指令：LABEL功能描述：设置镜像的标签延伸：镜像标签可以通过docker inspect查看格式：LABEL &lt; key&gt;&#x3D;&lt; value&gt; &lt; key&gt;&#x3D;&lt; value&gt; …提示：不同标签之间通过空格隔开注意：每条指令都会生成一个镜像层，Docker中镜像最多只能有127层，如果超出Docker Daemon就会报错，如LABEL ..&#x3D;.. &lt;假装这里有个换行&gt; LABEL ..&#x3D;..合在一起用空格分隔就可以减少镜像层数量，同样，可以使用连接符\\将脚本分为多行镜像会继承基础镜像中的标签，如果存在同名标签则会覆盖 指令：EXPOSE功能描述：设置镜像暴露端口，记录容器启动时监听哪些端口语法：EXPOSE &lt; port&gt; &lt; port&gt; …延伸：镜像暴露端口可以通过docker inspect查看提示：容器启动时，Docker Daemon会扫描镜像中暴露的端口，如果加入-P参数，Docker Daemon会把镜像中所有暴露端口导出，并为每个暴露端口分配一个随机的主机端口（暴露端口是容器监听端口，主机端口为外部访问容器的端口）注意：EXPOSE只设置暴露端口并不导出端口，只有启动容器时使用-P&#x2F;-p才导出端口，这个时候才能通过外部访问容器提供的服务 指令：ENV功能描述：设置镜像中的环境变量语法：ENV &lt; key&gt;&#x3D;&lt; value&gt;…|&lt; key&gt; &lt; value&gt;注意：环境变量在整个编译周期都有效，第一种方式可设置多个环境变量，第二种方式只设置一个环境变量提示：通过${变量名}或者 $变量名使用变量，使用方式${变量名}时可以用${变量名:-default} ${变量名:+cover}设定默认值或者覆盖值 ENV设置的变量值在整个编译过程中总是保持不变的 指令：ADD功能描述：复制文件到镜像中语法：ADD &lt; src&gt;… &lt; dest&gt;|[“&lt; src&gt;”,… “&lt; dest&gt;”]注意：当路径中有空格时，需要使用第二种方式 当src为文件或目录时，Docker Daemon会从编译目录寻找这些文件或目录，而dest为镜像中的绝对路径或者相对于WORKDIR的路径提示：src为目录时，复制目录中所有内容，包括文件系统的元数据，但不包括目录本身 src为压缩文件，并且压缩方式为gzip,bzip2或xz时，指令会将其解压为目录 如果src为文件，则复制文件和元数据 如果dest不存在，指令会自动创建dest和缺失的上级目录 指令：COPY功能描述：复制文件到镜像中语法：COPY &lt; src&gt;… &lt; dest&gt;|[“&lt; src&gt;”,… “&lt; dest&gt;”]提示：指令逻辑和ADD十分相似，同样Docker Daemon会从编译目录寻找文件或目录，dest为镜像中的绝对路径或者相对于WORKDIR的路径 指令：ENTRYPOINT功能描述：设置容器的入口程序语法：ENTRYPOINT [“executable”,”param1”,”param2”] ENTRYPOINT command param1 param2（shell方式）提示：入口程序是容器启动时执行的程序，docker run中最后的命令将作为参数传递给入口程序 入口程序有两种格式：exec、shell，其中shell使用&#x2F;bin&#x2F;sh -c运行入口程序，此时入口程序不能接收信号量 当Dockerfile有多条ENTRYPOINT时只有最后的ENTRYPOINT指令生效 如果使用脚本作为入口程序，需要保证脚本的最后一个程序能够接收信号量，可以在脚本最后使用exec或gosu启动传入脚本的命令注意：通过shell方式启动入口程序时，会忽略CMD指令和docker run中的参数 为了保证容器能够接受docker stop发送的信号量，需要通过exec启动程序；如果没有加入exec命令，则在启动容器时容器会出现两个进程，并且使用docker stop命令容器无法正常退出（无法接受SIGTERM信号），超时后docker stop发送SIGKILL，强制停止容器例子：FROM ubuntu &lt;换行&gt; ENTRYPOINT exec top -b 指令：VOLUME功能描述：设置容器的挂载点语法：VOLUME [“&#x2F;data”] VOLUME &#x2F;data1 &#x2F;data2提示：启动容器时，Docker Daemon会新建挂载点，并用镜像中的数据初始化挂载点，可以将主机目录或数据卷容器挂载到这些挂载点 指令：USER功能描述：设置RUN CMD ENTRYPOINT的用户名或UID语法：USER &lt; name&gt; 指令：WORKDIR功能描述：设置RUN CMD ENTRYPOINT ADD COPY指令的工作目录语法：WORKDIR &lt; Path&gt;提示：如果工作目录不存在，则Docker Daemon会自动创建 Dockerfile中多个地方都可以调用WORKDIR，如果后面跟的是相对位置，则会跟在上条WORKDIR指定路径后（如WORKDIR &#x2F;A WORKDIR B WORKDIR C，最终路径为&#x2F;A&#x2F;B&#x2F;C 指令：ARG功能描述：设置编译变量语法：ARG &lt; name&gt;[&#x3D;&lt; defaultValue&gt;]注意：ARG从定义它的地方开始生效而不是调用的地方，在ARG之前调用编译变量总为空，在编译镜像时，可以通过docker build –build-arg &lt; var&gt;&#x3D;&lt; value&gt;设置变量，如果var没有通过ARG定义则Daemon会报错 可以使用ENV或ARG设置RUN使用的变量，如果同名则ENV定义的值会覆盖ARG定义的值，与ENV不同，ARG的变量值在编译过程中是可变的，会对比使用编译缓存造成影响（ARG值不同则编译过程也不同）例子：ARG CONT_IMAG_VER &lt;换行&gt; RUN echo $CONT_IMG_VER ARG CONT_IMAG_VER &lt;换行&gt; RUN echo hello 当编译时给ARG变量赋值hello，则两个Dockerfile可以使用相同的中间镜像，如果不为hello，则不能使用同一个中间镜像 CMD ENTRYPOINT和RUN的区别RUN指令是设置编译镜像时执行的脚本和程序，镜像编译完成后，RUN指令的生命周期结束容器启动时，可以通过CMD和ENTRYPOINT设置启动项，其中CMD叫做容器默认启动命令，如果在docker run命令末尾添加command，则会替换镜像中CMD设置的启动程序；ENRTYPOINT叫做入口程序，不能被docker run命令末尾的command替换，而是将command当作字符串，传递给ENTRYPOINT作为参数","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/编程/java/并发编程","date":"2023-10-27T06:50:27.030Z","updated":"2023-10-27T06:50:27.030Z","comments":true,"path":"2023/10/27/王阁/技术/编程/java/并发编程/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E7%BC%96%E7%A8%8B/java/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/编程/scala/scala学习","date":"2023-10-27T06:50:27.030Z","updated":"2023-10-27T06:50:27.030Z","comments":true,"path":"2023/10/27/王阁/技术/编程/scala/scala学习/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E7%BC%96%E7%A8%8B/scala/scala%E5%AD%A6%E4%B9%A0/","excerpt":"title: scala学习 date: 2020-07-23 tags:","text":"title: scala学习 date: 2020-07-23 tags:","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/编程/python/summary/2020.3.17","date":"2023-10-27T06:50:27.030Z","updated":"2023-10-27T06:50:27.030Z","comments":true,"path":"2023/10/27/王阁/技术/编程/python/summary/2020.3.17/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E7%BC%96%E7%A8%8B/python/summary/2020.3.17/","excerpt":"","text":"python summary1the summary of recently knowledge which was about python ! ​ class and method","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/编程/datastructure/未命名","date":"2023-10-27T06:50:27.019Z","updated":"2023-10-27T06:50:27.019Z","comments":true,"path":"2023/10/27/王阁/技术/编程/datastructure/未命名/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E7%BC%96%E7%A8%8B/datastructure/%E6%9C%AA%E5%91%BD%E5%90%8D/","excerpt":"","text":"应用 sxsddsj-auth-center 没有更新，跳过应用 sxsddsj-file-server 更新，版本变更 7331 -&gt; 11168执行 kubectl set image deployments&#x2F;pre-sxsddsj-file-server app&#x3D;registry.lisong.pub:28500&#x2F;sxsddsj&#x2F;sxsddsj-file-server:v11168deployment.apps&#x2F;pre-sxsddsj-file-server image updated应用 sxsddsj-main 更新，版本变更 11025 -&gt; 11193执行 kubectl set image deployments&#x2F;pre-sxsddsj-main app&#x3D;registry.lisong.pub:28500&#x2F;sxsddsj&#x2F;main:v11193deployment.apps&#x2F;pre-sxsddsj-main image updated 应用 sxsddsj_web 更新，版本变更 11036 -&gt; 11198执行 kubectl set image deployments&#x2F;pre-sxsddsj-web webapp&#x3D;registry.lisong.pub:28500&#x2F;sxsddsj-fe&#x2F;sxsddsj_web:v11198deployment.apps&#x2F;pre-sxsddsj-web image updated应用 sxsddsj_big_screen 更新，版本变更 10931 -&gt; 11163执行 kubectl set image deployments&#x2F;pre-sxsddsj-big-web webapp&#x3D;registry.lisong.pub:28500&#x2F;sxsddsj-fe&#x2F;sxsddsj_big_screen:v11163deployment.apps&#x2F;pre-sxsddsj-big-web image updated 应用 sxsddsj-auth-center 更新，版本变更 11034 -&gt; 11098执行 kubectl set image deployments&#x2F;pre-sxsddsj-auth-center app&#x3D;registry.lisong.pub:28500&#x2F;sxsddsj&#x2F;auth-center:v11098deployment.apps&#x2F;pre-sxsddsj-auth-center image updated 1kubectl set image deployments/pre-sxsddsj-auth-center app=registry.lisong.pub:28500/sxsddsj/auth-center:v11230 1kubectl set image deployments/pre-sxsddsj-web webapp=registry.lisong.pub:28500/sxsddsj-fe/sxsddsj_web:v11431 1kubectl set image deployments/pre-sxsddsj-shield-data app=registry.lisong.pub:28500/sxsddsj/sxsddsj-shield-data-main:v11429","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/编程/c/python","date":"2023-10-27T06:50:27.017Z","updated":"2023-10-27T06:50:27.017Z","comments":true,"path":"2023/10/27/王阁/技术/编程/c/python/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E7%BC%96%E7%A8%8B/c/python/","excerpt":"","text":"python","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/编程/c/程序设计与数据结构","date":"2023-10-27T06:50:27.017Z","updated":"2023-10-27T06:50:27.017Z","comments":true,"path":"2023/10/27/王阁/技术/编程/c/程序设计与数据结构/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E7%BC%96%E7%A8%8B/c/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"title: 程序设计与数据结构date: 2023年 3月 7日tags: [程序设计、复试]password: 7FKBKZrTTTPG2LnC 程序设计与数据结构Hash冲突1.1 什么是hash12Hash叫做&quot;散列表&quot;，就是把任意长度的输入，通过散列算法，变成固定长度输出，该输出结果是散列值。其实这种转换是一种压缩映射，散列表的空间通常小于输入的空间，不同的输入可能会散列成相同的输出，所以不能从散列表来唯一的确定输入值。这就出现了Hash冲突。 1.2 Hash冲突1根据key（键）即经过一个函数f(key)得到的结果的作为地址去存放当前的key value键值对(这个是hashmap的存值方式)，但是却发现算出来的地址上已经被占用了。这就是所谓的hash冲突。 2、解决Hash冲突2.1开放定址法1该方法也叫做再散列法，其基本原理是：当关键字key的哈希地址p=H（key）出现冲突时，以p为基础，产生另一个哈希地址p1，如果p1仍然冲突，再以p为基础，产生另一个哈希地址p2，…，直到找出一个不冲突的哈希地址pi 。 2.2 再Hash法1这种方法就是同时构造多个不同的哈希函数： Hi=RH1（key） i=1，2，…，k。当哈希地址Hi=RH1（key）发生冲突时，再计算Hi=RH2（key）……，直到冲突不再产生。这种方法不易产生聚集，但增加了计算时间。 2.3链地址法（Java就是采用这种方法）1其基本思想: 将所有哈希地址为i的元素构成一个称为同义词链的单链表，并将单链表的头指针存在哈希表的第i个单元中，因而查找、插入和删除主要在同义词链中进行。链地址法适用于经常进行插入和删除的情况。 2.4 建立公共溢出区1这种方法的基本思想是：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。 排序","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/编程/bash/python_learn","date":"2023-10-27T06:50:27.016Z","updated":"2023-10-27T06:50:27.016Z","comments":true,"path":"2023/10/27/王阁/技术/编程/bash/python_learn/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E7%BC%96%E7%A8%8B/bash/python_learn/","excerpt":"","text":"python learn1.print 函数详细用法 print (value,…,sep&#x3D;’’,end&#x3D;’\\n’,file&#x3D;sys.stdout,flush&#x3D;False)","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/编程/bash/shell积累","date":"2023-10-27T06:50:27.016Z","updated":"2023-10-27T06:50:27.016Z","comments":true,"path":"2023/10/27/王阁/技术/编程/bash/shell积累/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E7%BC%96%E7%A8%8B/bash/shell%E7%A7%AF%E7%B4%AF/","excerpt":"","text":"shell collect&#x2F;Users&#x2F;wqkenqing&#x2F;Library&#x2F;Preferences&#x2F;IntelliJIdea2019.3&#x2F;scratches","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/编程/c/C复试","date":"2023-10-27T06:50:27.016Z","updated":"2023-10-27T06:50:27.016Z","comments":true,"path":"2023/10/27/王阁/技术/编程/c/C复试/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E7%BC%96%E7%A8%8B/c/C%E5%A4%8D%E8%AF%95/","excerpt":"","text":"文件操作操作函数打开文件 FILE * fopen ( const char * filename, const char * mode ); 12345//首先定义文件指针：fpFILE *fp;//用fopen()函数卡开文件， r——&gt;以只读方式打开fp = fopen(&quot;test.txt&quot;, &quot;r&quot;); //没有指定文件路径，则默认为当前工作目录。 fclose函数 12//fp 为文件指针，关闭文件代码如下：fclose(fp); 功能 函数名 字符输入函数 fgetc 字符输出函数 fputc 文本行输入函数 gets 文本行输出函数 fputs 格式化输入函数 fscanf 格式化输出函数 fprintf 二进制输入 fread 二进制输出 fwrite 12345678910111213141516#include &lt;stdio.h&gt;int main()&#123; FILE* fp; fp = fopen(&quot;D:\\\\codeFile\\\\test1.txt&quot;, &quot;r&quot;); if (fp != NULL) &#123; //feof(file stream )文件指针到达文件末尾 while (!feof(fp)) //读文件 printf(&quot;%c&quot;, fgetc(fp)); &#125; else printf(&quot;fail to open! \\n&quot;); fclose(fp); return 0;&#125; c语言中gets ，getschar 和fgets 的用法及三者之间的差别，还有scanf gets——从标准输入接收一串字符，遇到’\\n’时结束，但不接收’\\n’，把 ‘\\n’留存输入缓冲区；把接收的一串字符存储在形式参数指针指向的空间，并在最后自动添加一个’\\0’。getchar——从标准输入接收一个字符返回，多余的字符全部留在输入缓冲区。fgets——从文件或标准输入接收一串字符，遇到’\\n’时结束，把’\\n’也作为一个字符接收；把接收的一串字符存储在形式参数指针指向的空间，并在’\\n’后再自动添加一个’\\0’。 简单说，gets是接收一个不以’\\n’结尾的字符串，getchar是接收任何一个字符(包括’\\n’)，fgets是接收一个以’\\n’结尾的字符串。 scanf( )函数和gets( )函数都可用于输入字符串，但在功能上有区别。 gets可以接收空格。 scanf遇到空格、回车和Tab键都会认为输入结束，所有它不能接收空格","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/大数据/hive/hive20190109","date":"2023-10-27T06:50:27.014Z","updated":"2023-10-27T06:50:27.014Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/hive/hive20190109/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/hive/hive20190109/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/大数据/kafka/kafka小结","date":"2023-10-27T06:50:27.014Z","updated":"2023-10-27T06:50:27.014Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/kafka/kafka小结/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/kafka/kafka%E5%B0%8F%E7%BB%93/","excerpt":"","text":"1kafka细粒度学习与总结 相对干货 Kafka学习笔记——Kafka原理与使用详解 记录点 kafka组成部分 数据流转过程 备份 安全性 持久性 性能 api储备 消息传输机制(相关语义) zookeer的作用 着重点 备份同步(ISR) 消费请求处理(处理能力,出现总是的恢复) Consumer Rebalance 触发条件 consumer的增加或删除 broker的增加或删除 分区策略 Rangeassignor RoundRobinAssignor StickyAssignor 自定义 再均衡 均衡器 GroupCoordinator ConsumerCoordinator 原因 新的消费者入组 消息者宕机失联 消息者主动退组(leaveGroupRequest),fg 调用unsubscribe()取消订阅 消费组对应分区数量发生变化 分区管理 优先副本(分区broker) leader副本承担读写服务,分区leader被怼坏意味着该分区不可用.即broker节点中对应的leader副本多少,决定了该节点的负载高低. High Level Consumer将从某个Partition读取的最后一条消息的offset存于Zookeeper中（从0.8.2开始同时支持将Offset存于Zookeeper中和专用的Kafka Topic中）。 Q: 为什么kafka吞吐量高 A: 同一个topic同个CG下的consumer只有一个消费它. Q:为什么要支持存储于专用的Kafka Topic中？ KafkaConsumer 多线程思路: 应该不单是只开启多个consumer线程","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/大数据/kafka/kafka疑问点","date":"2023-10-27T06:50:27.014Z","updated":"2023-10-27T06:50:27.014Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/kafka/kafka疑问点/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/kafka/kafka%E7%96%91%E9%97%AE%E7%82%B9/","excerpt":"","text":"kafka疑问点consumer api High-level &amp;&amp; sample-level的区别 不同api下offset的维护.","categories":[],"tags":[]},{"title":"kafka原理","slug":"王阁/技术/大数据/kafka/kafka原理","date":"2023-10-27T06:50:27.014Z","updated":"2023-10-27T06:50:27.014Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/kafka/kafka原理/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/kafka/kafka%E5%8E%9F%E7%90%86/","excerpt":"","text":"[TOC] 正常kafka服务运行的组件组成 kafka zookeeper kafka运行主要的逻辑角色组成 producer consumer broker kafka 组成拓扑结构 broker topic partition AR ISR OSR consumer consumer_group offset producer ProducerQ: server.properies(服务端参数配置)相关参数 参数名 默认设置 建议设置 说明 zookeeper.connect listeners advertised.listeners broker.id log.dir,log.dirs message.max.bytes Q:listeners 与 advertised.listeners 是否有区别message.max.bytes修改对conusmer端的级联影响Q: HW &amp; LEO是什么,HW如何指定,有什么作用?HW: hight watermark 高水位LEO: log end offset 日志末尾偏移量 HW决定了能该topic能被消费的最大offset Q: HW如何决定topic最大可被消息offset?需要知道:HW 是由ISR中最小被同步partition的offset决定 HW是最小被同offset+1; LEO也是落地日志offset+1; 这也很好理解,因为ISR是作为数据备份队列.那么限制用户只能消费ISR队列中每个节点都已经同步的数据.这有利于保证如果消费不成功,并遇到broker发生异常,在重新选举broker后新选举的broker能尽可能有完整的数据备份(考虑到OSR也可能被设定参与选举).这样尽可能保证消息服务的稳定性. 但这里也抛出一个问题,就是若真发生了异常,HW后的数据能否保证不丢失! Q: kafka为什么没有完全采用同步复制或异步复制,而采用ISR模式 Q: 生产者必要参数 参数名 设置内容 说明 bootstrap.servers host:port,指定broker地址.可以不用完全按照完整的broker列表来设置 key. serializer value . serializer Q:producer发送消费涉及到的异常Q: producer发送消息的三种方法 fire-and- forget sync async Q:producer 一条信息到broker主要经历的流程 Interceptor Serializer Partitioner 分区器不一定是必须的,但后两者则是必需的. Q: 拦截器的主要用法与实际用途Q: kafka发送吞吐量受影响参数与可能原因T:kafka producer原理分析ConsumerQ:消费者与消费组,topic中分区与消费组与消费者的关系Q:消费者消费的内部逻辑Q: 如何获得每个分区最大offsetQ:consumer订阅的几种方式,与区别主要分为subscribe 与assign 前者能够自动再均衡,后者不行. Q:三种不同的订阅状态分别是什么Q:常见的反序列化协议Q:consumer poll(Duration timeout ),这个timeout起到什么作用.另timeout设置成0或Long.MAX_VALUE有什么区别Q:consumer 消费后提交的主要方式有几种,各有什么区别consumer poll()背后涉及到的逻辑简示 消费位移 消费者协调 组协调 消费者选举 分区分配的分发 再均衡逻辑 heartbroker Q:消费者位移信息存放位置之前老版本存放在zookeeper中,但因为这样kafka的消费性能会受到zookeeper,controller节点负载能力的影响.而zookeeper ,常常不止被一个组件使用.所以,在新版本中,kafka客户端将consumer offset的维护信息放在内部主题 __consumer_offsets中.这个主题中存放着各consumer group 与consumer对订阅topic的消费位移信息 Q:消费者多线程实现方式(大致三种)T: consumer异常提交与处理日志存储 kafka在服务器硬盘上的具体存储形式 日志格式的演变 v0 v1 v2 Q: 各版本间主要的区别Q:swap对kafka的影响服务端一些核心设计","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/大数据/yarn/YarnState","date":"2023-10-27T06:50:27.014Z","updated":"2023-10-27T06:50:27.014Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/yarn/YarnState/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/yarn/YarnState/","excerpt":"","text":"yarn相关参数提取rest web apihttp://namenode:8088/ws/v1/cluster/apps 获得json对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211&#123; &quot;apps&quot;:&#123; &quot;app&quot;:[ &#123; &quot;applicationType&quot;:&quot;MAPREDUCE&quot;, &quot;finalStatus&quot;:&quot;SUCCEEDED&quot;, &quot;trackingUrl&quot;:&quot;http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0007\\/&quot;, &quot;runningContainers&quot;:-1, &quot;clusterId&quot;:1558964977321, &quot;vcoreSeconds&quot;:349, &quot;preemptedResourceVCores&quot;:0, &quot;numAMContainerPreempted&quot;:0, &quot;allocatedMB&quot;:-1, &quot;id&quot;:&quot;application_1558964977321_0007&quot;, &quot;state&quot;:&quot;FINISHED&quot;, &quot;amHostHttpAddress&quot;:&quot;datanode2:8042&quot;, &quot;memorySeconds&quot;:538321, &quot;preemptedResourceMB&quot;:0, &quot;applicationTags&quot;:&quot;&quot;, &quot;startedTime&quot;:1561963496299, &quot;trackingUI&quot;:&quot;History&quot;, &quot;numNonAMContainerPreempted&quot;:0, &quot;amContainerLogs&quot;:&quot;http:\\/\\/datanode2:8042\\/node\\/containerlogs\\/container_1558964977321_0007_01_000001\\/hadoop&quot;, &quot;allocatedVCores&quot;:-1, &quot;diagnostics&quot;:&quot;&quot;, &quot;name&quot;:&quot;componet_code-jar-with-dependencies.jar&quot;, &quot;progress&quot;:100, &quot;finishedTime&quot;:1561963577074, &quot;user&quot;:&quot;hadoop&quot;, &quot;queue&quot;:&quot;default&quot;, &quot;elapsedTime&quot;:80775 &#125;, &#123; &quot;applicationType&quot;:&quot;MAPREDUCE&quot;, &quot;finalStatus&quot;:&quot;FAILED&quot;, &quot;trackingUrl&quot;:&quot;http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0006\\/&quot;, &quot;runningContainers&quot;:-1, &quot;clusterId&quot;:1558964977321, &quot;vcoreSeconds&quot;:197, &quot;preemptedResourceVCores&quot;:0, &quot;numAMContainerPreempted&quot;:0, &quot;allocatedMB&quot;:-1, &quot;id&quot;:&quot;application_1558964977321_0006&quot;, &quot;state&quot;:&quot;FINISHED&quot;, &quot;amHostHttpAddress&quot;:&quot;datanode1:8042&quot;, &quot;memorySeconds&quot;:261004, &quot;preemptedResourceMB&quot;:0, &quot;applicationTags&quot;:&quot;&quot;, &quot;startedTime&quot;:1559543218059, &quot;trackingUI&quot;:&quot;History&quot;, &quot;numNonAMContainerPreempted&quot;:0, &quot;amContainerLogs&quot;:&quot;http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0006_01_000001\\/hadoop&quot;, &quot;allocatedVCores&quot;:-1, &quot;diagnostics&quot;:&quot;Task failed task_1558964977321_0006_m_000000\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n&quot;, &quot;name&quot;:&quot;N\\/A&quot;, &quot;progress&quot;:100, &quot;finishedTime&quot;:1559543260162, &quot;user&quot;:&quot;hadoop&quot;, &quot;queue&quot;:&quot;default&quot;, &quot;elapsedTime&quot;:42103 &#125;, &#123; &quot;applicationType&quot;:&quot;MAPREDUCE&quot;, &quot;finalStatus&quot;:&quot;FAILED&quot;, &quot;trackingUrl&quot;:&quot;http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0005\\/&quot;, &quot;runningContainers&quot;:-1, &quot;clusterId&quot;:1558964977321, &quot;vcoreSeconds&quot;:181, &quot;preemptedResourceVCores&quot;:0, &quot;numAMContainerPreempted&quot;:0, &quot;allocatedMB&quot;:-1, &quot;id&quot;:&quot;application_1558964977321_0005&quot;, &quot;state&quot;:&quot;FINISHED&quot;, &quot;amHostHttpAddress&quot;:&quot;datanode1:8042&quot;, &quot;memorySeconds&quot;:239683, &quot;preemptedResourceMB&quot;:0, &quot;applicationTags&quot;:&quot;&quot;, &quot;startedTime&quot;:1559543157210, &quot;trackingUI&quot;:&quot;History&quot;, &quot;numNonAMContainerPreempted&quot;:0, &quot;amContainerLogs&quot;:&quot;http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0005_01_000001\\/hadoop&quot;, &quot;allocatedVCores&quot;:-1, &quot;diagnostics&quot;:&quot;Task failed task_1558964977321_0005_m_000004\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n&quot;, &quot;name&quot;:&quot;N\\/A&quot;, &quot;progress&quot;:100, &quot;finishedTime&quot;:1559543203631, &quot;user&quot;:&quot;hadoop&quot;, &quot;queue&quot;:&quot;default&quot;, &quot;elapsedTime&quot;:46421 &#125;, &#123; &quot;applicationType&quot;:&quot;MAPREDUCE&quot;, &quot;finalStatus&quot;:&quot;FAILED&quot;, &quot;trackingUrl&quot;:&quot;http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0004\\/&quot;, &quot;runningContainers&quot;:-1, &quot;clusterId&quot;:1558964977321, &quot;vcoreSeconds&quot;:206, &quot;preemptedResourceVCores&quot;:0, &quot;numAMContainerPreempted&quot;:0, &quot;allocatedMB&quot;:-1, &quot;id&quot;:&quot;application_1558964977321_0004&quot;, &quot;state&quot;:&quot;FINISHED&quot;, &quot;amHostHttpAddress&quot;:&quot;datanode2:8042&quot;, &quot;memorySeconds&quot;:269329, &quot;preemptedResourceMB&quot;:0, &quot;applicationTags&quot;:&quot;&quot;, &quot;startedTime&quot;:1559543112981, &quot;trackingUI&quot;:&quot;History&quot;, &quot;numNonAMContainerPreempted&quot;:0, &quot;amContainerLogs&quot;:&quot;http:\\/\\/datanode2:8042\\/node\\/containerlogs\\/container_1558964977321_0004_01_000001\\/hadoop&quot;, &quot;allocatedVCores&quot;:-1, &quot;diagnostics&quot;:&quot;Task failed task_1558964977321_0004_m_000000\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n&quot;, &quot;name&quot;:&quot;N\\/A&quot;, &quot;progress&quot;:100, &quot;finishedTime&quot;:1559543159096, &quot;user&quot;:&quot;hadoop&quot;, &quot;queue&quot;:&quot;default&quot;, &quot;elapsedTime&quot;:46115 &#125;, &#123; &quot;applicationType&quot;:&quot;MAPREDUCE&quot;, &quot;finalStatus&quot;:&quot;FAILED&quot;, &quot;trackingUrl&quot;:&quot;http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0003\\/&quot;, &quot;runningContainers&quot;:-1, &quot;clusterId&quot;:1558964977321, &quot;vcoreSeconds&quot;:172, &quot;preemptedResourceVCores&quot;:0, &quot;numAMContainerPreempted&quot;:0, &quot;allocatedMB&quot;:-1, &quot;id&quot;:&quot;application_1558964977321_0003&quot;, &quot;state&quot;:&quot;FINISHED&quot;, &quot;amHostHttpAddress&quot;:&quot;datanode1:8042&quot;, &quot;memorySeconds&quot;:227366, &quot;preemptedResourceMB&quot;:0, &quot;applicationTags&quot;:&quot;&quot;, &quot;startedTime&quot;:1559543073374, &quot;trackingUI&quot;:&quot;History&quot;, &quot;numNonAMContainerPreempted&quot;:0, &quot;amContainerLogs&quot;:&quot;http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0003_01_000001\\/hadoop&quot;, &quot;allocatedVCores&quot;:-1, &quot;diagnostics&quot;:&quot;Task failed task_1558964977321_0003_m_000004\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n&quot;, &quot;name&quot;:&quot;N\\/A&quot;, &quot;progress&quot;:100, &quot;finishedTime&quot;:1559543111009, &quot;user&quot;:&quot;hadoop&quot;, &quot;queue&quot;:&quot;default&quot;, &quot;elapsedTime&quot;:37635 &#125;, &#123; &quot;applicationType&quot;:&quot;MAPREDUCE&quot;, &quot;finalStatus&quot;:&quot;FAILED&quot;, &quot;trackingUrl&quot;:&quot;http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0002\\/&quot;, &quot;runningContainers&quot;:-1, &quot;clusterId&quot;:1558964977321, &quot;vcoreSeconds&quot;:206, &quot;preemptedResourceVCores&quot;:0, &quot;numAMContainerPreempted&quot;:0, &quot;allocatedMB&quot;:-1, &quot;id&quot;:&quot;application_1558964977321_0002&quot;, &quot;state&quot;:&quot;FINISHED&quot;, &quot;amHostHttpAddress&quot;:&quot;datanode2:8042&quot;, &quot;memorySeconds&quot;:267663, &quot;preemptedResourceMB&quot;:0, &quot;applicationTags&quot;:&quot;&quot;, &quot;startedTime&quot;:1559543001391, &quot;trackingUI&quot;:&quot;History&quot;, &quot;numNonAMContainerPreempted&quot;:0, &quot;amContainerLogs&quot;:&quot;http:\\/\\/datanode2:8042\\/node\\/containerlogs\\/container_1558964977321_0002_01_000001\\/hadoop&quot;, &quot;allocatedVCores&quot;:-1, &quot;diagnostics&quot;:&quot;Task failed task_1558964977321_0002_m_000004\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n&quot;, &quot;name&quot;:&quot;N\\/A&quot;, &quot;progress&quot;:100, &quot;finishedTime&quot;:1559543043050, &quot;user&quot;:&quot;hadoop&quot;, &quot;queue&quot;:&quot;default&quot;, &quot;elapsedTime&quot;:41659 &#125;, &#123; &quot;applicationType&quot;:&quot;MAPREDUCE&quot;, &quot;finalStatus&quot;:&quot;SUCCEEDED&quot;, &quot;trackingUrl&quot;:&quot;http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0001\\/&quot;, &quot;runningContainers&quot;:-1, &quot;clusterId&quot;:1558964977321, &quot;vcoreSeconds&quot;:411, &quot;preemptedResourceVCores&quot;:0, &quot;numAMContainerPreempted&quot;:0, &quot;allocatedMB&quot;:-1, &quot;id&quot;:&quot;application_1558964977321_0001&quot;, &quot;state&quot;:&quot;FINISHED&quot;, &quot;amHostHttpAddress&quot;:&quot;datanode1:8042&quot;, &quot;memorySeconds&quot;:619025, &quot;preemptedResourceMB&quot;:0, &quot;applicationTags&quot;:&quot;&quot;, &quot;startedTime&quot;:1559197897433, &quot;trackingUI&quot;:&quot;History&quot;, &quot;numNonAMContainerPreempted&quot;:0, &quot;amContainerLogs&quot;:&quot;http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0001_01_000001\\/hadoop&quot;, &quot;allocatedVCores&quot;:-1, &quot;diagnostics&quot;:&quot;&quot;, &quot;name&quot;:&quot;componet_code-jar-with-dependencies.jar&quot;, &quot;progress&quot;:100, &quot;finishedTime&quot;:1559197986948, &quot;user&quot;:&quot;hadoop&quot;, &quot;queue&quot;:&quot;default&quot;, &quot;elapsedTime&quot;:89515 &#125; ] &#125;&#125; 对json对象分解后获得如下字段备用 字段名 含义 applicationType 计算任务类型(mr spark) finalStatus 最终状态 runningContainers 正在运行的容器","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/大数据/flink/Flink记录","date":"2023-10-27T06:50:27.013Z","updated":"2023-10-27T06:50:27.013Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/flink/Flink记录/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/Flink%E8%AE%B0%E5%BD%95/","excerpt":"","text":"flink相关记录 yarn启动12345678910111213Usage: Required -n,--container &lt;arg&gt; Number of YARN container to allocate (=Number of Task Managers) Optional -D &lt;arg&gt; Dynamic properties -d,--detached Start detached -jm,--jobManagerMemory &lt;arg&gt; Memory for JobManager Container with optional unit (default: MB) -nm,--name Set a custom name for the application on YARN -q,--query Display available YARN resources (memory, cores) -qu,--queue &lt;arg&gt; Specify YARN queue. -s,--slots &lt;arg&gt; Number of slots per TaskManager -tm,--taskManagerMemory &lt;arg&gt; Memory per TaskManager Container with optional unit (default: MB) -z,--zookeeperNamespace &lt;arg&gt; Namespace to create the Zookeeper sub-paths for HA mode yarn-session.sh -n 8 -jm 2048 -tm 4096 -d参数解释： 12345678//-n 2 表示指定两个容器// -jm 1024 表示jobmanager 1024M内存// -tm 1024表示taskmanager 1024M内存//-d 任务后台运行//-nm,--name YARN上为一个自定义的应用设置一个名字//-q,--query 显示yarn中可用的资源 (内存, cpu核数)//-z,--zookeeperNamespace &lt;arg&gt; 针对HA模式在zookeeper上创建NameSpace//-id,--applicationId &lt;yarnAppId&gt; YARN集群上的任务id，附着到一个后台运行的yarn session中 命令: yarn-session.sh -n 10 -tm 8192 -s 32 yarn-session.sh -n 2 -tm 2048 -s 2 yarn-session.sh -n 1 -tm 1024 -s 2 yarn-session.sh -n 1 -tm 1024 -s 2yarn-session.sh -n 2 -tm 4096 -jm 4096 yarn-session.sh -id application_1563854073510_0006 flink run -m yarn-cluster -yn 2 -yjm 4096 -ytm 4096 2020年 01月 13日 星期一 16:39:12 yarn-session.sh -n 2 -tm 4096 -jm 4096","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/大数据/flink/flink","date":"2023-10-27T06:50:27.013Z","updated":"2023-10-27T06:50:27.013Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/flink/flink/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/flink/","excerpt":"title: flink学习 date: tags:","text":"title: flink学习 date: tags: base of flink checkpoint state time window flink envStreamaddSourcefunctionaddSink addSource基于集合 fromCollection(Collection) fromCollection(Iterator, Class) fromElements(T …) fromParallelCollection(SplittableIterator, Class) generateSequence(from,to) fromCollection 1234567891011public static void fromCollection() &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 5; i++) &#123; list.add(i + &quot;&quot;); &#125; DataStream&lt;String&gt; dstrem = env.fromCollection(list); dstrem.map(s -&gt; new Tuple2(s, 1)).returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String.class, Integer.class)) .print(); &#125; generateSequence 123456public static void fromGenerateSequence() &#123; DataStreamSource&lt;Long&gt; interStream = env.generateSequence(10, 1000); interStream.print(); &#125; 基于文件 readFile() 123456789101112131415161718DataStream&lt;String&gt; text = env.readTextFile(&quot;/Users/wqkenqing/Desktop/out/keyCount.txt&quot;); FlatMapFunction&lt;String, Tuple2&lt;String, String&gt;&gt; spliter = (String sentence, Collector&lt;Tuple2&lt;String, String&gt;&gt; out) -&gt; &#123; String ss[] = sentence.split(&quot;\\\\s+&quot;); out.collect(new Tuple2&lt;String, String&gt;(ss[0], ss[1])); &#125;; FilterFunction&lt;Tuple2&lt;String, String&gt;&gt; filter = (Tuple2&lt;String, String&gt; message) -&gt; &#123; if (message.f1.contains(&quot;m&quot;)) &#123; return true; &#125; return false; &#125;; ReduceFunction&lt;String&gt; reduceFunction = (m1, m2) -&gt; &#123; return m1 + m2; &#125;; text.flatMap(spliter).returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String.class, String.class)) .keyBy(0) .reduce(reduceFunction) .print(); 基于socket1 自定义Source分两部份,一是一些现有的source,但未集成至flink.另一种是纯自己写的source part one: kafkaSource hdfsSource kafaSource","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/大数据/flink/flink_book","date":"2023-10-27T06:50:27.013Z","updated":"2023-10-27T06:50:27.013Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/flink/flink_book/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/flink_book/","excerpt":"title: flink_book.md date: 2020-8-24 tags: [flink,book]","text":"title: flink_book.md date: 2020-8-24 tags: [flink,book] flink book","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/大数据/flink/flink任务提交","date":"2023-10-27T06:50:27.013Z","updated":"2023-10-27T06:50:27.013Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/flink/flink任务提交/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/flink%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4/","excerpt":"","text":"flink任务提交","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/大数据/flume/docker版flume使用","date":"2023-10-27T06:50:27.013Z","updated":"2023-10-27T06:50:27.013Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/flume/docker版flume使用/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/flume/docker%E7%89%88flume%E4%BD%BF%E7%94%A8/","excerpt":"","text":"flume in docker1公司flume的docker镜像,","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/大数据/hbase/hbase20190109","date":"2023-10-27T06:50:27.013Z","updated":"2023-10-27T06:50:27.013Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/hbase/hbase20190109/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/hbase/hbase20190109/","excerpt":"","text":"summary of today今日总结 hive外部表的创建与删除 hbase filter的使用 hbase rowkey 的使用 hive 外部表的创建,主要是 postion_gps_online表 hbase rowkey的使用,这里涉及到相关逻辑里的实现问题,如轨迹某段时间里所有设备的轨迹,rowkey 若设计成 device_id+”“+ts_time 形式,则会出现通过start_row与stop_row的模糊区间匹配未找到现成的方案.所以后 续rowkey设计成的是ts_time+”“+device_id的形式,这样可以只通过传时间就可以筛选出这个时间区间里的所有 device_id.","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/大数据/flink/2023/容错机制","date":"2023-10-27T06:50:27.013Z","updated":"2023-10-27T06:50:27.013Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/flink/2023/容错机制/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/2023/%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/大数据/flink/2023/flink_study","date":"2023-10-27T06:50:27.012Z","updated":"2023-10-27T06:50:27.013Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/flink/2023/flink_study/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/2023/flink_study/","excerpt":"","text":"Flink learnwindow窗口分类： 滚动窗口 滑动窗口 会话窗口 窗口分配器先区分是否为键控流 键控流用window(WindowAssigner) 非键控流用windowAll(WindowAssigner) 窗口分配器： 滚动窗口 滑动窗口 会话窗口 全局窗口 自定义窗口 内置窗口均基于时间 开始时间戳 结束时间戳 时间语义与水位线","categories":[],"tags":[]},{"title":"Mongodb使用","slug":"王阁/技术/大数据/Mongodb/常用命令","date":"2023-10-27T06:50:27.011Z","updated":"2023-10-27T06:50:27.011Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/Mongodb/常用命令/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/Mongodb/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"","text":"Mongodb导出 json mongoexport –host 172.23.8.16 –port&#x3D;1240 -u spark -p spark –db locateInfo –collection dataCollect –out locateInfo.json csvmongoexport –host 172.23.8.16 –port&#x3D;1240 -u spark -p spark –db locateInfo –collection dataCollect –csv -f deviceId, gnssTime –out locateInfo.dat","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/大数据/es/es小结","date":"2023-10-27T06:50:27.011Z","updated":"2023-10-27T06:50:27.011Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/es/es小结/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/es/es%E5%B0%8F%E7%BB%93/","excerpt":"","text":"12针对es的一些操作小结 es主要的重点在于查询,所以针对es的一些查询进行小结. es的查询分类 主要有以下 elasticsearch term 匹配索引值, macth匹配文本内容所以对text 类,用term会匹配不到 需要用match or match_phrase; settings的设置 123456789101112131415//静态设置：只能在索引创建时或者在状态为 closed index（闭合的索引）上设置index.number_of_shards //主分片数，默认为5.只能在创建索引时设置，不能修改index.shard.check_on_startup //是否应在索引打开前检查分片是否损坏，当检查到分片损坏将禁止分片被打开 false //默认值 checksum //检查物理损坏 true //检查物理和逻辑损坏，这将消耗大量内存和CPU fix //检查物理和逻辑损坏。有损坏的分片将被集群自动删除，这可能导致数据丢失index.routing_partition_size //自定义路由值可以转发的目的分片数。默认为 1，只能在索引创建时设置。此值必须小于index.number_of_shardsindex.codec //默认使用LZ4压缩方式存储数据，也可以设置为 best_compression，它使用 DEFLATE 方式以牺牲字段存储性能为代价来获得更高的压缩比例。 12345678910111213141516171819202122index.number_of_replicas //每个主分片的副本数。默认为 1。index.auto_expand_replicas //基于可用节点的数量自动分配副本数量,默认为 false（即禁用此功能）index.refresh_interval //执行刷新操作的频率，这使得索引的最近更改可以被搜索。默认为 1s。可以设置为 -1 以禁用刷新。index.max_result_window //用于索引搜索的 from+size 的最大值。默认为 10000index.max_rescore_window // 在搜索此索引中 rescore 的 window_size 的最大值index.blocks.read_only //设置为 true 使索引和索引元数据为只读，false 为允许写入和元数据更改。index.blocks.read // 设置为 true 可禁用对索引的读取操作index.blocks.write //设置为 true 可禁用对索引的写入操作。index.blocks.metadata // 设置为 true 可禁用索引元数据的读取和写入。index.max_refresh_listeners //索引的每个分片上可用的最大刷新侦听器数","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/大数据/flink/2023/Flink Connector","date":"2023-10-27T06:50:27.011Z","updated":"2023-10-27T06:50:27.011Z","comments":true,"path":"2023/10/27/王阁/技术/大数据/flink/2023/Flink Connector/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink/2023/Flink%20Connector/","excerpt":"这是主要是对常见的connector 进行一个梳理，日常使用时可以开箱即用","text":"这是主要是对常见的connector 进行一个梳理，日常使用时可以开箱即用 Flink Connector 主要分为 预定义的 source 和 sink Bundled Connectors cdc Connector 自定义connector 整理的connector 这里主要针对组件进行整理，具体的connector类型会作标记，有些是内置的，有些是自定义的，有些是三方的。 Kafka ConnectorSource kafka-connector(Bundled) USE **Kafka Consumer ** 12345Properties properties = new Properties();properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);properties.setProperty(&quot;group.id&quot;, &quot;test&quot;);DataStream&lt;String&gt; stream = env .addSource(new FlinkKafkaConsumer&lt;&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties)); 1234567FlinkKafkaConsumer&lt;String&gt; myConsumer = new FlinkKafkaConsumer&lt;&gt;(...);myConsumer.setStartFromEarliest(); // 尽可能从最早的记录开始myConsumer.setStartFromLatest(); // 从最新的记录开始myConsumer.setStartFromTimestamp(...); // 从指定的时间开始（毫秒）myConsumer.setStartFromGroupOffsets(); // 默认的方法 123456Map&lt;KafkaTopicPartition, Long&gt; specificStartOffsets = new HashMap&lt;&gt;();specificStartOffsets.put(new KafkaTopicPartition(&quot;myTopic&quot;, 0), 23L);specificStartOffsets.put(new KafkaTopicPartition(&quot;myTopic&quot;, 1), 31L);specificStartOffsets.put(new KafkaTopicPartition(&quot;myTopic&quot;, 2), 43L);myConsumer.setStartFromSpecificOffsets(specificStartOffsets); 12345678910final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();Properties properties = new Properties();properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);properties.setProperty(&quot;group.id&quot;, &quot;test&quot;);FlinkKafkaConsumer&lt;String&gt; myConsumer = new FlinkKafkaConsumer&lt;&gt;( java.util.regex.Pattern.compile(&quot;test-topic-[0-9]&quot;), new SimpleStringSchema(), properties); 1234567891011Properties properties = new Properties();properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);properties.setProperty(&quot;group.id&quot;, &quot;test&quot;);FlinkKafkaConsumer&lt;String&gt; myConsumer = new FlinkKafkaConsumer&lt;&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties);myConsumer.assignTimestampsAndWatermarks( WatermarkStrategy .forBoundedOutOfOrderness(Duration.ofSeconds(20)));DataStream&lt;String&gt; stream = env.addSource(myConsumer); Kafka Sink 12345Properties properties = new Properties(); properties.setProperty(“bootstrap.servers”, “localhost:9092”);FlinkKafkaProducer myProducer = new FlinkKafkaProducer( “my-topic”, // 目标 topic new SimpleStringSchema(), // 序列化 schema properties, // producer 配置 FlinkKafkaProducer.Semantic.EXACTLY_ONCE); // 容错stream.addSink(myProducer); ElasticsearchElasticsearchSource(Custom Source) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package sunrise.demo.source;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.source.RichSourceFunction;import org.apache.http.HttpHost;import org.elasticsearch.action.search.SearchRequest;import org.elasticsearch.action.search.SearchResponse;import org.elasticsearch.client.RequestOptions;import org.elasticsearch.client.RestClient;import org.elasticsearch.client.RestHighLevelClient;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.SearchHit;import org.elasticsearch.search.builder.SearchSourceBuilder;/** * @author kuiqwang * @emai wqkenqingto@163.com * @time 2023/2/22 * @desc */public class ElasticsearchSource extends RichSourceFunction&lt;String&gt; &#123; private final String index; private final String query; private final int interval; private boolean running; private RestHighLevelClient client; public ElasticsearchSource(String index, String query, int interval) &#123; this.index = index; this.query = query; this.interval = interval; &#125; @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); client = new RestHighLevelClient(RestClient.builder(new HttpHost(&quot;calculation02&quot;, 9200, &quot;http&quot;))); &#125; @Override public void run(SourceContext&lt;String&gt; ctx) throws Exception &#123; running = true; while (running) &#123; SearchRequest searchRequest = new SearchRequest(index); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();// searchSourceBuilder.query(QueryBuilders.matchQuery(&quot;message&quot;, query)); searchSourceBuilder.query(QueryBuilders.matchAllQuery()); searchRequest.source(searchSourceBuilder); SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT); for (SearchHit hit : searchResponse.getHits().getHits()) &#123; ctx.collect(hit.getSourceAsString()); &#125; Thread.sleep(interval); &#125; &#125; @Override public void cancel() &#123; running = false; try &#123; client.close(); &#125; catch (Exception e) &#123; // Ignore exception &#125; &#125;&#125; Elasticsearch Sink (Bundled) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import org.apache.flink.api.common.functions.RuntimeContext;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction;import org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer;import org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink;import org.apache.http.HttpHost;import org.elasticsearch.action.index.IndexRequest;import org.elasticsearch.client.Requests;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;DataStream&lt;String&gt; input = ...;List&lt;HttpHost&gt; httpHosts = new ArrayList&lt;&gt;();httpHosts.add(new HttpHost(&quot;127.0.0.1&quot;, 9200, &quot;http&quot;));httpHosts.add(new HttpHost(&quot;10.2.3.1&quot;, 9200, &quot;http&quot;));// 使用 ElasticsearchSink.Builder 创建 ElasticsearchSinkElasticsearchSink.Builder&lt;String&gt; esSinkBuilder = new ElasticsearchSink.Builder&lt;&gt;( httpHosts, new ElasticsearchSinkFunction&lt;String&gt;() &#123; public IndexRequest createIndexRequest(String element) &#123; Map&lt;String, String&gt; json = new HashMap&lt;&gt;(); json.put(&quot;data&quot;, element); return Requests.indexRequest() .index(&quot;my-index&quot;) .type(&quot;my-type&quot;) .source(json); &#125; @Override public void process(String element, RuntimeContext ctx, RequestIndexer indexer) &#123; indexer.add(createIndexRequest(element)); &#125; &#125;);// 批量请求的配置；下面的设置使 sink 在接收每个元素之后立即提交，否则这些元素将被缓存起来esSinkBuilder.setBulkFlushMaxActions(1);// 为内部创建的 REST 客户端提供一个自定义配置信息的 RestClientFactoryesSinkBuilder.setRestClientFactory( restClientBuilder -&gt; &#123; restClientBuilder.setDefaultHeaders(...); restClientBuilder.setMaxRetryTimeoutMillis(...); restClientBuilder.setPathPrefix(...); restClientBuilder.setHttpClientConfigCallback(...); &#125;);// 最后，构建并添加 sink 到作业管道中input.addSink(esSinkBuilder.build()); 123//要使用具有容错特性的 Elasticsearch Sinks，需要在执行环境中启用作业拓扑的 checkpoint：final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.enableCheckpointing(5000); // 每 5000 毫秒执行一次 checkpoint 处理失败的 Elasticsearch 请求 123456789101112131415161718192021222324DataStream&lt;String&gt; input = ...;input.addSink(new ElasticsearchSink&lt;&gt;( config, transportAddresses, new ElasticsearchSinkFunction&lt;String&gt;() &#123;...&#125;, new ActionRequestFailureHandler() &#123; @Override void onFailure(ActionRequest action, Throwable failure, int restStatusCode, RequestIndexer indexer) throw Throwable &#123; if (ExceptionUtils.findThrowable(failure, EsRejectedExecutionException.class).isPresent()) &#123; // 队列已满；重新添加文档进行索引 indexer.add(action); &#125; else if (ExceptionUtils.findThrowable(failure, ElasticsearchParseException.class).isPresent()) &#123; // 文档格式错误；简单地删除请求避免 sink 失败 &#125; else &#123; // 对于所有其他失败的请求，失败的 sink // 这里的失败只是简单的重新抛出，但用户也可以选择抛出自定义异常 throw failure; &#125; &#125;&#125;)); hivehiveSource (custom source) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package sunrise.demo.source;/** * * @author kuiqwang * @emai wqkenqingto@163.com * @time 2022/11/15 * @desc */import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.source.RichSourceFunction;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.ResultSet;public class HiveSource extends RichSourceFunction&lt;Tuple2&lt;String, String&gt;&gt; &#123; private Connection connection = null; private PreparedStatement ps = null; String con; String user; String pass; String sql; public HiveSource(String con, String user, String pass, String sql )&#123; this.user=user; this.pass=pass; this.con=con; this.sql=sql; &#125; //该方法主要用于打开数据库连接，下面的ConfigKeys类是获取配置的类 @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;);//加载数据库驱动// connection = DriverManager.getConnection(&quot;jdbc:mysql://106.54.170.224:10328&quot;, &quot;root&quot;, &quot;Bmsoft2020datateam&quot;);//获取连接 connection = DriverManager.getConnection(con, user, pass);//获取连接 ps = connection.prepareStatement(sql); &#125; @Override public void run(SourceContext&lt;Tuple2&lt;String, String&gt;&gt; sourceContext) throws Exception &#123; ResultSet resultSet = ps.executeQuery(); while (resultSet.next()) &#123; Tuple2&lt;String, String&gt; tuple = new Tuple2&lt;String, String&gt;(); tuple.setFields(resultSet.getString(1), resultSet.getString(2)); sourceContext.collect(tuple); &#125; &#125; @Override public void cancel() &#123; try &#123; super.close(); if (connection != null) &#123; connection.close(); &#125; if (ps != null) &#123; ps.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; hive sink (custom sink) &#x2F;&#x2F;暂略 hbasegithub上的一个开源的connector; flink-connector-hbase hbase source 略 hbase sink Redis flink-connector-redis 123456789101112131415161718192021222324252627282930package sunrise.demo.stream.mapper;import org.apache.flink.streaming.connectors.redis.common.mapper.RedisCommand;import org.apache.flink.streaming.connectors.redis.common.mapper.RedisCommandDescription;import org.apache.flink.streaming.connectors.redis.common.mapper.RedisMapper;import sunrise.demo.pojo.VideoEvent;/** * @author kuiqwang * @emai wqkenqingto@163.com * @time 2023/2/21 * @desc */public class RedisSinkMapper implements RedisMapper&lt;VideoEvent&gt; &#123; @Override public RedisCommandDescription getCommandDescription() &#123; return new RedisCommandDescription(RedisCommand.HSET,&quot;video-sink&quot;); &#125; @Override public String getKeyFromData(VideoEvent videoEvent) &#123; return videoEvent.getCamera(); &#125; @Override public String getValueFromData(VideoEvent videoEvent) &#123; return String.valueOf(videoEvent.getSpeed()); &#125;&#125; RabbitMQ123456&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-rabbitmq&lt;/artifactId&gt; &lt;version&gt;1.18-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; Source 1234567891011121314151617final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// checkpointing is required for exactly-once or at-least-once guaranteesenv.enableCheckpointing(...);final RMQConnectionConfig connectionConfig = new RMQConnectionConfig.Builder() .setHost(&quot;localhost&quot;) .setPort(5000) ... .build(); final DataStream&lt;String&gt; stream = env .addSource(new RMQSource&lt;String&gt;( connectionConfig, // config for the RabbitMQ connection &quot;queueName&quot;, // name of the RabbitMQ queue to consume true, // use correlation ids; can be false if only at-least-once is required new SimpleStringSchema())) // deserialization schema to turn messages into Java objects .setParallelism(1); // non-parallel source is only required for exactly-once 12345final RMQConnectionConfig connectionConfig = new RMQConnectionConfig.Builder() .setPrefetchCount(30_000) ... .build(); RabbitMQ Sink 12345final RMQConnectionConfig connectionConfig = new RMQConnectionConfig.Builder() .setPrefetchCount(30_000) ... .build(); JDBC 主要还是用的是Custom Source自己封装，不过官网也有相关connector。具体实现按需来。 Table Connector 虽然connector的来源包可能是一样的，但使用代码可能不一样，所以区别记录一下。 Formats这里的format 指连接器的数据格式，可能有 csv json avro cal cdc apache parquet apache orc Raw 在connector中有一个format参数，具体按数据内容格式指定就好 如下 1234567CREATE TABLE att_business ( id String, project_id String, project_code String) WITH ( &#x27;connector&#x27; = &#x27;filesystem&#x27;, &#x27;path&#x27; = &#x27;/Users/kuiqwang/Desktop/att_business.csv&#x27;, &#x27;format&#x27; = &#x27;csv&#x27;) Kafka Connector1234567891011121314CREATE TABLE KafkaTable ( `deviceStatus` String, `describe` String, `postionNo` STRING, `creatTime` TIMESTAMP(3) METADATA FROM &#x27;timestamp&#x27;) WITH ( &#x27;connector&#x27; = &#x27;kafka&#x27;, &#x27;topic&#x27; = &#x27;jllsd-flume-collect-from-yaobo-1&#x27;, &#x27;properties.bootstrap.servers&#x27; = &#x27;kafka01:9092&#x27;, &#x27;properties.group.id&#x27; = &#x27;testGroup&#x27;, &#x27;scan.startup.mode&#x27; = &#x27;earliest-offset&#x27;, &#x27;json.ignore-parse-errors&#x27; = &#x27;true&#x27;, &#x27;format&#x27; = &#x27;json&#x27;) 消息键和消息体格式 1234567891011121314151617CREATE TABLE KafkaTable ( `ts` TIMESTAMP(3) METADATA FROM &#x27;timestamp&#x27;, `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING) WITH ( &#x27;connector&#x27; = &#x27;kafka&#x27;, ... &#x27;key.format&#x27; = &#x27;json&#x27;, &#x27;key.json.ignore-parse-errors&#x27; = &#x27;true&#x27;, &#x27;key.fields&#x27; = &#x27;user_id;item_id&#x27;, &#x27;value.format&#x27; = &#x27;json&#x27;, &#x27;value.json.fail-on-missing-field&#x27; = &#x27;false&#x27;, &#x27;value.fields-include&#x27; = &#x27;ALL&#x27;) 重名的格式字段 1234567891011121314151617CREATE TABLE KafkaTable ( `k_version` INT, `k_user_id` BIGINT, `k_item_id` BIGINT, `version` INT, `behavior` STRING) WITH ( &#x27;connector&#x27; = &#x27;kafka&#x27;, ... &#x27;key.format&#x27; = &#x27;json&#x27;, &#x27;key.fields-prefix&#x27; = &#x27;k_&#x27;, &#x27;key.fields&#x27; = &#x27;k_version;k_user_id;k_item_id&#x27;, &#x27;value.format&#x27; = &#x27;json&#x27;, &#x27;value.fields-include&#x27; = &#x27;EXCEPT_KEY&#x27;) 要启用加密和认证相关的安全配置，只需将安全配置加上 “properties.” 前缀配置在 Kafka 表上即可。下面的代码片段展示了如何配置 Kafka 表以使用 PLAIN 作为 SASL 机制并提供 JAAS 配置： 123456789101112CREATE TABLE KafkaTable ( `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING, `ts` TIMESTAMP(3) METADATA FROM &#x27;timestamp&#x27;) WITH ( &#x27;connector&#x27; = &#x27;kafka&#x27;, ... &#x27;properties.security.protocol&#x27; = &#x27;SASL_PLAINTEXT&#x27;, &#x27;properties.sasl.mechanism&#x27; = &#x27;PLAIN&#x27;, &#x27;properties.sasl.jaas.config&#x27; = &#x27;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\&quot;username\\&quot; password=\\&quot;password\\&quot;;&#x27;) 另一个更复杂的例子，使用 SASL_SSL 作为安全协议并使用 SCRAM-SHA-256 作为 SASL 机制： 12345678910111213141516171819202122CREATE TABLE KafkaTable ( `user_id` BIGINT, `item_id` BIGINT, `behavior` STRING, `ts` TIMESTAMP(3) METADATA FROM &#x27;timestamp&#x27;) WITH ( &#x27;connector&#x27; = &#x27;kafka&#x27;, ... &#x27;properties.security.protocol&#x27; = &#x27;SASL_SSL&#x27;, /* SSL 配置 */ /* 配置服务端提供的 truststore (CA 证书) 的路径 */ &#x27;properties.ssl.truststore.location&#x27; = &#x27;/path/to/kafka.client.truststore.jks&#x27;, &#x27;properties.ssl.truststore.password&#x27; = &#x27;test1234&#x27;, /* 如果要求客户端认证，则需要配置 keystore (私钥) 的路径 */ &#x27;properties.ssl.keystore.location&#x27; = &#x27;/path/to/kafka.client.keystore.jks&#x27;, &#x27;properties.ssl.keystore.password&#x27; = &#x27;test1234&#x27;, /* SASL 配置 */ /* 将 SASL 机制配置为 as SCRAM-SHA-256 */ &#x27;properties.sasl.mechanism&#x27; = &#x27;SCRAM-SHA-256&#x27;, /* 配置 JAAS */ &#x27;properties.sasl.jaas.config&#x27; = &#x27;org.apache.kafka.common.security.scram.ScramLoginModule required username=\\&quot;username\\&quot; password=\\&quot;password\\&quot;;&#x27;) Upsert Kafka SQL 12345678910111213141516171819202122232425262728293031323334CREATE TABLE pageviews_per_region ( user_region STRING, pv BIGINT, uv BIGINT, PRIMARY KEY (user_region) NOT ENFORCED) WITH ( &#x27;connector&#x27; = &#x27;upsert-kafka&#x27;, &#x27;topic&#x27; = &#x27;pageviews_per_region&#x27;, &#x27;properties.bootstrap.servers&#x27; = &#x27;...&#x27;, &#x27;key.format&#x27; = &#x27;avro&#x27;, &#x27;value.format&#x27; = &#x27;avro&#x27;);CREATE TABLE pageviews ( user_id BIGINT, page_id BIGINT, viewtime TIMESTAMP, user_region STRING, WATERMARK FOR viewtime AS viewtime - INTERVAL &#x27;2&#x27; SECOND) WITH ( &#x27;connector&#x27; = &#x27;kafka&#x27;, &#x27;topic&#x27; = &#x27;pageviews&#x27;, &#x27;properties.bootstrap.servers&#x27; = &#x27;...&#x27;, &#x27;format&#x27; = &#x27;json&#x27;);-- 计算 pv、uv 并插入到 upsert-kafka sinkINSERT INTO pageviews_per_regionSELECT user_region, COUNT(*), COUNT(DISTINCT user_id)FROM pageviewsGROUP BY user_region; JDBC 常见的关系型数据库的应用 mysql 123456789101112131415161718192021222324-- 在 Flink SQL 中注册一张 MySQL 表 &#x27;users&#x27;CREATE TABLE MyUserTable ( id BIGINT, name STRING, age INT, status BOOLEAN, PRIMARY KEY (id) NOT ENFORCED) WITH ( &#x27;connector&#x27; = &#x27;jdbc&#x27;, &#x27;url&#x27; = &#x27;jdbc:mysql://localhost:3306/mydatabase&#x27;, &#x27;table-name&#x27; = &#x27;users&#x27;);-- 从另一张表 &quot;T&quot; 将数据写入到 JDBC 表中INSERT INTO MyUserTableSELECT id, name, age, status FROM T;-- 查看 JDBC 表中的数据SELECT id, name, age, status FROM MyUserTable;-- JDBC 表在时态表关联中作为维表SELECT * FROM myTopicLEFT JOIN MyUserTable FOR SYSTEM_TIME AS OF myTopic.proctimeON myTopic.key = MyUserTable.id; Elasticsearch 官网的connector 只能作为sink的存在。 1234567891011CREATE TABLE myUserTable ( user_id STRING, user_name STRING uv BIGINT, pv BIGINT, PRIMARY KEY (user_id) NOT ENFORCED) WITH ( &#x27;connector&#x27; = &#x27;elasticsearch-7&#x27;, &#x27;hosts&#x27; = &#x27;http://calculation02:9200&#x27;, &#x27;index&#x27; = &#x27;sxsddsj-file-system&#x27;); Hbase12345678910111213141516171819202122232425-- 在 Flink SQL 中注册 HBase 表 &quot;mytable&quot;CREATE TABLE hTable ( rowkey INT, family1 ROW&lt;q1 INT&gt;, family2 ROW&lt;q2 STRING, q3 BIGINT&gt;, family3 ROW&lt;q4 DOUBLE, q5 BOOLEAN, q6 STRING&gt;, PRIMARY KEY (rowkey) NOT ENFORCED) WITH ( &#x27;connector&#x27; = &#x27;hbase-1.4&#x27;, &#x27;table-name&#x27; = &#x27;mytable&#x27;, &#x27;zookeeper.quorum&#x27; = &#x27;localhost:2181&#x27;);-- 用 ROW(...) 构造函数构造列簇，并往 HBase 表写数据。-- 假设 &quot;T&quot; 的表结构是 [rowkey, f1q1, f2q2, f2q3, f3q4, f3q5, f3q6]INSERT INTO hTableSELECT rowkey, ROW(f1q1), ROW(f2q2, f2q3), ROW(f3q4, f3q5, f3q6) FROM T;-- 从 HBase 表扫描数据SELECT rowkey, family1, family3.q4, family3.q6 FROM hTable;-- temporal join HBase 表，将 HBase 表作为维表SELECT * FROM myTopicLEFT JOIN hTable FOR SYSTEM_TIME AS OF myTopic.proctimeON myTopic.key = hTable.rowkey; 123456789101112131415161718192021222324-- 在 Flink SQL 中注册 HBase 表 &quot;mytable&quot;CREATE TABLE hTable ( rowkey INT, info ROW&lt;remark String&gt;, info ROW&lt;name STRINGT&gt;, PRIMARY KEY (rowkey) NOT ENFORCED) WITH ( &#x27;connector&#x27; = &#x27;hbase-2.12&#x27;, &#x27;table-name&#x27; = &#x27;cs_user1&#x27;, &#x27;zookeeper.quorum&#x27; = &#x27;kafka01:2181&#x27;);-- 用 ROW(...) 构造函数构造列簇，并往 HBase 表写数据。-- 假设 &quot;T&quot; 的表结构是 [rowkey, f1q1, f2q2, f2q3, f3q4, f3q5, f3q6]INSERT INTO hTableSELECT rowkey, ROW(f1q1), ROW(f2q2, f2q3), ROW(f3q4, f3q5, f3q6) FROM T;-- 从 HBase 表扫描数据SELECT rowkey, family1, family3.q4, family3.q6 FROM hTable;-- temporal join HBase 表，将 HBase 表作为维表SELECT * FROM myTopicLEFT JOIN hTable FOR SYSTEM_TIME AS OF myTopic.proctimeON myTopic.key = hTable.rowkey; 123456789CREATE TABLE hTable ( rowkey String, info ROW&lt;name String&gt;, PRIMARY KEY (rowkey) NOT ENFORCED) WITH ( &#x27;connector&#x27; = &#x27;hbase-2.2&#x27;, &#x27;table-name&#x27; = &#x27;cs_user1&#x27;, &#x27;zookeeper.quorum&#x27; = &#x27;kafka01:2181&#x27;) FileSystem SQL Connector 常规文件上文提到过，单独列出来主要是官网提供了一些如文件夹监听的功能 1234567891011121314151617181920CREATE TABLE MyUserTable ( column_name1 INT, column_name2 STRING, ... part_name1 INT, part_name2 STRING) PARTITIONED BY (part_name1, part_name2) WITH ( &#x27;connector&#x27; = &#x27;filesystem&#x27;, -- required: specify the connector &#x27;path&#x27; = &#x27;file:///path/to/whatever&#x27;, -- required: path to a directory &#x27;format&#x27; = &#x27;...&#x27;, -- required: file system connector requires to specify a format, -- Please refer to Table Formats -- section for more details &#x27;partition.default-name&#x27; = &#x27;...&#x27;, -- optional: default partition name in case the dynamic partition -- column value is null/empty string -- optional: the option to enable shuffle data by dynamic partition fields in sink phase, this can greatly -- reduce the number of file for filesystem sink but may lead data skew, the default value is false. &#x27;sink.shuffle-by-partition.enable&#x27; = &#x27;...&#x27;, ...) Directory watching12345678910CREATE TABLE MyUserTableWithFilepath ( column_name1 INT, column_name2 STRING, `file.path` STRING NOT NULL METADATA) WITH ( &#x27;connector&#x27; = &#x27;filesystem&#x27;, &#x27;path&#x27; = &#x27;file:///path/to/whatever&#x27;, &#x27;format&#x27; = &#x27;json&#x27;) Streaming Sink1234567891011121314151617181920212223242526272829303132CREATE TABLE kafka_table ( user_id STRING, order_amount DOUBLE, log_ts TIMESTAMP(3), WATERMARK FOR log_ts AS log_ts - INTERVAL &#x27;5&#x27; SECOND) WITH (...);CREATE TABLE fs_table ( user_id STRING, order_amount DOUBLE, dt STRING, `hour` STRING) PARTITIONED BY (dt, `hour`) WITH ( &#x27;connector&#x27;=&#x27;filesystem&#x27;, &#x27;path&#x27;=&#x27;...&#x27;, &#x27;format&#x27;=&#x27;parquet&#x27;, &#x27;sink.partition-commit.delay&#x27;=&#x27;1 h&#x27;, &#x27;sink.partition-commit.policy.kind&#x27;=&#x27;success-file&#x27;);-- streaming sql, insert into file system tableINSERT INTO fs_table SELECT user_id, order_amount, DATE_FORMAT(log_ts, &#x27;yyyy-MM-dd&#x27;), DATE_FORMAT(log_ts, &#x27;HH&#x27;) FROM kafka_table;-- batch sql, select with partition pruningSELECT * FROM fs_table WHERE dt=&#x27;2020-05-20&#x27; and `hour`=&#x27;12&#x27;; 1234567891011121314151617181920212223242526272829303132333435CREATE TABLE kafka_table ( user_id STRING, order_amount DOUBLE, ts BIGINT, -- time in epoch milliseconds ts_ltz AS TO_TIMESTAMP_LTZ(ts, 3), WATERMARK FOR ts_ltz AS ts_ltz - INTERVAL &#x27;5&#x27; SECOND -- Define watermark on TIMESTAMP_LTZ column) WITH (...);CREATE TABLE fs_table ( user_id STRING, order_amount DOUBLE, dt STRING, `hour` STRING) PARTITIONED BY (dt, `hour`) WITH ( &#x27;connector&#x27;=&#x27;filesystem&#x27;, &#x27;path&#x27;=&#x27;...&#x27;, &#x27;format&#x27;=&#x27;parquet&#x27;, &#x27;partition.time-extractor.timestamp-pattern&#x27;=&#x27;$dt $hour:00:00&#x27;, &#x27;sink.partition-commit.delay&#x27;=&#x27;1 h&#x27;, &#x27;sink.partition-commit.trigger&#x27;=&#x27;partition-time&#x27;, &#x27;sink.partition-commit.watermark-time-zone&#x27;=&#x27;Asia/Shanghai&#x27;, -- Assume user configured time zone is &#x27;Asia/Shanghai&#x27; &#x27;sink.partition-commit.policy.kind&#x27;=&#x27;success-file&#x27;);-- streaming sql, insert into file system tableINSERT INTO fs_table SELECT user_id, order_amount, DATE_FORMAT(ts_ltz, &#x27;yyyy-MM-dd&#x27;), DATE_FORMAT(ts_ltz, &#x27;HH&#x27;) FROM kafka_table;-- batch sql, select with partition pruningSELECT * FROM fs_table WHERE dt=&#x27;2020-05-20&#x27; and `hour`=&#x27;12&#x27;;","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/专题/技术专题","date":"2023-10-27T06:50:27.010Z","updated":"2023-10-27T06:50:27.010Z","comments":true,"path":"2023/10/27/王阁/技术/专题/技术专题/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E4%B8%93%E9%A2%98/%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/专题/sql专题/sql基础知识","date":"2023-10-27T06:50:27.010Z","updated":"2023-10-27T06:50:27.010Z","comments":true,"path":"2023/10/27/王阁/技术/专题/sql专题/sql基础知识/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E4%B8%93%E9%A2%98/sql%E4%B8%93%E9%A2%98/sql%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/专题/数据建模/数据仓库建设方案","date":"2023-10-27T06:50:27.010Z","updated":"2023-10-27T06:50:27.010Z","comments":true,"path":"2023/10/27/王阁/技术/专题/数据建模/数据仓库建设方案/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E4%B8%93%E9%A2%98/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E8%AE%BE%E6%96%B9%E6%A1%88/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/专题/数据建模/数据指标体系","date":"2023-10-27T06:50:27.010Z","updated":"2023-10-27T06:50:27.010Z","comments":true,"path":"2023/10/27/王阁/技术/专题/数据建模/数据指标体系/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E4%B8%93%E9%A2%98/%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1/%E6%95%B0%E6%8D%AE%E6%8C%87%E6%A0%87%E4%BD%93%E7%B3%BB/","excerpt":"","text":"1指标=数据+业务场景，能够指导业务制定下一步行动方案。 一个好的指标的应该能够解决以下5W的问题： 1、 初识指标与指标体系1.1、使用场景（who、when、where)解决指标的维度问题，通过定义维度可以明确指标所能支持的分析场景，例如【体脂率】可以支持性别、年龄段、地区等维度，那对应的可以支持对不同性别、年龄段、地区人群的分析。 1.1.2、指标定义（what)解决指标的计算口径问题，大多数情况下需要解决的是同名不同义、同义不同名的问题，如下图的销售额、上架数量两个指标所示。 1.1.3、指标用途（why）解决指标的逻辑问题，明确指标与指标之间的逻辑关系，如：销售利润&#x3D;销售额-采购成本-头程税费-退税差额，毛利润&#x3D;销售利润-呆滞计提-资金占用利息。 1.2 指标体系1.2.1、海盗指标法（AARRR) Acquisition (获取) Activation （激活） Retention (留存) Revenue （收入） Referral (推荐) 这个模型对于流量→收入转化的指标建设有相当的指导意义，适用于大部分的互联网公司。但对于传统电商这类关注供应链、管理成本的企业来说，这套指标体系并不能覆盖所有的场景，因此我们主要采用的是第一关键指标法作为指标体系建设的理论基础。 1.2.2、第一关键指标法核心思想:在任意一个时间点，肯定只有一个最关键的指标,但随着业务的发展关注重点会有变化。 1.2.3 指标体系搭建过程1.2.3.1. 确定第一关键指标 1.2.3.2. 划分模块​ 1. 2.3.3. 梳理指标逻辑关系确定各个模块的核心关注指标之后，我们从第一关键指标开始，从上往下梳理指标之间的逻辑关系。 原文地址 指标 2、指标体系构建方法-四个模型2.1 构建数据指标体系的方法概括数据指标体系建设的方法可以总结为三个步骤 明确业务目标 理清用户生命周期及行为路径 指标分层治理 以上三个步骤涉及了 OSM(Object,Strategy,Measure) AARRR(Acquisition,Activation,Retention,Revenue,Referral) UJM(User, Journey, Map) MECE (Mutually Exclusive, Collectively Exhaustive) 四个模型 1.OSM模型-明确业务目标，数据赋能业务 OSM模型是 Object, Strategy, Measure的缩写。 数据服务于业务才能赋能业务，数据脱离业务，那么数据就会失去其价值。 我们在建立数据指标体系之前，一定要清晰的了解业务目标，也就是模型中的O,Object。换句话说，业务的目标也就是业务的核心KPI，了解业务的核心KPI能够帮助我们快速理清指标体系的方向。 了解业务目标方向之后，就需要制定相应的行动策略，也就是模型中的S,Strategy。行动策略的制定可以根据产品生命周期或者用户行为路径进行拆解，也就是把业务的核心KPI拆解到产品生命周期(AARRR)或者用户行为路径(UJM)当中，在整条链路当中分析可以提升核心KPI的点。 就需要我们制定较细的评估指标，也就是模型中的M,Measure。评估指标的制定是将产品链路或者行为路径中的各个核心KPI进行下钻细分，这里用到的方法就是麦肯锡著名的MECE模型，需保证每个细分指标是完全独立且相互穷尽的。 总结一下OSM模型的内容及其与AARRR,UJM,MECE模型之间的关系，OSM模型是指标体系建设的指导思想，理解业务KPI是OSM模型的核心；制定行动策略是实现业务KPI的手段，而AARRR和UJM模型是实现策略制定的方法论；制定细分指标是评估业务策略优劣的方法，而MECE模型制定细分指标的方法论。 2. AARRR模型和UJM模型–理清用户生命周期以及行为路径AARRR和UJM模型都是路径模型，二者原理相似 不过AARRR是从产品角度出发，简而言之就是 拉新，促活，留存，付费，推广。 UJM是从用户角度出发，描述了用户进入产品的整个路径流程，即注册，登陆，加购，购买，复购链路流程。 3.MECE模型–指标体系分级治理 指标体系分级治理 示例：以GMV为例，用三个步骤，四个模型教会你搭建指标体系的方法 Q：指标分级治理拆这么细有什么用？","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/hexo/spark/算子/Spark-Windos函数","date":"2023-10-27T06:50:27.010Z","updated":"2023-10-27T06:50:27.010Z","comments":true,"path":"2023/10/27/王阁/技术/hexo/spark/算子/Spark-Windos函数/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/spark/%E7%AE%97%E5%AD%90/Spark-Windos%E5%87%BD%E6%95%B0/","excerpt":"title: spark-windows函数 date: 2020-7-20 tags: [spark,window] spark windows函数","text":"title: spark-windows函数 date: 2020-7-20 tags: [spark,window] spark windows函数 spark-windows函数","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/hexo/oldblog/blog22","date":"2023-10-27T06:50:27.009Z","updated":"2023-10-27T06:50:27.009Z","comments":true,"path":"2023/10/27/王阁/技术/hexo/oldblog/blog22/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog22/","excerpt":"","text":"#测试环境搭建小结因一些原因，最近协助搭建测试服务器，主要涉及到了一服务器系统安装，环境配置，参数调优，软件使用，自动化建设等内容，因为主要是协助，所以着重小结我参与的部份 参数调优这里的参数调优主要针对的是服务器的调优，主要是针对出现问题后的调优，这次软件使用上大致问题有 oom ioStream 端口数不足 pid分配不够等问题 om对oom的调优主要的动作有调大分配给jvm的内存，但光调大内存不一定能解决问题，当遇到大量创建线程，但linux服务器允许该用户的执行的线程数不够时，会报无法创建的问题，严重会致使无法执行。所以还需要调大用户的进程数，查看用户信息通过ulimit -a ioStream这个是io问题，经分析出现这个问题的主要原因应该该用户下限制了最大文件打开数，所以通过ulimit -n numbers即可调大该值但有时退出后可能会重新复原，所以需要长久变更可设置在.bashrc文件中 端口数不足这是并发测试web接口时，发送的请求动作的完成需要时间，本机设置的端口区间可能无法满足大批量的端口需求，所以需要进行重新设置，主要方式是调大区间，调低端口被释放时间等 第一步，修改&#x2F;etc&#x2F;sysctl.conf文件，在文件中添加如下行： net.ipv4.ip_local_port_range &#x3D; 1024 65000 这表明将系统对本地端口范围限制设置为1024~65000之间。请注意，本地端口范围的最小值必须大于或等于1024；而端口范围的最大值则应小于或等于65535。修改完后保存此文件。解决方案]1解决方案2 pid分配不够等问题pid分配不够","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/hexo/old/sqoop实现方案","date":"2023-10-27T06:50:27.008Z","updated":"2023-10-27T06:50:27.008Z","comments":true,"path":"2023/10/27/王阁/技术/hexo/old/sqoop实现方案/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/old/sqoop%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88/","excerpt":"","text":"调研结果通过具体调研，本次迭代能直接通过sqoop直接实现的任务有 mysql to hbase (单表) mysql to hive (单表) mysql to hive (整库) hive to mysql （单表） 其它则在数据治理模块中的任务编排功能实现后，再借助任务编排功能实现具体有mysql to hbase （整库）hbase to mysqlhive to mysql(整库) sqoop执行环境：192.168.10.100user：rootpassword: HyYg123 mysql to hbase单表导入sqoop import –connect jdbc:mysql:&#x2F;&#x2F;192.168.10.210:3306&#x2F;hy_hydd –username yg_reader –password yg987654321 –table t_user –hbase-table test:t_user –column-family info –delete-target-dir –hbase-create-table –hbase-row-key id –hbase-bulkload 支持覆盖 整库导入sqoop import-all-tables –connect jdbc:mysql:&#x2F;&#x2F;192.168.10.210:3306&#x2F;hy_hydd –driver com.mysql.jdbc.Driver –username ‘yg_reader’ –password ‘yg987654321’ –hbase-create-table –hbase-table ‘test:hy_hydd’ –column-family ‘info’ –hbase-bulkload sqoop mysql to hbase的效果是直接导入到一个表中（该库中的所有表，都导入到一个表中） hbase to mysql无法直接导出,需要借助hive、hdfs等策略 mysql to hive单表导入1234567891011121314sqoop import \\--connect jdbc:mysql://192.168.10.210:3306/hy_hydd \\--username yg_reader \\--password yg987654321 \\--table t_user \\-m 1 \\--hive-import \\--hive-table t_user \\--hive-overwrite \\--create-hive-table \\--fields-terminated-by &#x27;,&#x27; \\--lines-terminated-by &#x27;\\n&#x27; 全库导入sqoop import-all-tables -Dorg.apache.sqoop.splitter.allow_text_splitter&#x3D;true –connect jdbc:mysql:&#x2F;&#x2F;192.168.10.210:3306&#x2F;$db –username yg_reader –password yg987654321 –hive-database $db –hive-import –hive-overwrite -m 1 hive to myqsl单表sqoop export –connect jdbc:mysql:&#x2F;&#x2F;192.168.10.210:3306&#x2F;hive_test –table t_user –username root –password yg987654321 –hcatalog-table t_user 整库","categories":[],"tags":[]},{"title":"准备小结","slug":"王阁/技术/hexo/old/准备小结","date":"2023-10-27T06:50:27.008Z","updated":"2023-10-27T06:50:27.008Z","comments":true,"path":"2023/10/27/王阁/技术/hexo/old/准备小结/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/old/%E5%87%86%E5%A4%87%E5%B0%8F%E7%BB%93/","excerpt":"准备小结","text":"准备小结 hdfs存储机制是怎样的?client端发送写文件请求，namenode检查文件是否存在，如果已存在，直接返回错误信息，否则，发送给client一些可用namenode节点client将文件分块，并行存储到不同节点上datanode上，发送完成后，client同时发送信息给namenode和datanodenamenode收到的client信息后，发送确信信息给datanodedatanode同时收到namenode和datanode的确认信息后，提交写操作。 hadoop中combiner的作用是什么?当map生成的数据过大时，带宽就成了瓶颈，怎样精简压缩传给Reduce的数据，又不影响最终的结果呢。有一种方法就是使用Combiner，Combiner号称本地的Reduce，Reduce最终的输入，是Combiner的输出。 你们数据库怎么导入hive 的,有没有出现问题在导入hive的时候，如果数据库中有blob或者text字段，会报错，解决方案在sqoop笔记中。在将数据由Oracle数据库导入到Hive时，发现带有clob字段的表的数据会错乱，出现一些字段全为NULL的空行。由于在项目中CLOB字段没有实际的分析用途，因此考虑将CLOB字段去掉。 hdfs-site.xml的3个主要属性?dfs.name.dir决定的是元数据存储的路径以及DFS的存储方式(磁盘或是远端)dfs.data.dir决定的是数据存储的路径fs.checkpoint.dir用于第二Namenode 下列哪项通常是集群的最主要瓶颈磁盘 IO答案：C 磁盘首先集群的目的是为了节省成本，用廉价的 pc 机，取代小型机及大型机。小型机和大型机有什么特点？1.cpu 处理能力强2.内存够大，所以集群的瓶颈不可能是 a 和 d3.如果是互联网有瓶颈，可以让集群搭建内网。每次写入数据都要通过网络（集群是内网），然后还要写入 3 份数据，所以 IO 就会打折扣。 关于 SecondaryNameNode 哪项是正确的？它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间 mapreduce的原理?MapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果，得到最终结果。简单地说，MapReduce就是”任务的分解与结果的汇总”。在Hadoop中，用于执行MapReduce任务的机器角色有两个：一个是JobTracker；另一个是TaskTracker，JobTracker是用于调度工作的，TaskTracker是用于执行工作的。一个Hadoop集群中只有一台JobTracker。在分布式计算中，MapReduce框架负责处理了并行编程中分布式存储、工作调度、负载均衡、容错均衡、容错处理以及网络通信等复杂问题，把处理过程高度抽象为两个函数：map和reduce，map负责把任务分解成多个任务，reduce负责把分解后多任务处理的结果汇总起来。需要注意的是，用MapReduce来处理的数据集（或任务）必须具备这样的特点：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都可以完全并行地进行处理。 HDFS存储的机制?写流程：client链接namenode存数据namenode记录一条数据位置信息（元数据），告诉client存哪。client用hdfs的api将数据块（默认是64M）存储到datanode上。datanode将数据水平备份。并且备份完将反馈client。client通知namenode存储块完毕。namenode将元数据同步到内存中。另一块循环上面的过程。 读流程举一个简单的例子说明mapreduce是怎么来运行的 ?MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。 Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。Mapper任务的执行过程详解 每个Mapper任务是一个Java进程，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后，转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下六个阶段： 第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit)的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值128MB，输入文件有两个，一个是32MB，一个是 172MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由 一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。 第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一 行的起始位置(单位是字节)，“值”是本行的文本内容。 第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会 调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。 第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、 山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer 任务运行的数量。默认只有一个Reducer任务。第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值 对&lt;2,2&gt;、&lt;1,3&gt;、&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是&lt;1,3&gt;、&lt;2,1&gt;、&lt;2,2&gt;。如果有第六阶段，那么进入 第六阶段 如果没有，直接输出到本地的Linux文件中。 第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。 归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。 Reducer任务的执行过程详解每个Reducer任务是一个java进程。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为三个阶段：第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。最后把这些输出的键值对写入到HDFS文件中。在整个MapReduce程序的开发过程中，我们最大的工作量是覆盖map函数和覆盖reduce函数。 了解hashMap 和hashTable吗介绍下，他们有什么区别。为什么重写equals还要重写hashcode因为equals比较的是内容是一致.但hashcode 说一下map的分类和常见的情况 hashmap,hashtable,treemap,LinkedHashMap 根据键得到值，因此不允许键重复(重复了覆盖了),但允许值重复 Hashmap是一个最常用的Map 它根据键的HashCode值存储数据,根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的 最多只允许一条记录的键为Null;允许多条记录的值为 Null; HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。 如果需要同步，可以用 Collections的synchronizedMap方法使HashMap具有同步的能力，或者使用ConcurrentHashMap HashtableHashtable与 HashMap类似,它继承自Dictionary类,不同的是:它不允许记录的键或者值为空; 它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了 Hashtable在写入时会比较慢 LinkedHashMap是 HashMap 的一个子类，保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的.也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比 HashMap 慢，不过有种情况例外，当 HashMap 容量很大，实际数据较少时，遍历起来可能会比 LinkedHashMap 慢，因为 LinkedHashMap 的遍历速度只和实际数据有关，和容量无关，而 HashMap 的遍历速度和他的容量有关 TreeMap实现 SortMap 接口,能够把它保存的记录根据键排序, 默认是按键值的升序排序，也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的 HashMap，链表法存储，entry[]数组，线程不安全，可能死锁 concurrentHashMap，segment数组，每个segent下维护一组entry[]数组，每个segment是一把锁，线程安全 LinkedHashMap Object若不重写hashCode()的话，hashCode()如何计算出来的？hashcode采用的是 spark1. spark的有几种部署模式，每种模式特点？本地模式本地模式分三类 local：只启动一个executor local[k]: 启动k个executor local[*]：启动跟cpu数目相同的 executor cluster模式cluster模式肯定就是运行很多机器上了，但是它又分为以下三种模式，区别在于谁去管理资源调度。（说白了，就好像后勤管家，哪里需要资源，后勤管家要负责调度这些资源） standalone模式分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础 Spark on yarn模式分布式部署集群，资源和任务监控交给yarn管理粗粒度资源分配方式，包含cluster和client运行模式cluster 适合生产，driver运行在集群子节点，具有容错功能client 适合调试，dirver运行在客户端 2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？Spark core是其它组件的基础，spark的内核主要包含：有向循环图、RDD、Lingage、Cache、broadcast等 SparkStreaming是一个对实时数据流进行高通量、容错处理的流式处理系统将流式计算分解成一系列短小的批处理作业 Spark sql：能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询 MLBase是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。 GraphX是Spark中用于图和图并行计算 spark有哪些组件master：管理集群和节点，不参与计算。worker：计算节点，进程本身不参与计算，和master汇报。Driver：运行程序的main方法，创建spark context对象。spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。client：用户提交程序的入口。 https://blog.csdn.net/yirenboy/article/details/47441465","categories":[],"tags":[{"name":"小结","slug":"小结","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E5%B0%8F%E7%BB%93/"}]},{"title":"大数据分享","slug":"王阁/技术/hexo/old/大数据相关分享","date":"2023-10-27T06:50:27.008Z","updated":"2023-10-27T06:50:27.008Z","comments":true,"path":"2023/10/27/王阁/技术/hexo/old/大数据相关分享/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/old/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E5%88%86%E4%BA%AB/","excerpt":"开头语工欲善其事，必先利其器","text":"开头语工欲善其事，必先利其器 本次分享,是我在公司的第一次分享,我考虑后,将本次分享的主要内容分为了三大部块.先是针对相关基础组件分类介绍.再介绍下通过对这些组件进行组织配搭的大数据基础环境架构.再结合我的一些经历,为大家介绍下相关的应用与产品落地. 技术栈简介 数据采集 数据存储 数据治理(清洗&amp;处理) 数据应用 产品落地 我又根据不同组件的特性将他们分 采集类 存储类 计算处理类 传输类 管理类 其它类 下面开始具体介绍 采集类数据源: 日志 业务数据 公网数据(爬虫) 文本数据 出行数据(gps,手机定位等) sqoop flume crawler datax kettle elk Flume(水槽) 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单,可拓展 Sqoop是一个在结构化数据和Hadoop之间进行批量数据迁移的工具，结构化数据可以是Mysql、Oracle等RDBMS。Sqoop底层用MapReduce程序实现抽取、转换、加载，MapReduce天生的特性保证了并行化和高容错率，而且相比Kettle等传统ETL工具，任务跑在Hadoop集群上，减少了ETL服务器资源的使用情况。在特定场景下，抽取过程会有很大的性能提升。 crawler , jsoup ,httpclient, nutch 等. elk 集中式日志系统 ELK 协议栈详解 存储类 hdfs hbase hive mongdb redis RDBMS hdfs* 分布式文件存储系统 * 提供了高可靠性、高扩展性和高吞吐率的数据存储服务 * hdfs典型结构：物理结构+逻辑结构 * 文件线性切割成Block：偏移量（offset） * Block分散存储在集群节点中 * 单一文件Block大小一致，文件与文件可以不一致 * Block可以设置副本数，副本分散在不同的节点中 * 副本数不要超过节点数量 * 文件上传可以设置Block大小和副本数 * 已上传的文件Block副本数可以调整，大小不变 * 只支持一次写入多次读取，同一时刻只有一个写入者 * 只能追加，不能修改 hbaseBase是一个构建在HDFS上的分布式列存储系统；Base是基于Google BigTable模型开发的，典型的key&#x2F;value系统；Base是Apache Hadoop生态系统中的重要一员，主要用于海量结构化数据存储； 大：一个表可以有数十亿行，上百万列；无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有截然不同的列；面向列：面向列（族）的存储和权限控制，列（族）独立检索；稀疏：空（null）列并不占用存储空间，表可以设计的非常稀疏；数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳；数据类型单一：Hbase中的数据都是字符串，没有类型 openTSDB基于Hbase的分布式的，可伸缩的时间序列数据库。主要用途，就是做监控系统；譬如收集大规模集群（包括网络设备、操作系统、应用程序）的监控数据并进行存储，查询。 solr &amp; Phoenix二级索引 hiveive 是一个基于 Hadoop 文件系统之上的数据仓库架构。它可以将结构化的数据文件映射为一张数据库表，并提供简单的 sql 查询功能。还可以将 sql 语句转换为 MapReduce 任务运行。底部计算引擎还可以用用Tez, spark等. ImpalaImpala是Cloudera公司推出，提供对HDFS、Hbase数据的高性能、低延迟的交互式SQL查询功能。 基于Hive使用内存计算，兼顾数据仓库、具有实时、批处理、多并发等优点 对内存依赖大,稳定性不如hive 相比hive数据仓库,impala针对的量级相关少些,但会有效率的提升.但一般来讲,数据仓库一类需求对时间上的要要求一般不会太高,所以常规方式一般就符合大多数需求. 计算处理类 mapreduce mapreduce on oozie ,on tez spark flink mapreduceMapreduce是一个计算框架，既然是做计算的框架，那么表现形式就是有个输入（input），mapreduce操作这个输（input），通过本身定义好的计算模型，得到一个输出（output），这个输出就是我们所需要的结果。我们要学习的就是这个计算模型的运行规则。在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，每个阶段都是用键值对（key&#x2F;value）作为输入（input）和输出（output）。而程序员要做的就是定义好这两个阶段的函数：map函数和reduce函数。 分布式计算；移动计算而不移动数据。 spark相比一二代计算引擎,在兼并了一二代的特色之外,还引放了流计算这一能力,还丰富了计算函数.其中比较有代表性的主要就是spark&amp;storm.也就是说这代计算引擎兼具无边界数据与有边界数据同样的处理能力.同时还具有DAG特性.这里主要介绍spark spark主要组成有以下 spark-core spark-streaming spark-sql spark-mlib spark-graphX。 spark-core是一个提供内存计算的框架,其他的四大框架都是基于spark core上进行计算的,所以没有spark core,其他的框架是浮云.spark-core的主要内容就是对RDD的操作RDD的创建 -&gt;RDD的转换 -&gt;RDD的缓存 -&gt;RDD的行动 -&gt;RDD的输出 spark-streaming中使用离散化流（discretized stream）作为抽象的表示，叫做DStream。它是随时间推移而收集数据的序列，每个时间段收集到的数据在DStream内部以一个RDD的形式存在。DStream支持从kafka，flume,hdfs,s3等获取输入。DStream也支持两种操作，即转化操作和输出操作 spark-sqlSpark SQL 提供了查询结构化数据及计算结果等信息的接口.查询结果以 Datasets and DataFrames 形式返回 … flink&#x2F;blink略 传输类kafkaKafka是分布式发布-订阅消息系统,一个分布式的，可划分的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。日常中常与spark-streaming结合实用,为其提供无边界数据 管理类 Hue cloudera-managerue与cm 都是由cloudera提供,后面cloudera将hue开源给了apache.如果基础集群环境是采用的是开源自主搭建,可考虑引入hue.另一些大数据服务公司,有集成打包自己的一些大数据产品,如cdh等.但这些服务收费,涉及到成本问题.所以如何选用,需要相关斟酌. 其它类 zookeeper ,yarn等zookeeper在集中基础环境中主要作为配置分享中心,与kafka,hbase等组件集成.yarn则作为资源管理组件,可以与mapreduce ,spark等集成 各类组件架构以上,已经大致介绍了各类工具,基本了解了相应的特性和使用场景,而根据它们的特性,进行合理的配备,架构,从而实现一个功能全面,稳定的大数据环境. 于我个人经历与平时了解来讲,一般的架构主要如下另: 总得来说,各类组件供选型一般来讲都不是单一的.所以,我们的大数据环境各部份组件都是插销式可插拔的.所以不同公司可能不一而同,具体看自身需求和实际情况.比如上图中的storm流式计算模块,就可以替换成spark-streaming等. 通过对上图的架构的拆解,再组合,可能还会有以下组织架构. 数据仓库可以理解为上图中间部份.作为一个数据集市的存在,算作数据中心的一部份. ODS：是数据仓库第一层数据，直接从原始数据过来的，经过简单地处理，比喻：字段体重的数据为175cm等数据。 DW*：这个是数据仓库的第二层数据，DWD和DWS很多情况下是并列存在的，这一层储存经过处理后的标准数据，比喻订单、用户、页面点击流量等数据。 ADS：这个是数据仓库的最后一层数据，为应用层数据，直接可以给业务人员使用。 星型模型 星型模型中有两个重要的概念：事实表和维度表。事实表：一些主键ID的集合，没有存放任何实际的内容维度表：存放详细的数据信息，有唯一的主键ID。如上面的关键词表、用户表等等。 数据中心:概念相对更大一些,可能即作为具体平台产品集合,也可能是一个团队行政划分.总得来说,是如 大数据基础平台 数据仓库 DMP平台 相关应用平台如推荐系统,报表系统,可视化平台等. 数据中台:这个是由阿里于15年率先提出.主导思想是大中台,小前台.这块暂无特别明确的解释说法,但现在也有不少公司效仿.我个人从它的主导思想”大中台,小前台”的理解是,这个可能是体量更大,壁垒更少的一个数据集成体.比如阿里系的旗下公司,数据流都会归集到中台,同时阿里系下的公司也能获得不仅自己公司数据中心归集的数据反馈,还能获得阿里中台整合后流出的反馈数据. 应用落地公共服务 交通出行 智慧城市 … 产品应用 用户画像 征信模型 推荐系统 精确营销 前沿科学(无人驾驶,人工智能,AR等) 结语以上,就是我今天分享的主要内容.今天的主题是”器”,但对这些工具的讲解浅尝辄止,在实际的开发实战中涉及的情况是更为复杂,需要掌握的内容更多,深度也更深.我这里主要是想抛砖引玉,为大家提供一点自己的理解,若能有所帮助,不胜荣幸. 另外,工具始终是工具,菜刀再利也要厨子好,才能做好菜.所以如何利用这些工具,与我们的业务结合,实现我们想要的价值,这是我一直在探索的,也愿与各位同仁一同前行. 附上图中涉及到的技术栈","categories":[],"tags":[{"name":"日常总结","slug":"日常总结","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/"}]},{"title":"kafka学习","slug":"王阁/技术/hexo/kafka/kafka实现","date":"2023-10-27T06:50:27.007Z","updated":"2023-10-27T06:50:27.007Z","comments":true,"path":"2023/10/27/王阁/技术/hexo/kafka/kafka实现/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/kafka/kafka%E5%AE%9E%E7%8E%B0/","excerpt":"[ x ] Consumer Group里只会被某一个Consumer消费 ,Kafka还允许不同Consumer Group同时消费同一条消息，这一特性可以为消息的多元化处理提供支持。","text":"[ x ] Consumer Group里只会被某一个Consumer消费 ,Kafka还允许不同Consumer Group同时消费同一条消息，这一特性可以为消息的多元化处理提供支持。 kafka 发送模式通过producer.type设置,可以设置producer的发送模式,具体参数据有producer.type&#x3D;false即同步(默认就是同步),设置为true为异步,即以batch形式像broker发送信息.(这里的batch可以设置)还有一种oneway.即通过对ack的设置即可实现,ack&#x3D;0时,即为oneway,只管发,不管是否接收成功.-1则是全部副本接收成功才算成功. kakfa消费模式 at last one at most one exactly one","categories":[],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://www.wqkenqing.ren/daydoc/tags/kafka/"}]},{"title":"Lambda&Stream.md","slug":"王阁/技术/hexo/old/Lamda积累","date":"2023-10-27T06:50:27.007Z","updated":"2023-10-27T06:50:27.007Z","comments":true,"path":"2023/10/27/王阁/技术/hexo/old/Lamda积累/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/old/Lamda%E7%A7%AF%E7%B4%AF/","excerpt":"Lambda&amp;Stream积累","text":"Lambda&amp;Stream积累 LambdaLambda主要是一个类语法长糖,尽量为java引入函数编程等实现,细节后续再补充 Streamjava8提拱的新特性之一就有stream.stream主要是针对集合的处理类.提供了一系列集合处理方式.配合使用lambda写出简介优美的代码 Stream的使用通过如 123456List&lt;Integer&gt; list = new ArrayList&lt;&gt;();list.stream();//即可以开启串行流;list.parallelStream().filter(a -&gt; &#123; return a &gt; 20; &#125;);//开启并行流 串行流即内部单线程顺序执行,并行则是启用多线程执行.后者并不一定效率就比前者高.因为并行执行启用分配线程资源时同样要消耗时间和资源,在一定量级下,前者的执行效率一度要高过后者. 我这里对三种对集合的处理形式的比较,可以简单参考一下 stream 串行流 parallelStream 并行流 常规循环式 1234567891011121314151617181920212223242526272829303132List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 100000; i++) &#123; list.add(getRandomNum()); &#125; DateUtil.setBegin(); list.stream().filter(a -&gt; &#123; return a &gt; 20; &#125;); DateUtil.setStop(); System.out.println(&quot;串行耗时&quot;+DateUtil.calCostTime()); DateUtil.setBegin(); list.parallelStream().filter(a -&gt; &#123; return a &gt; 20; &#125;); DateUtil.setStop(); System.out.println(&quot;并行耗时&quot;+DateUtil.calCostTime()); int count = 0; DateUtil.setBegin(); for (int l : list) &#123; if (l &gt; 20) &#123; count++; &#125; &#125; DateUtil.setStop(); System.out.println(&quot;循环耗时&quot;+DateUtil.calCostTime()); 经由相当量次的测试后,我觉得如果要对集合中的数据进行遍历操作,根据量级的不同,建议低量级还是采用普通循环,量级特别大,可考虑用并行流.书写方便,又不是大批量数据处理操作可以直接采用串行流 Stream的操作分类 Intermediate Terminal Short-circuiting","categories":[],"tags":[{"name":"日常总结","slug":"日常总结","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/"}]},{"title":"mapreduce组件总结","slug":"王阁/技术/hexo/old/mapreduce组件总结","date":"2023-10-27T06:50:27.007Z","updated":"2023-10-27T06:50:27.007Z","comments":true,"path":"2023/10/27/王阁/技术/hexo/old/mapreduce组件总结/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/old/mapreduce%E7%BB%84%E4%BB%B6%E6%80%BB%E7%BB%93/","excerpt":"mapreduce组件总结","text":"mapreduce组件总结 相关组件大致有 Inputformat Inputsplit ReadRecorder mapper Combiner Partioner Reduce GroupComparator Reduce shuffle 1shuffle 被称为mapreduce的核心,一个真正让奇迹发生的地方.但它到底是什么呢?简练的讲,它就是 map out 到 reduce in 这段过程中对数据的处理过程. shuffle过程中主要发生的操作有,Partion,Sort,spill,merge,copy,sort,merge.(还有可能有combine操作) 具体流程是map out后,Collector 对out后的数据进行处理. 数据将会写入到内存缓冲区,该内存缓冲区的数据达到80%后,会开启一个溢写线程,在磁盘本地创建一个文件.如果reduce设置了多个分区,写入buffer区的数据,会被打上一个分区标记.通过sortAndSpill()方法进行指对数据按分区号,key排序.最后溢出的文件是分区的,按key有序的文件.若buffer区中的20%一直未被填满,buffer写入进程不会断.但若达到100%,Buffer写入进程则会阻塞.并在buffer区中的数据全部spill完后才会再开启. (buffer区的内存默认是100M),spill过程中,若设置过combiner.则会对数据先进行combiner逻辑处理,再将处理后的数据写出 spill完成后则会对本地的spill后的文件进行Merge.即把多个spill后的文件进行合并,并排序.最后会行成一个有序文件 当1个Map Task 完成后,reduce 就会开启copy进程(默认是5个线程).这个过程中会通过http请求去各taskTracker(nodemanager),拉取相应的spill&amp;merge后的文件.当copy完成后,则又会对数据进行merge.这个过程中同样有个类似map shuffle 中的buffer 溢写的阶段. 这个过程同样会触发combiner组件.这里的merge数据源有三种 memory to memory memory to disk disk to disk默认1是不开启的. copy phase 完成后,是reduceTask 中的 sort phase即对merge 中的文件继续进行sort and group . 当sort phase 完成.则开启reduce phase .到此shuffle正式完成. ##二次排序 1 mapreduce 常见的辅助排序 partitioner key的比较Comparator 分组函数Grouping Comparator joinmap join ,semi join ,reduce join","categories":[],"tags":[{"name":"bigdata","slug":"bigdata","permalink":"http://www.wqkenqing.ren/daydoc/tags/bigdata/"}]},{"title":"kafka","slug":"王阁/技术/hexo/old/kafka","date":"2023-10-27T06:50:27.007Z","updated":"2023-10-27T06:50:27.007Z","comments":true,"path":"2023/10/27/王阁/技术/hexo/old/kafka/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/old/kafka/","excerpt":"kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic sparkstreamingkafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic flumetest","text":"kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic sparkstreamingkafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic flumetest kafka-console-producer.sh –broker-list localhost:9092 –topic flumetest :创建生产者 kafka-console-consumer.sh –bootstrap-server namenode:9092 –topic flume-ng Kafka相关小结kafka 相关指令kafka-server-start.sh config&#x2F;server.properties &amp; 启动kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic topic_name :创建topickafka-console-producer.sh –broker-list localhost:9092 –topic topic_name :创建生产者 kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic topic_name :创建消费者 kafka-console-producer.sh –broker-list namenode:9092 –topic sparkstreaming 删除group kafka-consumer-groups –bootstrap-server 192.168.10.100:9092,192.168.10.101:9092,192.168.10.102:9092 —group traffic_history —delete kafka java apikafka 虽然搭建较为简单,但想要对针它编程体验还是有些问题.初步使用下来明显感觉对版本的强约束性.以我线上版本 为例,我java项目对应的版本则是 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt; &lt;version&gt;0.8.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.8.2.1&lt;/version&gt; &lt;/dependency&gt; 以上版本搭配经由我亲测通过","categories":[],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://www.wqkenqing.ren/daydoc/tags/kafka/"}]},{"title":"mapreduce组件总结","slug":"王阁/技术/hexo/old/spark学习3","date":"2023-10-27T06:50:27.007Z","updated":"2023-10-27T06:50:27.008Z","comments":true,"path":"2023/10/27/王阁/技术/hexo/old/spark学习3/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/old/spark%E5%AD%A6%E4%B9%A03/","excerpt":"spark-core,spark-streaming再深造","text":"spark-core,spark-streaming再深造 spark go on初始规划 spark-corespark-streaming","categories":[],"tags":[{"name":"bigdata","slug":"bigdata","permalink":"http://www.wqkenqing.ren/daydoc/tags/bigdata/"}]},{"title":"","slug":"王阁/技术/hexo/CDH/cdh集成phoenix","date":"2023-10-27T06:50:27.006Z","updated":"2023-10-27T06:50:27.006Z","comments":true,"path":"2023/10/27/王阁/技术/hexo/CDH/cdh集成phoenix/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/CDH/cdh%E9%9B%86%E6%88%90phoenix/","excerpt":"","text":"cdh集成phoenix","categories":[],"tags":[]},{"title":"","slug":"王阁/技术/hexo/Thread/多线程","date":"2023-10-27T06:50:27.006Z","updated":"2023-10-27T06:50:27.006Z","comments":true,"path":"2023/10/27/王阁/技术/hexo/Thread/多线程/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/Thread/%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Flink集成安装至CDH","slug":"王阁/工作/旸谷数据中心/基础/Flink集成CDH","date":"2023-10-27T06:50:27.001Z","updated":"2023-10-27T06:50:27.001Z","comments":true,"path":"2023/10/27/王阁/工作/旸谷数据中心/基础/Flink集成CDH/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E5%B7%A5%E4%BD%9C/%E6%97%B8%E8%B0%B7%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83/%E5%9F%BA%E7%A1%80/Flink%E9%9B%86%E6%88%90CDH/","excerpt":"","text":"集成flink至CDH中 CDH集中flink准备工作需要准备的相关内容有： java （已有） maven （已有） flink 安装包 制作parcel flink相关下载 123456注：可不需要提前下载flink下载地址：https://archive.apache.org/dist/flink/flink-1.12.0/flink-1.12.0-bin-scala_2.12.tgzparcel制作工具下载地址：https://github.com/pkeropen/flink-parcel.git（github提供的工具包） flink镜像制作1234567891011121314151617181920212223cd /root/flink-parcel## 修改flink-parcel.propertiesvim flink-parcel.properties## 再按图修改CDH支持版本## 制镜像./build.sh parcel## 生成文件夹 FLINK-1.12.4-BIN-SCALA_2.11_build## 制作yarn形式的csd文件./build.sh csd_on_yarn## 生成FLINK_ON_YARN-1.12.4.jar包## 将FLINK_ON_YARN-1.12.4.jar 挪动至scm-server的 /opt/cloudera/csd 路径下mv FLINK_ON_YARN-1.12.4.jar /opt/cloudera/csd/## FLINK-1.12.4-BIN-SCALA_2.11_build 挪到至之前的/var/www/html/路径下，作为局域网安装源mv FLINK-1.12.4-BIN-SCALA_2.11_build /var/www/html/flinkcd /var/www/html/flink## 创建成repo仓库createrepo . cloudera manager中添加flink 出现flink parcel ，点击下载，分配、激活 添加对应服务这里在上面步骤中生成了 FLINK_ON_YARN-1.12.4.jar后添加到路径后，若在添加服务中看不到内容，则在这里要 12345678## 重启cloudera-serversystemctl restart cloudera-scm-server## 重启agentsystemctl restart cloudera-scm-agent## 查看日志tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log 经过上述操作后，一般即可在添加服务中找到flink-yarn的添加选项Tips: 123这里其实有一些暗坑，就是前面的选择cdh区间的操作中，要注意，即如图示进行配置，我这里是6.3.2，则要配置成如上图才行。不然可能会出现cdh版本适配区间不一致的问题 接下来，像正常安装cdh组件一样安装flink-yarn服务 但要真的让flink服务成功添加还要进行一些参数配置和修改 修改 Kerberos因为集群中暂时还未集成Kerberos所以这里的配置内容要删除，不然，会出现启动问题。 添加hadoop环境变量 因为flink1.10后开始应该就不用再集中hadoop的包，而是直接配置hadoop环境变量或从1.11分支上拉项目，自己打包。 大致这里两个参数设置后，服务应该就能正常添加了。 添加成功后的效果","categories":[],"tags":[{"name":"flink、CDH","slug":"flink、CDH","permalink":"http://www.wqkenqing.ren/daydoc/tags/flink%E3%80%81CDH/"}]},{"title":"","slug":"王阁/回顾/备战/Untitled","date":"2023-10-27T06:50:26.999Z","updated":"2023-10-27T06:50:26.999Z","comments":true,"path":"2023/10/27/王阁/回顾/备战/Untitled/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/27/%E7%8E%8B%E9%98%81/%E5%9B%9E%E9%A1%BE/%E5%A4%87%E6%88%98/Untitled/","excerpt":"","text":"需求整理业主表示想要时间轴好用。 需求梳理 时间轴联动优化 比如当时是2022.1.11~2022.3.11。那么下面全局都是这个范围 游标定位放大缩小时，游标脱离了原视野范围。 上述优化项可能带来的性能问题，要进行优化。 123456789101112131415161718192021222324252627282930# 创建 sxsddsj_shield_data_0 topickafka-topics.sh --create --bootstrap-server your-bootstrap-server --topic sxsddsj_shield_data_0 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_1 topickafka-topics.sh --create --bootstrap-server your-bootstrap-server --topic sxsddsj_shield_data_1 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_2 topickafka-topics.sh --create --bootstrap-server your-bootstrap-server --topic sxsddsj_shield_data_2 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_3 topickafka-topics.sh --create --bootstrap-server your-bootstrap-server --topic sxsddsj_shield_data_3 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_4 topickafka-topics.sh --create --bootstrap-server your-bootstrap-server --topic sxsddsj_shield_data_4 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_5 topickafka-topics.sh --create --bootstrap-server your-bootstrap-server --topic sxsddsj_shield_data_5 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_6 topickafka-topics.sh --create --bootstrap-server your-bootstrap-server --topic sxsddsj_shield_data_6 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_7 topickafka-topics.sh --create --bootstrap-server your-bootstrap-server --topic sxsddsj_shield_data_7 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_8 topickafka-topics.sh --create --bootstrap-server your-bootstrap-server --topic sxsddsj_shield_data_8 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_9 topickafka-topics.sh --create --bootstrap-server your-bootstrap-server --topic sxsddsj_shield_data_9 --partitions 3 --replication-factor 1 123456789101112131415161718192021222324252627282930# 创建 sxsddsj_shield_data_0 topickafka-topics.sh --create --bootstrap-server kafka01:9092 --topic sxsddsj_shield_data_0 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_1 topickafka-topics.sh --create --bootstrap-server kafka01:9092 --topic sxsddsj_shield_data_1 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_2 topickafka-topics.sh --create --bootstrap-server kafka01:9092 --topic sxsddsj_shield_data_2 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_3 topickafka-topics.sh --create --bootstrap-server kafka01:9092 --topic sxsddsj_shield_data_3 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_4 topickafka-topics.sh --create --bootstrap-server kafka01:9092 --topic sxsddsj_shield_data_4 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_5 topickafka-topics.sh --create --bootstrap-server kafka01:9092 --topic sxsddsj_shield_data_5 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_6 topickafka-topics.sh --create --bootstrap-server kafka01:9092 --topic sxsddsj_shield_data_6 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_7 topickafka-topics.sh --create --bootstrap-server kafka01:9092 --topic sxsddsj_shield_data_7 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_8 topickafka-topics.sh --create --bootstrap-server kafka01:9092 --topic sxsddsj_shield_data_8 --partitions 3 --replication-factor 1# 创建 sxsddsj_shield_data_9 topickafka-topics.sh --create --bootstrap-server kafka01:9092 --topic sxsddsj_shield_data_9 --partitions 3 --replication-factor 1","categories":[],"tags":[]},{"title":"自动化思路","slug":"王阁/运营/未分类/自动化思路","date":"2023-10-26T03:03:20.000Z","updated":"2023-10-27T06:50:27.036Z","comments":true,"path":"2023/10/26/王阁/运营/未分类/自动化思路/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2023/10/26/%E7%8E%8B%E9%98%81/%E8%BF%90%E8%90%A5/%E6%9C%AA%E5%88%86%E7%B1%BB/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%80%9D%E8%B7%AF/","excerpt":"","text":"链路自动化思路自动化视频去水印尝试通过脚本实现自动化. 步骤： 拾取视频地址 传递视频地址信息给 https://dlpanda.com/ 下载视频到指定地址","categories":[{"name":"运营","slug":"运营","permalink":"http://www.wqkenqing.ren/daydoc/categories/%E8%BF%90%E8%90%A5/"}],"tags":[]},{"title":"SparkRDD算子","slug":"王阁/技术/hexo/spark/算子/RDD算子","date":"2020-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.010Z","comments":true,"path":"2020/07/16/王阁/技术/hexo/spark/算子/RDD算子/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2020/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/spark/%E7%AE%97%E5%AD%90/RDD%E7%AE%97%E5%AD%90/","excerpt":"","text":"RDD算子合集 transform * map * flatMap * filter * distinct * union * intersection * substract * cartesian * maptoPair * flatMapToPair * combineByKey * reduceByKey * foldByKey * sortByKey * groupByKey * cogroup * substractByKey * join * fullOuterJoin * leftOuterJoin * rightOuterJoin Action * first * take","categories":[],"tags":[{"name":"spark","slug":"spark","permalink":"http://www.wqkenqing.ren/daydoc/tags/spark/"},{"name":"rdd","slug":"rdd","permalink":"http://www.wqkenqing.ren/daydoc/tags/rdd/"},{"name":"算子","slug":"算子","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E7%AE%97%E5%AD%90/"}]},{"title":"ELK-beats","slug":"王阁/技术/运维/ELK/logstash/beats","date":"2020-07-09T16:00:00.000Z","updated":"2023-10-27T06:50:27.031Z","comments":true,"path":"2020/07/10/王阁/技术/运维/ELK/logstash/beats/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2020/07/10/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/ELK/logstash/beats/","excerpt":"本文主要分为两大块,一是官网beat的介绍,与相应配置的说明.二是被官方收录的beat","text":"本文主要分为两大块,一是官网beat的介绍,与相应配置的说明.二是被官方收录的beat beatselastic官网常规的beats主要有五种 Filebeat 日志文件 Metricbeat 指标 Packetbeat 网络数据 Winlogbeat windows事件日志 AuditBeat 审计数据 HeartBeat 运行时间监控 Functionbeat 无需要服务器的采集器 Filebeat","categories":[],"tags":[{"name":"beats","slug":"beats","permalink":"http://www.wqkenqing.ren/daydoc/tags/beats/"}]},{"title":"logstash使用","slug":"王阁/技术/运维/ELK/logstash/logstash","date":"2020-07-08T16:00:00.000Z","updated":"2023-10-27T06:50:27.031Z","comments":true,"path":"2020/07/09/王阁/技术/运维/ELK/logstash/logstash/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2020/07/09/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/ELK/logstash/logstash/","excerpt":"","text":"logstash使用logstash使用管道方式进行日志的收集处理与输出 一般包括三个阶段 输入input-&gt;处理filter-&gt;输出output 横线之上的内容是在logstash引入codec概念之前的主要流程,但引入codec之后则有所不同. 这里需要纠正之前的一个概念。Logstash 不只是一个input | filter | output 的数据流，而是一个input | decode | filter | encode | output 的数据流！codec 就是用来 decode、encode 事件的。 即在接收到数据后,可以对数据进行再编码和解码后输出 即配置文件组成部份一般如下: 12345678910input &#123; &#125;filter &#123; &#125;output &#123; &#125; 执行指令 1logstash -f demo.conf 所以这里针对logstash主要的内容还是针对不同组件模块进行配置 elasticsearch12345678910output &#123;#stdout &#123; codec =&gt; rubydebug &#125;elasticsearch &#123;hosts =&gt; [&quot;127.0.0.1:9200&quot;]template_overwrite =&gt; trueindex =&gt; &quot;rediscluster-%&#123;+YYYY.MM.dd&#125;&quot;workers =&gt; 5&#125;&#125; syslogsyslog.conf,rsyslog.conf","categories":[],"tags":[{"name":"logstash","slug":"logstash","permalink":"http://www.wqkenqing.ren/daydoc/tags/logstash/"},{"name":"log","slug":"log","permalink":"http://www.wqkenqing.ren/daydoc/tags/log/"}]},{"title":"elk使用","slug":"王阁/技术/大数据/es/elk使用","date":"2020-06-16T16:00:00.000Z","updated":"2023-10-27T06:50:27.011Z","comments":true,"path":"2020/06/17/王阁/技术/大数据/es/elk使用/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2020/06/17/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/es/elk%E4%BD%BF%E7%94%A8/","excerpt":"","text":"elk使用","categories":[],"tags":[{"name":"logstash","slug":"logstash","permalink":"http://www.wqkenqing.ren/daydoc/tags/logstash/"},{"name":"beats","slug":"beats","permalink":"http://www.wqkenqing.ren/daydoc/tags/beats/"}]},{"title":"elasticsearch dsl 详解","slug":"王阁/技术/大数据/es/esDSL详解","date":"2020-06-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.011Z","comments":true,"path":"2020/06/16/王阁/技术/大数据/es/esDSL详解/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2020/06/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/es/esDSL%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"DSL dsl the full name isDomain Specific Language it is divided to two parts Leaf query clauses: particular query such as :term, match,range Compound query clauses: Compound query clauses wrap other leaf or compound queries and are used to combine multiple queries in a logical fashion (such as the bool or dis_max query), or to alter their behaviour (such as the constant_score query). Match all query全文查询，用于对分词的字段进行搜索。会用查询字段的分词器对查询的文本进行分词生成查询。可用于短语查询、模糊查询、前缀查询、临近查询等查询场景 query_all 1234567GET /_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; Full text queriesmatchmatch_phrase对句子,进行短语查询 match_phrase_prefixmulti_matchcommon termsquery_stringsimple_query_string","categories":[],"tags":[{"name":"dsl","slug":"dsl","permalink":"http://www.wqkenqing.ren/daydoc/tags/dsl/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://www.wqkenqing.ren/daydoc/tags/elasticsearch/"}]},{"title":"kafka极客","slug":"王阁/技术/大数据/kafka/kafka极客","date":"2020-06-14T16:00:00.000Z","updated":"2023-10-27T06:50:27.014Z","comments":true,"path":"2020/06/15/王阁/技术/大数据/kafka/kafka极客/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2020/06/15/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/kafka/kafka%E6%9E%81%E5%AE%A2/","excerpt":"kafka 极客时间","text":"kafka 极客时间","categories":[],"tags":[{"name":"kakfa","slug":"kakfa","permalink":"http://www.wqkenqing.ren/daydoc/tags/kakfa/"},{"name":"极客时间","slug":"极客时间","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"}]},{"title":"python爬虫篇","slug":"王阁/技术/编程/python/reptile/python爬虫篇","date":"2020-06-09T16:00:00.000Z","updated":"2023-10-27T06:50:27.030Z","comments":true,"path":"2020/06/10/王阁/技术/编程/python/reptile/python爬虫篇/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2020/06/10/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E7%BC%96%E7%A8%8B/python/reptile/python%E7%88%AC%E8%99%AB%E7%AF%87/","excerpt":"","text":"uses packages requests","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://www.wqkenqing.ren/daydoc/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E7%88%AC%E8%99%AB/"}]},{"title":"docker启动mysql","slug":"王阁/日常/运维/docker启动mysql","date":"2020-06-03T16:00:00.000Z","updated":"2023-10-27T06:50:27.035Z","comments":true,"path":"2020/06/04/王阁/日常/运维/docker启动mysql/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2020/06/04/%E7%8E%8B%E9%98%81/%E6%97%A5%E5%B8%B8/%E8%BF%90%E7%BB%B4/docker%E5%90%AF%E5%8A%A8mysql/","excerpt":"docker 的方式快速安装mysql","text":"docker 的方式快速安装mysql 123docker run --net host -p 3306:3306 --name jd_user -e MYSQL_ROOT_PASSWORD=jd123456 -d mysql","categories":[],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://www.wqkenqing.ren/daydoc/tags/mysql/"},{"name":"docker","slug":"docker","permalink":"http://www.wqkenqing.ren/daydoc/tags/docker/"},{"name":"运维","slug":"运维","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"python小结5","slug":"王阁/技术/编程/python/summary/python5小结","date":"2020-06-03T16:00:00.000Z","updated":"2023-10-27T06:50:27.030Z","comments":true,"path":"2020/06/04/王阁/技术/编程/python/summary/python5小结/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2020/06/04/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E7%BC%96%E7%A8%8B/python/summary/python5%E5%B0%8F%E7%BB%93/","excerpt":"csv, json ,poi处理","text":"csv, json ,poi处理 csv 12import csv 主要用到的方式 csv.reader() csv.writer() 读csv和写csv json12345678910111213141516171819202122import json##针对对象## 将python对象编码成json字符串, 返回json串json.dumps()## 将已编码的json串解码为python对象，返回python对应的数据类型json.loads()##针对文件的操作json.dump()json.load()# 写入 JSON 数据with open(&#x27;data.json&#x27;, &#x27;w&#x27;) as f: json.dump(data, f) # 读取数据with open(&#x27;data.json&#x27;, &#x27;r&#x27;) as f: data = json.load(f) elasticsearch12import","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://www.wqkenqing.ren/daydoc/tags/python/"},{"name":"json","slug":"json","permalink":"http://www.wqkenqing.ren/daydoc/tags/json/"},{"name":"xml","slug":"xml","permalink":"http://www.wqkenqing.ren/daydoc/tags/xml/"},{"name":"poi","slug":"poi","permalink":"http://www.wqkenqing.ren/daydoc/tags/poi/"}]},{"title":"python小结4","slug":"王阁/技术/编程/python/summary/python小结4","date":"2020-06-01T16:00:00.000Z","updated":"2023-10-27T06:50:27.030Z","comments":true,"path":"2020/06/02/王阁/技术/编程/python/summary/python小结4/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2020/06/02/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E7%BC%96%E7%A8%8B/python/summary/python%E5%B0%8F%E7%BB%934/","excerpt":"python 常见的connect and operate","text":"python 常见的connect and operate python 小结4python connect redis通过pip install redis 在python3中安装redis的包 1234pool =redis.ConnectionPool(host=&#x27;jd_cloud&#x27;, port=6380, db=0, password=&#x27;test123&#x27;)r = redis.Redis(connection_pool=pool) python connect kafkakafka producerkafka consumer多种consumerpython 文本处理","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://www.wqkenqing.ren/daydoc/tags/python/"},{"name":"connect","slug":"connect","permalink":"http://www.wqkenqing.ren/daydoc/tags/connect/"}]},{"title":"python小结3","slug":"王阁/技术/编程/python/summary/python小结3","date":"2020-05-28T16:00:00.000Z","updated":"2023-10-27T06:50:27.030Z","comments":true,"path":"2020/05/29/王阁/技术/编程/python/summary/python小结3/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2020/05/29/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E7%BC%96%E7%A8%8B/python/summary/python%E5%B0%8F%E7%BB%933/","excerpt":"","text":"python小结3模块面向对象类与实例python类的声()明会在类名上加os.rmdir() eg 123class Student(object): pass 构造函数 12def __init__(self,field1,field2): pass 访问控制python中 私有属性是通过 __来标识,用__标识后则表示该属性是这个类的私有属性,则意味着直接通过一般的*.__是难以访问到的. 如果是_的形式,则也是声明将这个认作是私有变量的属性,但却是可以直接通过*._*的形式进行访问的. &#96;注意:__的属性并不是完全不能用实例直接访问,如Student类为例通过student._Stuent__name的形式也能直接访问 从这里来看,这一点是不太符合面向对象编程的精神的,也就是说python解释器不会从根本上防止你乱搞,取用哪种编程规范主要还是要靠自觉 &#96; 继承与多态总得来讲与java类似.但因为python是动态语言,所以它符合”鸭子类型”的特点,即在多态使用时,只要具有同样的方法,即可接收调用. 获取对象信息通过types模块下的type()方法来获取对象类型 通过isinstance()来判断是否是这个类,可用于测试继承,也可通过逗号分隔,测试是不是这些类其中之一通过dir()获取一个对象的所有属性和方法 实例属性与类属性python 是动态语言,所以可以在创建实例后再赋值 eg: 1234student=Stuent()student.score=90 即这score是后续才赋值上去的. 这就是实例属性 而本身类需要绑定一些属性呢,这就可以直接定义在class中,即类属性. 1234class Student(object): name=&#x27;ken&#x27; age=18 面向对象进阶使用__slots__因为动态语言的特性,python实例在创建后还能再绑定方法与属性. 这里如果我们想要约束只绑定某些属性时可以通过 slots(a,b)的形式来操作. 注意:该类若被子类继承,在子类未定义__slots__时,子类实例是不会生效的. @property类似java getter @feild.setter 多重继承因为python是动态语言,所以能够继承多个类,但需要考虑下设计理念,即扩展继承时,扩展继承的类可以命名为**Runnable,与 XXMixIn 定制类的方法inititerstr 等方法,这些方法统一后面来整理 枚举类元类有些类似反射后面再定向研究 错误与调试try…except…finally.. 文件读写读文件要以读文件的模式打开一个文件对象，使用Python内置的open()函数，传入文件名和标示符： file open 最后需要file.close 可以通过with open(“path”,’’) as f : 的写法来简写,这样写不用写f.close() open(‘path’,’rb’) &#x2F;&#x2F;这样写才是读文件 读时,f.read()会将文件全量写到内存里去,所以小文件这么简写还能接受,但若文件较大,内存会有较大压力. f.read(size),则是一次读取内容的大小. f.readlines()则是读成list的形式. open(‘path’,’wb’)&#x2F;&#x2F;这样获取的文件操作实例是写对象 f.write(“content”) 同样也可以用with的写法来获取对象 前者写对象的获取的实例,若文件存在,则会覆盖该文件 open(‘path’,’wba’)这样的写法是续写文件 内存数据操作(StringIO ByteIO)操作文件和目录os.path.joinos.pathos.path.exists() os.mkdir()os.rmdir() os.path.abspath() 常用包datetime.datetime 即datetime包下的datetime类 123import datetime.datetimeor from datetime import datetime 获取当前时间datetime.now() 获取指定的时间dt&#x3D;datetime(2020,05,23,12,32) 转成timestampdt.timestamp() timestamp to datetime datetime.fromtimestamp(t) 转成格林制式时间datetime.utcfromtimestamp(t) str转成datetime collectionsnamedtupledeque (“双向列表”)defaultdict使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict： OrderedDict注意，OrderedDict的Key会按照插入的顺序排列，不是Key本身排序： ChainMap(再研究)CounterCounter实际上也是dict的一个子类，上面的结果可以看出每个字符出现的次数。","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://www.wqkenqing.ren/daydoc/tags/python/"}]},{"title":"mac python版本替换","slug":"王阁/日常/运维/mac python版本替换","date":"2020-05-24T16:00:00.000Z","updated":"2023-10-27T06:50:27.035Z","comments":true,"path":"2020/05/25/王阁/日常/运维/mac python版本替换/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2020/05/25/%E7%8E%8B%E9%98%81/%E6%97%A5%E5%B8%B8/%E8%BF%90%E7%BB%B4/mac%20python%E7%89%88%E6%9C%AC%E6%9B%BF%E6%8D%A2/","excerpt":"","text":"1mac python版本替换","categories":[],"tags":[{"name":"mac python2 python3","slug":"mac-python2-python3","permalink":"http://www.wqkenqing.ren/daydoc/tags/mac-python2-python3/"}]},{"title":"flink学习","slug":"王阁/技术/hexo/flink/Flink","date":"2019-07-30T16:00:00.000Z","updated":"2023-10-27T06:50:27.006Z","comments":true,"path":"2019/07/31/王阁/技术/hexo/flink/Flink/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/31/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/flink/Flink/","excerpt":"flink内容记录","text":"flink内容记录 搭建创建maven项目12345678910mvn archetype:generate \\ -DarchetypeGroupId=org.apache.flink \\ -DarchetypeArtifactId=flink-quickstart-java \\ -DarchetypeVersion=1.6.1 \\ -DgroupId=my-flink-project \\ -DartifactId=my-flink-project \\ -Dversion=0.1 \\ -Dpackage=myflink \\ -DinteractiveMode=false 123mvn clean package -Dmaven.test.skip=true 1flink run -c myflink.demo.SocketTextStreamWordCount my-flink-project-0.1.jar 127.0.0.1 9000 DataStream APIflink程序工作解剖图 执行环境flink支持 获取已经存在的flink环境 创建一个本地环境 创建一个远程环境 DataSource预置sourceSocket-based socketTextStream(); File-based Transfomations map flatMap filter keyBy reduce fold 合计 min max sum 窗口","categories":[],"tags":[{"name":"flink","slug":"flink","permalink":"http://www.wqkenqing.ren/daydoc/tags/flink/"}]},{"title":"hdfs命令","slug":"王阁/技术/hexo/hadoop/hdfs/hdfs命令","date":"2019-07-16T16:00:00.000Z","updated":"2023-10-27T06:50:27.007Z","comments":true,"path":"2019/07/17/王阁/技术/hexo/hadoop/hdfs/hdfs命令/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/17/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/hadoop/hdfs/hdfs%E5%91%BD%E4%BB%A4/","excerpt":"hdfs常用命令","text":"hdfs常用命令 count 1该命令选项显示指定路径下的文件夹数量、文件数量、文件总大小信息，如图4-6所示。 du 1统计目录下各文件大小 touchz 12创建空白文件 -stat 12“%b %n %o %r %Y”依次表示文件大小、文件名称、块大小、副本数、访问时间。","categories":[],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"http://www.wqkenqing.ren/daydoc/tags/hdfs/"}]},{"title":"sparkstreaming 窗口操作","slug":"王阁/技术/hexo/spark/stream2","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.010Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/spark/stream2/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/spark/stream2/","excerpt":"sparkstreaming时间窗口设置","text":"sparkstreaming时间窗口设置 说明通过sparkstreaming设置窗口函数,可达到如,每10秒计算前30秒内数据的效果 如上 主要有两个参数 窗口大小 滑动距离 val windowedWordCounts &#x3D; pairs.reduceByKeyAndWindow(_ + _, Seconds(30), Seconds(10)) 如上 常用api Transformation Meaning window(windowLength, slideInterval) Return a new DStream which is computed based on windowed batches of the source DStream. countByWindow(windowLength,slideInterval) Return a sliding window count of elements in the stream. reduceByWindow(func, windowLength,slideInterval) reduceByKeyAndWindow(func,windowLength, slideInterval, [numTasks]) reduceByKeyAndWindow(func, invFunc,windowLength, slideInterval, [numTasks]) countByValueAndWindow(windowLength,slideInterval, [numTasks])","categories":[],"tags":[{"name":"sparkstreaming","slug":"sparkstreaming","permalink":"http://www.wqkenqing.ren/daydoc/tags/sparkstreaming/"}]},{"title":"宽窄依赖","slug":"王阁/技术/hexo/spark/宽窄依赖","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.010Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/spark/宽窄依赖/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/spark/%E5%AE%BD%E7%AA%84%E4%BE%9D%E8%B5%96/","excerpt":"spark依赖说明","text":"spark依赖说明 种类spark的依赖关系大致有两类 narrow dependency wide dependency 说明narrow dependency父Partition &#x3D;&#x3D;&#x3D;&gt; 子partition 多对一或一对一 flatMap ,mapToPair ,map ,filter等算子父partition &#x3D;&#x3D;&#x3D;&gt; 子partition 一对多 reduce ,group by 等. stage当一个dag串联遇到宽依赖时形成stage.一个stage对应一个task.这个task的并行度由最后一个依赖决定.应该就是说由wide dependency 的具体并行度决度.如reduce ,partition&#x3D;3.就3的并行度.这里的参数可以设置. wide dependency 必定对应的有shuffle.但shuffle不一定是wide dependency 如sort orderby join 即可能发生shuffle也可能不,具体看情况. pipeline一个stage划分好后.一条数据的具体运算逻辑是会一直走完所有计算逻辑后才会落地.这是与mapreduce的区别mapreduce是计算逻辑走完落地,再启动,计算又落地. 所以说spark的效率比mapreduce高也是有这个原因.dag串联后,运算优先.","categories":[],"tags":[{"name":"spark dependency","slug":"spark-dependency","permalink":"http://www.wqkenqing.ren/daydoc/tags/spark-dependency/"}]},{"title":"java八大数据类型总结","slug":"王阁/技术/hexo/oldblog/blog1","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.008Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog1/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog1/","excerpt":"此处简介","text":"此处简介 java八大数据类型总结[TOC] 负数在电脑中的存储是用（该数值的绝对值的反码+1表示）最高位是符号位，1表示负数，0表示正数负数换算规则:负数的二进制=负数绝对值的二进制取反码+1。 byte类型: byte类型，使用一个字节存放一个数据，一个字节占八位，所以它取值范围是:1000 0000 ~ 0111 1111(-128-127) short类型 short类型，使用两个字节存放一个数据，两个字节16位二进制表示，那么它的取值范围就是： 1000 0000 0000 0000 ~ 0111 1111 1111 1111(-32768-32767)换算同上述byte类型 char类型 char在java中占据两个字节，即用16位表示一个char类型的数据。由于char是无符号的所以其表示范围是0-65536.当计算超过其表示范围时，系统会自动将结果转换为int类型。 boolean类型: boolean类型占用一个字节，八位二进制表示。boolean类型只有两个值true和flase。 float类型 其他特殊表示: 1.当指数部分和小数部分全为0时,表示0值,有+0和-0之分(符号位决定),0x00000000表示正0,0x80000000表示负0. 2.指数部分全1,小数部分全0时,表示无穷大,有正无穷和负无穷,0x7f800000表示正无穷,0xff800000表示负无穷. 3.指数部分全1,小数部分不全0时,表示NaN,分为QNaN和SNaN,Java中都是NaN. 可以看出浮点数的取值范围是:2^(-149)~~(2-2^(-23))*2^127,也就是Float.MIN_VALUE和Float.MAX_VALUE. double类型 double类型占8个字节，一共是64位二进制表示。数符加尾数占48位，指数符加指数占16位.取值换算方式和float的换算方式一样。但是在使用float和double时最好先分析好目标数据的精度和性能要求，如果能够使用float满足的坚决不适用double，因为double类型使用内存占用是float的两倍，运算速度远不如float。 short类型 short类型，使用两个字节存放一个数据，两个字节16位二进制表示，那么它的取值范围就是： 1000 0000 0000 0000 ~ 0111 1111 1111 1111(-32768-32767)换算同上述byte类型。short类型使用时除了要注意取值范围 int类型 int类型，在Java中使用的是四个字节保存一个数据，一共是32为二进制表示，同上述的一样,取值范围:1000 0000 0000 0000 0000 0000 0000 0000 - 0111 1111 1111 1111 1111 1111 1111 1111 (-2^32~2^31) long类型 long类型是Java的基础类型，使用8个字节存储一个数值，一共是64位二进制数。取值范围是（-2^64-2^63 八大常用类型的最大值与最小值float max=3.4028235E38 float min=1.4E-45 double max=1.7976931348623157E308 double max=4.9E-324 byte max=127 byte min=-128 char max=? char min= short max=32767 short min=-32768 int max=2147483647 int min=-2147483648 long max=9223372036854775807 long min=-9223372036854775808 | 数据类型 | byte类型 | short类型 |char类型|boolean类型|float类型|int类型|double类型|long类型|| :——– | ——–:| :——:|所占字节数 | 2| 2|2|8|4|4|8|8|","categories":[],"tags":[]},{"title":"异常处理机制小结","slug":"王阁/技术/hexo/oldblog/blog10","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.008Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog10/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog10/","excerpt":"此处简介","text":"此处简介 异常处理机制小结 在 Java 中，所有的异常都有一个共同的祖先 Throwable（可抛出）。Throwable 指定代码中可用异常传播机制通过 Java 应用程序传输的任何问题的共性。 Throwable： 有两个重要的子类：Exception（异常）和 Error（错误），二者都是 Java 异常处理的重要子类，各自都包含大量子类。 Error（错误）:是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。 Exception（异常）:是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 类及其子类表示“JVM 常用操作”引发的错误。例如，若试图使用空值对象引用、除数为零或数组越界，则分别引发运行时异常（NullPointerException、 ArithmeticException）和 ArrayIndexOutOfBoundException。 注意：异常和错误的区别：异常能被程序本身可以处理，错误是无法处理。 通常，Java的异常(包括Exception和Error)分为可查的异常（checked exceptions）和不可查的异常（unchecked exceptions）。 可查异常（编译器要求必须处置的异常）：正确的程序在运行中，很容易出现的、情理可容的异常状况。可查异常虽然是异常状况，但在一定程度上它的发生是可以预计的，而且一旦发生这种异常状况，就必须采取某种方式进行处理。","categories":[],"tags":[]},{"title":"git小结","slug":"王阁/技术/hexo/oldblog/blog12","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.008Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog12/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog12/","excerpt":"此处简介","text":"此处简介 git小结常用指令小结 git init 初始化 git add 将工作区的变更提交至暂存区 git commit -m 将暂存区的内容提交至版本库 git log 查看记录 git reflog 操作记录 git –hard commit id 回到对应的id版本下 git status 查看状态 git log –pretty&#x3D;oneline 简约查看log git diff 查看文件与版本库中的差异 git checkout – file 文件在工作区的修改全部撤销 git remote add origin &#x67;&#x69;&#x74;&#64;&#103;&#x69;&#116;&#x68;&#117;&#98;&#46;&#x63;&#x6f;&#x6d;:michaelliao&#x2F;learngit.git 添加远程库 git push -u origin master 推送分支 git clone &#x67;&#105;&#116;&#x40;&#103;&#105;&#x74;&#x68;&#x75;&#x62;&#x2e;&#x63;&#x6f;&#109;:michaelliao&#x2F;gitskills.git 克隆 git checkout -b dev 创建并切换分支 git branch dev 创建分支 git checkout dev 切换分支 git merge dev 合并分支 git branch -d 删除分支 git stash 紧急切分支时，将工作区的变更内容暂存起来。 git stash list 查看stash列表 git stash apply 回复当前分支stash内容 git stash pop 删除stash内容 git branch -D 强行删除 git checkout -b dev origin&#x2F;dev 拉下远程分支 git push origin branch-name 推送分支 git pull 从远程库拉取更新 git push origin branch-name 将本地库中的更新推送至远程库 git branch –set-upstream branch-name origin&#x2F;branch-name 建立本地分支与远程库的联系 git tag 用于新建一个标签，默认为HEAD，也可以指定一个commit id；打标签 git tag -a -m “blablabla…”可以指定标签信息； git tag -s -m “blablabla…”可以用PGP签名标签； git push origin git push origin –tags git tag -d git push origin :refs&#x2F;tags&#x2F;","categories":[],"tags":[]},{"title":"maven小结","slug":"王阁/技术/hexo/oldblog/blog13","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.008Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog13/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog13/","excerpt":"此处简介","text":"此处简介 maven小结什么是maven就是一款帮助程序员构建项目的工具,我们只需要告诉Maven需要哪些Jar 包，它会帮助我们下载所有的Jar，极大提升开发效率。 Maven规定的目录结构Maven基本命令 -v:查询Maven版本本命令用于检查maven是否安装成功。Maven安装完成之后，在命令行输入mvn -v，若出现maven信息，则说明安装成功 compile：编译 test:测试项目 package:打包 clean:删除target文件夹 install:安装 将当前项目放到Maven的本地仓库中。供其他项目使用 什么是Maven仓库？Maven仓库用来存放Maven管理的所有Jar包。分为：本地仓库 和 本地仓库。 本地仓库Maven本地的Jar包仓库。 中央仓库Maven官方提供的远程仓库。 当项目编译时，Maven首先从本地仓库中寻找项目所需的Jar包，若本地仓库没有，再到Maven的中央仓库下载所需Jar包。 什么是“坐标”？在Maven中，坐标是Jar包的唯一标识，Maven通过坐标在仓库中找到项目所需的Jar包。 如下代码中，groupId和artifactId构成了一个Jar包的坐标。 12345&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt; groupId:所需Jar包的项目名artifactId:所需Jar包的模块名version:所需Jar包的版本号 传递依赖 与 排除依赖 传递依赖：如果我们的项目引用了一个Jar包，而该Jar包又引用了其他Jar包，那么在默认情况下项目编译时，Maven会把直接引用和简洁引用的Jar包都下载到本地。 排除依赖：如果我们只想下载直接引用的Jar包，那么需要在pom.xml中做如下配置：(将需要排除的Jar包的坐标写在中) 123456&lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;/exclusion&gt;&lt;/exclusions&gt; 依赖冲突若项目中多个Jar同时引用了相同的Jar时，会产生依赖冲突，但Maven采用了两种避免冲突的策略，因此在Maven中是不存在依赖冲突的。短路优先本项目——&gt;A.jar——&gt;B.jar——&gt;X.jar本项目——&gt;C.jar——&gt;X.jar声明优先若引用路径长度相同时，在pom.xml中谁先被声明，就使用谁。 聚合什么是聚合？将多个项目同时运行就称为聚合。如何实现聚合？只需在pom中作如下配置即可实现聚合： 12345&lt;modules&gt; &lt;module&gt;../模块1&lt;/module&gt; &lt;module&gt;../模块2&lt;/module&gt; &lt;module&gt;../模块3&lt;/module&gt; &lt;/modules&gt; 继承什么是继承？在聚合多个项目时，如果这些被聚合的项目中需要引入相同的Jar，那么可以将这些Jar写入父pom中，各个子项目继承该pom即可。如何实现继承？父pom配置：将需要继承的Jar包的坐标放入标签即可。 123456789&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 子pom配置： 12345&lt;parent&gt; &lt;groupId&gt;父pom所在项目的groupId&lt;/groupId&gt; &lt;artifactId&gt;父pom所在项目的artifactId&lt;/artifactId&gt; &lt;version&gt;父pom所在项目的版本号&lt;/version&gt;&lt;/parent&gt; Maven本地资源库 通常情况下，可改变默认的 .m2 目录下的默认本地存储库文件夹到其他更有意义的名称，例如 当你建立一个 Maven 的项目，Maven 会检查你的 pom.xml 文件，以确定哪些依赖下载。首先，Maven 将从本地资源库获得 Maven 的本地资源库依赖资源，如果没有找到，然后把它会从默认的 Maven 中央存储库 – http://repo1.maven.org/maven2/ 查找下载","categories":[],"tags":[]},{"title":"Vim笔记","slug":"王阁/技术/hexo/oldblog/blog15","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.008Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog15/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog15/","excerpt":"此处简介","text":"此处简介 Vim笔记vi有三种工作模式： 命令模式 插入模式 和编辑模式。 插入模式 命令 作用a 在光标后附加文本A 在本行行末附加文本i 在光标前插入文本I 在本行开始插入文本o 在光标下插入新行O 在光标上插入新行 定位命令命令 作用h、方向左键 左移一个字符j、方向下键 下移一行k、方向上键 上移一行l、方向右键 右移一个字符$ 移至行尾0 移至行首H 移至屏幕上端M 移至屏幕中央L 移至屏幕下端:set nu 设置行号:set nonu 取消行号ggG 到第一行到最后一行nG 到第n行:n 到第n行 删除命令命令 作用x 删除光标所在处字符nx 删除光标所在处后n个字符dd 删除光标所在行，ndd删除n行dG 删除光标所在行到文件末尾的内容D 删除从光标所在处到行尾的内容:n1,n2d 删除指定范围的行 复制和剪切命令命令 作用yy、Y 复制当前行nyy、nY 复制当前行一下n行dd 剪切当前行ndd 剪切当前行以下n行p、P 粘贴在当前光标所在行下或行上 注：在vi中，剪切就是删除之后再粘贴 替换和取消命令命令 作用r 取代光标所在处字符R 从光标所在处开始替换字符，按Esc结束u 取消上一步操作 注：比如改变单个字符，先输入r，再输入需要更改的字符。比如将字符a改成b。这适合用于少量修改时使用 搜索和替换命令命令 作用&#x2F;string 向前搜索指定字符串搜索时忽略大小写 :set icn 搜索指定字符串的下一个出现位置:%s&#x2F;old&#x2F;new&#x2F;g 全文替换指定字符串:n1,n2s&#x2F;old&#x2F;new&#x2F;g 在一定范围内替换指定字符串 注：n是从前往后，N是从后往前找set noic是设置大小写敏感:n1,n2s&#x2F;old&#x2F;new&#x2F;c 替换时进行询问是否真的替换 ZZ与:wq的作用一样，都是保存退出对于readonly文件，如果是root或者改文件所有者，即使该文件没有写权限，使用:wq!也能保存该修改之后的文件。仅仅保存但不退出 :w另存为 :w &#x2F;root&#x2F;file.bak 其它命令导入文件 :r 文件名在vi中执行命令 :! 命令定义快捷键 :map 快捷键 触发命令范例： :map ^P I# 注：^p是这样输入的 ctrl+v+v –&gt; ^p :map ^B 0x连续行注释 :n1,n2s&#x2F;^&#x2F;#&#x2F;g 注：^表示行首 :n1,n2s&#x2F;^#&#x2F;&#x2F;g :n1,n2s&#x2F;^&#x2F;//&#x2F;g替换 :ab huhuimail &#x68;&#x75;&#104;&#117;&#x69;&#x63;&#x73;&#64;&#x67;&#109;&#97;&#105;&#x6c;&#x2e;&#99;&#111;&#109; 取消ab命令 :unan huhuimail:r !date 在vi中加入命令执行的结果快捷键插入邮箱 :map ^e &#105;&#104;&#x75;&#104;&#117;&#x69;&#x63;&#x73;&#64;&#103;&#x6d;&#97;&#105;&#108;&#x2e;&#x63;&#111;&#109; 修改用户vim设置修改用户vim设置，比如能永久保存快捷键vi ~&#x2F;.vimrc缺省这个文件是空的，然后可以写入一些快捷键","categories":[],"tags":[]},{"title":"Restful架构","slug":"王阁/技术/hexo/oldblog/blog16","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.008Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog16/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog16/","excerpt":"此处简介","text":"此处简介 Restful架构什么是RESTful架构Representational State Transfer:表现层状态转化 REST的名称”表现层状态转化”中，省略了主语。”表现层”其实指的是”资源”（Resources）的”表现层”。 所谓”资源”，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。 “资源”是一种信息实体，它可以有多种外在表现形式。我们把”资源”具体呈现出来的形式，叫做它的”表现层”（Representation）。 访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。 互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生”状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是”表现层状态转化”。 客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。 综合上面的解释，我们总结一下什么是RESTful架构： 每一个URI代表一种资源； 客户端和服务器之间，传递这种资源的某种表现层； 客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。 最常见的一种设计错误，就是URI包含动词。因为”资源”表示一种实体，所以应该是名词，URI不应该有动词，动词应该放在HTTP协议中。 举例来说，某个URI是&#x2F;posts&#x2F;show&#x2F;1，其中show是动词，这个URI就设计错了，正确的写法应该是&#x2F;posts&#x2F;1，然后用GET方法表示show。 如果某些动作是HTTP动词表示不了的，你就应该把动作做成一种资源。比如网上汇款，从账户1向账户2汇款500元，错误的URI是： POST &#x2F;accounts&#x2F;1&#x2F;transfer&#x2F;500&#x2F;to&#x2F;2 正确的写法是把动词transfer改成名词transaction，资源不能是动词，但是可以是一种服务： POST &#x2F;transaction HTTP&#x2F;1.1 Host: 127.0.0.1 from&#x3D;1&amp;to&#x3D;2&amp;amount&#x3D;500.00 另一个设计误区，就是在URI中加入版本号： http://www.example.com/app/1.0/foo http://www.example.com/app/1.1/foo http://www.example.com/app/2.0/foo 因为不同的版本，可以理解成同一种资源的不同表现形式，所以应该采用同一个URI。版本号可以在HTTP请求头信息的Accept字段中进行区分（参见Versioning REST Services）：Accept: vnd.example-com.foo+json; version&#x3D;1.0 Accept: vnd.example-com.foo+json; version&#x3D;1.1 Accept: vnd.example-com.foo+json; version&#x3D;2.0","categories":[],"tags":[]},{"title":"kafka小结","slug":"王阁/技术/hexo/oldblog/blog17","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.008Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog17/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog17/","excerpt":"此处简介","text":"此处简介 kafka小结消息系统术语kafka特性 分布式的 可分区的 可复制的 在普通的消息系统的功上，还有自己独特的设计 Kafka将消息以topic为单位进行归纳。将向Kafka topic发布消息的程序成为producers.将预订topics并消费消息的程序成为consumer.Kafka以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个broker.producers通过网络将消息发送到Kafka集群，集群向消费者提供消息， 客户端和服务端通过TCP协议通信。Kafka提供了Java客户端，并且对多种语言都提供了支持。 Topics 和Logs 先来看一下Kafka提供的一个抽象概念:topic.一个topic是对一组消息的归纳。对每个topic，Kafka 对它的日志进行了分区 一个topic是对一组消息的归纳。对每个topic，Kafka 对它的日志进行了分区， 每个分区都由一系列有序的、不可变的消息组成，这些消息被连续的追加到分区中。分区中的每个消息都有一个连续的序列号叫做offset,用来在分区中唯一的标识这个消息。 kafka常用指令收集查看topic的详细信息kafka-topics.sh -zookeeper 127.0.0.1:2181 -describe -topic topic name 为topic增加副本kafka-reassign-partitions.sh -zookeeper 127.0.0.1:2181 -reassignment-json-file json&#x2F;partitions-to-move.json -execute创建topickafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic name为topic增加partitionkafka-topics.sh –zookeeper 127.0.0.1:2181 –alter –partitions 20 –topic namekafka生产者客户端命令kafka-console-producer.sh –broker-list localhost:9092 –topic namekafka消费者客户端命令kafka-console-consumer.sh -zookeeper localhost:2181 –from-beginning –topic namekafka服务启动kafka-server-start.sh -daemon ..&#x2F;config&#x2F;server.properties删除topickafka-run-class.sh kafka.admin.DeleteTopicCommand –topic testKJ1 –zookeeper 127.0.0.1:2181kafka-topics.sh –zookeeper localhost:2181 –delete –topic testKJ1查看consumer组内消费的offsetkafka-run-class.sh kafka.tools.ConsumerOffsetChecker –zookeeper localhost:2181 –group test –topic name","categories":[],"tags":[]},{"title":"flume小结","slug":"王阁/技术/hexo/oldblog/blog18","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.009Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog18/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog18/","excerpt":"此处简介","text":"此处简介 flume小结此次flume环境的搭建是针对实际日志业务，整个过程还算顺利针对flume的引入更多的偏向应用层面。所以更多的要熟悉相关配置与参数的设置 flume的整体构思采用的是flume框架中的flume-ng。整体架构如下图log_product环节尚有争议，主要针对flume环节进行小结。原从效率上考虑，打算在跳板机上搭建直接接入hadoop的单flume节点，因为网络权限等问题，无法直接写入所以放弃。转而改为在hadoop环境中也引入一个flume节点(flume-server)。因client是单节点，所以没有必要引入fail-over机制。因此flume-server也是单节点。 写入hdfs时有三个参数要注意rollSizerollCountrollInterval这三个参数对写入单个hdfs文件时的大小，行，时间。 flume-ng agent -n agent1 -c conf -f flume-client.properties -Dflume.root.logger&#x3D;DEBUG,console flume-ng agent -n agent1 -c conf -f flume-client.properties -Dflume.root.logger&#x3D;DEBUG,console &amp; flume-ng agent -n a1 -c ..&#x2F;conf -f flume-server.properties -Dflume.root.logger&#x3D;DEBUG,console &amp;","categories":[],"tags":[]},{"title":"JVM问题","slug":"王阁/技术/hexo/oldblog/blog19","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.009Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog19/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog19/","excerpt":"此处简介","text":"此处简介 JVM问题1、堆内存溢出2、持久代内存溢出3、系统频繁FGC 框架使用不当4、错误使用框架提供API5、日志框架使用不当OS内存溢出6、某系统物理内存溢出数据库问题7、慢SQL问题 案例1、堆内存溢出JVM基础知识1、Jvm内存分为三个大区，young区，old区和perm区；其中young区又包含三个区：Edgn区、S0、S1区2、young区和old区属于heap区，占据堆内存；perm区称为持久代，不占据堆内存。堆内存溢出性能问题发现过程 查看服务器上报错日志，发现有如下报错信息［java.lang.OutOfMemoryError: Java heap space］；根据报错信息确定是jvm 堆内存空间不够导致，于是使用jvm命令（下图）查看，发现此时old区内存空间已经被占满了，同时使用jvisualvm监控工具也发现old区空间被占满（右图），整个heap区空间已经无法再容纳新对象进入。建议考虑大量数据一次性写入内存场景 持久代内存溢出现象 压测某系统接口，压测前1分钟左右tps 400多，之后Tps直降为零，后台报错日志：java.lang.OutOfMemoryError:PermGen space，通过jvm监控工具查看持久代（perm区）空间被占满，Old区空闲； 问题定位过程通过注释代码块定位问题，考虑到perm区溢出大部分跟类对象大量创建有关，故锁定问题在序列化框架使用可能有问题；对于比较棘手难解决的perm溢出问题，作者构建了一个perm区溢出的场景，可以采用如下定位方案1、添加jvm dump配置-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath&#x3D;&#x2F;data&#x2F;dump.bin2、安装eclipse mat分析工具3、将dump文件导入eclipse，点击［Leak Suspects］，找到跟公司有关的代码进行分析此处不过多讲解，大家可以去网上查阅资料学习解决办法跟开发沟通后选择去掉msgpack0.6版本框架，采用java原成序列化框架，修改后系统tps稳定在400多，gc情况正常修改前后gc情况对比修复前类似问题如何避免 1、去掉项目无用jar包2、避免大量使用类对象、大量使用反射案例3、频繁FGC（1）系统某接口频繁FGC问题排查：先查JVM内存信息找可疑对象从内存对象实例信息中发现跟mysql连接有关，然后检测mysql配置信息 发现系统采用的是 spring框架的数据源，没有用连接池； 思考使用连接池有什么好处？连接复用、减少连接重复建立和销毁造成的大量资源消耗 然后换做hikaricp连接池做对比测试 &lt;bean id&#x3D;”dataSource” class&#x3D;”com.zaxxer.hikari.HikariDataSource”压测半小时未出现fgc，问题得到解决类似问题如何避免 1、研发规范统一DB连接池，避免研发误用2、减少大对象、临时对象使用 案例4、错误使用框架提供API现象某系统本身业务逻辑处理能力很快（研发本机自测tps可以到达2w多），但是接入到framework框架后，TPS最高只能到达300笔&#x2F;S左右，而且系统负载很低 问题排查根据这种现象说明系统可能是堵在了某块方法上，根据这种情况一般采用线程dump的方式来查看系统具体哪些线程出现异常情况，通过线程dump 发现 ［TIMED_WAITING］状态的业务线程占比很高根据线程dump信息，找到公司包名开头的信息，然后从下往上查看线程dump信息，从信息中我们可以看到 framework.servlet.fServlet.doPost：框架api封装了servlet dopost方法做了某些操作framework.servlet.fServlet.execute：框架api执行serveltframework.process.fProcessor.process：框架api进行自身逻辑处理framework.filter.impl.AuthFilter.before：框架使用过滤器进行用户权限过滤 。。。。。。然后就是进行http请求操作由此我们断定，就是在框架进行权限校验这块堵住了。之后跟开发沟通这块的问题即可 分析思路压测端 ：net 服务器 jvm服务端：net 服务器 nginx tomcat jvm（应用程序）算法 db（mysql redis）","categories":[],"tags":[]},{"title":"String StringBuffer StringBuilder","slug":"王阁/技术/hexo/oldblog/blog2","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.009Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog2/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog2/","excerpt":"此处简介","text":"此处简介 String StringBuffer StringBuilder String s &#x3D;new String(“ok”) （1）String ok1&#x3D;new String(“ok”);（2）String ok2&#x3D;“ok”;我相信很多人都知道这两种方式定义字符串，但他们之间的差别又有多少人清楚呢。画出这两个字符串的内存示意图： 1String ok1=new String(“ok”)。首先会在堆内存申请一块内存存储字符串ok,ok1指向其内存块对象。同时还会检查字符串常量池中是否含有ok字符串,若没有则添加ok到字符串常量池中。所以 new String()可能会创建两个对象. String ok2=“ok”。 先检查字符串常量池中是否含有ok字符串,如果有则直接指向, 没有则在字符串常量池添加ok字符串并指向它.所以这种方法最多创建一个对象，有可能不创建对象所以String ok1=new String(“ok”);//创建了两个对象String ok2=“ok”;//没有创建对象 比较类中的数值是否相等使用equals(),比较两个包装类的引用是否指向同一个对象时用== 123String ok=&quot;ok&quot;;String ok1=new String(&quot;ok&quot;);System.out.println(ok==ok1);//fasle 明显不是同一个对象，一个指向字符串常量池，一个指向new出来的堆内存块，new的字符串在编译期是无法确定的。所以输出false 123String ok=&quot;apple1&quot;;String ok1=&quot;apple&quot;+1;System.out.println(ok==ok1);//true String ok=&quot;apple1&quot;; int temp=1; String ok1=&quot;apple&quot;+temp; System.out.println(ok==ok1) Intern()方法但我们可以通过intern()方法扩展常量池。 intern()是扩充常量池的一个方法,当一个String实例str调用intern()方法时,java会检查常量池中是否有相同的字符串,如果有则返回其引用,如果没有则在常量池中增加一个str字符串并返回它的引用。 String类具有immutable(不能改变)性质,当String变量需要经常变换时,会产生很多变量值,应考虑使用StringBuffer提高效率。在开发时，注意String的创建方法 使用System.out.println(obj.hashcode())输出的时对象的哈希码， 而非内存地址。在Java中是不可能得到对象真正的内存地址的，因为Java中堆是由JVM管理的不能直接操作。 只能说此时打印出的Hash码表示了该对象在JAVA虚拟机中的内存位置， Java虚拟机会根据该hash码最终在真正的的堆空间中给该对象分配一个地址. 但是该地址 是不能通过java提供的api获取的 String变量连接新字符串会改变hashCode值，变量是在JVM中“连接——断开”； StringBuffer变量连接新字符串不会改变hashCode值，因为变量的堆地址不变。 StringBuilder变量连接新字符串不会改变hashCode值，因为变量的堆地址不变。 比较String、StringBuffer、StringBuilder性能 String类由于Java中的共享设计，在修改变量值时使其反复改变栈中的对于堆的引用地址，所以性能低。 StringBuilder是线性不安全的，适合于单线程操作，其性能比StringBuffer略高。 StringBuffer和StringBuilder类设计时改变其值，其堆内存的地址不变，避免了反复修改栈引用的地址，其性能高。 当String使用引号创建字符串时，会先去字符串池中找，找到了就返回，找不到就在字符串池中增加一个然后返回，这样由于共享提高了性能。 而new String()无论内容是否已经存在，都会开辟新的堆空间，栈中的堆内存也会改变。 性能简介StringBuilder&gt;StringBuffer&gt;String http://www.jb51.net/article/78057.htm StringBuffer中的setLength与delete的效率比较 前者主要是通过将底层的storage数组长度设置为0 后者则是另复制一份至另一空间，长度设为0所以后则的效率会相对慢一点","categories":[],"tags":[]},{"title":"hbase","slug":"王阁/技术/hexo/oldblog/blog20","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.009Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog20/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog20/","excerpt":"此处简介","text":"此处简介 hbase shell命令 描述alter 修改列族（column family）模式count 统计表中行的数量create 创建表describe 显示表相关的详细信息delete 删除指定对象的值（可以为表，行，列对应的值，另外也可以指定时间戳的值）deleteall 删除指定行的所有元素值disable 使表无效drop 删除表enable 使表有效exists 测试表是否存在exit 退出hbase shellget 获取行或单元（cell）的值incr 增加指定表，行或列的值list 列出hbase中存在的所有表put 向指向的表单元添加值tools 列出hbase所支持的工具scan 通过对表的扫描来获取对用的值status 返回hbase集群的状态信息shutdown 关闭hbase集群（与exit不同）truncate 重新创建指定表version 返回hbase版本信息","categories":[],"tags":[]},{"title":"java---GC机制","slug":"王阁/技术/hexo/oldblog/blog11","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.008Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog11/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog11/","excerpt":"此处简介","text":"此处简介 java—GC机制JVM会有一个运行时数据区来管理内存 程序计数器(Program Counter Register) 虚拟机栈(VM Stack) 本地方法栈(Native Method Stack) 方法区(Method Area) 堆(Heap) What? – 哪些内存需要回收？而其中程序计数器、虚拟机栈、本地方法栈是每个线程私有的内存空间，随线程而生，随线程而亡。例如栈中每一个栈帧中分配多少内存基本上在类结构去诶是哪个下来时就已知了，因此这3个区域的内存分配和回收都是确定的，无需考虑内存回收的问题。 但方法区和堆就不同了，一个接口的多个实现类需要的内存可能不一样，我们只有在程序运行期间才会知道会创建哪些对象，这部分内存的分配和回收都是动态的，GC主要关注的是这部分内存。 总而言之，GC主要进行回收的内存是JVM中的方法区和堆；涉及到多线程(指堆)、多个对该对象不同类型的引用(指方法区)，才会涉及GC的回收。 When? – 什么时候回收？堆 123在面试中经常会碰到这样一个问题（事实上笔者也碰到过）：如何判断一个对象已经死去？很容易想到的一个答案是：对一个对象添加引用计数器。每当有地方引用它时，计数器值加1；当引用失效时，计数器值减1.而当计数器的值为0时这个对象就不会再被使用，判断为已死。是不是简单又直观。然而，很遗憾。这种做法是错误的！（面试时可千万别这样回答哦，我就是不假思索这样回答，然后就。。）为什么是错的呢？事实上，用引用计数法确实在大部分情况下是一个不错的解决方案，而在实际的应用中也有不少案例，但它却无法解决对象之间的循环引用问题。比如对象A中有一个字段指向了对象B，而对象B中也有一个字段指向了对象A，而事实上他们俩都不再使用，但计数器的值永远都不可能为0，也就不会被回收，然后就发生了内存泄露。。 在Java，C#等语言中，比较主流的判定一个对象已死的方法是：可达性分析(Reachability Analysis).所有生成的对象都是一个称为”GC Roots”的根的子树。从GC Roots开始向下搜索，搜索所经过的路径称为引用链(Reference Chain)，当一个对象到GC Roots没有任何引用链可以到达时，就称这个对象是不可达的（不可引用的），也就是可以被GC回收了 12无论是引用计数器还是可达性分析，判定对象是否存活都与引用有关！那么，如何定义对象的引用呢？ 强引用(Strong Reference):Object obj &#x3D; new Object();只要强引用还存在，GC永远不会回收掉被引用的对象。 软引用(Soft Reference)：描述一些还有用但非必需的对象。在系统将会发生内存溢出之前，会把这些对象列入回收范围进行二次回收（即系统将会发生内存溢出了，才会对他们进行回收。） 弱引用(Weak Reference):程度比软引用还要弱一些。这些对象只能生存到下次GC之前。当GC工作时，无论内存是否足够都会将其回收（即只要进行GC，就会对他们进行回收。） 虚引用(Phantom Reference):一个对象是否存在虚引用，完全不会对其生存时间构成影响。 方法区What部分我们已经提到，GC主要回收的是堆和方法区中的内存，而上面的How主要是针对对象的回收，他们一般位于堆内。那么，方法区中的东西该怎么回收呢？ 关于方法区中需要回收的是一些废弃的常量和无用的类 废弃的常量的回收。这里看引用计数就可以了。没有对象引用该常量就可以放心的回收了。 无用的类的回收。什么是无用的类呢？ 该类所有的实例都已经被回收。也就是Java堆中不存在该类的任何实例； 加载该类的ClassLoader已经被回收； 该类对应的java.lang.Class对象没有任何地方被引用，无法在任何地方通过反射访问该类的方法。 12总而言之，对于堆中的对象，主要用可达性分析判断一个对象是否还存在引用，如果该对象没有任何引用就应该被回收。而根据我们实际对引用的不同需求，又分成了4中引用，每种引用的回收机制也是不同的。对于方法区中的常量和类，当一个常量没有任何对象引用它，它就可以被回收了。而对于类，如果可以判定它为无用类，就可以被回收了。 How? – 如何回收？标记-清除(Mark-Sweep)算法分为两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。缺点：效率问题，标记和清除两个过程的效率都不高；空间问题，会产生很多碎片。 复制算法将可用内存按容量划分为大小相等的两块，每次只用其中一块。当这一块用完了，就将还存活的对象复制到另外一块上面，然后把原始空间全部回收。高效、简单。缺点：将内存缩小为原来的一半。 标记-整理(Mark-Compat)算法标记过程与标记-清除算法过程一样，但后面不是简单的清除，而是让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存。 分代收集(Generational Collection)算法 新生代中，每次垃圾收集时都有大批对象死去，只有少量存活，就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集； 老年代中，其存活率较高、没有额外空间对它进行分配担保，就应该使用“标记-整理”或“标记-清理”算法进行回收。 ####一些收集器 Serial收集器单线程收集器，表示在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。”Stop The World”. ParNew收集器实际就是Serial收集器的多线程版本。 并发(Parallel):指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态； 并行(Concurrent):指用户线程与垃圾收集线程同时执行，用户程序在继续运行，而垃圾收集程序运行于另一个CPU上。 Parallel Scavenge收集器该收集器比较关注吞吐量(Throughout)(CPU用于用户代码的时间与CPU总消耗时间的比值)，保证吞吐量在一个可控的范围内。 CMS(Concurrent Mark Sweep)收集器CMS收集器是一种以获得最短停顿时间为目标的收集器。 G1(Garbage First)收集器从JDK1.7 Update 14之后的HotSpot虚拟机正式提供了商用的G1收集器，与其他收集器相比，它具有如下优点：并行与并发；分代收集；空间整合；可预测的停顿等。 本部分主要分析了三种不同的垃圾回收算法：Mark-Sweep, Copy, Mark-Compact. 每种算法都有不同的优缺点，也有不同的适用范围。而JVM中对垃圾回收器并没有严格的要求，不同的收集器会结合多个算法进行垃圾回收。#####内存分配Java技术体系中所提倡的自动内存管理最终可以归结为自动化的解决2个问题：给对象分配内存以及回收分配给对象的内存。#####对象优先在Eden分配大多数情况下，对象在新生代Eden区分配。当Eden区没有足够的内存时，虚拟机将发起一次Minor GC。 Minor GC(新生代GC):指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC发生的非常频繁。 Full GC&#x2F;Major GC(老年代GC):指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC。 大对象直接进老年代大对象是指需要大量连续内存空间的Java对象（例如很长的字符串以及数组）。 长期存活的对象将进入老年代JVM为每个对象定义一个对象年龄计数器。 如果对象在Eden出生并经历过第一次Minor GC后仍然存活，并且能够被Survivor容纳，则应该被移动到Survivor空间中，并且年龄对象设置为1； 对象在Survivor区中每熬过一次Minor GC，年龄就会增加1岁，当它的年龄增加到一定程度(默认为15岁，可通过参数-XX:MaxTenuringThreshold设置)，就会被晋升到老年代中。 要注意的是：JVM并不是永远的要求对象的年龄必须达到MaxTenuringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一般，年龄大于等于该年龄的对象就可以直接进入老年代，无需等到MaxTenuringThreshold中要求的年龄。 空间分配担保 在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，则进行Minor GC是安全的； 如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，则急促检查老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管它是有风险的； 如果小于或者HandePromotionFailure设置为不允许冒险，则这时要改为进行一次Full GC.","categories":[],"tags":[]},{"title":"文本处理小结","slug":"王阁/技术/hexo/oldblog/blog14","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.008Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog14/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog14/","excerpt":"此处简介","text":"此处简介 文本处理小结此处的文本处理主要针对的是日常工作中主要遇到的一些场景小结，小结主要围绕技能展开，但不局限于某一技术点， 文本处理的主要类型 排序 去重 合并 切割 取集 打乱排序 模糊匹配 替换 1总得来说，目前主要的文本处理手段有linux指令、shell脚本、java脚本、mr脚本、excel、sublime等几类，下现主要也围绕这部份展开 排序目前主要涉及到的排序手段是linux中的sort指令，所以这里对sort进行展开小结####sort排序sort排序主要的操作有sort命令是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出。sort命令既可以从特定的文件，也可以从stdin中获取输入。 #####sort 指令后面常接的Options sort原理sort将文件&#x2F;文本的每一行作为一个单位，相互比较，比较原则是从首字符向后，依次按ASCII码值进行比较，最后将他们按升序输出。 排序有时往往伴随着去重，而sort则对应的有去重指令即sort -u file:忽略相同行(这里的-u其实以对应的是unique，而unique的主要作用还是去重，所以在去重部份再展开总结) sort的常用指令(-n、-r、-k、-t)-t:指定分隔符-n:指定以按数字的大小的形式进行排序-k:指定按那一列-r:-r是以相反顺序 careful-k 有一些复杂用法，即 -k选项的语法格式： FStart.CStart Modifie,FEnd.CEnd Modifier ——-Start——–,——-End——– FStart.CStart 选项 , FEnd.CEnd 选项 这个语法格式可以被其中的逗号,分为两大部分，Start部分和End部分。Start部分也由三部分组成，其中的Modifier部分就是我们之前说过的类似n和r的选项部分。我们重点说说Start部分的FStart和C.Start。C.Start也是可以省略的，省略的话就表示从本域的开头部分开始。FStart.CStart，其中FStart就是表示使用的域，而CStart则表示在FStart域中从第几个字符开始算“排序首字符”。同理，在End部分中，你可以设定FEnd.CEnd，如果你省略.CEnd，则表示结尾到“域尾”，即本域的最后一个字符。或者，如果你将CEnd设定为0(零)，也是表示结尾到“域尾”。 从公司英文名称的第二个字母开始进行排序： $ sort -t ‘ ‘ -k 1.2 facebook.txt baidu 100 5000 sohu 100 4500 google 110 5000 guge 50 3000 使用了-k 1.2，表示对第一个域的第二个字符开始到本域的最后一个字符为止的字符串进行排序。你会发现baidu因为第二个字母是a而名列榜首。sohu和 google第二个字符都是o，但sohu的h在google的o前面，所以两者分别排在第二和第三。guge只能屈居第四了。 只针对公司英文名称的第二个字母进行排序，如果相同的按照员工工资进行降序排序： $ sort -t ‘ ‘ -k 1.2,1.2 -nrk 3,3 facebook.txt baidu 100 5000 google 110 5000 sohu 100 4500 guge 50 3000 由于只对第二个字母进行排序，所以我们使用了-k 1.2,1.2的表示方式，表示我们“只”对第二个字母进行排序。（如果你问“我使用-k 1.2怎么不行？”，当然不行，因为你省略了End部分，这就意味着你将对从第二个字母起到本域最后一个字符为止的字符串进行排序）。对于员工工资进行排 序，我们也使用了-k 3,3，这是最准确的表述，表示我们“只”对本域进行排序，因为如果你省略了后面的3，就变成了我们“对第3个域开始到最后一个域位置的内容进行排序” 了。 总得来说Linux的sort排序功能就能满足绝大部份应用场景 去重排序完后，往往涉及到的场景更多的是去重相较于排序而言，日常中去重的手段会更多一些 去重(linux方式)linux中的去重指令首先是刚才在前文本提到的 sort -u:对文本进行排序，去重，并对重复的只保留一份。 而在日常中，结合去重可能会产生更多的应用场景，即取交集、并集等 大致来讲linux 主要的去重指令是uniq uniquniq的Options主要有 uniq :默认只是将重复的保留一行 而通过uniq实现交集与并集主要通过-d与-u实现 uniq -d是只显示重复出现的行列uniq -u是只显示不重复的列 这里要注意uniq -f -s的使用uniq -f nubmber :即指定忽略多少栏位开如计重 uniq -s number:即指定忽略多少字任开始计重 uniq -f -s :同时出现时则按先按栏位移，再按字符移。 通过uniq实现去重要先排序 针对时常出现的应用场景提供一个思路两个文本中有重复内容，但只想去掉前一个文件与后一个文件中的重复内容，保留前一个文件中的非重复内容可以通过 sort file1 file2 | uniq -d &gt;temp sort file1 temp|uniq -u 即思路是:先将两个文件中的重复内容找出并写入临时文件，再将前一个文件与临时文件合并，排序去重，保留只出现一次的文件内容 其它去重方式要去重，通过java方式也能轻松实现即主要利用set等的非重复内容的特性。进行实现 切割即将文件进行切割，出于取样，或测试的需求考虑，可能需要从大文本中切割出一些小文件来。对文件的切割也有很多实现方式但从实现方式上更推荐linux指令式 切割linux指令式涉及到切割的linux指令主要有split split命令可以将一个大文件分割成很多个小文件，有时需要将文件分割成更小的片段，比如为提高可读性，生成日志等。 具体实现都是指令式，需要注意的地方较少，不记得时则翻阅相关文档 切割的其它方式主要擅长的还有java方式 打乱排序1打乱排序主要用的方式有awk与excel的方式 awk方式awk ‘BEGIN{ 100000*srand();}{ printf “%s %s\\n”, rand(), $0}’ t |sort -k1n | awk ‘{gsub($1FS,””); print $0}’ excel方式即通过在文本中再另加一列，生成随机数，然后对随机数列进行排序从列达到打乱的效果 模糊匹配文本的模糊匹配有较多应该场景,可以再多总结 模糊匹配主要有在shell脚本中针对contains操作 和vim中的匹配操作","categories":[],"tags":[]},{"title":"爬虫之nutch","slug":"王阁/技术/hexo/oldblog/blog25","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.009Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog25/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog25/","excerpt":"此处简介","text":"此处简介 这阵子主要研究的爬虫方向，主要以java为语言基础,nutch为自动框架，jsoup作为自主爬虫插件开发基础，进行了一些有针对性的实站，在这些过程中，也遇到了一些问题和心得，觉得有必要总结一下 爬虫之nutchnutch使用体验与感悟在这里之所以以nutch开篇，因为在接触爬虫之前就一直有听过它的大名，知道它是我们java语言栈中的爬虫利器，而且因为它，诞生了hadoop这一重器，后者在如今天大数据技术圈的名头，应该是无人不晓吧。所以，多种因素的促使下，让我入了nutch的坑。而本文虽是不仅针对nutch的总结，但主要还是以nutch为主。 经过一阵的实际体验,nutch给我的感觉，老实说确实是强,百科中说nutch的目的是“Nutch 致力于让每个人能很容易, 同时花费很少就可以配置世界一流的Web搜索引擎. 为了完成这一宏伟的目标, Nutch必须能够做到”,能有这么宏伟的目标，自然得有相应的实力来支撑。但就我体验来说，它确实是强，经过简单的配置后，几个简单的指令就能对页面进行抓取，但我觉得也有几个比较明显的让人体验不太好的地方 对动态生成的网页内容的抓取不理想，或要通过其它的插件，但插件的引入也不如想像般顺利 版本太多，且很多版本之间的差异明显。在后面会尝试说明一下。 资料太少，有很多场景国外都没有有效的解决策略（公网检索下），国内环境就更差了，为了搜集资料我多次去过国家图书馆，逛过书店等，都没太找到特别详尽且有针对性的资料，所以nutch的整个踩坑过程是很痛苦的，我后面会将我目前调使的nutch抓取服务配置和一些踩坑进行详细的总结，我觉得，就我之前对网上的搜集资料的掌握的情况来看，接下来我给出的总结，或者能带来一些更可靠的价值。综上，我对nutch的感悟是又爱又恨。爱得是它确实是能很便利的帮忙爬取一些东西，恨的是它又不完全是那么便利 nutch搭建与踩坑回顾前文中已经提到过nutch有很多版本，目前最新版本2.3.1，它的多个版本间是有很多差异的，比如1.X的版本间的持久层是没有抽象出来的，所以1.X版本的持久化形式比较单一。而2.X版本中持久层被抽象出来，通过一不同的配置即可实现多元的持久层存储。如mysql、hbase、avro等。但在2.3.X开始就不支持mysql，官网是明确公示过，或者说从2.2.X开始就不支持了，但就实际操作来看2.2.X还是能适配mysql的，而2.3.X在我的实际操作中是不行的。另在这里说一下，我的持久层主要以mysql为主，后面可能还会再试试hbase，而一开始不采用hbase不是我没尝试，而是尝试过，发现nutch目前适配的hbase版本太低，而我服务器上已经搭建的hbase集群环境的版本相对高了不少，无法适配。我想没有必要为此就针对hbase的版本进行更替，所以采用mysql作为持久层，这是坑一。在使用nutch之前，对其是一点都不了解的，或者说对爬虫需求也都是不熟悉的。所以，对其的功能是寄于厚望，一开始就配一个网站的首页url，就设置多线程，多层抓取。但经过多次实站后，发现有些网站中一部份或大部份都不能抓取。再随着深入观察发现，很多是动态生成的内容。更甚者，有些内容的加载是页面加载后，再发送ajax来加载页面的一些关键内容。而原本考虑nutch是支持插件扩展的，原想直接通过检索，求助于互联网，希望直接就能找到一款ajax扩展的插件，然而，几经周折，都未成行。这是坑二 其它还有一些小坑，具体我在下面搭建环节具体结细节处再提，虽小，但影响也挺大，而且比较坑人。以上都是主要通过人力暂未能直接解决的坑。所以先列出来，下面，我回顾下我的nutch搭建环节，其中会给出一些小坑与相应的解决方案 搭建nutch的搭建分为分布式与单机版，我目前主要涉猎的是nutch单机版。具体的版本选择，我尝试过nutch1.2与nutch1.7以上的所有版本，1.X与2.X的主要差异在于2.X将持久层给抽象出来了。总得来说nuthc2.x的实用性相对高些。而我以我目前具体使用的版本2.2.1为例，进行介绍step 1.获取nutch2.2.1 http://archive.apache.org/dist/这个url能定位到apache很多软件和历史版本。进入这里，然后找到nutch，下载相应的版本（该连接有时可能打不开，翻墙试试）。step 2. 对ivy下的ivy.xml与ivysetting.xml进行修改。这里就有之前提到的坑一了，这里可能配置对持久层的依赖了。我这以mysql配置。 修改${APACHE_NUTCH_HOME}&#x2F;ivy&#x2F;ivy.xml文件 将以下行的注释取消 org=”mysql” name=”mysql-connector-java” rev=”5.1.18″ conf=”*-&gt;default”/&gt; 修改以下行。从默认的 org=&quot;org.apache.gora&quot; name=&quot;gora-core&quot; rev=&quot;0.3&quot; conf=&quot;*-&gt;default&quot;/&gt;改成 org=&quot;org.apache.gora&quot; name=&quot;gora-core&quot; rev=&quot;0.2.1&quot; conf=&quot;*-&gt;default&quot;/&gt; 取消以下行的注释 org=&quot;org.apache.gora&quot; name=&quot;gora-sql&quot; rev=&quot;0.1.1-incubating&quot; conf=&quot;*-&gt;default&quot; /&gt; 如果按默认的不做修改，将会在抓取网页时遇到以下错误。 然后配置ivysetting.xml，这个文件类似maven的.setting.xml文件，主要修改相应的软件仓库源，默认地址可能会出现下载缓慢的情况，建议换成国内源，贴一个我配的阿里的。 速度杠杠的。这时就可以进行编译了，通过终端进nutch文件地址进执行ant 然后配置持久层的配置文件，gora.properties 接下来配另mysql的映射文件gora-sql-mapping.xml，可以这么说，这个地方出现的坑是我遇到的坑最多的地方之一。因为我是事后总结，平日也有工任务，所以，在出现坑的时侯，我首要的考虑的是解决的它，所以解决这些问题后，可能能记得是大致是什么状况，但无法具体复现，我在这以总结，提练式的对这些小坑进行叙述，就不贴具体的错误日志了。 首先，文件中的默认配置id的大小是512.这个对于以unicode，准确的说以utf8为编码格式的mysql来说是过长的，会在inject就报sql初始化错误，id too long解决方式将id的长度改小，我设置的是180，nutch默认一般以target url拼接成id，所以一般来讲，180的id是妥妥够用的。 类似的问题还会出现一些如text ,content过长的问题，初次搭建我这先建议改小试试，后面还有根治的方式。 修改了gora-sql-mapping.xml文件后，执行抓取指令，以bin&#x2F;nutch crawl urlfile -threads number -deepth num 这是nutch 普遍用的一个指行指令，这个指令执行时会默认将数据持久化到webpage的表中，这种方式缺点是不太灵活，复杂任务的时候不好操作。 另一种bin&#x2F;crawl urlfile -crawlId name (这个name会作为对应的表名组成部份) “solrrl” “num”用于指定解析程度这种方式相对而言更灵活，后面我主要采用这种方式。这里有个小细节，bin&#x2F;crawl 这个脚本是可以尝试更改的，它默认执行任务时只创建了50，而这种方式无法像上面那样指定创建线程数，所以我更改了crawl文件的初始值，将参数调值2000.所以可以根据自己需求，酌情更改。 bin&#x2F;nutch parse 有些fetch 任务执行完成后，parse数据同步至数据库时可能会产生一些错误，那么可以通过这个指令尝试进行解析。示例：bin&#x2F;nutch parse -crawlId name -all这里容易出现前面提到过的一个问题，就是在解析时，content 或text 中的内容格式不对，或text 内容中出现在了emoji等情况，都可能使解析任务中断。我经过一翻调整，算是有一个统一解决方案：在这里贴出来参考1、将数据库编码格式设置为utf8mb4；2、将text和content的gora-mapping.xml文件中添加jdbc-type&#x3D;“text” 或jdbc-type&#x3D;”blob”的设置，即指明其数据库中对应的类型，避免长度等问题3、上两者结合的一个情况，text存入文中的内容有编码格式，相对读取轻松，blob以二进制形式存放，取值需要转码，所以在不出错的情况下优先设为text，而这时，数据库设为utf8mb4时就不要在jdbc url 中设置utf8格式了，这样反而会出现问题。 至此 nutch日常的主要使用指令就这两个，还有些 如果bin&#x2F;nutch fetch bin&#x2F;nutch gernate这些相对出错较少，出错的影响和对策网上也相对较多，就不再缀述了。 至此爬虫服务的总结就要告一段落了，这篇文章体量相对较大，我是断断续续逐步完成的，所以，后面更多是凭回忆出的之前深刻影响且在网上少有明确解决方案的一些问题，给出我的解决方式，所以，如果这篇总结针对的算是nutch使用过程中，尝试更进一步的助力，而非特别基础的。如果看到此篇文章的你对nutch还有其它的一些问题，可以尝试留言，我们一起探讨。","categories":[],"tags":[]},{"title":"Hadoop总结(第一版---HDFS篇)","slug":"王阁/技术/hexo/oldblog/blog24","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.009Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog24/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog24/","excerpt":"此处简介","text":"此处简介 Hadoop总结(第一版—HDFS篇)接触hadoop生态也有大半年了，一直碎片化的查阅，学习了一些博客和书籍。随着使用的深入，对一些常用的模块有较熟练的使用，也有写过一些日常小结，但对hadoop本身背后的原理没有系统性的学习，回顾来看，就提升曲线来说，现在亟需总结这环节，这也是本文的由来 什么是hadoop？ hadoop是稳定的，高容错的，可大规模布署的分布式文件，存储，并行编程框架。本文默认是已经有hadoop使用经验,所以暂不涉及具体的hadoop生态的各组件的部署和调优细节，后续单开文章来总结。但在具体讲解时会涉及说参数配置会对相关组件参生影响具体而言，hadoop核心组件内容有：hdfs、mapredcue。所以接下来的总结主要针对这两在核心组件展开 HDFS篇什么是HDFS？分布式文件系统：分布式文件系统是一种允许文件通过网络在多台主机上分享的 文件的系统，可让多机器上的多用户分享文件和存储空间HDFS:（Hadoop Distribute File System）即hadoop分布式文件系统 主要用于适合运行在通用硬件上的分布式文件系统，特点是高度容错，适合布署在廉价服务器上，具有高吞吐量的数据访问等特点 1. 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。 2. 运行在廉价的机器上。 3. 适合大数据的处理。多大？多小？HDFS默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。 4. 适用于一次写入、多次查询的情况 5. 不支持并发写情况，小文件不合适。因为小文件也占用一个块，小文件越多（1000个1k文件）块越 多，NameNode压力越大。hdfs得部署在linux系统上 HDFS的具体内容文件、节点、数据块 HDFS主要是是围绕着这三个关键词设计的. 数据块 Block：在HDFS中，每个文件都是采用的分块的方式存储，每个block放在不同的dataNode节点上，每个block的标识是一个三元组(block id , numBytes,generationStamp),block id 具有唯一性，具体分配是由namenode节点设置，然后在datanode上建立对应的Block文件，同时建立对应的block meta文件(问题是block meta文件存放位，block size可以通过配置文件设置，所以修改block size会对以前持续化的数据有何影响?） Packet:是HDFS文件在DFSClient与DataNode之间通信的过程中文件的形式(一般一个Block对应多个Packet) Chunk:是通过程中具体传输的文件单位，发送过程以Packet的方式进行，但 一个packet包含多个Chunk,同时对于每个chunk进行checksum计算，生成checksum bytes。 PacketPacket的结构：数据包和heatbeat包 一个Packet数据包的组成结构主要分为 Packet Header 、PacketData Packet Header 中又分为：Packet Data部分是一个Packet的实际数据部分。主要内容有 一个4字节校验 Checksum Chunk部分，Chunk部分最大为512字节Packet创建过程：首先将字节流数据写入一个buffer缓冲区中，也就是从偏移量为25的位置（checksumStart）开 始写Packet数据Chunk的Checksum部分，从偏移量为533的位置（dataStart）开始写Packet数据的Chunk Data部分，直到一个Packet创建完成为止。 注意：当写一个文件的最后一个Block的最后一个Packet时，如果一个Packet的大小未能达到最大长度，也就是上图对应的缓冲区 中，Checksum与Chunk Data之间还保留了一段未被写过的缓冲区位置，在发送这个Packet之前，会检查Chunksum与Chunk Data之间的缓冲区是否为空白缓冲区（gap），如果有则将Chunk Data部分向前移动，使得Chunk Data 1与Chunk Checksum N相邻，然后才会被发送到DataNode节点 hdsf架构(主要组成是节点）主要的构成角色有：Client、NameNode、SecondayNameNode、DataNode Client：系统使用者，调用HDFS API操作文件；与NN交互获取文件元数据;与DN交互进行数据读写, 注意：写数据时文件切分由Client完成 Namenode：Master节点 （也称元数据节点）是系统唯一的管理者。负责元数据的管理(名称空间和数据块映射信息);配置副本策略；处理客户端请求 Datanode：数据存储节点(也称Slave节点)，存储实际的数据；执行数据块的读写；汇报存储信息给NN Secondary NameNode：备胎，namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode careful: 注意：在hadoop 2.x 版本，当启用 hdfs ha 时，将没有这一角色热冷备份说明：热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作 冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失hdfs构架原则 元数据与数据分离：文件本身的属性（即元数据）与文件所持有的数据分离 主&#x2F;从架构：一个HDFS集群是由一个NameNode和一定数目的DataNode组成 一次写入多次读取：HDFS中的文件在任何时间只能有一个Writer。当文件被创建，接着写入数据，最后，一旦文件被关闭，就不能再修改。 移动计算比移动数据更划算：数据运算，越靠近数据，执行运算的性能就越好，由于hdfs数据分布在不同机器上，要让网络的消耗最低，并提高系统的吞吐量，最佳方式是将运算的执行移到离它要处理的数据更近的地方，而不是移动数据 针对第四条的解释：在上文中交代到hdfs中的文件是以block的形式存放在集群中，所以一个文件可以是被切分成很多block存放在集群的机器中针对第四条，移动计算比移动数据更划算，是因为，从理论上讲，集群的计算能力是很方便扩展的，如服务器的硬件提升，或增加服务器等。但网络带宽等资源却很容易达到瓶颈或增加经济负担，所以将计算转移至每个block就近的机器进行计算，会比将所有的block合到一个机器上再进行计算要划算，所以，叫移动计算要比移动数据划算 NameNodeNameNode是整个文件系统的管理与计算节点，是HDFS中最复杂的一个实体，它与管理着HDFS文件系统中最重要的两个关系 HDFS文件系统中的文件目录树，以及文件的数据块索引，即每个文件对应的数据块列表 数据块和数据节点的对应关系，即某一块数据块保存在哪些数据节点的信息 第一个关系即目录树、元数据和数据块的索引信息持久化到物理存储中，具体的实现是保存在命名空间的镜像fsimage和编辑日志edits中，careful：在fsimage中，并没有记录每一个block对应到那几个Datanodes的对应表信息 第二个关系是在NameNode启动后，每个DataNode对本地的磁盘进行扫描，将本DataNode上保存的block信息上报至NameNode,Namenode在接收到每个Datanode的块信息汇报后，将接收到的块信息，以及其所在的Datanode信息等保存在内存中。HDFS就是通过这种块信息汇报的方式来完成 block -&gt; Datanodes list的对应表构建（careful）类似于数据库中的检查点，为了避免edits日志过大，在Hadoop1.X 中，SecondaryNameNode会按照时间阈值（比如24小时）或者edits大小阈值（比如1G），周期性的将fsimage和edits的合 并，然后将最新的fsimage推送给NameNode。而在Hadoop2.X中，这个动作是由Standby NameNode来完成.由此可看出，这两个文件一旦损坏或丢失，将导致整个HDFS文件系统不可用在hadoop1.X为了保证这两种元数据文件的高可用性，一般的做法，将dfs.namenode.name.dir设置成以逗号分隔的多个目录，这多个目录至少不要在一块磁盘上，最好放在不同的机器上，比如：挂载一个共享文件系统fsimage\\edits 是序列化后的文件，想要查看或编辑里面的内容，可通过 hdfs 提供的 oiv\\oev 命令，命令: hdfs oiv （offline image viewer） 用于将fsimage文件的内容转储到指定文件中以便于阅读,，如文本文件、XML文件，该命令需要以下参数：-i (必填参数) –inputFile 输入FSImage文件-o (必填参数) –outputFile 输出转换后的文件，如果存在，则会覆盖-p (可选参数） –processor 将FSImage文件转换成哪种格式： (Ls|XML|FileDistribution).默认为Ls示例：hdfs oiv -i &#x2F;data1&#x2F;hadoop&#x2F;dfs&#x2F;name&#x2F;current&#x2F;fsimage_0000000000019372521 -o &#x2F;home&#x2F;hadoop&#x2F;fsimage.txt命令：hdfs oev (offline edits viewer 离线edits查看器）的缩写， 该工具只操作文件因而并不需要hadoop集群处于运行状态。示例: hdfs oev -i edits_0000000000000042778-0000000000000042779 -o edits.xml支持的输出格式有binary（hadoop使用的二进制格式）、xml（在不使用参数p时的默认输出格式）和stats（输出edits文件的统计信息）由此可以总结到：NameNode管理着DataNode，接收DataNode的注册、心跳、数据块提交等信息的上报，并且在心跳中发送数据块复制、删除、恢复等指令；同时，NameNode还为客户端对文件系统目录树的操作和对文件数据读写、对HDFS系统进行管理提供支持另 Namenode 启动后会进入一个称为安全模式的特殊状态。处于安全模式 的 Namenode 是不会进行数据块的复制的。 Namenode 从所有的 Datanode 接收心跳信号和块状态报告。 块状态报告包括了某个 Datanode 所有的数据 块列表。每个数据块都有一个指定的最小副本数。当 Namenode 检测确认某 个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全 (safely replicated) 的；在一定百分比（这个参数可配置）的数据块被 Namenode 检测确认是安全之后（加上一个额外的 30 秒等待时间）， Namenode 将退出安全模式状态。接下来它会确定还有哪些数据块的副本没 有达到指定数目，并将这些数据块复制到其他 Datanode 上。 ##### Secondary NameNode 在HA cluster中又称为standby node主要作用： 1.如上文提到的合并fsimage和eits日志，将eits日志文件大小控制在一个限度下 大致流程如下 namenode 响应 Secondary namenode 请求，将 edit log 推送给 Secondary namenode ， 开始重新写一个新的 edit log Secondary namenode 收到来自 namenode 的 fsimage 文件和 edit log Secondary namenode 将 fsimage 加载到内存，应用 edit log ， 并生成一 个新的 fsimage 文件 Secondary namenode 将新的 fsimage 推送给 Namenode Namenode 用新的 fsimage 取代旧的 fsimage ， 在 fstime 文件中记下检查 点发生的时 HDFS写文件 1.x 默认的block大小是64M 2.X版本默认block的大小是 128M 如上图所示 + Client预先设置的block参数切分FIle CLient向NameNode发送写数据请求， NameNode，记录block信息，并返回可用的DataNode(具体的返回规则参考下文) client向DataNode发送block1；发送过程是以流式写入具体流程是 将64M的block1按64k的packet划分 然后将第一个packet发送给host2 host2接收完后，将第一个packet发送给host1，同时client想host2发送第二个packet host1接收完第一个packet后，发送给host3，同时接收host2发来的第二个packet 以此类推，如图红线实线所示，直到将block1发送完毕 host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。 client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。 发送完block1后，再向host7，host8，host4发送block2 当客户端向 HDFS 文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副 本系数设置为 3 ，当本地临时文件累积到一个数据块的大小时，客户端会从 Namenode 获取一个 Datanode 列表用于存放副本。然后客户端开始向第一个 Datanode 传输数据，第一个 Datanode 一小部分一小部分 (4 KB) 地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中 第二个 Datanode 节点。第二个 Datanode 也是这样，一小部分一小部分地接收数据，写入本地 仓库，并同时传给第三个 Datanode 。最后，第三个 Datanode 接收数据并存储在本地。因此， Datanode 能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的 方式从前一个 Datanode 复制到下一个 写入的过程，按hdsf默认设置，1T文件，我们需要3T的存储，3T的网络流量 在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去 挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份 hdfs读文件 客户端通过调用FileSystem对象的open()方法来打开希望读取的文件，对于HDFS来说，这个对象时分布文件系统的一个实例； DistributedFileSystem通过使用RPC来调用NameNode以确定文件起始块的位置，同一Block按照重复数会返回多个位置，这些位置按照Hadoop集群拓扑结构排序，距离客户端近的排在前面 前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流，客户端对这个输入流调用read()方法 存储着文件起始块的DataNode地址的DFSInputStream随即连接距离最近的DataNode，通过对数据流反复调用read()方法，将数据从DataNode传输到客户端 到达块的末端时，DFSInputStream会关闭与该DataNode的连接，然后寻找下一个块的最佳DataNode，这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流 一旦客户端完成读取，就对FSDataInputStream调用close()方法关闭文件读取 block持续化结构 DataNode节点上一个Block持久化到磁盘上的物理存储结构，如下图所示： 每个Block文件（如上图中blk_1084013198文件）都对应一个meta文件（如上图中blk_1084013198_10273532.meta文件），Block文件是一个一个Chunk的二进制数据（每个Chunk的大小是512字节），而meta文件是与每一个Chunk对应的Checksum数据，是序列化形式存储 — 至上我们大致了解了HDFS。正如上文提到的Hadoop的特点，高可能，高容错性。若光从上文提到的特性可能还不足以说明，如NameNode环节就提到了NameNode的重要作用，但若NameNode出现了故障，对整个机集会是毁灭性的打击，于是Hadoop也引入其它的一些手段来保存高可用，高容错。接下来我们就来探讨下 Hadoop HA的引入 HA：High Available即高可用性集群，是保证业务连续性的有效解决方案，一般有两个或两个以上的节点，且分为活动节点及备用节点。 在HA具体实现方法不同的情况下，HA框架的流程是一致的, 不一致的就是如何存储和管理日志。在Active NN和Standby NN之间要有个共享的存储日志的地方，Active NN把EditLog写到这个共享的存储日志的地方，Standby NN去读取日志然后执行，这样Active和Standby NN内存中的HDFS元数据保持着同步。一旦发生主从切换Standby NN可以尽快接管Active NN的工作; 默认并未启用 hdfs ha。 SPOF方案回顾： Secondary NameNode：它不是HA，它只是阶段性的合并edits和fsimage，以缩短集群启动的时间。当NN失效的时候，Secondary NN并无法立刻提供服务，Secondary NN甚至无法保证数据完整性：如果NN数据丢失的话，在上一次合并后的文件系统的改动会丢失 Backup NameNode (HADOOP-4539)：它在内存中复制了NN的当前状态，算是Warm Standby，可也就仅限于此，并没有failover等。它同样是阶段性的做checkpoint，也无法保证数据完整性3. 手动把name.dir指向NFS（Network File System），这是安全的Cold Standby，可以保证元数据不丢失，但集群的恢复则完全靠手动4. Facebook AvatarNode：Facebook有强大的运维做后盾，所以Avatarnode只是Hot Standby，并没有自动切换，当主NN失效的时候，需要管理员确认，然后手动把对外提供服务的虚拟IP映射到Standby NN，这样做的好处是确保不会发生脑裂的场景。其某些设计思想和Hadoop 2.0里的HA非常相似，从时间上来看，Hadoop 2.0应该是借鉴了Facebook的做法 5. PrimaryNN 与StandbyNN之间通过NFS来共享FsEdits、FsImage文件，这样主备NN之间就拥有了一致的目录树和block信息；而block的 位置信息，可以根据DN向两个NN上报的信息过程中构建起来。这样再辅以虚IP，可以较好达到主备NN快速热切的目的。但是显然，这里的NFS又引入了新的SPOF6. 在主备NN共享元数据的过程中，也有方案通过主NN将FsEdits的内容通过与备NN建立的网络IO流，实时写入备NN，并且保证整个过程的原子性。这种方案，解决了NFS共享元数据引入的SPOF，但是主备NN之间的网络连接又会成为新的问题 hadoop2.X ha 原理: hadoop2.x之后，Clouera提出了QJM&#x2F;Qurom Journal Manager，这是一个基于Paxos算法实现的HDFS HA方案，它给出了一种较好的解决思路和方案,示意图如下： 基本原理就是用2N+1台 JN 存储EditLog，每次写数据操作有大多数（&gt;&#x3D;N+1）返回成功时即认为该次写成功，数据不会丢失了。当然这个算法所能容忍的是最多有N台机器挂掉，如果多于N台挂掉，这个算法就失效了。这个原理是基于Paxos算法 在HA架构里面SecondaryNameNode这个冷备角色已经不存在了，为了保持standby NN时时的与主Active NN的元数据保持一致，他们之间交互通过一系列守护的轻量级进程JournalNode + 任何修改操作在 Active NN上执行时，JN进程同时也会记录修改log到至少半数以上的JN中，这时 Standby NN 监测到JN 里面的同步log发生变化了会读取 JN 里面的修改log，然后同步到自己的的目录镜像树里面， 当发生故障时，Active的 NN 挂掉后，Standby NN 会在它成为Active NN 前，读取所有的JN里面的修改日志，这样就能高可靠的保证与挂掉的NN的目录镜像树一致，然后无缝的接替它的职责，维护来自客户端请求，从而达到一个高可用的目的 QJM方式来实现HA的主要优势： 1. 不需要配置额外的高共享存储，降低了复杂度和维护成本 2. 消除spof 3. 系统鲁棒性(Robust:健壮)的程度是可配置 4. JN不会因为其中一台的延迟而影响整体的延迟，而且也不会因为JN的数量增多而影响性能（因为NN向JN发送日志是并行的） datanode的fencing: 确保只有一个NN能命令DN。HDFS-1972中详细描述了DN如何实现fencing 1. 每个NN改变状态的时候，向DN发送自己的状态和一个序列号 2. DN在运行过程中维护此序列号，当failover时，新的NN在返回DN心跳时会返回自己的active状态和一个更大的序列号。DN接收到这个返回则认为该NN为新的active 3. 如果这时原来的active NN恢复，返回给DN的心跳信息包含active状态和原来的序列号，这时DN就会拒绝这个NN的命令 客户端fencing：确保只有一个NN能响应客户端请求，让访问standby nn的客户端直接失败。在RPC层封装了一层，通过FailoverProxyProvider以重试的方式连接NN。通过若干次连接一个NN失败后尝试连接新的NN，对客户端的影响是重试的时候增加一定的延迟。客户端可以设置重试此时和时间 Hadoop提供了ZKFailoverController角色，部署在每个NameNode的节点上，作为一个deamon进程, 简称zkfc， FailoverController主要包括三个组件: 1. HealthMonitor: 监控NameNode是否处于unavailable或unhealthy状态。当前通过RPC调用NN相应的方法完成 2. ActiveStandbyElector: 管理和监控自己在ZK中的状态 3. ZKFailoverController 它订阅HealthMonitor 和ActiveStandbyElector 的事件，并管理NameNode的状态 ZKFailoverController主要职责： 1. 健康监测：周期性的向它监控的NN发送健康探测命令，从而来确定某个NameNode是否处于健康状态，如果机器宕机，心跳失败，那么zkfc就会标记它处于一个不健康的状态 2. 会话管理：如 果NN是健康的，zkfc就会在zookeeper中保持一个打开的会话，如果NameNode同时还是Active状态的，那么zkfc还会在 Zookeeper中占有一个类型为短暂类型的znode，当这个NN挂掉时，这个znode将会被删除，然后备用的NN，将会得到这把锁，升级为主 NN，同时标记状态为Active 3. 当宕机的NN新启动时，它会再次注册zookeper，发现已经有znode锁了，便会自动变为Standby状态，如此往复循环，保证高可靠，需要注意，目前仅仅支持最多配置2个NN 4. master选举：如上所述，通过在zookeeper中维持一个短暂类型的znode，来实现抢占式的锁机制，从而判断那个NameNode为Active状态 hadoop2.x Federation： 单Active NN的架构使得HDFS在集群扩展性和性能上都有潜在的问题，当集群大到一定程度后，NN进程使用的内存可能会达到上百G，NN成为了性能的瓶颈 常用的估算公式为1G对应1百万个块，按缺省块大小计算的话，大概是64T (这个估算比例是有比较大的富裕的，其实，即使是每个文件只有一个块，所有元数据信息也不会有1KB&#x2F;block) 为了解决这个问题,Hadoop 2.x提供了HDFS Federation, 示意图如下： 多个NN共用一个集群里的存储资源，每个NN都可以单独对外提供服务 每个NN都会定义一个存储池，有单独的id，每个DN都为所有存储池提供存储 DN会按照存储池id向其对应的NN汇报块信息，同时，DN会向所有NN汇报本地存储可用资源情况 如果需要在客户端方便的访问若干个NN上的资源，可以使用客户端挂载表，把不同的目录映射到不同的NN，但NN上必须存在相应的目录 设计优势： 改动最小，向前兼容；现有的NN无需任何配置改动；如果现有的客户端只连某台NN的话 分离命名空间管理和块存储管理 客户端挂载表：通过路径自动对应NN、使Federation的配置改动对应用透明 (与上面ha方案中介绍的最多2个NN冲突？) 至此hadoop中的hdfs高可用特性，高容错的实现又有了更深理解，但针对hdfs还一层设计实现机架感知 机架感知分布式的集群通常包含非常多的机器，由于受到机架槽位和交换机网口的限制，通常大型的分布式集群都会跨好几个机架，由多个机架上的机器共同组成一个分布 式集群。机架内的机器之间的网络速度通常都会高于跨机架机器之间的网络速度，并且机架之间机器的网络通信通常受到上层交换机间网络带宽的限制。 具体到hadoop集群，由于hadoop的HDFS对数据文件的分布式存放是按照分块block存储，每个block会有多个副本(默认为3)，并且为了数据的安全和高效，所以hadoop默认对3个副本的存放策略为：在本地机器的hdfs目录下存储一个block 在另外一个rack的某个datanode上存储一个block在该机器的同一个rack下的某台机器上存储最后一个block这样的策略可以保证对该block所属文件的访问能够优先在本rack下找到，如果整个rack发生了异常，也可以在另外的rack上找到该block的副本。这样足够的高效，并且同时做到了数据的容错。hadoop对机架的感知并非是自适应的，亦即，hadoop集群分辨某台slave机器是属于哪个rack并非是只能的感知的，而是需要 hadoop的管理者人为的告知hadoop哪台机器属于哪个rack，这样在hadoop的namenode启动初始化时，会将这些机器与rack的对 应信息保存在内存中，用来作为对接下来所有的HDFS的写块操作分配datanode列表时（比如3个block对应三台datanode）的选择 datanode策略，做到hadoop allocate block的策略：尽量将三个副本分布到不同的rack。具体实现本文不在深究，在此附上网上的一些解决方式机架感知实现1 机架感知实现2 至此对hdfs的理解与总结告一段落，后续有了新的理解再进行补充","categories":[],"tags":[]},{"title":"Python","slug":"王阁/技术/hexo/oldblog/blog21","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.009Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog21/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog21/","excerpt":"此处简介","text":"此处简介 the record for python Python 知识点标识\b符号在 Python 里，标识符有字母、数字、下划线组成。在 Python 中，所有标识符可以包括英文、数字以及下划线(_)，但不能以数字开头。 以下划线开头的标识符是有特殊意义的。以单下划线开头 _foo 的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用 from xxx import * 而导入； 以双下划线开头的 __foo 代表类的私有成员；以双下划线开头和结尾的 foo 代表 Python 里特殊方法专用的标识，如 init() 代表类的构造函数。 Python 可以同一行显示多条语句，方法是用分号 ; 分开， 行和缩进python 最具特色的就是用缩进来写模块。缩进的空白数量是可变的，但是所有代码块语句必须包含相同的缩进空白数量，这个必须严格执行因此，在 Python 的代码块中必须使用相同数目的行首缩进空格数。建议你在每个缩进层次使用 单个制表符 或 两个空格 或 四个空格 , 切记不能混用 多行语句Python语句中一般以新行作为为语句的结束符。但是我们可以使用斜杠（ \\）将一行的语句分为多行显示， Python 引号Python 可以使用引号( ‘ )、双引号( “ )、三引号( ‘’’ 或 “”” ) 来表示字符串，引号的开始与结束必须的相同类型的。其中三引号可以由多行组成，编写多行文本的快捷语法，常用语文档字符串，在文件的特定地点，被当做注释。 Python注释python 中多行注释使用三个单引号(‘’’)或三个双引号(“””)。 Python空行函数之间或类的方法之间用空行分隔，表示一段新的代码的开始。类和函数入口之间也用一行空行分隔，以突出函数入口的开始。空行与代码缩进不同，空行并不是Python语法的一部分。书写时不插入空行，Python解释器运行也不会出错。但是空行的作用在于分隔两段不同功能或含义的代码，便于日后代码的维护或重构。记住：空行也是程序代码的一部分。 Python 变量类型变量存储在内存中的值。这就意味着在创建变量时会在内存中开辟一个空间。基于变量的数据类型，解释器会分配指定内存，并决定什么数据可以被存储在内存中。因此，变量可以指定不同的数据类型，这些变量可以存储整数，小数或字符。 变量赋值 Python 中的变量赋值不需要类型声明。每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。等号（&#x3D;）用来给变量赋值。 python 标准数据类型python 定义了一些标准类型，用于存储各种类型的数据。Python有五个标准的数据类型：Numbers（数字）String（字符串）List（列表）Tuple（元组）Dictionary（字典） Python数字数字数据类型用于存储数值。他们是不可改变的数据类型，这意味着改变数字数据类型会分配一个新的对象。当你指定一个值时，Number对象就会被创建：您也可以使用del语句删除一些对象的引用。del语句的语法是： del var1[,var2[,var3[….,varN]]]] 您可以通过使用del语句删除单个或多个对象的引用。例如： del vardel var_a, var_b Python支持四种不同的数字类型： int（有符号整型） long（长整型[也可以代表八进制和十六进制]） float（浮点型） complex（复数） Python字符串字符串或串(String)是由数字、字母、下划线组成的一串字符。 一般记为 :s&#x3D;”a1a2···an”(n&gt;&#x3D;0)它是编程语言中表示文本的数据类型。 python的字串列表有2种取值顺序: 从左到右索引默认0开始的，最大范围是字符串长度少1 从右到左索引默认-1开始的，最大范围是字符串开头 当使用以冒号分隔的字符串，python返回一个新的对象，结果包含了以这对偏移标识的连续的内容，左边的开始是包含了下边界。上面的结果包含了s[1]的值l，而取到的最大范围不包括上边界，就是s[5]的值p。 加号（+）是字符串连接运算符，星号（*）是重复操作 Python列表List（列表） 是 Python 中使用最频繁的数据类型。 python元组元组是另一个数据类型，类似于List（列表）。 元组用”()”标识。内部元素用逗号隔开。但是元组不能二次赋值，相当于只读列表 Python 字典","categories":[],"tags":[]},{"title":"java中枚举的使用","slug":"王阁/技术/hexo/oldblog/blog3","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.009Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog3/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog3/","excerpt":"此处简介","text":"此处简介 java中枚举的使用enum 的全称为 enumeration 是 JDK 1.5 中引入的新特性，存放在 java.lang 包中。 原始的接口定义常量 语法（定义） 遍历、switch 等常用操作 enum 对象的常用方法介绍 给 enum 自定义属性和方法 EnumSet，EnumMap 的应用 enum 的原理分析 总结 原始的接口定义常量 123456789public interface IConstants &#123; String MON = &quot;Mon&quot;; String TUE = &quot;Tue&quot;; String WED = &quot;Wed&quot;; String THU = &quot;Thu&quot;; String FRI = &quot;Fri&quot;; String SAT = &quot;Sat&quot;; String SUN = &quot;Sun&quot;;&#125; 语法（定义） 创建枚举类型要使用 enum 关键字，隐含了所创建的类型都是 java.lang.Enum 类的子类（java.lang.Enum 是一个抽象类）。枚举类型符合通用模式 Class Enum&lt;E extends Enum&gt;，而 E 表示枚举类型的名称。枚举类型的每一个值都将映射到 protected Enum(String name, int ordinal) 构造函数中，在这里，每个值的名称都被转换成一个字符串，并且序数设置表示了此设置被创建的顺序。 12345678910111213package com.hmw.test;/** * 枚举测试类 * @author &lt;a href=&quot;mailto:hemingwang0902@126.com&quot;&gt;何明旺&lt;/a&gt; */public enum EnumTest &#123; MON, TUE, WED, THU, FRI, SAT, SUN;&#125;这段代码实际上调用了7次 Enum(String name, int ordinal)：new Enum&lt;EnumTest&gt;(&quot;MON&quot;,0);new Enum&lt;EnumTest&gt;(&quot;TUE&quot;,1);new Enum&lt;EnumTest&gt;(&quot;WED&quot;,2); 遍历、switch 等常用操作 public class Test { public static void main(String[] args) { for (EnumTest e : EnumTest.values()) { System.out.println(e.toString()); } System.out.println(&quot;----------------我是分隔线------------------&quot;); EnumTest test = EnumTest.TUE; switch (test) &#123; case MON: System.out.println(&quot;今天是星期一&quot;); break; case TUE: System.out.println(&quot;今天是星期二&quot;); break; // ... ... default: System.out.println(test); break; &#125; &#125; &#125; enum 对象的常用方法介绍int compareTo(E o) 比较此枚举与指定对象的顺序。 Class getDeclaringClass() 返回与此枚举常量的枚举类型相对应的 Class 对象。 String name() 返回此枚举常量的名称，在其枚举声明中对其进行声明。 int ordinal() 返回枚举常量的序数（它在枚举声明中的位置，其中初始常量序数为零）。 String toString() 返回枚举常量的名称，它包含在声明中。 static &lt;T extends Enum&gt; T valueOf(Class enumType, String name) 返回带指定名称的指定枚举类型的枚举常量。 给 enum 自定义属性和方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344给 enum 对象加一下 value 的属性和 getValue() 的方法：package com.hmw.test;/** * 枚举测试类 * * @author &lt;a href=&quot;mailto:hemingwang0902@126.com&quot;&gt;何明旺&lt;/a&gt; */public enum EnumTest &#123; MON(1), TUE(2), WED(3), THU(4), FRI(5), SAT(6) &#123; @Override public boolean isRest() &#123; return true; &#125; &#125;, SUN(0) &#123; @Override public boolean isRest() &#123; return true; &#125; &#125;; private int value; private EnumTest(int value) &#123; this.value = value; &#125; public int getValue() &#123; return value; &#125; public boolean isRest() &#123; return false; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; System.out.println(&quot;EnumTest.FRI 的 value = &quot; + EnumTest.FRI.getValue()); &#125;&#125;输出结果：EnumTest.FRI 的 value = 5 EnumSet，EnumMap 的应用 12345678910111213141516171819public class Test &#123; public static void main(String[] args) &#123; // EnumSet的使用 EnumSet&lt;EnumTest&gt; weekSet = EnumSet.allOf(EnumTest.class); for (EnumTest day : weekSet) &#123; System.out.println(day); &#125; // EnumMap的使用 EnumMap&lt;EnumTest, String&gt; weekMap = new EnumMap(EnumTest.class); weekMap.put(EnumTest.MON, &quot;星期一&quot;); weekMap.put(EnumTest.TUE, &quot;星期二&quot;); // ... ... for (Iterator&lt;Entry&lt;EnumTest, String&gt;&gt; iter = weekMap.entrySet().iterator(); iter.hasNext();) &#123; Entry&lt;EnumTest, String&gt; entry = iter.next(); System.out.println(entry.getKey().name() + &quot;:&quot; + entry.getValue()); &#125; &#125;&#125; 原理分析 enum 的语法结构尽管和 class 的语法不一样，但是经过编译器编译之后产生的是一个class文件。该class文件经过反编译可以看到实际上是生成了一个类，该类继承了java.lang.Enum。EnumTest 经过反编译(javap com.hmw.test.EnumTest 命令)之后得到的内容如下： 123456789101112131415public class com.hmw.test.EnumTest extends java.lang.Enum&#123; public static final com.hmw.test.EnumTest MON; public static final com.hmw.test.EnumTest TUE; public static final com.hmw.test.EnumTest WED; public static final com.hmw.test.EnumTest THU; public static final com.hmw.test.EnumTest FRI; public static final com.hmw.test.EnumTest SAT; public static final com.hmw.test.EnumTest SUN; static &#123;&#125;; public int getValue(); public boolean isRest(); public static com.hmw.test.EnumTest[] values(); public static com.hmw.test.EnumTest valueOf(java.lang.String); com.hmw.test.EnumTest(java.lang.String, int, int, com.hmw.test.EnumTest);&#125; 所以，实际上 enum 就是一个 class，只不过 java 编译器帮我们做了语法的解析和编译而已。总结 可以把 enum 看成是一个普通的 class，它们都可以定义一些属性和方法，不同之处是：enum 不能使用 extends 关键字继承其他类，因为 enum 已经继承了 java.lang.Enum（java是单一继承）。","categories":[],"tags":[]},{"title":"List学习","slug":"王阁/技术/hexo/oldblog/blog4","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.009Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog4/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog4/","excerpt":"此处简介","text":"此处简介 List学习 ArrayList不是线程安全的，只能用在单线程环境下，多线程环境下可以考虑用Collections.synchronizedList(List l)函数返回一个线程安全的ArrayList类，也可以使用concurrent并发包下的CopyOnWriteArrayList类 ArrayList实现了Serializable接口，因此它支持序列化，能够通过序列化传输，实现了RandomAccess接口，支持快速随机访问，实际上就是通过下标序号进行快速访问，实现了Cloneable接口，能被克隆。 每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。从上述代码中可以看出，数组进行扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量的增长大约是其原容量的1.5倍。这种操作的代价是很高的，因此在实际使用时，我们应该尽量避免数组容量的扩张。当我们可预知要保存的元素的多少时，要在构造ArrayList实例时，就指定其容量，以避免数组扩容的发生。或者根据实际需求，通过调用ensureCapacity方法来手动增加ArrayList实例的容量。 Object oldData[] &#x3D; elementData;&#x2F;&#x2F;为什么要用到oldData[]乍一看来后面并没有用到关于oldData， 这句话显得多此一举！但是这是一个牵涉到内存管理的类， 所以要了解内部的问题。 而且为什么这一句还在if的内部，这跟elementData &#x3D; Arrays.copyOf(elementData, newCapacity); 这句是有关系的，下面这句Arrays.copyOf的实现时新创建了newCapacity大小的内存，然后把老的elementData放入。好像也没有用到oldData，有什么问题呢。问题就在于旧的内存的引用是elementData， elementData指向了新的内存块，如果有一个局部变量oldData变量引用旧的内存块的话，在copy的过程中就会比较安全，因为这样证明这块老的内存依然有引用，分配内存的时候就不会被侵占掉，然后copy完成后这个局部变量的生命期也过去了，然后释放才是安全的。不然在copy的的时候万一新的内存或其他线程的分配内存侵占了这块老的内存，而copy还没有结束，这将是个严重的事情。 关于ArrayList和Vector区别如下： ArrayList在内存不够时默认是扩展50% + 1个，Vector是默认扩展1倍。 Vector提供indexOf(obj, start)接口，ArrayList没有。 Vector属于线程安全级别的，但是大多数情况下不使用Vector，因为线程安全需要更大的系统开销。 ArrayList还给我们提供了将底层数组的容量调整为当前列表保存的实际元素的大小的功能。它可以通过trimToSize方法来实现。代码如下： 关于ArrayList的源码，给出几点比较重要的总结： 1、注意其三个不同的构造方法。无参构造方法构造的ArrayList的容量默认为10，带有Collection参数的构造方法，将Collection转化为数组赋给ArrayList的实现数组elementData。 2、注意扩充容量的方法ensureCapacity。ArrayList在每次增加元素（可能是1个，也可能是一组）时，都要调用该方法来确保足够的容量。当容量不足以容纳当前的元素个数时，就设置新的容量为旧的容量的1.5倍加1，如果设置后的新容量还不够，则直接新容量设置为传入的参数（也就是所需的容量），而后用Arrays.copyof()方法将元素拷贝到新的数组（详见下面的第3点）。从中可以看出，当容量不够时，每次增加元素，都要将原来的元素拷贝到一个新的数组中，非常之耗时，也因此建议在事先能确定元素数量的情况下，才使用ArrayList，否则建议使用LinkedList。 3、ArrayList的实现中大量地调用了Arrays.copyof()和System.arraycopy()方法。我们有必要对这两个方法的实现做下深入的了解。 http://www.cnblogs.com/ITtangtang/p/3948555.html . LinkedList是用链表结构存储数据的，很适合数据的动态插入和删除，随机访问和遍历速度比较慢。另外，接口中没有定义的方法get，remove，insertList，专门用于操作表头和表尾元素，可以当作堆栈、队列和双向队列使用。LinkedList没有同步方法。如果多个线程同时访问一个List，则必须自己实现访问同步。一种解决方法是在创建 List时构造一个同步的List： List list &#x3D; Collections.synchronizedList(new LinkedList(…)); 查看Java源代码，发现当数组的大小不够的时候，需要重新建立数组，然后将元素拷贝到新的数组内，ArrayList和Vector的扩展数组的大小不同。 http://www.tuicool.com/articles/iQZBFb http://www.cnblogs.com/azai/archive/2010/12/09/1901272.html 简化 Collections.synchronizedList(List l) Serializable 、RandomAccess 、Cloneable ensureCapacity 从上面的源码剖析可以看出这三种List实现的一些典型适用场景，如果经常对数组做随机插入操作，特别是插入的比较靠前，那么LinkedList的性能优势就非常明显，而如果都只是末尾插入，则ArrayList更占据优势，如果需要线程安全，则使用Vector或者创建线程安全的ArrayList。 在使用基于数组实现的ArrayList 和Vector 时我们要指定初始容量，因为我们在源码中也看到了，在添加时首先要进行容量的判断，如果容量不够则要创建新数组，还要将原来数组中的数据复制到新数组中，这个过程会减低效率并且会浪费资源。","categories":[],"tags":[]},{"title":"MapReduce的核心思想","slug":"王阁/技术/hexo/oldblog/blog26","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.009Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog26/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog26/","excerpt":"此处简介","text":"此处简介 mapreduce在hadoop中更多的承担的是计算的角色 什么是MapReduce？mapreduce源于谷歌公司为研究大规模数据处理而研发出的一种并行计算模型和方法。它将并行编程中的难点和有相对门槛的方法进行高度封装，而给开发者一套接口，让开发者理专注于自己的业务逻辑，即可让自己的代码运行在分布式集群中，大大降低了开发一些并发程序的门槛。从字面上就可以知道，MapReduce分为Map(映射)、Reduce(规约) MapReduce的核心思想MapReduce主要是两种经典函数： 映射（Mapping）将一个整体按某种规则映射成N份。并对这N份进行同一种操作。 规约（Reducing）将N份文件，按某种策略进行合并。 MapReduce的角色与动作 MapReduce包含四个组成部分，分别为Client、JobTracker、TaskTracker和Task，下面我们详细介绍这四个组成部分。 Client：作业提交的发起者， 每一个 Job 都会在用户端通过 Client 类将应用程序以及配置参数 Configuration 打包成 JAR 文件存储在 HDFS，并把路径提交到 JobTracker 的 master 服务，然后由 master 创建每一个 Task（即 MapTask 和 ReduceTask） 将它们分发到各个 TaskTracker 服务中去执行。 JobTracker：初始化作业，分配作业，与TaskTracker通信，协调整个作业 JobTracke负责资源监控和作业调度。JobTracker 监控所有TaskTracker 与job的健康状况，一旦发现失败，就将相应的任务转移到其他节点；同时，JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器，而调度器会在资源出现空闲时，选择合适的任务使用这些资源。在Hadoop 中，任务调度器是一个可插拔的模块，用户可以根据自己的需要设计相应的调度器。 TaskTracker：TaskTracker 会周期性地通过Heartbeat 将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）。TaskTracker 使用“slot”等量划分本节点上的资源量。“slot”代表计算资源（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop 调度器的作用就是将各个TaskTracker 上的空闲slot 分配给Task 使用。slot 分为Map slot 和Reduce slot 两种，分别供Map Task 和Reduce Task 使用。TaskTracker 通过slot 数目（可配置参数）限定Task 的并发度。 Task ： Task 分为Map Task 和Reduce Task 两种，均由TaskTracker 启动。HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。 Map Task 执行过程如下图 所示：由该图可知，Map Task 先将对应的split 迭代解析成一个个key&#x2F;value 对，依次调用用户 自定义的map() 函数进行处理，最终将临时结果存放到本地磁盘上, 其中临时数据被分成若干个partition，每个partition 将被一个Reduce Task 处理。 Reduce Task 执行过程下图所示。该过程分为三个阶段 提交作业 在作业提交之前，需要对作业进行配置 程序代码，主要是自己书写的MapReduce程序 输入输出路径 其他配置，如输出压缩等 配置完成后，通过JobClient来提交 作业的初始化 客户端提交完成后，JobTracker会将作业加入队列，然后进行调度，默认的调度方法是FIFO调试方式任务的分配 TaskTracker和JobTracker之间的通信与任务的分配是通过心跳机制完成的。 TaskTracker会主动向JobTracker询问是否有作业要做，如果自己能做，那么就会申请作业任务，这个任务可以使Map，也可能是Reduce任务任务的执行 申请到任务后，TaskTracker会做如下事情： 拷贝代码到本地 拷贝任务的信息到本地 启动Jvm运行任务状态与任务的更新 任务在运行过程中，首先会将自己的状态汇报给TaskTracker，然后由TaskTracker汇总报给JobTracker 任务进度是通过计数器来实现的。 作业的完成 JobTracker 是在接受到最后一个任务运行完成后，才会将任务标志为成功 此时会做删除中间结果等善后处理工作 MapReduce任务执行流程与数据处理流程详解就任务流程而言，上文中有些或已经涉及，有些也介绍的比较详细了。但就整体而言，不是特别具体，或这个整体性，所以在此我们再集中总结一下，即使重复内容，就也当加深印象吧。 任务执行流程详解 通过jobClient提交当mapreduce的job提交至JobTracker,提交的具体信息大致会有，conf配置内容，path，相关的Map，Reduce函数等。（数据的切片会在client上完成） JobTracker中的Master服务会完成创建一个Task（MapTask与ReduceTask），并将这个任务加载至任务队列中，等待TaskTracker来获取。同进JobTracker会处于一种监听状态，监听所有TaskTracker与Job的健康情况。面对故障时，提供服务转移等。同时它还会记录任务的执行进度，资源的使用情况，提交至任务调度器，由调度器进行资源的调度。 TaskTracker会主动向jobTacker去询问是否有任务，如果有，就会申请到作业，作业可以是map,也可以是reduce任务。TaskTracker获取的不仅是任务，还有相关的处理代码也会copy一份至本地，然后TaskTracker再针对所分配的split进行处理。TaskTracker还会周期性地通过HearBeat将本节点上的资源的使用情况和任务的进行进度汇报给JobTracker，同时接收JobTracker发过来的相关命令并执行， 当TaskTracker中的任务完成后会上报给JobTracker,而当最后一个TaskTracker完成后，JobTracker才会将任务标志为成功，并执行一些如删除中间结果等善后工作。 从这里我们知道了mapreduce中任务执行流程，但mapreduce作为hadoop的计算框架，它对数据的处理流程我们还没明确涉及，所以接下来我们再深入了解下具体的数据处理流程 数据处理流程详解这里的数据处理流程，主要指的是mapreduce执行后，Task中对split的任务具体处理的这一流程，其中还包括，数据的切片，mapTask完成后将中间结果上传等动作。下面我们来具体讨论当jobClient向JobTracker提交了任务后，数据处理流程也随之开始 在Client端将输入源的数据进行切片(split)，具体的切片机制参考后面 JobTracker中的MRAppMaster将每个Split信息计算出需要的MapTask的实例数量，然后向集群申请机器启动相应数量的mapTask进程 进程启动后，根据给定的数据切片范围进行数据处理，主要流程为a)通过inputFormat来获取RecordReader读取数据，并形成输入的KV对b)将输入KV对传递给用户定义的map（）方法，做逻辑处理，并map()方法的输出的kv对手机到缓存中(这里用到了缓存机制)简而言之map中输入时要做的事是1.反射构造InputFormat.2.反射构造InputSplit.3.创建RecordReader.4.反射创建MapperRunner 而map输出时相对复杂，主要涉及到的有Partitioner，shuffle，sort，combiner等概念，我们就来一一讨论。在map（）方法执行后，map阶段是会有处理的数据输出，正常来说，就是每个split对应的每一行。如上图MapRunner的next为false时，对输入数据的map完成，这时对存内中这些map的数据，会对其进行sort,如果我们事先设置的有combiner那么，还会对sort完的数据执行combiner(一个类reduce操作，不过是对本地数据的reduce，使用它的原则是combiner的输入不会影响到reduce计算的最终输入，例如：如果计算只是求总数，最大值，最小值可以使用combiner，但是做平均值计算使用combiner的话，最终的reduce计算结果就会出错。)，然后开始map的内容spill到磁盘中，如果频繁的spill对磁盘会带来较大的损耗和效率影响。所以引入了一个写缓冲区的概念，即每一个Map Task都拥有一个“环形缓冲区”作为Mapper输出的写缓冲区。写缓冲区大小默认为100MB（通过属性io.sort.mb调整），当写缓冲区的数据量达到一定的容量限额时（默认为80%，通过属性io.sort.spill.percent调整），后台线程开始将写缓冲区的数据溢写到本地磁盘。在数据溢写的过程中，只要写缓冲区没有被写满，Mappper依然可以将数据写出到缓冲区中；否则Mapper的执行过程将被阻塞，直到溢写结束。溢写以循环的方式进行（即写缓冲区的数据量大致限额时就会执行溢写），可以通过属性mapred.local.dir指定写出的目录。spill结束前 溢写线程将数据最终写出到本地磁盘之前，首先根据Reducer的数目对这部分数据进行分区（即每一个分区中的数据会被传送至同一个Reducer进行处理，分区数目与Reducer数据保持一致）即 partition，partition是一个类inputSplit()的操作，即根据有多少个ReduceTask生成多少个partition,并通过jobTracker指定给相应的ReduceTask。当spill完成后，本地磁盘中会有多个溢出文件存在。在MapTask结束前，这些文件会根据相应的分区进行合并，并排序，合并可能发生多次，具体由io.sort.factor控制一次最多合并多少个文件。 如果溢写文件个数超过3（通过属性min.num.spills.for.combine设置），会对合并且分区排序后的结果执行Combine过程（如果MapReduce有设置Combiner），而且combine过程在不影响最终结果的前提下可能会被执行多次；否则不会执行Combine过程（相对而言，Combine开销过大）。 注意：Map Task执行过程中，Combine可能出现在两个地方：写缓冲区溢写过程中、溢写文件合并过程中。 注意：Mapper的一条输出结果（由key、value表示）写出到写缓冲区之前，已经提前计算好相应的分区信息，即分区的过程在数据写入写缓冲区之前就已经完成，溢写过程实际是写缓冲区数据排序的过程（先按分区排序，如果分区相同时，再按键值排序）。 这里涉及到MapReduce的两个组件：Comparator、Partitioner。(由于篇幅的原因，这里暂不引入对这两个组件的源码分析，和自定义方式，后面有机会则单开文章讨论) 在将map输出结果作为reducTask中的输入时，会涉及到磁盘写入，网络传输等资源的限制，所以对出于节省资源的考虑，可以在对map的输出结果进行压缩。默认情况下，压缩是不被开启的，可以通过属性mapred.compress.map.output、mapred.map.output.compression.codec进行相应设置。 当MapTask任务结束后，被指定的分区ReduceTask会立即开始执行，即开始拷贝对应MapTask分区中的输出结果。Reduce Task的这个阶段被称为“Copy Phase”。Reduce Task拥有少量的线程用于并行地获取Map Tasks的输出结果，默认线程数为5，可以通过属性mapred.reduce.parallel.copies进行设置。同样：如果Map Task的输出结果足够小，它会被拷贝至Reduce Task的缓冲区中；否则拷贝至磁盘。当缓冲区中的数据达到一定量（由属性mapred.job.shuffle.merge.percent、mapred.inmem.merge.threshold），这些数据将被合并且溢写到磁盘。如果Combine过程被指定，它将在合并过程被执行，用来减少需要写出到磁盘的数据量。 随着拷贝文件中磁盘上的不断积累，一个后台线程会将它们合并为更大地、有序的文件，用来节省后期的合并时间。如果Map Tasks的输出结合使用了压缩机制，则在合并的过程中需要对数据进行解压处理。 当Reduce Task的所有Map Tasks输出结果均完成拷贝，Reduce Task进入“Sort Phase”（更为合适地应该被称为“Merge Phase”，排序在Map阶段已经被执行），该阶段在保持原有顺序的情况下进行合并。这种合并是以循环方式进行的，循环次数与合并因子（io.sort.factor）有关。sort phase 通常不是合并成一个文件，而是略过磁盘操作，直接将数据合并输入至Reduce方法中 这次合并的数据可以结合内存、磁盘两部分进行操作），即“Reduce Phase”。 通常这里还有一个“Group”的阶段，这个阶段决定着哪些键值对属于同一个键。如果没有特殊设置，只有在Map Task输出时那些键完全一样的数据属于同一个键，但这是可以被改变的。 描述至这里终于能引用mapreduce中相当重要的一个概念，即shuffle。这个词在这里该怎么定义，我暂未找到个一个比较满意的答案，但我比较喜欢有人把这个比作是搓完牌一桌子人，在下一局开始前的整个过程。即Shuffle操作，涉及到数据的partition、sort、[combine]、spill、[comress]、[merge]、copy、[combine]、merge、group，而这些操作不但决定着程序逻辑的正确性，也决定着MapReduce的运行效率。 shuffle完后进入了ReduceTask的reduce()方法中在Reduce Phase的过程中，它处理的是所有Map Tasks输出结果中某一个分区中的所有数据，这些数据整体表现为一个根据键有序的输入，对于每一个键都会相应地调用一次Reduce Function（同一个键对应的值可能有多个，这些值将作为Reduce Function的参数） 至此MapReduce的逻辑过程基本描述完成，虽然洋洋洒洒可能会有数千字，但本文的出发点就不是简析，而更多是自我概念原理部份的总结，所以力求整个流程完整详细。后面我配上一些网络图片，方便大家快速理解，结合文字加深印象。","categories":[],"tags":[]},{"title":"sed & awk小结","slug":"王阁/技术/hexo/oldblog/blog23","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.009Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog23/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog23/","excerpt":"此处简介","text":"此处简介 sed &amp; awk小结一直想认真掌握sed与awk已经很久了，但一直未找个以特别详细的资料和时间来做这件事，正好这两天受到启发，转而翻墙搜索国外资源，有了很大的收获，趁次机会攻克下来 前言sed与awk总得来说是两样东西，本身无直接关联，做在日常使用时两者经常使用到，并且常常混合使用，所以此次小结放在一起，分总式结构进行小结 sedsed相较awk更偏于工具一点，全称应该是strem editor (即流式编辑器)。面向的是一行一行内容 使用形式sed [-nefr] [动作]选项与参数：-n ：使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到终端上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。-e ：直接在命令列模式上进行 sed 的动作编辑；-f ：直接将 sed 的动作写在一个文件内， -f filename 则可以运行 filename 内的 sed 动作；-r ：sed 的动作支持的是延伸型正规表示法的语法。(默认是基础正规表示法语法)-i ：直接修改读取的文件内容，而不是输出到终端。 动作说明： [n1[,n2]]functionn1, n2 ：不见得会存在，一般代表『选择进行动作的行数』，举例来说，如果我的动作是需要在 10 到 20 行之间进行的，则『 10,20[动作行为] 』 function：a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚；i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)；p ：列印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s&#x2F;old&#x2F;new&#x2F;g 就是啦！ awk尝试看awk已有些时日，整体效率不太好，但大体也有思路，现对awk进行一些简单总结，后面则进行实例操作awk的细节小点比较多，一次或无法完全总结全，但总体感觉下来发现熟悉大体模式，具体细节可以再通过即时检索解决 awk命令的基本格式是: awk &#39;/search_pattern/ &#123; action_to_t[](http://)ake_on_matches; another_action; &#125;&#39; file_to_parse 其中searach 与action都可省略其中之一，若action省略，那么action默认为print操作如何search省略，那么默认action针对的是每一行如 附几个操作实例如 这里是以空白区分了列，通过$后加不同的数字，表示不同的列，$0表示这一行，$1表示第一列，类推。 awk的内置变量 FILENAME:当前输入文件的名称 FNR:当前输入文件数 FS:当前环境中的分隔符，默认是空白 NF:输入文件的每行对应的列数 NR:当前是第几个记录 OFS:列输出时的分隔符，默认是空白 ORS:记录输出时的分隔符，默认是新起一行 RS输入记录中的分隔符，默认是newline character 正如以上内置变量的存在，所以，awk在正式使用进可能面临更多复杂情况，而之前那种简单模式可以无法应对。于是，常用的awk的使用形态扩充为 awk ‘BEGIN { action; }&#x2F;search&#x2F; { action; }END { action; }’ input_file 这里引入了BEGIN与END两个部份，用于做一些初始化或善后处理。 awk的一些常见的匹配操作 awk ‘&#x2F;sa&#x2F;‘ file awk ‘$2 ~ &#x2F;^sa&#x2F;‘ file$number表示只匹配该列~ 表示“是”!~表示“不是” 至此一这就总结了一些较为基础的awk使用，就一般工作而言，已经能应对很多场景了。后续我会再进行更为详细的总结待续~~~","categories":[],"tags":[]},{"title":"java中IO小结","slug":"王阁/技术/hexo/oldblog/blog7","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.010Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog7/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog7/","excerpt":"此处简介","text":"此处简介 java中IO小结FileFile类,相关api createNewFiles(); mkdir(); mkdirs() IO流按方向，分输入，输出流in、out流当中有缓冲区，即stream.read(),可以一次性读取stream.read(Byte[]byte);针对缓冲流还有两个方法mark与resetmarkSupported 判断该输入流能支持mark 和 reset 方法。 mark用于标记当前位置；在读取一定数量的数据(小于readlimit的数据)后使用reset可以回到mark标记的位置。 FileInputStream不支持mark&#x2F;reset操作；BufferedInputStream支持此操作； mark(readlimit)的含义是在当前位置作一个标记，制定可以重新读取的最大字节数，也就是说你如果标记后读取的字节数大于readlimit，你就再也回不到回来的位置了。 通常InputStream的read()返回-1后，说明到达文件尾，不能再读取。除非使用了mark&#x2F;reset。 流的类型 FileInputStream ObjectInputStream DataInputStream BufferedInputStream BufferedReader ByteArrayInputStream CharArrayReader Console FileReader PipedInputStream PipedReader PushbackInputStream StringReader 流的名称 说明 FileInputStream FileInputStream 从文件系统中的某个文件中获得输入字节。 ObjectInputStream ObjectInputStream 对以前使用 ObjectOutputStream 写入的基本数据和对象进行反序列化。 DataInputStream 数据输入流允许应用程序以与机器无关方式从底层输入流中读取基本 Java 数据类型。 BufferedInputStream BufferedInputStream 为另一个输入流添加一些功能，即缓冲输入以及支持 mark 和 reset 方法的能力 ByteArrayInputStream ByteArrayInputStream 包含一个内部缓冲区，该缓冲区包含从流中读取的字节。 CharArrayReader 此类实现一个可用作字符输入流的字符缓冲区 Console 此类包含多个方法，可访问与当前 Java 虚拟机关联的基于字符的控制台设备（如果有）。 FileReader 用来读取字符文件的便捷类 PushbackInputStream PushbackInputStream 为另一个输入流添加性能，即“推回 (push back)”或“取消读取 (unread)”一个字节的能力。 Java I&#x2F;O默认是不缓冲流的，所谓“缓冲”就是先把从流中得到的一块字节序列暂存在一个被称为buffer的内部字节数组里，然后你可以一下子取到这一整块的字节数据，没有缓冲的流只能一个字节一个字节读，效率孰高孰低一目了然。有两个特殊的输入流实现了缓冲功能，一个是我们常用的BufferedInputStream 一些小结带array的流自带缓冲区。支持mark与reset方法其他的要套缓冲流 存在readLine()方法的流 BufferReader read(char[],0,len)的理解将len从0位开始装入char[]中，然后通过String.valueOf(char[])转换成字符串","categories":[],"tags":[]},{"title":"map总结","slug":"王阁/技术/hexo/oldblog/blog9","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.010Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog9/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog9/","excerpt":"此处简介","text":"此处简介 map总结Hashtable├-HashMap└-WeakHashMap 通用Map，用于在应用程序中管理映射，通常在 java.util 程序包中实现 HashMap Hashtable Properties LinkedHashMap IdentityHashMap TreeMap WeakHashMap ConcurrentHashMap HashMap 1最常用的Map,它根据键的HashCode 值存储数据,根据键可以直接获取它的值，具有很快的访问速度。HashMap最多只允许一条记录的键为Null(多条会覆盖);允许多条记录的值为 Null。非同步的。 Hashtable 1234与 HashMap类似,不同的是:key和value的值均不允许为null;它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了Hashtale在写入时会比较慢。LinkedHashMap保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的.在遍历的时候会比HashMap慢。key和value均允许为空，非同步的。 TreeMap + 增强for循环使用方便，但性能较差，不适合处理超大量级的数据。 + 迭代器的遍历速度要比增强for循环快很多，是增强for循环的2倍左右。 + 使用entrySet遍历的速度要比keySet快很多，是keySet的1.5倍左右。","categories":[],"tags":[]},{"title":"java中的形参与实参的理解","slug":"王阁/技术/hexo/oldblog/blog8","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.010Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog8/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog8/","excerpt":"此处简介","text":"此处简介 java中的形参与实参的理解值传递：方法调用时，实际参数把它的值传递给对应的形式参数，函数接收的是原始值的一个copy，此时内存中存在两个相等的基本类型，即实际参数和形式参数，后面方法中的操作都是对形参这个值的修改，不影响实际参数的值。 引用传递：也称为传地址。方法调用时，实际参数的引用(地址，而不是参数的值)被传递给方法中相对应的形式参数，函数接收的是原始值的内存地址；在方法执行中，形参和实参内容相同，指向同一块内存地址，方法执行中对引用的操作将会影响到实际对象。 *值传递：方法调用时，实际参数将它的值传递给对应的形式参数，函数接收到的是原始值的副本，此时内存中存在两个相等的基本类型，若方法中对形参执行处理操作，并不会影响实际参数的值。 *引用传递：方法调用时，实际参数的引用（是指地址，而不是参数的值）被传递给方法中相应的形式参数，函数接收到的是原始值的内存地址，在方法中，形参与实参的内容相同，方法中对形参的处理会影响实参的值。 1）形参为基本类型时，对形参的处理不会影响实参。2）形参为引用类型时，对形参的处理会影响实参。3）String,Integer,Double等immutable类型的特殊处理，可以理解为值传递，形参操作不会影响实参对象。","categories":[],"tags":[]},{"title":"反射小结","slug":"王阁/技术/hexo/oldblog/blog6","date":"2019-07-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.010Z","comments":true,"path":"2019/07/16/王阁/技术/hexo/oldblog/blog6/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/oldblog/blog6/","excerpt":"此处简介","text":"此处简介 反射小结什么是反射反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为java语言的反射机制。对于反射我的理解是：正java的核心思想，万物对象，那么类的类也可被描述，如类也会有名，属性，方法的特性。皆是对类的类型的一种描述。在jvm中反射机制的存在让java也拥有了动态的特性，即我们可以在运行时才确定类的类型，即相应的方法与属性 反射的功能 在运行时获取其类的相关特性，如名称，属性，方法 可以构造一个类的对象 动态代理 反射的三种形式 Class.forName(“ “);通过Class的静态方法加载相应的相对路径，获得指定的类的类型 类.class获取类的类型 实例.getClass(); 要访问私有属性，要设置setAccessibleClass cls ~~Field file &#x3D;cls.getDeclaredFields();","categories":[],"tags":[]},{"title":"updateStateByKey&mapStateWithKey","slug":"王阁/技术/hexo/spark/stream","date":"2019-07-14T16:00:00.000Z","updated":"2023-10-27T06:50:27.010Z","comments":true,"path":"2019/07/15/王阁/技术/hexo/spark/stream/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/07/15/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/spark/stream/","excerpt":"spark中如何实现全局count","text":"spark中如何实现全局count 说明两种方式都可以实现对同一key的累计统计 区别updateStateByKey会返回无增量数据的状态,所以会相对较大的数据资源开销mapStateWithKey 相当于增量统计 使用updateStateByKey : 1234567891011121314151617public static Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt; updateFunctionByUpdate() &#123; Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt; updateFunction = (values, s1) -&gt; &#123; Integer newSum = 0; if (s1.isPresent()) &#123; newSum = s1.get(); &#125; Iterator&lt;Integer&gt; i = values.iterator(); while (i.hasNext()) &#123; newSum += i.next(); &#125; return Optional.of(newSum); &#125;; return updateFunction; &#125; mapStateWithKey : 1234567891011public static Function3&lt;String, Optional&lt;Integer&gt;, State&lt;Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; updateFunctionByMap() &#123; Function3&lt;String, Optional&lt;Integer&gt;, State&lt;Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; updateFunction2 = (word, one, state) -&gt; &#123; int sum = one.or(0) + (state.exists() ? state.get() : 0); Tuple2&lt;String, Integer&gt; output = new Tuple2&lt;String, Integer&gt;(word, sum); state.update(sum); return output; &#125;; return updateFunction2; &#125;","categories":[],"tags":[{"name":"sparkstream","slug":"sparkstream","permalink":"http://www.wqkenqing.ren/daydoc/tags/sparkstream/"}]},{"title":"flume记录","slug":"王阁/技术/大数据/flume/Flume","date":"2019-06-18T16:00:00.000Z","updated":"2023-10-27T06:50:27.013Z","comments":true,"path":"2019/06/19/王阁/技术/大数据/flume/Flume/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/06/19/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE/flume/Flume/","excerpt":"flume几种source and sink实际操作","text":"flume几种source and sink实际操作 soruceflume自身就支持多种source.chanel这里暂用memsink to hdfs 简单测试几种source 12345678910111213141516171819202122232425262728293031323334353637# 配置Agenta1.sources = r1a1.sinks = k1a1.channels = c1# 配置Sourcea1.sources.r1.type = execa1.sources.r1.channels = c1a1.sources.r1.deserializer.outputCharset = UTF-8# 配置需要监控的日志输出目录a1.sources.r1.command = tail# 配置Sinka1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.useLocalTimeStamp = truea1.sinks.k1.hdfs.path = hdfs://namenode:9000/flume/events/%Y-%ma1.sinks.k1.hdfs.filePrefix = %Y-%m-%d-%Ha1.sinks.k1.hdfs.fileSuffix = .loga1.sinks.k1.hdfs.minBlockReplicas = 1a1.sinks.k1.hdfs.fileType = DataStreama1.sinks.k1.hdfs.writeFormat = Text#a1.sinks.k1.hdfs.rollInterval = 86400a1.sinks.k1.hdfs.rollSize =0a1.sinks.k1.hdfs.rollCount =0#a1.sinks.k1.hdfs.idleTimeout = 0# 配置Channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# 将三者连接a1.sources.r1.channel = c1a1.sinks.k1.channel = c1 命令flume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f kafka.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f demo2.conf -n a1 -Dflume.root.logger&#x3D;INFO,console16228 解决Flume采集数据时在HDFS上产生大量小文件的问题以上conf为例 a1为agent的名称demo.conf为flume配置文件的名称-c指向log4j.properties文件和flume_env.sh文件所在目录。–Dflume.root.logger&#x3D;INFO,console 在终端输出运行日志 查阅flume配置参数，如下： rollSize默认值：1024，当临时文件达到该大小（单位：bytes）时，滚动成目标文件。如果设置成0，则表示不根据临时文件大小来滚动文件。 rollCount默认值：10，当events数据达到该数量时候，将临时文件滚动成目标文件，如果设置成0，则表示不根据events数据来滚动文件。 round默认值：false，是否启用时间上的”舍弃”，类似于”四舍五入”，如果启用，则会影响除了%t的其他所有时间表达式； roundValue默认值：1，时间上进行“舍弃”的值； roundUnit 默认值：seconds，时间上进行”舍弃”的单位，包含：second,minute,hour 当设置了round、roundValue、roundUnit参数收，需要在sink指定的HDFS路径上指定按照时间生成的目录的格式，例如有需求，每采集1小时就在HDFS目录上生成一个目录，里面存放这1小时内采集到的数据。 flume file to avro12345678910111213141516171819202122#########a1 agent#####a1.sources = r1a1.sinks = k1a1.channels = c1# 配置Sourcea1.sources.r1.type = execa1.sources.r1.channels = c1a1.sources.r1.deserializer.outputCharset = UTF-8a1.sources.r1.command = cat /data/upload/theme_item_pool.csv# 配置需要监控的日志输出目录a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.sinks.k1.type = avroa1.sinks.k1.hostname = namenodea1.sinks.k1.port = 4444a1.sinks.k1.requiredAcks = 1a1.sinks.k1.channel = c1 flume avro to kafka123456789101112131415161718192021#########a1 agent#####a1.sources = r1a1.sinks = k1a1.channels = c1# 配置Sourcea1.sources.r1.type = execa1.sources.r1.channels = c1a1.sources.r1.deserializer.outputCharset = UTF-8a1.sources.r1.command = cat /data/upload/theme_item_pool.csv# 配置需要监控的日志输出目录a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.sinks.k1.type = avroa1.sinks.k1.hostname = namenodea1.sinks.k1.port = 4444a1.sinks.k1.requiredAcks = 1a1.sinks.k1.channel = c1 avro source123456789101112131415161718192021222324252627282930#test avro sourcesa1.sources=r1a1.channels=c1a1.sinks=k1a1.sources.r1.type = avroa1.sources.r1.channels=c1a1.sources.r1.bind=localhosta1.sources.r1.port=55555#Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity = 100#sink配置a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.useLocalTimeStamp = truea1.sinks.k1.hdfs.path = hdfs://namenode:8020/flume/events/%Y-%ma1.sinks.k1.hdfs.filePrefix = %Y-%m-%d-%Ha1.sinks.k1.hdfs.fileSuffix = .loga1.sinks.k1.hdfs.minBlockReplicas = 1a1.sinks.k1.hdfs.fileType = DataStreama1.sinks.k1.hdfs.writeFormat = Text#a1.sinks.k1.hdfs.rollInterval = 86400a1.sinks.k1.hdfs.rollSize =0a1.sinks.k1.hdfs.rollCount =0#a1.sinks.k1.hdfs.idleTimeout = 0a1.sinks.k1.channel = c1 avro to file1234567891011121314151617181920212223#test avro sourcesa1.sources=r1a1.channels=c1a1.sinks=k1a1.sources.r1.type = execa1.sources.r1.channels = c1a1.sources.r1.deserializer.outputCharset = UTF-8a1.sources.r1.command =curl http://192.168.10.104:8088/ws/v1/cluster/apps#Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity = 100#sink配置a1.sinks.k1.type=file_rolla1.sinks.k1.channel=c1a1.sinks.k1.sink.directory=/data/hadoop/flume-ng/config/log 123456789101112131415161718192021222324252627282930313233343536373839### 多个sink#test avro sourcesa1.sources=r1a1.channels=c1 c2a1.sinks=k1 k2a1.sources.r1.type = netcata1.sources.r1.bind=192.168.10.101a1.sources.r1.port=55555a1.sources.r1.channels = c1 c2#Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity = 100###channel2a1.channels.c2.type = memorya1.channels.c2.capacity=1000a1.channels.c2.transactionCapacity = 100#sink配置#a1.sinks.k1.type=loggera1.sinks.k1.type=file_rolla1.sinks.k1.channel=c1a1.sinks.k1.sink.directory=/data/hadoop/flume-ng/config/log###sink2a1.sinks.k2.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k2.brokerList = namenode:9092a1.sinks.k2.topic = carstreama1.sinks.k2.batchSize = 100a1.sinks.k2.requiredAcks = 1a1.sinks.k2.channel=c2 目录变化12345678910111213141516171819a1.sources=r1a1.channels=c1a1.sinks=k1a1.sources.r1.type = spooldira1.sources.r1.spoolDir = /data/hadoop/flume-ng/config/loga1.sources.r1.fileHeader = truea1.sources.r1.channels = c1#Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity = 100a1.sinks.k1.type=loggera1.sinks.k1.channel=c1 tailDir123456789101112131415161718192021222324252627282930313233# in this case called &#x27;a1&#x27;a1.sources = s1a1.channels = c1a1.sinks = k1# For each one of the sources, the type is defineda1.sources.s1.type = org.apache.flume.source.taildir.TaildirSourcea1.sources.s1.positionFile = /opt/cdhmoduels/apache-flume-1.5.0-cdh5.3.6-bin/taidir/dirsource/taildir_position.jsona1.sources.s1.filegroups = f1a1.sources.s1.filegroups.f1 = /data/hadoop/flume-ng/log/# The channel can be defined as follows.a1.sources.s1.channels = c1a1.sinks.k1.channel = c1# Each sink&#x27;s type must be defineda1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = /flume/event/taildira1.sinks.k1.hdfs.filePrefix = hive-log#Specify the channel the sink should use# Each channel&#x27;s type is defined.a1.channels.c1.type = memory# Other config values specific to each type of channel(sink or source)# can be defined as well# In this case, it specifies the capacity of the memory channela1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 1000 操作记录flume-ng avro-client -c conf -H localhost -p 55555 -F &#x2F;data&#x2F;upload&#x2F;trip_coord.csv &#x2F;data&#x2F;upload&#x2F;trip_coord.csv flume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f avro.conf -n a1 -Dflume.root.logger&#x3D;INFO,console flume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f avro2.conf -n a1 -Dflume.root.logger&#x3D;INFO,console flume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f curl.conf -n a1 -Dflume.root.logger&#x3D;INFO,console flume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f self2.conf -n a1 -Dflume.root.logger&#x3D;INFO,console flume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f kafka.conf -n a1 -Dflume.root.logger&#x3D;INFO,console flume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f tcp_logger.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f avro3.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f tcp_hdfs.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f avro.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f avro2.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f tcp.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f tcp4.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f syslog.conf -n a1 -Dflume.root.logger&#x3D;INFO,console flume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f http_logger.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f api_logger.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f api_logger_twosink.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f kafka.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f demo4.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f tcp.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f tcp_two_sink.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f spool.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f taildir.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f udp.conf -n a1 -Dflume.root.logger&#x3D;INFO,consoleflume-ng agent -c &#x2F;etc&#x2F;flume-ng&#x2F;conf&#x2F; -f udp_two_sink.conf -n a1 -Dflume.root.logger&#x3D;INFO,console2020-7-22 docker run –restart&#x3D;unless-stopped -d -p 9200:9200 -p 9300:9300 -e “discovery.type&#x3D;single-node” –name&#x3D;”hankoues” elasticsearch:7.1.0 docker pull docker.io&#x2F;cerebrodocker pull docker.io&#x2F;cerebro","categories":[],"tags":[]},{"title":"Yarn配置细节","slug":"王阁/技术/hexo/old/Yarn配置","date":"2019-06-12T16:00:00.000Z","updated":"2023-10-27T06:50:27.007Z","comments":true,"path":"2019/06/13/王阁/技术/hexo/old/Yarn配置/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/06/13/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/old/Yarn%E9%85%8D%E7%BD%AE/","excerpt":"此处简介","text":"此处简介 Yarn配置细节##内存,核数设置 12345678910111213141516171819202122232425262728293031323334 &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;3072&lt;/value&gt;&lt;/property&gt;&lt;!--该配置用于配置任务请求时的资源. --&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt; &lt;value&gt;-Xmx3276m&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;","categories":[],"tags":[]},{"title":"flume记录","slug":"王阁/技术/hexo/old/flume记录","date":"2019-06-12T16:00:00.000Z","updated":"2023-10-27T06:50:27.007Z","comments":true,"path":"2019/06/13/王阁/技术/hexo/old/flume记录/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/06/13/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/old/flume%E8%AE%B0%E5%BD%95/","excerpt":"此处简介","text":"此处简介 flume记录from kafka 123456789101112131415161718192021222324252627282930313233343536373839a1.sources = source1a1.sources.source1.type = org.apache.flume.source.kafka.KafkaSourcea1.sources.source1.channels = c1a1.sources.source1.batchSize = 5000a1.sources.source1.batchDurationMillis = 2000a1.sources.source1.zookeeperConnect = localhost:2181#a1.sources.source1.kafka.brokerList = localhost:9092a1.sources.source1.kafka.bootstrap.servers = localhost:9092a1.sources.source1.topic = flumetesta1.sources.source1.kafka.consumer.group.id = custom.g.ida1.channels = c1a1.channels.c1.type = memorya1.channels.c1.capacity = 10000a1.channels.c1.transactionCapacity = 10000a1.channels.c1.byteCapacityBufferPercentage = 20a1.channels.c1.byteCapacity = 800000a1.sinks = k1a1.sinks.k1.type = file_rolla1.sinks.k1.channel = c1a1.sinks.k1.sink.directory = /home/hadoop/testfile/flume 这里也有版本匹配的问题.经过多番尝试,这里的组合版本是flume1.6+kafka_2.11-2.2.0.tgz其它版本可能会有request header 问题.另外还遇到了指定topic 和 zookeeper的问题. 执行语句:flume-ng agent -n a1 -c conf -f kafka.properties -Dflume.root.logger&#x3D;INFO,console flume 采集到kafka1234567891011121314151617181920212223242526272829agent.sources=r1agent.sinks=k1agent.channels=c1agent.sources.r1.type=execagent.sources.r1.command=tail /root/tomcat/logs/catalina.outagent.sources.r1.restart=trueagent.sources.r1.batchSize=1000agent.sources.r1.batchTimeout=3000agent.sources.r1.channels=c1agent.channels.c1.type=memoryagent.channels.c1.capacity=102400agent.channels.c1.transactionCapacity=1000agent.channels.c1.byteCapacity=134217728agent.channels.c1.byteCapacityBufferPercentage=80agent.sinks.k1.channel=c1agent.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSinkagent.sinks.k1.kafka.topic=sparkstreamingagent.sinks.k1.kafka.zookeeperConnect=47.102.199.215:2181#agent.sinks.k1.kafka.bootstrap.servers=47.102.199.215:9092agent.sinks.k1.kafka.brokerList =47.102.199.215:9092agent.sinks.k1.serializer.class=kafka.serializer.StringEncoderagent.sinks.k1.flumeBatchSize=1000agent.sinks.k1.useFlumeEventFormat=true","categories":[],"tags":[]},{"title":"SparkSql","slug":"王阁/技术/hexo/spark/sql/SparkSql","date":"2019-06-12T16:00:00.000Z","updated":"2023-10-27T06:50:27.010Z","comments":true,"path":"2019/06/13/王阁/技术/hexo/spark/sql/SparkSql/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/06/13/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/spark/sql/SparkSql/","excerpt":"1spark sql 相关内容","text":"1spark sql 相关内容 sparksql","categories":[],"tags":[]},{"title":"CDH搭建细节","slug":"王阁/技术/hexo/CDH/搭建","date":"2019-05-26T16:00:00.000Z","updated":"2023-10-27T06:50:27.006Z","comments":true,"path":"2019/05/27/王阁/技术/hexo/CDH/搭建/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/05/27/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/CDH/%E6%90%AD%E5%BB%BA/","excerpt":"…..","text":"….. CDH安装准备 ubuntu ulimit","categories":[],"tags":[]},{"title":"spark操作.md","slug":"王阁/技术/hexo/spark/spark操作","date":"2019-05-16T16:00:00.000Z","updated":"2023-10-27T06:50:27.010Z","comments":true,"path":"2019/05/17/王阁/技术/hexo/spark/spark操作/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/05/17/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/spark/spark%E6%93%8D%E4%BD%9C/","excerpt":"spark编程积累","text":"spark编程积累 spark编程Inputhdfs操作hdfs比较常规,直接通过context.textfile(path) &#x2F;&#x2F;即可实现 hbasehbase 则要通过newAPIHadoopRDD来实现 123JavaPairRDD&lt;ImmutableBytesWritable, Result&gt; javaRDD = jsc.newAPIHadoopRDD(HbaseOperate.getConf(), TableInputFormat.class, ImmutableBytesWritable.class, Result.class); 这里要特别说明的是,这里的conf承担了更多的责任,如指定表名,指定scan传输字符串等. 123456Configuration hconf = HbaseOperate.getConf(); Scan scan = new Scan(); hconf.set(TableInputFormat.INPUT_TABLE, &quot;company&quot;); hconf.set(TableInputFormat.SCAN, convertScanToString(scan)); 参考以上这段代码 另 1234static String convertScanToString(Scan scan) throws IOException &#123; ClientProtos.Scan proto = ProtobufUtil.toScan(scan); return Base64.encodeBytes(proto.toByteArray()); &#125; 以上是为实现scan指令传输字符的封装. 两者底层都是通过persist实现","categories":[],"tags":[]},{"title":"hbaes操作","slug":"王阁/技术/hexo/hadoop/hbase/hbase操作","date":"2019-05-16T16:00:00.000Z","updated":"2023-10-27T06:50:27.007Z","comments":true,"path":"2019/05/17/王阁/技术/hexo/hadoop/hbase/hbase操作/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/05/17/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/hadoop/hbase/hbase%E6%93%8D%E4%BD%9C/","excerpt":"对hbase常规api进行封装","text":"对hbase常规api进行封装 hbase日常api类封装","categories":[],"tags":[{"name":"常规api封装","slug":"常规api封装","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E5%B8%B8%E8%A7%84api%E5%B0%81%E8%A3%85/"}]},{"title":"hadoop高可用模式搭建","slug":"王阁/技术/hexo/hadoop/hadoopHA搭建","date":"2019-05-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.007Z","comments":true,"path":"2019/05/16/王阁/技术/hexo/hadoop/hadoopHA搭建/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/05/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/hadoop/hadoopHA%E6%90%AD%E5%BB%BA/","excerpt":"发现对hadoop的相关版本的组件,进程还有些模糊,借着针对hadoopHA模式搭建的过程,对hadoop进行一次细统的回顾.","text":"发现对hadoop的相关版本的组件,进程还有些模糊,借着针对hadoopHA模式搭建的过程,对hadoop进行一次细统的回顾. hadoop HA搭建与总结什么是HAHA即高可用 HA相关配置core-site.xml基本一致 hdfs-site.xml这里有明显差别hadoop2.X与hadoop1.X的高可能中的明显差异就是从这里开始的.2.x 引入了nameservice. 该nameservice可支持最大两个namenode.1.x img 和edits统一放置在namenode上.2.x 则通过journalnodes来共享edits日志. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;!-- 为namenode集群定义一个services name --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns1&lt;/value&gt; &lt;/property&gt; &lt;!-- nameservice 包含哪些namenode，为各个namenode起名 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ns1&lt;/name&gt; &lt;value&gt;namenode,datanode1&lt;/value&gt; &lt;/property&gt; &lt;!-- 名为master188的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.namenode&lt;/name&gt; &lt;value&gt;namenode:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 名为master189的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.datanode1&lt;/name&gt; &lt;value&gt;datanode1:9000&lt;/value&gt; &lt;/property&gt; &lt;!--名为master188的namenode的http地址和端口号，用来和web客户端通讯 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.namenode&lt;/name&gt; &lt;value&gt;namenode:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 名为master189的namenode的http地址和端口号，用来和web客户端通讯 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.datanode1&lt;/name&gt; &lt;value&gt;datanode1:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- namenode间用于共享编辑日志的journal节点列表 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://namenode:8485;datanode1:8485;datanode2:8485/ns1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定该集群出现故障时，是否自动切换到另一台namenode --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled.ns1&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- journalnode 上用于存放edits日志的目录 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop_store/dfs/data/dfs/journalnode&lt;/value&gt; &lt;/property&gt; &lt;!-- 客户端连接可用状态的NameNode所用的代理类 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns1&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 一旦需要NameNode切换，使用ssh方式进行操作 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 如果使用ssh进行故障切换，使用ssh通信时用的密钥存储的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- connect-timeout超时时间 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop_store/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop_store/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; mapreduce-site.xml变动不大 yarn-site.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122&lt;?xml version=&quot;1.0&quot;?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;configuration&gt; &lt;!-- 启用HA高可用性 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定resourcemanager的名字 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yrc&lt;/value&gt; &lt;/property&gt; &lt;!-- 使用了2个resourcemanager,分别指定Resourcemanager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定rm1的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;namenode&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定rm2的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;datanode1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定当前机器master188作为rm1 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt; &lt;value&gt;rm1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper集群机器 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;namenode:2181,datanode1:2181,datanode2:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- NodeManager上运行的附属服务，默认是mapreduce_shuffle --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;kuiqwang&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://namenode:19888/tmp/logs/hadoop/logs/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop_store/logs/yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop_store/logs/userlogs&lt;/value&gt; &lt;/property&gt;&lt;!--内存,核数大小配置 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;3072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;3072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt; &lt;value&gt;-Xmx3276m&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; HA过程中主要用到的操作命令当配置文件完成后,先启动journalnode,以助namenode 和standby node 共享edits文件hadoop-daemon.sh 然后再进行namdnode格式化,hadoop namenode -format进行namenode格式化当namenode格式化完成后可以先启动该节点的namenodehadoop-daemon.sh start namenode然后再在另一namdnode节点执行hdfs namenode -bootstrapStandby到这可以将之前的journalnode停用,然后start-dfs.sh 因为要用到zookeeper协助同步配置文件与操作日志,所以这里可以先对zookeeper进行hdfs内容的格式化hdfs zkfc –formatZK然后启动FailOver进程hadoop-daemon.sh start zkfc至此则是这些进程然后启用yarn.即start-yarn.sh到这里HA过程中用到的一些常用指令大致总结完成 至此 hadoop HA的常规总结完成.后续再补充一些细节,如standy 节点切的,与切换机制.HA背后的运作机制,与效果","categories":[],"tags":[]},{"title":"hdfs操作细节","slug":"王阁/技术/hexo/hadoop/hdfs/hdfs操作","date":"2019-05-15T16:00:00.000Z","updated":"2023-10-27T06:50:27.007Z","comments":true,"path":"2019/05/16/王阁/技术/hexo/hadoop/hdfs/hdfs操作/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2019/05/16/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/hadoop/hdfs/hdfs%E6%93%8D%E4%BD%9C/","excerpt":"针对hdfs一些较细节的api封装","text":"针对hdfs一些较细节的api封装 hdfs操作常规操作 创建文件 写数据 删除文件 上传文件 下载文件 断点续写 123错误： java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try 原因： 无法写入；我的环境中有3个datanode，备份数量设置的是3。在写操作时，它会在pipeline中写3个机器。默认replace-datanode-on-failure.policy是DEFAULT,如果系统中的datanode大于等于3，它会找另外一个datanode来拷贝。目前机器只有3台，因此只要一台datanode出问题，就一直无法写入成功。 1234567891011121314&lt;property&gt; &lt;name&gt;dfs.client.block.write.replace-datanode-on-failure.enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.block.write.replace-datanode-on-failure.policy&lt;/name&gt; &lt;value&gt;NEVER&lt;/value&gt; &lt;/property&gt; 对于dfs.client.block.write.replace-datanode-on-failure.enable，客户端在写失败的时候，是否使用更换策略，默认是true没有问题对于，dfs.client.block.write.replace-datanode-on-failure.policy，default在3个或以上备份的时候，是会尝试更换结点尝试写入datanode。而在两个备份的时候，不更换datanode，直接开始写。对于3个datanode的集群，只要一个节点没响应写入就会出问题，所以可以关掉。","categories":[],"tags":[]},{"title":"hive总结","slug":"王阁/技术/hexo/old/hive总结","date":"2018-12-24T15:07:45.000Z","updated":"2023-10-27T06:50:27.007Z","comments":true,"path":"2018/12/24/王阁/技术/hexo/old/hive总结/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2018/12/24/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/old/hive%E6%80%BB%E7%BB%93/","excerpt":"Hive相关点小结","text":"Hive相关点小结 启动指令 hive &#x3D;&#x3D; hive –service cli不需要启动server，使用本地的metastore，可以直接做一些简单的数据操作和测试。 启动hiveserver2hive –service hiveserver2 beeline工具测试使用jdbc方式连接beeline -u jdbc:hive2:&#x2F;&#x2F;localhost:10000 1.managed table管理表。删除表时，数据也删除了 2.external table外部表。删除表时，数据不删 建表:CREATE TABLE IF NOT EXISTS t2(id int,name string,age int)COMMENT ‘xx’ &#x2F;&#x2F;注释ROW FORMAT DELIMITED &#x2F;&#x2F;行分隔符FIELDS TERMINATED BY ‘,’ &#x2F;&#x2F;字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改STORED AS TEXTFILE ; 外部表: CREATE TABLE IF NOT EXISTS t2(id int,name string,age int) COMMENT ‘xx’ ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ STORED AS TEXTFILE ; 分区表，桶表分区表Hive中有分区表的概念。我们可以看到分区表具有重要的性能，而且分区表还可以将数据以一种符合逻辑的方式进行组织，比如分层存储。Hive的分区表，是把数据放在满足条件的分区目录下CREATE TABLE t3(id int,name string,age int) PARTITIONED BY (Year INT, Month INT) &#x2F;&#x2F;按照年月进行分区 ROW FORMAT DELIMITED &#x2F;&#x2F;行分隔符 FIELDS TERMINATED BY ‘,’ ; &#x2F;&#x2F;字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改load data local inpath ‘&#x2F;home&#x2F;zpx&#x2F;customers.txt’ into table t3 partition 分桶表这样做，在查找数据的时候就可以跨越多个桶，直接查找复合条件的数据了。速度快，时间成本低。Hive中的桶表默认使用的机制也是hash。CREATE TABLE t4(id int,name string,age int) CLUSTERED BY (id) INTO 3 BUCKETS &#x2F;&#x2F;创建3个通桶表，按照字段id进行分桶 ROW FORMAT DELIMITED &#x2F;&#x2F;行分隔符 FIELDS TERMINATED BY ‘,’ ; load data local inpath ‘&#x2F;home&#x2F;centos&#x2F;customers.txt’ into table t4 ; 导入数据load data local inpath ‘&#x2F;home&#x2F;zpx&#x2F;customers.txt’ into table t2 ; &#x2F;&#x2F;local上传文件load data inpath ‘&#x2F;user&#x2F;zpx&#x2F;customers.txt’ [overwrite] into table t2 &#x2F;&#x2F;分布式文件系统上移动文件 建视图Hive也可以建立视图，是一张虚表，方便我们进行操作. create view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id &#x3D; b.cid ; Hive的严格模式Hive提供了一个严格模式，可以防止用户执行那些产生意想不到的不好的影响的查询。使用了严格模式之后主要对以下3种不良操作进行控制： 1.分区表必须指定分区进行查询。2.order by时必须使用limit子句。3.不允许笛卡尔积。 Hive的动态分区像分区表里面存储了数据。我们在进行存储数据的时候，都是明确的指定了分区。在这个过程中Hive也提供了一种比较任性化的操作，就是动态分区，不需要我们指定分区目录，Hive能够把数据进行动态的分发,我们需要将当前的严格模式设置成非严格模式，否则不允许使用动态分区set hive.exec.dynamic.partition.mode&#x3D;nonstrict&#x2F;&#x2F;设置非严格模式 Hive的排序Hive也提供了一些排序的语法，包括order by,sort by。 order by&#x3D;MapReduce的全排序sort by&#x3D;MapReduce的部分排序distribute by&#x3D;MapReduce的分区 selece …….from …… order by 字段；&#x2F;&#x2F;按照这个字段全排序 selece …….from …… sort by 字段； &#x2F;&#x2F;按照这个字段局部有序 selece 字段…..from …… distribute by 字段；&#x2F;&#x2F;按照这个字段分区特别注意的是： 在上面的最后一个distribute by使用过程中，按照排序的字段要出现在最左侧也就是select中有这个字段，因为我们要告诉MapReduce你要按照哪一个字段分区，当然获取的数据中要出现这个字段了。类似于我们使用group by的用法，字段也必须出现在最左侧，因为数据要包含这个字段，才能按照这个字段分组，至于Hive什么时候会自行的开启MapReduce，那就是在使用聚合的情况下开启，使用select …from ….以及使用分区表的selece ….from……where …..不会开启 distribute by与sort by可以组合使用，但是distribute by要放在前边，因为MapReduce要先分区，后排序，再归并 select 字段a,……..from …….distribute by字段a，sort by字段如果distribute by与sort by使用的字段一样，则可以使用cluster by 字段替代：select 字段a,……..from …….cluster by 字段 函数 show functions; 展示相关函数 desc function split; desc function extended split; &#x2F;&#x2F;查看函数的扩展信息 用户自定义函数（UDF）具体步骤如下： （1）.自定义类（继承UDF，或是GenericUDF。GenericUDF是更为复杂的抽象概念，但是其支持更好的null值处理同时还可以处理一些标准的UDF无法支持的编程操作）。（2）.导出jar包，通过命令添加到hive的类路径。$hive&gt;add jar xxx.jar（3）.注册函数$hive&gt;CREATE TEMPORARY FUNCTION 函数名 AS ‘具体类路径：包.类’;（4）.使用 $hive&gt;select 函数名(参数);自定义实现类如下(继承UDF)：","categories":[],"tags":[{"name":"bigdata","slug":"bigdata","permalink":"http://www.wqkenqing.ren/daydoc/tags/bigdata/"}]},{"title":"hbase积累.md","slug":"王阁/技术/hexo/old/hbase积累","date":"2018-06-04T02:54:48.000Z","updated":"2023-10-27T06:50:27.007Z","comments":true,"path":"2018/06/04/王阁/技术/hexo/old/hbase积累/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2018/06/04/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/old/hbase%E7%A7%AF%E7%B4%AF/","excerpt":"hbase积累","text":"hbase积累 细节点1.Rowkey设计原则 1.1 长度原则 rowkey 在hbase以二进制码流,可以是任意字符串, 最大长度是64kb,实际应用主要是100~100bytes 长度尽量为8的整数倍,因为现在的系统主要是64位,内存8字节对齐.控制在16字节,符合操作系统特性 1.2 散列原则:因为hbase是分布式存储,rowkey的高位尽量是散列字段,散列性弱的尽量放在低位段.如Time AND Device_id的组合,相对而言device_id 应该量级较小,散列性高.而TIME散列性低,如果TIME放在高位,可能造成数据在某个RegeionServer上堆积的情况.所以较合理的rowkey组合应是device_id+time. 1.3 RowKey唯一原则：必须在设计上保证其唯一性.hbase 中以KeyValue形式存储,key若重复,行内容则会被覆盖. 2.Hbase的Regeion热点问题解决因为在创建表是没有提前预分区,创建的表默认就只会有一个region,这个region的rowkey是没有边界的,即没有startkey与stopkey.数据在写入时,都会写入到这个region.随着数据的不断增加,达到某个阈值时,才会split成2个region.在这个过程中就会产生所有数据囤积在一个regionServer上,出现热点问题.另在split时,会占用集群的I/O资源.通过预分区可以解决该问题 2.1 预分区预分区,”预”字是核心.我们在建表时,预先对表中要存放的数据形式和可能的量级,心中必然会有所估量,即这里应预估数据量.若数据量较大,则在建表时又应该预分区.即根据数据形式,量级,事先预设好一定量的region,后面数据写入时,则会写入到相应的分区.从而避免热点,减少split. 2.1.2 salting(加盐)hbase rowkey设计,避免热点,常会用到该操作,这里的加盐本身不是加密操作,而是在原数据前加入一些随机数据,从而起到分散不同region的作用. 2.1.3 预习区具体方案 hbase预分区的相关操作,如shell形式,可直接在hbase shell操作.如 [https://blog.csdn.net/xiao_jun_0820/article/details/24419793](Hbase shell 预分区操作.) java形式[https://blog.csdn.net/qq_20641565/article/details/56482407](Hbase 预分区 java API形式) 以上操作形式有个问题就是rowkey是随机生成的,虽起到了散列存储,避免了热点堆积,但因为加盐的缘故,想要直接的获取某行数据较为困难.若针对的是高频使用的数据,则会出现问题. 2.1.4 hash分区 在原先预分区的基础上,通过相关规则将原数据hash,从而获得这个原数据对应在哪个分区,使当拿到相关原数据,就能推演出相关rowkey.从而能准确的get数据. hbase优化确定优化目标沟通交流后，业务方更看重降低成本。数据量梳理后略有降低，保证吞吐，无长期请求堆积前提下可以放宽延时要求。为了更快的进行优化，放宽稳定性可以要求接受短期波动。另外，该分组的RegionServer之前存在不稳定的问题，这次优化也一并解决。","categories":[],"tags":[{"name":"日常总结","slug":"日常总结","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/"}]},{"title":"spark学习","slug":"王阁/技术/hexo/old/spark学习","date":"2018-03-04T03:12:58.000Z","updated":"2023-10-27T06:50:27.007Z","comments":true,"path":"2018/03/04/王阁/技术/hexo/old/spark学习/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2018/03/04/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/old/spark%E5%AD%A6%E4%B9%A0/","excerpt":"spark 学习","text":"spark 学习 1spark 作为主流的实时计算引擎,需要高度掌握 spark介绍Apache Spark是一用于实时处理的开源集群计算框架.持多种语言编程,Spark Streaming有高吞吐量和容错能力强等特点.数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算,而结果也能保存在很多地方，如HDFS，数据库等。另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。 优点 易用 容错 spark体系整合 RDD详解RDD是什么RDD：Spark的核心概念是RDD (resilientdistributed dataset)，指的是一个只读的，可分区的分布式数据集，这个数据集的全部或部分可以缓存在内存中，在多次计算间重用。 另:RDD即弹性分布式数据集，有容错机制并可以被并行操作的元素集合，具有只读、分区、容错、高效、无需物化、可以缓存、RDD依赖等特征。RDD只是数据集的抽象，分区内部并不会存储具体的数据。 RDD的五个特性 有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。 有一个函数计算每一个分片，这里指的是下面会提到的compute函数. 对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖. 可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。 可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。(存储的是一个表，可以将处理的分区“本地化”). 1234567891011//只计算一次 protected def getPartitions: Array[Partition] //对一个分片进行计算，得出一个可遍历的结果 def compute(split: Partition, context: TaskContext): Iterator[T] //只计算一次，计算RDD对父RDD的依赖 protected def getDependencies: Seq[Dependency[_]] = deps //可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce @transient val partitioner: Option[Partitioner] = None //可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置 protected def getPreferredLocations(split: Partition): Seq[String] = Nil 为什么会产生RDDRDD数据集 并行集合 接收一个已经存在的集合,然后进行各种并行计算.并行化集合是通过调用SparkContext的parallelize方法，在一个已经存在的Scala集合上创建（一个Seq对象）。集合的对象将会被拷贝，创建出一个可以被并行操作的分布式数据集。 Hadoop数据集 Spark可以将任何Hadoop所支持的存储资源转化成RDD，只要文件系统是HDFS，或者Hadoop支持的任意存储系统即可，如本地文件（需要网络文件系统，所有的节点都必须能访问到）、HDFS、Cassandra、HBase、Amazon S3等，Spark支持文本文件、SequenceFiles和任何Hadoop InputFormat格式。 此两种类型的RDD都可以通过相同的方式进行操作，从而获得子RDD等一系列拓展，形成lineage血统关系图。 Spark RDD算子 Transformation不触发提交作业，完成作业中间处理过程。 DStream什么是DStreamDiscretized Stream :代表持续性的数据流和经过各种Spark原语操作后的结果数据流,在内部实现上是一系列连续的RDD来表示.每个RDD含有一段时间间隔内的数据,如下图 计算则由spark engine来完成 spark java因为我是主要掌握的语言是java,从效率上来考虑,这里 参考博客https://blog.csdn.net/wangxiaotongfan/article/details/51395769 RDD详解https://blog.csdn.net/zuochang_liu/article/details/81459185 spark streaming学习https://blog.csdn.net/hellozhxy/article/details/81672845 spark java 使用指南https://blog.csdn.net/t1dmzks/article/details/70198430 sparkRDD算子介绍https://blog.csdn.net/wxycx11111/article/details/79123482 sparkRDD入门介绍https://github.com/zhaikaishun/spark_tutorial RDD算子介绍","categories":[],"tags":[{"name":"学习spark","slug":"学习spark","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E5%AD%A6%E4%B9%A0spark/"}]},{"title":"spark学习2","slug":"王阁/技术/hexo/old/spark学习2","date":"2018-03-04T03:12:58.000Z","updated":"2023-10-27T06:50:27.007Z","comments":true,"path":"2018/03/04/王阁/技术/hexo/old/spark学习2/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2018/03/04/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/old/spark%E5%AD%A6%E4%B9%A02/","excerpt":"spark学习2spark 运行的四种模式","text":"spark学习2spark 运行的四种模式 本地模式如 1./bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[1] ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100 standlone模式client.&#x2F;bin&#x2F;spark-submit –class org.apache.spark.examples.SparkPi –master spark:&#x2F;&#x2F;spark001:7077 –executor-memory 1G –total-executor-cores 1 .&#x2F;lib&#x2F;spark-examples-1.3.1-hadoop2.4.0.jar 100 cluster.&#x2F;bin&#x2F;spark-submit –class org.apache.spark.examples.SparkPi –master spark:&#x2F;&#x2F;spark001:7077 –deploy-mode cluster –supervise –executor-memory 1G –total-executor-cores 1 .&#x2F;lib&#x2F;spark-examples-1.3.1-hadoop2.7.0.jar 100 Yarn模式client模式123client模式：结果xshell可见：./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --executor-memory 1G --num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.7.0.jar 100 cluster模式.&#x2F;bin&#x2F;spark-submit –class org.apache.spark.examples.SparkPi –master yarn-cluster –executor-memory 1G –num-executors 1 .&#x2F;lib&#x2F;spark-examples-1.3.1-hadoop2.4.0.jar 100 spark sql","categories":[],"tags":[{"name":"学习spark2","slug":"学习spark2","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E5%AD%A6%E4%B9%A0spark2/"}]},{"title":"spark算子","slug":"王阁/技术/hexo/old/spark算子","date":"2018-03-04T03:12:05.000Z","updated":"2023-10-27T06:50:27.008Z","comments":true,"path":"2018/03/04/王阁/技术/hexo/old/spark算子/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2018/03/04/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/old/spark%E7%AE%97%E5%AD%90/","excerpt":"spark 算子","text":"spark 算子 1234sparkRDD封装的函数方法又称算子,通过这些算子可以对RDD进行相关处理,从而获我们想要的结果,因为可能涉及的算子较多.因此单独开篇进行粒度更细,更集中的总结.总得来讲spark的算子,本就是scala集合的一些高阶用法. Transformation(转换)不触发提交作业，完成作业中间处理过程。 parallelize (并行化)将一个存在的集合，变成一个RDD ,返回的是一个JavaRDD[T]** in scala ** 1sc.parallelize(List(&quot;shenzhen&quot;, &quot;is a beautiful city&quot;)) ** in java ** 1JavaRDD&lt;String&gt; javaStringRDD = sc.parallelize(Arrays.asList(&quot;shenzhen&quot;, &quot;is a beautiful city&quot;)); makeRDD只有scala版本的才有makeRDD ,与parallelize类似. textFile调用SparkContext.textFile()方法，从外部存储中读取数据来创建 RDD** in scala ** 1var lines = sc.textFile(inpath) 12// java JavaRDD&lt;String&gt; lines = sc.textFile(inpath); filter对RDD数据进行过滤 map接收一个函数,并将这个函数作用于RDD中的每个元素.RDD 中对应**元素的值 map是一对一的关系 ** flatMap有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap() ,faltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器 distinct去重,我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销很大 union两个RDD进行合并 intersectionRDD1.intersection(RDD2) 返回两个RDD的交集，** 并且去重 **intersection 需要混洗数据，比较浪费性能 subtractRDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重 cartesiancartesian(RDD2) 返回RDD1和RDD2的笛卡儿积，这个开销非常大 mapToPair将元素该成key-value形式 flatMapToPair差异同mapToPair combineByKey该方法主要针对不同分区的同一key进行元素合并函数操作.需要对pairRDD进行 createCombiner 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过,要么就和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建那个键对应的累加器的初始值 mergeValue 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并 mergeCombiners 于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。 reduceByKey接收一个函数，按照相同的key进行reduce操 foldByKey该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V ,与reduce不同的是 foldByKey开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素 sortByKeySortByKey用于对pairRDD按照key进行排序，第一个参数可以设置true或者false，默认是true groupByKeygroupByKey会将RDD[key,value] 按照相同的key进行分组，形成RDD[key,Iterable[value]]的形式， 有点类似于sql中的groupby，例如类似于mysql中的group_concat cogroupgroupByKey是对单个 RDD 的数据进行分组，还可以使用一个叫作 cogroup() 的函数对多个共享同一个键的 RDD 进行分组RDD1.cogroup(RDD2) 会将RDD1和RDD2按照相同的key进行分组，得到(key,RDD[key,Iterable[value1],Iterable[value2]])的形式 subtractByKey类似于subtrac，删掉 RDD 中键与 other RDD 中的键相同的元素 join可以把RDD1,RDD2中的相同的key给连接起来，类似于sql中的join操作RDD1.join(RDD2) fullOuterJoin全连接 leftOuterJoinrightOuterJoinActionfirst返回第一个元素 takerdd.take(n)返回第n个元素 collectrdd.collect() 返回 RDD 中的所有元素 countrdd.count() 返回 RDD 中的元素个数 countByValue各元素在 RDD 中出现的次数 返回{(key1,次数),(key2,次数),…(keyn,次数)} reduce并行整合RDD中所有数据 fold和 reduce() 一 样， 但是提供了初始值num,每个元素计算时，先要合这个初始值进行折叠, 注意，这里会按照每个分区进行fold，然后分区之间还会再次进行fold toprdd.top(n)按照降序的或者指定的排序规则，返回前n个元素 takeOrderedrdd.take(n)对RDD元素进行升序排序,取出前n个元素并返回，也可以自定义比较器（这里不介绍），类似于top的相反的方法 foreach对 RDD 中的每个元素使用给定的函数 countByKey以RDD{(1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)}为例 rdd.countByKey会返回{(1,1),(2,2),(3,3)} collectAsMap将pair类型(键值对类型)的RDD转换成map, 还是上面的例子 saveAsTextFilesaveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。 saveAsSequenceFilesaveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。 saveAsObjectFilesaveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。 saveAsHadoopFilesaveAsNewAPIHadoopFilemapPartitionsmapPartitionsWithIndexHashPartitionerRangePartitioner自定义分区","categories":[],"tags":[{"name":"spark学习","slug":"spark学习","permalink":"http://www.wqkenqing.ren/daydoc/tags/spark%E5%AD%A6%E4%B9%A0/"}]},{"title":"sqoop记录","slug":"王阁/技术/hexo/old/sqoop记录","date":"2018-03-04T02:54:48.000Z","updated":"2023-10-27T06:50:27.008Z","comments":true,"path":"2018/03/04/王阁/技术/hexo/old/sqoop记录/","link":"","permalink":"http://www.wqkenqing.ren/daydoc/2018/03/04/%E7%8E%8B%E9%98%81/%E6%8A%80%E6%9C%AF/hexo/old/sqoop%E8%AE%B0%E5%BD%95/","excerpt":"将Mysql数据导入Hive中","text":"将Mysql数据导入Hive中 命令: 12345678sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:mysql://211.159.172.76:3306/solo--username root --password 125323Wkq --table tablename --hive-import --hive-table tablename 整库导入1234sqoop import-all-tables --connect jdbc:mysql://211.159.172.76:3306/ --username root --password 125323Wkq --hive-database solo -m 10 --create-hive-table --fields-terminated-by &quot;\\t&quot;--hive-import --hive-database qianyang --hive-overwrite sqoop import-all-tables -Dorg.apache.sqoop.splitter.allow_text_splitter&#x3D;true –connect jdbc:mysql:&#x2F;&#x2F;211.159.172.76:3306&#x2F;solo –username root –password 125323Wkq –hive-database blog –create-hive-table –hive-import –hive-overwrite -m 10 单表导入sqoop import –connect jdbc:mysql:&#x2F;&#x2F;211.159.172.76:3306&#x2F;solo –username root –password 125323Wkq –table b3_solo_article –target-dir &#x2F;blog&#x2F;article –hive-import –hive-database blog–fields-terminated-by “\\t” –hive-table article –hive-overwrite–m 10 sqoop import –connect jdbc:mysql:&#x2F;&#x2F;211.159.172.76:3306&#x2F;solo –username root –password 125323Wkq –table b3_solo_article –target-dir &#x2F;blog&#x2F;article –hive-import –hive-database blog –create-hive-table –hive-table article –hive-overwrite -m 1","categories":[],"tags":[{"name":"日常总结","slug":"日常总结","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/"}]}],"categories":[{"name":"回顾","slug":"回顾","permalink":"http://www.wqkenqing.ren/daydoc/categories/%E5%9B%9E%E9%A1%BE/"},{"name":"运营","slug":"运营","permalink":"http://www.wqkenqing.ren/daydoc/categories/%E8%BF%90%E8%90%A5/"}],"tags":[{"name":"小结","slug":"小结","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E5%B0%8F%E7%BB%93/"},{"name":"日常总结","slug":"日常总结","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/"},{"name":"kafka","slug":"kafka","permalink":"http://www.wqkenqing.ren/daydoc/tags/kafka/"},{"name":"bigdata","slug":"bigdata","permalink":"http://www.wqkenqing.ren/daydoc/tags/bigdata/"},{"name":"flink、CDH","slug":"flink、CDH","permalink":"http://www.wqkenqing.ren/daydoc/tags/flink%E3%80%81CDH/"},{"name":"spark","slug":"spark","permalink":"http://www.wqkenqing.ren/daydoc/tags/spark/"},{"name":"rdd","slug":"rdd","permalink":"http://www.wqkenqing.ren/daydoc/tags/rdd/"},{"name":"算子","slug":"算子","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E7%AE%97%E5%AD%90/"},{"name":"beats","slug":"beats","permalink":"http://www.wqkenqing.ren/daydoc/tags/beats/"},{"name":"logstash","slug":"logstash","permalink":"http://www.wqkenqing.ren/daydoc/tags/logstash/"},{"name":"log","slug":"log","permalink":"http://www.wqkenqing.ren/daydoc/tags/log/"},{"name":"dsl","slug":"dsl","permalink":"http://www.wqkenqing.ren/daydoc/tags/dsl/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://www.wqkenqing.ren/daydoc/tags/elasticsearch/"},{"name":"kakfa","slug":"kakfa","permalink":"http://www.wqkenqing.ren/daydoc/tags/kakfa/"},{"name":"极客时间","slug":"极客时间","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"},{"name":"python","slug":"python","permalink":"http://www.wqkenqing.ren/daydoc/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E7%88%AC%E8%99%AB/"},{"name":"mysql","slug":"mysql","permalink":"http://www.wqkenqing.ren/daydoc/tags/mysql/"},{"name":"docker","slug":"docker","permalink":"http://www.wqkenqing.ren/daydoc/tags/docker/"},{"name":"运维","slug":"运维","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E8%BF%90%E7%BB%B4/"},{"name":"json","slug":"json","permalink":"http://www.wqkenqing.ren/daydoc/tags/json/"},{"name":"xml","slug":"xml","permalink":"http://www.wqkenqing.ren/daydoc/tags/xml/"},{"name":"poi","slug":"poi","permalink":"http://www.wqkenqing.ren/daydoc/tags/poi/"},{"name":"connect","slug":"connect","permalink":"http://www.wqkenqing.ren/daydoc/tags/connect/"},{"name":"mac python2 python3","slug":"mac-python2-python3","permalink":"http://www.wqkenqing.ren/daydoc/tags/mac-python2-python3/"},{"name":"flink","slug":"flink","permalink":"http://www.wqkenqing.ren/daydoc/tags/flink/"},{"name":"hdfs","slug":"hdfs","permalink":"http://www.wqkenqing.ren/daydoc/tags/hdfs/"},{"name":"sparkstreaming","slug":"sparkstreaming","permalink":"http://www.wqkenqing.ren/daydoc/tags/sparkstreaming/"},{"name":"spark dependency","slug":"spark-dependency","permalink":"http://www.wqkenqing.ren/daydoc/tags/spark-dependency/"},{"name":"sparkstream","slug":"sparkstream","permalink":"http://www.wqkenqing.ren/daydoc/tags/sparkstream/"},{"name":"常规api封装","slug":"常规api封装","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E5%B8%B8%E8%A7%84api%E5%B0%81%E8%A3%85/"},{"name":"学习spark","slug":"学习spark","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E5%AD%A6%E4%B9%A0spark/"},{"name":"学习spark2","slug":"学习spark2","permalink":"http://www.wqkenqing.ren/daydoc/tags/%E5%AD%A6%E4%B9%A0spark2/"},{"name":"spark学习","slug":"spark学习","permalink":"http://www.wqkenqing.ren/daydoc/tags/spark%E5%AD%A6%E4%B9%A0/"}]}
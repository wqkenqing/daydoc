{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"themes/3-hexo/source/css/gitalk.css","path":"css/gitalk.css","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/mobile.styl","path":"css/mobile.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/avatar.jpg","path":"img/avatar.jpg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/brown-papersq.png","path":"img/brown-papersq.png","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/gov.png","path":"img/gov.png","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/school-book.png","path":"img/school-book.png","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/article-list-background.jpeg","path":"img/article-list-background.jpeg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/alipay.jpg","path":"img/alipay.jpg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/iconfont.js","path":"js/iconfont.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/search.js","path":"js/search.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/script.js","path":"js/script.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/titleTip.js","path":"js/titleTip.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/jquery.pjax.js","path":"js/jquery.pjax.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/img/weixin.jpg","path":"img/weixin.jpg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/icomoon.svg","path":"css/fonts/icomoon.svg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/icomoon.eot","path":"css/fonts/icomoon.eot","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/icomoon.ttf","path":"css/fonts/icomoon.ttf","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/icomoon.woff","path":"css/fonts/icomoon.woff","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/iconfont.svg","path":"css/fonts/iconfont.svg","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/iconfont.eot","path":"css/fonts/iconfont.eot","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/iconfont.ttf","path":"css/fonts/iconfont.ttf","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/iconfont.woff2","path":"css/fonts/iconfont.woff2","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/brown-paper.styl","path":"css/hl_theme/brown-paper.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/iconfont.woff","path":"css/fonts/iconfont.woff","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/atom-dark.styl","path":"css/hl_theme/atom-dark.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/fonts/selection.json","path":"css/fonts/selection.json","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/darcula.styl","path":"css/hl_theme/darcula.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/github-gist.styl","path":"css/hl_theme/github-gist.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/github.styl","path":"css/hl_theme/github.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/gruvbox-dark.styl","path":"css/hl_theme/gruvbox-dark.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/atom-light.styl","path":"css/hl_theme/atom-light.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/gruvbox-light.styl","path":"css/hl_theme/gruvbox-light.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/kimbie-dark.styl","path":"css/hl_theme/kimbie-dark.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/railscasts.styl","path":"css/hl_theme/railscasts.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/rainbow.styl","path":"css/hl_theme/rainbow.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/kimbie-light.styl","path":"css/hl_theme/kimbie-light.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/sublime.styl","path":"css/hl_theme/sublime.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/school-book.styl","path":"css/hl_theme/school-book.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/sunburst.styl","path":"css/hl_theme/sunburst.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/css/hl_theme/zenbum.styl","path":"css/hl_theme/zenbum.styl","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/gitment.js","path":"js/gitment.js","modified":1,"renderable":1},{"_id":"themes/3-hexo/source/js/gitalk.js","path":"js/gitalk.js","modified":1,"renderable":1},{"_id":"themes/tree/source/404.html","path":"404.html","modified":1,"renderable":1},{"_id":"themes/tree/source/favicon.ico","path":"favicon.ico","modified":1,"renderable":1},{"_id":"themes/tree/source/css/main.css","path":"css/main.css","modified":1,"renderable":1},{"_id":"themes/tree/source/js/main.js","path":"js/main.js","modified":1,"renderable":1},{"_id":"themes/tree/source/lib/jquery.pjax.js","path":"lib/jquery.pjax.js","modified":1,"renderable":1},{"_id":"themes/tree/source/Tree.png","path":"Tree.png","modified":1,"renderable":1},{"_id":"themes/tree/source/lib/jquery-3.4.1.min.js","path":"lib/jquery-3.4.1.min.js","modified":1,"renderable":1},{"_id":"themes/tree/source/lib/font-awesome/font-awesome.min.css","path":"lib/font-awesome/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/tree/source/lib/highlight/darcula.css","path":"lib/highlight/darcula.css","modified":1,"renderable":1},{"_id":"themes/tree/source/lib/highlight/highlight.pack.js","path":"lib/highlight/highlight.pack.js","modified":1,"renderable":1},{"_id":"themes/tree/source/lib/valine/Valine-1.3.10-min.js","path":"lib/valine/Valine-1.3.10-min.js","modified":1,"renderable":1},{"_id":"themes/tree/source/lib/busuanzi/2.3/busuanzi.pure.mini.js","path":"lib/busuanzi/2.3/busuanzi.pure.mini.js","modified":1,"renderable":1},{"_id":"themes/tree/source/lib/valine/av-3.0.4-min.js","path":"lib/valine/av-3.0.4-min.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"160fc677cb4c117e9970f50c384bc5e399104202","modified":1609814138000},{"_id":"themes/3-hexo/.gitignore","hash":"86a50fa08e69cab561892aa5edef24f9081bbde1","modified":1609814139000},{"_id":"themes/3-hexo/LICENSE","hash":"b04140c5f682db2b300428f97bb164fd7f5f18bd","modified":1609814139000},{"_id":"themes/3-hexo/_config.yml","hash":"5b44cf532f76c1d308f145f06506f19ffa56d7d3","modified":1619147053683},{"_id":"themes/3-hexo/README.md","hash":"d2c42534ceabcb7cd2e58ed994bf7d6f734d1f3b","modified":1609814139000},{"_id":"source/categories/index.md","hash":"943b799ede68d50f8fa11c6611069add907b5e7b","modified":1619159531189},{"_id":"source/_posts/.Ulysses-Group.plist","hash":"d7000dd72bc33acbcea760c87ff0ab26264e1913","modified":1609814138000},{"_id":"source/_posts/.Ulysses-Settings.plist","hash":"63228c3d354addefda06bb6e02bbbe5801b6fe56","modified":1609814138000},{"_id":"source/tags/index.md","hash":"eccac8fa45f4f0df0158eb43e779e0af47904acd","modified":1619159334674},{"_id":"source/_posts/plan/8-3.md","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1609814136000},{"_id":"source/_posts/plan/test.md","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1609814136000},{"_id":"themes/3-hexo/languages/en.yml","hash":"53ae29ae1237fc7822df85a6d2f8da6f0078625e","modified":1609814138000},{"_id":"themes/3-hexo/languages/zh-CN.yml","hash":"d2c6d86fe2ff03e6ee9bbc16dff8efe5b47ac297","modified":1609814138000},{"_id":"themes/3-hexo/layout/index.ejs","hash":"27ea3dac053d501b79bbef5117b4f3aff063d8cd","modified":1609814138000},{"_id":"themes/3-hexo/layout/post.ejs","hash":"4abd16c0f5e3f51103d23b73710d695dc7fdc5d2","modified":1609814138000},{"_id":"themes/3-hexo/layout/indexs.md","hash":"13945d230f93c8e446d668df7c24cfbac5f98aae","modified":1609814138000},{"_id":"source/_posts/日常/test.drawio","hash":"cace4e8f4f70808098c0a37fe65c6ebe41d9678f","modified":1609814138000},{"_id":"source/_posts/技术/hive/hive20190109.md","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1609814137000},{"_id":"source/_posts/技术/program/mutiThread.md","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1609814137000},{"_id":"source/_posts/日常/vue/vue.md","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1609814138000},{"_id":"source/_posts/日常/考试/电子科大2019招生收集.md","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1609814138000},{"_id":"themes/3-hexo/source/css/gitalk.css","hash":"58177ce227c50ee359fbf99a4fdd26058887afc5","modified":1609814139000},{"_id":"themes/3-hexo/source/css/mobile.styl","hash":"59a4abd36cc8ff2107f1fcb3c0fe48d0492c9030","modified":1609814139000},{"_id":"themes/3-hexo/source/css/style.styl","hash":"7090b81f34e26172670c68da6a366ca186523779","modified":1609814139000},{"_id":"themes/3-hexo/source/img/avatar.jpg","hash":"e32c82ae27515c8c162ef420cdf61c5ad453ee7a","modified":1609814139000},{"_id":"themes/3-hexo/source/img/brown-papersq.png","hash":"3a1332ede3a75a3d24f60b6ed69035b72da5e182","modified":1609814139000},{"_id":"themes/3-hexo/source/img/gov.png","hash":"f31c9f47faedf7f33b9580d6284ab891fb697560","modified":1609814139000},{"_id":"themes/3-hexo/source/img/school-book.png","hash":"711ec983c874e093bb89eb77afcbdf6741fa61ee","modified":1609814139000},{"_id":"themes/3-hexo/source/img/article-list-background.jpeg","hash":"4fdf8b3e53dd02d6ee6360aebfadb0cba1fb5633","modified":1609814139000},{"_id":"themes/3-hexo/source/img/alipay.jpg","hash":"e457d1d3dfefbbd824d154cf756a2c6d10b812a2","modified":1609814139000},{"_id":"themes/3-hexo/source/js/iconfont.js","hash":"3a0869ca1b09af07d82987e343a3bc4cb9558ecb","modified":1609814139000},{"_id":"themes/3-hexo/source/js/search.js","hash":"c80c9a231ee040c7adc07a477793873fb85ce8bc","modified":1609814139000},{"_id":"themes/3-hexo/source/js/script.js","hash":"53b8bfe0ffa5ff153e5d759c6f2de00daadb8a07","modified":1609814139000},{"_id":"themes/3-hexo/source/js/titleTip.js","hash":"81dca549063e29ba3a4a278f0f4388eba8a2167b","modified":1609814139000},{"_id":"themes/3-hexo/source/js/jquery.pjax.js","hash":"191c49fdb40dff115a49cfd2b30dffb888d86550","modified":1609814139000},{"_id":"themes/3-hexo/layout/_partial/comment.ejs","hash":"5507b4dfab2032345e012a0c5356f63b01395157","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/article_copyright.ejs","hash":"0ebb17d001cb7bb7606c616c380049a2e7124496","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/article.ejs","hash":"4b483bb1b4175bad05c8ef9e2567c2b2c4cb2e98","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/copyright.ejs","hash":"f66939a8c9d5258948b47842b8b4495e6ec45988","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/dashang.ejs","hash":"bc94eee27701b67d238f328737b578e8270989eb","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/friends.ejs","hash":"d11092791e5c140ff81f2aefa0d1b051f403239d","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/footer.ejs","hash":"b5f53df47c469e10ebcef75c6b794b12f098c969","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/header.ejs","hash":"2eec2a1a0828998f89976d668a3a8c7070316e53","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/full-toc.ejs","hash":"a734c26d86da6697003ed27672c1b9b82b216c82","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/mathjax.ejs","hash":"c2e5cef2377884cd79e5f686fe4f74b082744306","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/nav-left.ejs","hash":"ca0b05101ded955684d305f494304d6a2ec285f6","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/nav-right.ejs","hash":"3f9d91215ff36a6bcdaaf86e5b028dafc5a6a6fc","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/toc-ref.ejs","hash":"6406251dabda66ef686d4c15edbc3061b6d828b8","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/meta.ejs","hash":"8a9e93f9cbe80763264018290da0b14b4bbe8ac5","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/tag.ejs","hash":"ef321fa796add484ff8f9f9ad61675994645d802","modified":1609814138000},{"_id":"source/_posts/回顾/hive/hive回顾.md","hash":"ed81dd52325f38418cd9b3982fccb244d24e1af3","modified":1619158380673},{"_id":"source/_posts/技术/Mongodb/常用命令.md","hash":"9b525b2730f34d76ae25b60be9689cf0b709af6c","modified":1609814136000},{"_id":"source/_posts/技术/bash/order.sh","hash":"d397b03a42c88fbbc942eeac6405acb43986bbbf","modified":1609814136000},{"_id":"source/_posts/技术/bash/python_learn.md","hash":"192223791b938a3b4d66d09f871b65e0a61701b9","modified":1609814136000},{"_id":"source/_posts/技术/bash/shell积累.md","hash":"5f08a93942adfc0918b907b56a2ea0ed95d6ffeb","modified":1609814136000},{"_id":"source/_posts/技术/cdh/hue.md","hash":"3c265dd7172db122da06ae44ae5dcd9c2a684f15","modified":1609814136000},{"_id":"source/_posts/技术/docker/Dockerfile基础&编写.md","hash":"0e659c4319ceb79642d613b0dfc58e6eec262758","modified":1609814136000},{"_id":"source/_posts/技术/docker/docker操作.md","hash":"2c01164e0a786209f30c39707c47747dcddbdbe0","modified":1609814136000},{"_id":"source/_posts/技术/docker/docker镜像.md","hash":"9bb19dbba9d88dac54ef3dc1909d64814fff85bc","modified":1609814136000},{"_id":"source/_posts/技术/flink/Flink记录.md","hash":"b3fda122e4cdeb9b23c32e73c7e148352dbfba20","modified":1609814136000},{"_id":"source/_posts/技术/flink/flink.md","hash":"214c5ddf653e6e22ebb3e55bf1afbcbb981fdbab","modified":1609814136000},{"_id":"source/_posts/技术/docker/docker.md","hash":"c1c0f5779f77a59d5154510fcd830667f76a372a","modified":1609814136000},{"_id":"source/_posts/技术/flink/flink_book.md","hash":"40a193b2ce5a8b79eb33da75d67c875ae2d731b9","modified":1609814136000},{"_id":"source/_posts/技术/flume/Dockerfile","hash":"c10775d31a0e83a904ee4fe22e9ec3ff5e804723","modified":1609814136000},{"_id":"source/_posts/技术/flume/Flume.md","hash":"f1d40860f4c84b82f6fb933eee6af7fb1fa2b0e7","modified":1609814136000},{"_id":"source/_posts/技术/flume/new","hash":"6096bd830b2670319ae8df19da0fcd60a5aac84e","modified":1609814136000},{"_id":"source/_posts/技术/flume/docker版flume使用.md","hash":"2a1881b032e7fbad928162bbafc6ee1c86792534","modified":1609814136000},{"_id":"source/_posts/技术/front_end/nginx部署.md","hash":"1e2d52b1ab0135205b4db49431cc84cd5458141c","modified":1609814136000},{"_id":"source/_posts/技术/front_end/调研.md","hash":"daef2a130c6affc8f890488fd386bacacd3daf8e","modified":1609814136000},{"_id":"source/_posts/技术/es/elk使用.md","hash":"62fdda4c038029cf78e8d75452b1d7610fe9da26","modified":1609814136000},{"_id":"source/_posts/技术/es/es小结.md","hash":"98ff350b1532c441e09c0ab3468211d5317aa10f","modified":1609814136000},{"_id":"source/_posts/技术/es/esDSL详解.md","hash":"6f4f8400e6075836a6d1d13b9c190b9e20f67012","modified":1609814136000},{"_id":"source/_posts/技术/es/es的docker安装.md","hash":"01871a18ebc3c5da64d344c34bf7d98357d84e68","modified":1609814136000},{"_id":"source/_posts/技术/es/es常用命令.md","hash":"c89741069da2e6ee3c137144ff758cef6681f313","modified":1609814136000},{"_id":"source/_posts/技术/es/共享数据集合.md","hash":"ff832f2c4ba4d8151e39fe93081d4bf4c45a28b4","modified":1609814136000},{"_id":"source/_posts/技术/es/es详细.md","hash":"82520ba00753ac9e11ced6828d353cd4b6c6faf4","modified":1609814136000},{"_id":"source/_posts/技术/es/积累.md","hash":"76d2f4d06145aaaad1b5492cc8f5d96413227eb5","modified":1609814136000},{"_id":"source/_posts/技术/hbase/hbase20190109.md","hash":"034801100edb1e74928e9296c1a86e34af70fbcc","modified":1609814137000},{"_id":"source/_posts/技术/gps/JT809.md","hash":"99c9bae4972f831ed3e622a6555055283983cc6b","modified":1609814136000},{"_id":"source/_posts/技术/hive/hivsql","hash":"cad143521dd36bbbbb19527546c2d3b27ac16a70","modified":1609814137000},{"_id":"source/_posts/技术/kafka/kafka原理.md","hash":"376f363ebea8068f56a8fc7c05d994f3aacfb251","modified":1609814137000},{"_id":"source/_posts/技术/kafka/kafka小结.md","hash":"e89ac4d03f692389fc98cfb878a571d71b6488da","modified":1609814137000},{"_id":"source/_posts/技术/platform/platform.md","hash":"f4ee2adc9b12f138615c8786644669ac1afda98f","modified":1609814137000},{"_id":"source/_posts/技术/kafka/kafka极客.md","hash":"0e47881922acf8a929f78723409fc9c6db80ba55","modified":1609814137000},{"_id":"source/_posts/技术/kafka/kafka疑问点.md","hash":"9289ad2ca59fbae023817f90f4b56adbdbaccfcd","modified":1609814137000},{"_id":"source/_posts/技术/owncloud/账号.md","hash":"9656647fc6627e915cb8dcf7a98c8811a680f32b","modified":1609814137000},{"_id":"source/_posts/技术/ubuntu/desk_gui.md","hash":"b7fb00d45fdbf9f7bedcba667fe935dce4b0926d","modified":1609814137000},{"_id":"source/_posts/技术/springboot/how to set parm.md","hash":"3650e44c9f630aaa0c29dad8f1c91596c50500c1","modified":1609814137000},{"_id":"source/_posts/技术/video/北风spark.md","hash":"078dabaca7dad1510dfc69c7745a1aadcef2535d","modified":1609814137000},{"_id":"source/_posts/技术/ubuntu/linux自启服务.md","hash":"d20bb5d223f0a68b64f352296f0f501b6da4ce05","modified":1609814137000},{"_id":"source/_posts/技术/yarn/YarnState.md","hash":"2835278d616af19e05c096e62f39813bb24a50f4","modified":1609814137000},{"_id":"source/_posts/技术/集群/.bashrc","hash":"a068feed27316b5be47dfd02656b198771a1f317","modified":1609814138000},{"_id":"source/_posts/技术/yd_server_config/README.md","hash":"b7cf4cbad171ef97615d9c7ce473839ce2dcf0f6","modified":1609814138000},{"_id":"source/_posts/日常/blog_sync_project/plan.md","hash":"cade8ecf043f66ab9281c49e6b27ef738c14908f","modified":1609814138000},{"_id":"source/_posts/日常/prepare/tourist","hash":"2bc298523f2529aadd88efa1dbc5605625bf8761","modified":1609814138000},{"_id":"source/_posts/日常/prepare/collect_es.sh","hash":"5602f68038b37fbcfc5ced7b82cca0d6f0c7f642","modified":1609814138000},{"_id":"source/_posts/日常/运维/about云账号密码.md","hash":"b36382fdee3ca9c84db6f65f568caf3aaeb6e066","modified":1609814138000},{"_id":"source/_posts/技术/集群/私有集群.md","hash":"da1abcdbda9ff007bdb51bbc7903d30a0843738c","modified":1609814138000},{"_id":"source/_posts/日常/prepare/数据储备.md","hash":"c09faacb0831a59b6370b11fd18cecdd18efe073","modified":1609814138000},{"_id":"source/_posts/日常/考试/考试相关.md","hash":"e825144a0f7266e691498fb77bc7742c0428a855","modified":1609814138000},{"_id":"source/_posts/日常/运维/docker启动mysql.md","hash":"a0a47a53e06d95fd44bcc857564f5b29594e3ebe","modified":1609814138000},{"_id":"source/_posts/日常/运维/jumpserver.md","hash":"20922b6335ea6d2a22260fb99fd66b14fed48020","modified":1609814138000},{"_id":"source/_posts/日常/运维/tmux上手.md","hash":"0b799cdc0a658a770d711a8ca46bc1b34def5bbd","modified":1609814138000},{"_id":"source/_posts/日常/运维/京东云.md","hash":"8c1315ccc759495a1afeaa1148a1abfa6c7999f0","modified":1609814138000},{"_id":"source/_posts/日常/运维/博客不同主题切换.md","hash":"e9778376d2d42cdf125b99f702a536543ce2d234","modified":1609814138000},{"_id":"source/_posts/日常/运维/mac python版本替换.md","hash":"d4da190119ff23ff6b63e416713a3866d58549a4","modified":1609814138000},{"_id":"source/_posts/日常/运维/操作记录.md","hash":"c8e8a8066075890e2ae1e79714247f94a48e20b1","modified":1609814138000},{"_id":"source/_posts/日常/运维/运维操作.md","hash":"4f08b51d27796d9ac78d93f8ad397c532fd5b7ab","modified":1609814138000},{"_id":"themes/3-hexo/source/img/weixin.jpg","hash":"8dafb22561698d0758fba9ea2a45abf6ad3512a2","modified":1609814139000},{"_id":"source/_posts/技术/hexo/Thread/多线程.md","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1609814137000},{"_id":"source/_posts/技术/python/learn/pytest.py","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1609814137000},{"_id":"themes/3-hexo/source/css/_partial/comment.styl","hash":"2a9b5ffb759be85545a89f6d1194579a800f51a5","modified":1609814138000},{"_id":"themes/3-hexo/source/css/_partial/dashang.styl","hash":"f6447a2ac407228e1d53e3455db2919ac0e9f094","modified":1609814138000},{"_id":"themes/3-hexo/source/css/_partial/fade.styl","hash":"4f687cbc74caf8a0887f5e89250284a9bce8b5c1","modified":1609814138000},{"_id":"themes/3-hexo/source/css/_partial/font.styl","hash":"ea78c9f23b3f44b6c665721ebe74eac9675b6499","modified":1609814138000},{"_id":"themes/3-hexo/source/css/_partial/full-toc.styl","hash":"0143711c1221cb4e70a3db866754d79c8a81d253","modified":1609814138000},{"_id":"themes/3-hexo/source/css/_partial/nav-right.styl","hash":"e3c30d618d9e1b7a6a3274cff2ca42f054e99ddd","modified":1609814138000},{"_id":"themes/3-hexo/source/css/_partial/nav-left.styl","hash":"4f1621f52e24ea35fa5bb32ce1cee2c7de9d0dc2","modified":1609814138000},{"_id":"themes/3-hexo/source/css/_partial/post.styl","hash":"62c9100d12753d4ee828ccc8fcc62d03ad17c504","modified":1609814138000},{"_id":"themes/3-hexo/source/css/_partial/nprogress.styl","hash":"65efbddd23a264e7d1e85f4073228526770e833c","modified":1609814138000},{"_id":"themes/3-hexo/source/css/fonts/icomoon.svg","hash":"37ac1ef28b03f46bf3ad2606c86f0e1ec3e4405f","modified":1609814138000},{"_id":"themes/3-hexo/source/css/_partial/num-load.styl","hash":"4b996440bba8ec755aa70bc6d074d7dbba55ec0c","modified":1609814138000},{"_id":"themes/3-hexo/source/css/fonts/icomoon.eot","hash":"b6195bedc1cb2f9cfcb26cc27021f2e94be2ab0a","modified":1609814138000},{"_id":"themes/3-hexo/source/css/fonts/icomoon.ttf","hash":"eb976d8b8559fcddfc2658a03a4350cb566fc06b","modified":1609814138000},{"_id":"themes/3-hexo/source/css/fonts/icomoon.woff","hash":"3985d29416bb9b19f50a2f20f2bbbce47f10af8d","modified":1609814138000},{"_id":"themes/3-hexo/source/css/fonts/iconfont.svg","hash":"13974fe35fca836e870a960ecb11b7eca2e036f8","modified":1609814138000},{"_id":"themes/3-hexo/source/css/fonts/iconfont.eot","hash":"b14b8624988ff069aff3145f88c0d7ac49052bd3","modified":1609814138000},{"_id":"themes/3-hexo/source/css/fonts/iconfont.ttf","hash":"140829ecf12d30c6e18d8dc6dc0c188a66addd25","modified":1609814138000},{"_id":"themes/3-hexo/source/css/fonts/iconfont.woff2","hash":"b0317a0b2ebb1181a8bf5a97d03556dd54538645","modified":1609814138000},{"_id":"themes/3-hexo/source/css/hl_theme/brown-paper.styl","hash":"03af387edcc1cf8c18d12e9c440fd51b6cf425b6","modified":1609814138000},{"_id":"themes/3-hexo/source/css/fonts/iconfont.woff","hash":"0d2d4559f1ac4fa801eb8cc099fa5bf9dcf955ef","modified":1609814138000},{"_id":"themes/3-hexo/source/css/hl_theme/atom-dark.styl","hash":"f3eb4e5feda9cbd6242ccf44ca064e2979b5d719","modified":1609814138000},{"_id":"themes/3-hexo/source/css/fonts/selection.json","hash":"57c7f100019d57b512aab509185cb0a6eb9aa4c8","modified":1609814138000},{"_id":"themes/3-hexo/source/css/hl_theme/darcula.styl","hash":"2bfc14f27ccca108b4b3755782de8366e8bd001e","modified":1609814138000},{"_id":"themes/3-hexo/source/css/hl_theme/github-gist.styl","hash":"5e05b19832c1099bd9d284bc3ed00dc8a3d7ee23","modified":1609814138000},{"_id":"themes/3-hexo/source/css/hl_theme/github.styl","hash":"53276ff1f224f691dfe811e82c0af7f4476abf5d","modified":1609814138000},{"_id":"themes/3-hexo/source/css/hl_theme/gruvbox-dark.styl","hash":"315ad610d303caba9eac80a7d51002193a15478a","modified":1609814138000},{"_id":"themes/3-hexo/source/css/hl_theme/atom-light.styl","hash":"553987211d3323a7dfc0b08786b183a3435978c9","modified":1609814138000},{"_id":"themes/3-hexo/source/css/hl_theme/gruvbox-light.styl","hash":"1bece084b1dbbbd4af064f05feffd8c332b96a48","modified":1609814139000},{"_id":"themes/3-hexo/source/css/hl_theme/kimbie-dark.styl","hash":"e9c190f9ffc37a13cac430512e4e0c760205be4a","modified":1609814139000},{"_id":"themes/3-hexo/source/css/hl_theme/railscasts.styl","hash":"a6e8cfd2202afd7893f5268f3437421e35066e7b","modified":1609814139000},{"_id":"themes/3-hexo/source/css/hl_theme/rainbow.styl","hash":"e5c37646a9d9c1094f9aab7a7c65a4b242e8db00","modified":1609814139000},{"_id":"themes/3-hexo/source/css/hl_theme/kimbie-light.styl","hash":"0c3ccd0d64e7504c7061d246dc32737f502f64e4","modified":1609814139000},{"_id":"themes/3-hexo/source/css/hl_theme/sublime.styl","hash":"501d75ef0f4385bea24d9b9b4cc434ba68d4be27","modified":1609814139000},{"_id":"themes/3-hexo/source/css/hl_theme/school-book.styl","hash":"51659351b391a2be5c68728bb51b7ad467c5e0db","modified":1609814139000},{"_id":"themes/3-hexo/source/css/hl_theme/sunburst.styl","hash":"2aa9817e68fb2ed216781ea04b733039ebe18214","modified":1609814139000},{"_id":"themes/3-hexo/source/css/hl_theme/zenbum.styl","hash":"933a3b196d01254dea5e6f48105ea15e210ae000","modified":1609814139000},{"_id":"themes/3-hexo/layout/_partial/comments/click2show.ejs","hash":"fa6675230f8c313236604e26926b142f4f418bdd","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/comments/disqus.ejs","hash":"cd0022ce7e6d6efb07a00e87477cdf791f7f6703","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/comments/utteranc.ejs","hash":"c76773b96860940083baf16470b7b80ac098e645","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/comments/gitalk.ejs","hash":"fbd3c7d72c8354d700918390c6cbfc0a11408277","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/comments/gitment.ejs","hash":"f16442568b43d034faaa8e3507f5ae8da34c7b72","modified":1609814138000},{"_id":"themes/3-hexo/layout/_partial/comments/livere.ejs","hash":"e820aa16b5ed4e024616b5e2d424925820d43e56","modified":1609814138000},{"_id":"source/_posts/技术/ELK/logstash/logstash.md","hash":"8a8fa9c5af50ffe7c984b0152422116a322722c4","modified":1609814136000},{"_id":"source/_posts/技术/ELK/logstash/beats.md","hash":"13ccfa50b86b07a6f3b54bb38401dc450affcbd0","modified":1609814136000},{"_id":"source/_posts/技术/bash/awk/awk.md","hash":"a6de788ec5734f01c73c0d829b48fa330c6581a8","modified":1609814136000},{"_id":"source/_posts/技术/bash/yd/manager.sh","hash":"0e80b49b931a190d82d18a01f8e3653358730950","modified":1609814136000},{"_id":"source/_posts/技术/flume/config/kafka_spool.conf","hash":"41b85fd6c333bc8bb79feb3e30c501140b5b4229","modified":1609814136000},{"_id":"source/_posts/技术/front_end/接口规划/data-manager.md","hash":"4bc19fe31e5c18ad9c1660bc49db12709d683d9b","modified":1609814136000},{"_id":"source/_posts/技术/flume/config/kafka_test.conf","hash":"dbf73bdd638421794768058f519b72ac8e71c06c","modified":1609814136000},{"_id":"source/_posts/技术/front_end/conf/h-ui.conf","hash":"a418c3060155d16d8a8f25642be5b276557b744b","modified":1609814136000},{"_id":"source/_posts/技术/hexo/CDH/搭建.md","hash":"c83761940ea352cf876c6debcce4f2bde948900e","modified":1609814137000},{"_id":"source/_posts/技术/hexo/flink/Flink.md","hash":"75baf11271d2d1c652a054310f87c94f74297283","modified":1609814137000},{"_id":"source/_posts/技术/hexo/hadoop/hadoopHA搭建.md","hash":"2119376e0cd9bcb10f88bed1fbe4e1b745380e3e","modified":1609814137000},{"_id":"source/_posts/技术/hexo/kafka/kafka实现.md","hash":"1135fefdcba5f6d3ceacdd0a36bb291c44eefad4","modified":1609814137000},{"_id":"source/_posts/技术/hexo/old/Lamda积累.md","hash":"861f7b5d5ad4d33ada02a734d481709b0c0f3b23","modified":1609814137000},{"_id":"source/_posts/技术/hexo/old/Yarn配置.md","hash":"0ae26ff179f5f485764b59374fbc17c931137a1d","modified":1609814137000},{"_id":"source/_posts/技术/hexo/old/hive总结.md","hash":"c06086950058bde49d1ff3e32f5a3c725ccc3db1","modified":1609814137000},{"_id":"source/_posts/技术/hexo/old/kafka.md","hash":"89bc210a375d350c132fa7eedb2ddb7d37aa783c","modified":1609814137000},{"_id":"source/_posts/技术/hexo/old/mapreduce组件总结.md","hash":"5a0aeac124b43c3b1381651ac82953bfd0db979b","modified":1609814137000},{"_id":"source/_posts/技术/hexo/old/hbase积累.md","hash":"da4e75edc5f28bb0d201ad5d58682a9418f443b9","modified":1609814137000},{"_id":"source/_posts/技术/hexo/old/flume记录.md","hash":"eae71d4c4a6e2f3a22bece4b15504608c0797077","modified":1609814137000},{"_id":"source/_posts/技术/hexo/old/spark学习2.md","hash":"f1e65f13a193befb0b59e669ead317321dae2699","modified":1609814137000},{"_id":"source/_posts/技术/hexo/old/spark学习.md","hash":"667e9410164eddbbba770e65267b9a9e3d767419","modified":1609814137000},{"_id":"source/_posts/技术/hexo/old/spark学习3.md","hash":"b447834f1657e90941d501b35c5e6c9c0241a378","modified":1609814137000},{"_id":"source/_posts/技术/hexo/old/spark算子.md","hash":"03e50b658aaa48b45b6dff2d284266db4f9e07f0","modified":1609814137000},{"_id":"source/_posts/技术/hexo/old/sqoop记录.md","hash":"1f329c3712a793ac4d41b77e581c81fe4b3df7da","modified":1609814137000},{"_id":"source/_posts/技术/hexo/old/准备小结.md","hash":"71a0e8749cef20a20943b15ddd108260f466578f","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog1.md","hash":"47d55382f66957411b13c4ee9bb2ca83e68d7314","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog10.md","hash":"d834d6bddaaa4c4e0380f874554fd8f45a59016d","modified":1609814137000},{"_id":"source/_posts/技术/hexo/old/大数据相关分享.md","hash":"ddf61eb694f7ad9ff95b9f037e0c885bf0ca5178","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog11.md","hash":"b8e69ff57a9a399836ba4ea96eed38eeabba9f00","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog13.md","hash":"da65357065f36be9436bcad5edeecac6774f2196","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog12.md","hash":"bab5c6773666d45fa02a9059c88bbd7d2aca7a95","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog14.md","hash":"f4ba5b32fdbdd93c5e64dea182e1250bb816175a","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog16.md","hash":"a24db979d17656856d0ef2cc84d70ec6d8d49183","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog15.md","hash":"b3e86cece9a274ddb694d12119d56e118f106fc9","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog17.md","hash":"d75e855f70fe3a0ac30ca1603ad643ee01a1ecfd","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog20.md","hash":"3ab08d260c55ce28e39d97de918fc989b17c9fc1","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog21.md","hash":"3ee31197784eafa09c201e449ba70b14b90d001c","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog18.md","hash":"e45acfe2addcd7d23fed2d27ff40949db6151264","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog19.md","hash":"8e1f27b3326733501dc5f5ee0bfdb3945a0c85bc","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog2.md","hash":"c61de9d382b5e766a00e9a937c5d93a037c2dc27","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog22.md","hash":"c9167686de0fa68b931c4fbb84463b75dddebc03","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog24.md","hash":"81c0f12e7e600241ff755973514fdee27b4c1afd","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog3.md","hash":"5e8410e1a88bd2ed5c307a08caad64284905f76f","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog26.md","hash":"bd0eaf6cf085e44f53c6bfdedd7b367cc20b6eed","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog4.md","hash":"9c65d63e63b7c6eddfcaec483999fe931b5ef7af","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog25.md","hash":"d85ca95f07750fe4e0b643a004bdb8ff0cc4ab32","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog6.md","hash":"58168b18d603b2ee40e215eb5d03d2efcd29e760","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog7.md","hash":"aab107c8f131ec3ec6bc63adbe2ff26ead76128c","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog8.md","hash":"f3c4742e662ce04a1d23954015b1ed729f28d7da","modified":1609814137000},{"_id":"source/_posts/技术/hexo/oldblog/blog9.md","hash":"490d2e65320a2a5f468733079e5e95687b7b6f0d","modified":1609814137000},{"_id":"source/_posts/技术/hexo/spark/spark操作.md","hash":"d165953ebef7f594c023ce9a4162eb2663fdb815","modified":1609814137000},{"_id":"source/_posts/技术/hexo/spark/stream2.md","hash":"ff5dcfc7d90f224b6f53c0d1bcc3c2790479bde4","modified":1609814137000},{"_id":"source/_posts/技术/hexo/spark/stream.md","hash":"d13cf889063e11907ad3c79e383bf030833d4cbc","modified":1609814137000},{"_id":"source/_posts/技术/hexo/spark/宽窄依赖.md","hash":"987ad1b935e8ec4da7eb636974f6625844293edb","modified":1609814137000},{"_id":"source/_posts/技术/python/reptile/python爬虫篇.md","hash":"5cdefa03f0730a793e99dbc28ff6945cd8020bb6","modified":1609814137000},{"_id":"source/_posts/技术/python/summary/python5小结.md","hash":"4e98dd570e55637dbc8730a69bdd5fc5c69c52a1","modified":1609814137000},{"_id":"source/_posts/技术/python/summary/2020.3.17.md","hash":"daaf2c6aab92da999da919edac04efded9437734","modified":1609814137000},{"_id":"source/_posts/技术/python/summary/python小结3.md","hash":"a32b45ab2bebd17d49502e08e1b9eec48f072d5a","modified":1609814137000},{"_id":"source/_posts/技术/python/summary/python小结4.md","hash":"56969d6f6692bf86f03126f2dfce6ad7ac6a2f0a","modified":1609814137000},{"_id":"source/_posts/技术/yd_server_config/sjksh_store_info/common.env","hash":"330a6bb209e6edbc9a85d865b488a9abba9ab0ea","modified":1609814138000},{"_id":"source/_posts/技术/yd_server_config/sjksh_store_info/docker-compose.yaml","hash":"3dfccab6992f9d370c559b6726803b52553a161f","modified":1609814137000},{"_id":"source/_posts/技术/yg_config/hy-sjksh-info-dev/common.env","hash":"eccf42bf56ca8b0f7d56bd7f2c74eaf472b8d61e","modified":1609814138000},{"_id":"source/_posts/技术/yg_config/hy-sjksh-info-dev/docker-compose.yaml","hash":"e187928e3cddeea01a6d36d99ec66bb75c4cdae9","modified":1609814138000},{"_id":"source/_posts/技术/集群/hbase/hbase-site.xml","hash":"ff4f104748e5db04e020cac7152a5c3efb972ffa","modified":1609814138000},{"_id":"source/_posts/技术/编程/scala/scala学习.md","hash":"b219d29cc6e01984fe96516119ef3cc579aa8072","modified":1609814138000},{"_id":"source/_posts/技术/集群/hbase/regionservers","hash":"766750ef731ae4b65be6cce62f5caea41caaf1fe","modified":1609814138000},{"_id":"source/_posts/技术/集群/hdfs/core-site.xml","hash":"a33634b73e27bd33fb05bfcaca4a17e524676378","modified":1609814138000},{"_id":"source/_posts/技术/集群/hdfs/hdfs-site.xml","hash":"6d343a668cea1959f257d12fec11fd4eb4a0b101","modified":1609814138000},{"_id":"source/_posts/技术/集群/hdfs/mapred-site.xml","hash":"4b4a46bef2c62dd83efdcbd69049b1ad94c8c483","modified":1609814138000},{"_id":"source/_posts/技术/集群/hdfs/slaves","hash":"766750ef731ae4b65be6cce62f5caea41caaf1fe","modified":1609814138000},{"_id":"source/_posts/技术/集群/hdfs/yarn-site.xml","hash":"d293153cfeeefb26de22efa34605a07429b8b574","modified":1609814138000},{"_id":"source/_posts/技术/集群/zookeeper/zoo.cfg","hash":"d2c8575592e4094c88cf98f76123659f87e9d9b1","modified":1609814138000},{"_id":"source/_posts/日常/blog_sync_project/bash/git_pull.sh","hash":"a6d1d62cc170957269c8fc8803e8248a80c748bb","modified":1609814138000},{"_id":"source/_posts/技术/hexo/oldblog/blog23.md","hash":"85439360268fb8a3e082be525cf4576c9d51545b","modified":1609814137000},{"_id":"source/_posts/技术/hexo/spark/算子/test.py","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1609814137000},{"_id":"themes/3-hexo/source/js/gitment.js","hash":"59a1e03f2b0ce61dd9bd405d3c52d3e07cc10dec","modified":1609814139000},{"_id":"source/_posts/技术/bash/awk/script/add.awk","hash":"57bc16c323bbfeb4dc2a9875786669d7fa9ee579","modified":1609814136000},{"_id":"source/_posts/技术/bash/awk/script/cal.awk","hash":"e8bfd6aed26595de5fd42c4fe15460be4755d606","modified":1609814136000},{"_id":"source/_posts/技术/bash/awk/script/example.txt","hash":"fd58a4c2cf6fb45e166eee156063c346406a6296","modified":1609814136000},{"_id":"source/_posts/技术/bash/awk/script/test.txt","hash":"5c93fb28e07fb71e2b1f898471b60e0182e33472","modified":1609814136000},{"_id":"source/_posts/技术/bash/awk/script/number.awk","hash":"5e378c28e2e2f28288e940039e417c6e3a48e74b","modified":1609814136000},{"_id":"source/_posts/技术/bash/awk/script/test.awk","hash":"1417a59d4ba4a1567dddafacd957f17779561310","modified":1609814136000},{"_id":"source/_posts/技术/hexo/hadoop/hbase/hbase操作.md","hash":"7667bc6213ab0fb8f3de98ca2724eda041ea0d44","modified":1609814137000},{"_id":"source/_posts/技术/hexo/hadoop/hdfs/hdfs命令.md","hash":"53029b898515be2d9382ff215365926219be211f","modified":1609814137000},{"_id":"source/_posts/技术/hexo/hadoop/hdfs/hdfs操作.md","hash":"d74376e48e408e11768c9d33bd9caf2288711ccb","modified":1609814137000},{"_id":"source/_posts/技术/hexo/spark/sql/SparkSql.md","hash":"2d7d0a3edb2a2c8350981875c1b137231f40d826","modified":1609814137000},{"_id":"source/_posts/技术/hexo/spark/算子/RDD算子.md","hash":"7acbdd650431cdd3948320c9e307fa0e3e5563f9","modified":1609814137000},{"_id":"source/_posts/技术/hexo/spark/算子/Spark-Windos函数.md","hash":"4e579249a0a5e839a0094e782f6a2df8b3f993cb","modified":1609814137000},{"_id":"source/_posts/技术/hexo/Users/wqkenqing/Desktop/qiniu_idea/GKW7SA.png","hash":"0572ad0bc852ab7226dcccaede7d6134ec815acf","modified":1609814137000},{"_id":"source/_posts/技术/hexo/Users/wqkenqing/Desktop/qiniu_idea/WSZpdf.png","hash":"fa71d43078812f75743fd86860d45b8c12bb66f0","modified":1609814137000},{"_id":"themes/3-hexo/source/js/gitalk.js","hash":"a75ead28e6a1fab2a006cc7332ca2d2e868ce8e1","modified":1609814139000},{"_id":"public/atom.xml","hash":"8506eabc8bb68fe0612ebe71c6637aa21b4f1330","modified":1619147074504},{"_id":"public/content.json","hash":"2165a51091e88b7fcc4f08f4a550e66c810725dd","modified":1619147074504},{"_id":"public/search.xml","hash":"839f26aff5b91a43d995e4519606676d154762b1","modified":1619147074504},{"_id":"public/sitemap.xml","hash":"c5bc141c03113461b87ff0ec775f98f4f94440e0","modified":1619147074504},{"_id":"public/categories/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/日常/vue/vue/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/日常/考试/电子科大2019招生收集/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/日常/blog_sync_project/plan/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/yd_server_config/README/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/日常/运维/about云账号密码/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/集群/私有集群/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/日常/prepare/数据储备/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/日常/运维/jumpserver/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/日常/运维/tmux上手/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/日常/运维/京东云/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/日常/运维/博客不同主题切换/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/日常/运维/运维操作/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/编程/scala/scala学习/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/yg_config/hy-sjksh-info-dev/docker-compose/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/hive/hive20190109/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/program/mutiThread/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/hbase/hbase20190109/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/kafka/kafka小结/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/platform/platform/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/kafka/kafka疑问点/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/owncloud/账号/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/ubuntu/desk_gui/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/springboot/how to set parm/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/video/北风spark/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/ubuntu/linux自启服务/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/yarn/YarnState/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/hexo/Thread/多线程/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/hexo/kafka/kafka实现/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/hexo/old/Lamda积累/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/hexo/old/kafka/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/hexo/old/mapreduce组件总结/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/hexo/old/spark学习3/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/hexo/old/准备小结/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/hexo/old/大数据相关分享/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/hexo/oldblog/blog22/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/python/summary/2020.3.17/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/yd_server_config/sjksh_store_info/docker-compose/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/hexo/spark/算子/Spark-Windos函数/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/plan/8-3/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/plan/test/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/bash/python_learn/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/bash/shell积累/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/cdh/hue/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/docker/Dockerfile基础&编写/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/docker/docker操作/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/docker/docker镜像/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/flink/Flink记录/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/flink/flink/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/docker/docker/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/flink/flink_book/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/flume/docker版flume使用/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/front_end/nginx部署/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/front_end/调研/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/es/es小结/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/es/es的docker安装/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/es/共享数据集合/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/es/es详细/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/es/积累/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/gps/JT809/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/front_end/接口规划/data-manager/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2021/01/05/技术/bash/awk/awk/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2020/05/29/技术/python/summary/python小结3/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2020/05/25/日常/运维/mac python版本替换/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2020/02/25/技术/es/es常用命令/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/31/技术/hexo/flink/Flink/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/17/技术/hexo/hadoop/hdfs/hdfs命令/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog10/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog11/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog13/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog1/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog12/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog14/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog16/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog15/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog17/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog20/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog21/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog18/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog19/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog3/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog24/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog26/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog4/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog6/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog25/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog7/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog9/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog8/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/spark/stream2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/spark/宽窄依赖/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/16/技术/hexo/oldblog/blog23/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/07/15/技术/hexo/spark/stream/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/06/19/技术/flume/Flume/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/06/13/技术/hexo/old/Yarn配置/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/06/13/技术/hexo/old/flume记录/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/06/13/技术/hexo/spark/sql/SparkSql/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/05/27/技术/hexo/CDH/搭建/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/05/17/技术/hexo/spark/spark操作/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/05/17/技术/hexo/hadoop/hbase/hbase操作/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/05/16/技术/hexo/hadoop/hadoopHA搭建/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2019/05/16/技术/hexo/hadoop/hdfs/hdfs操作/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2018/12/24/技术/hexo/old/hive总结/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2018/06/04/技术/hexo/old/hbase积累/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2018/03/04/技术/hexo/old/spark学习2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2018/03/04/技术/hexo/old/spark学习/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2018/03/04/技术/hexo/old/spark算子/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/2018/03/04/技术/hexo/old/sqoop记录/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/page/2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/page/3/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/page/4/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/page/5/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/page/6/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/page/7/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/page/8/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/page/9/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/page/10/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/page/11/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2018/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2018/03/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2018/06/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2018/12/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2019/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2019/page/2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2019/page/3/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2019/page/4/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2019/05/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2019/06/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2019/07/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2019/07/page/2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2019/07/page/3/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2020/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2020/02/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2020/05/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2021/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2021/page/2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2021/page/3/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2021/page/4/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2021/page/5/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2021/page/6/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2021/page/7/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2021/01/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2021/01/page/2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2021/01/page/3/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2021/01/page/4/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2021/01/page/5/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2021/01/page/6/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/archives/2021/01/page/7/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/page/2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/page/3/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/page/4/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/page/5/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/page/6/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/page/7/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/page/8/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/page/9/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/page/10/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/page/11/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/mac-python2-python3/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/flink/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/kafka/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/日常总结/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/bigdata/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/学习spark2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/学习spark/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/spark学习/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/小结/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/sparkstreaming/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/sparkstream/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/spark-dependency/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/python/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/常规api封装/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/tags/hdfs/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1619147074504},{"_id":"public/CNAME","hash":"160fc677cb4c117e9970f50c384bc5e399104202","modified":1619147074504},{"_id":"public/img/gov.png","hash":"f31c9f47faedf7f33b9580d6284ab891fb697560","modified":1619147074504},{"_id":"public/img/avatar.jpg","hash":"e32c82ae27515c8c162ef420cdf61c5ad453ee7a","modified":1619147074504},{"_id":"public/img/school-book.png","hash":"711ec983c874e093bb89eb77afcbdf6741fa61ee","modified":1619147074504},{"_id":"public/img/brown-papersq.png","hash":"3a1332ede3a75a3d24f60b6ed69035b72da5e182","modified":1619147074504},{"_id":"public/css/fonts/icomoon.eot","hash":"b6195bedc1cb2f9cfcb26cc27021f2e94be2ab0a","modified":1619147074504},{"_id":"public/img/alipay.jpg","hash":"e457d1d3dfefbbd824d154cf756a2c6d10b812a2","modified":1619147074504},{"_id":"public/css/fonts/icomoon.ttf","hash":"eb976d8b8559fcddfc2658a03a4350cb566fc06b","modified":1619147074504},{"_id":"public/css/fonts/icomoon.svg","hash":"37ac1ef28b03f46bf3ad2606c86f0e1ec3e4405f","modified":1619147074504},{"_id":"public/css/fonts/icomoon.woff","hash":"3985d29416bb9b19f50a2f20f2bbbce47f10af8d","modified":1619147074504},{"_id":"public/css/fonts/iconfont.eot","hash":"b14b8624988ff069aff3145f88c0d7ac49052bd3","modified":1619147074504},{"_id":"public/css/fonts/iconfont.ttf","hash":"140829ecf12d30c6e18d8dc6dc0c188a66addd25","modified":1619147074504},{"_id":"public/css/fonts/iconfont.woff2","hash":"b0317a0b2ebb1181a8bf5a97d03556dd54538645","modified":1619147074504},{"_id":"public/css/fonts/iconfont.svg","hash":"13974fe35fca836e870a960ecb11b7eca2e036f8","modified":1619147074504},{"_id":"public/css/fonts/iconfont.woff","hash":"0d2d4559f1ac4fa801eb8cc099fa5bf9dcf955ef","modified":1619147074504},{"_id":"public/img/article-list-background.jpeg","hash":"4fdf8b3e53dd02d6ee6360aebfadb0cba1fb5633","modified":1619147074504},{"_id":"public/lib/blog-encrypt.js","hash":"91fda12d550323056762c5408623eb8a1d201ecc","modified":1619147074504},{"_id":"public/css/blog-encrypt.css","hash":"22e25b0d16ea053d25eb971e038c817a4c9cb584","modified":1619147074504},{"_id":"public/css/mobile.css","hash":"5998f6fc27998596beb1e40e4bc3c43be2ed764c","modified":1619147074504},{"_id":"public/js/search.js","hash":"c80c9a231ee040c7adc07a477793873fb85ce8bc","modified":1619147074504},{"_id":"public/js/titleTip.js","hash":"81dca549063e29ba3a4a278f0f4388eba8a2167b","modified":1619147074504},{"_id":"public/css/hl_theme/brown-paper.css","hash":"500c8e750373f6656ff49a7857c871ceedcf8777","modified":1619147074504},{"_id":"public/css/hl_theme/atom-dark.css","hash":"88d11052a24e8100af6248eb4dbe1ce7b0e96408","modified":1619147074504},{"_id":"public/css/hl_theme/github.css","hash":"e05a0806a508a26b9f3f3794b6b588ec6504ad3f","modified":1619147074504},{"_id":"public/css/hl_theme/github-gist.css","hash":"7a41c1c479d09df875f99f1f6d94aac42e9e2ad0","modified":1619147074504},{"_id":"public/css/hl_theme/darcula.css","hash":"4341074bae4bc9f0b86e32b623e27babc0159b6e","modified":1619147074504},{"_id":"public/css/hl_theme/gruvbox-dark.css","hash":"8c440d9b4ee19ac03eaee3c6af78ba52e5ba5535","modified":1619147074504},{"_id":"public/css/hl_theme/gruvbox-light.css","hash":"30514aaa242a34647aa666cfca4fc74c595ea8f2","modified":1619147074504},{"_id":"public/css/hl_theme/atom-light.css","hash":"d31edb9816dae6b01410028bceb91757a962f780","modified":1619147074504},{"_id":"public/css/hl_theme/kimbie-dark.css","hash":"728527fcc308da454722c119b89e6da3025bd1e3","modified":1619147074504},{"_id":"public/css/hl_theme/rainbow.css","hash":"7ff4251938076ddb7e4e49413db82653e5b61321","modified":1619147074504},{"_id":"public/css/hl_theme/sublime.css","hash":"f65c5b116d9213afb9c324384a2f3bc86cb71121","modified":1619147074504},{"_id":"public/css/hl_theme/kimbie-light.css","hash":"0c61926c989163faefb031d27bce3e287d6e10f2","modified":1619147074504},{"_id":"public/css/hl_theme/railscasts.css","hash":"511f2fd2a84d426e5da5cb17880cc08f73beb002","modified":1619147074504},{"_id":"public/css/hl_theme/school-book.css","hash":"ffbbcd13a74ac2404262c50b7a43053dfd0096ff","modified":1619147074504},{"_id":"public/css/hl_theme/zenbum.css","hash":"0a78f74a93568e20b32ca7427c719e9bae9a0b55","modified":1619147074504},{"_id":"public/css/hl_theme/sunburst.css","hash":"8a135abac1512cf430d1d1ad2304b79afa1a4d6e","modified":1619147074504},{"_id":"public/css/style.css","hash":"6a3cefe3d405eaf28307e26766808d6526e6e993","modified":1619147074504},{"_id":"public/css/gitalk.css","hash":"58177ce227c50ee359fbf99a4fdd26058887afc5","modified":1619147074504},{"_id":"public/js/iconfont.js","hash":"3a0869ca1b09af07d82987e343a3bc4cb9558ecb","modified":1619147074504},{"_id":"public/js/jquery.pjax.js","hash":"191c49fdb40dff115a49cfd2b30dffb888d86550","modified":1619147074504},{"_id":"public/js/script.js","hash":"53b8bfe0ffa5ff153e5d759c6f2de00daadb8a07","modified":1619147074504},{"_id":"public/css/fonts/selection.json","hash":"047b615ea32dc48dae5b964061427d41feaaafdf","modified":1619147074504},{"_id":"public/js/gitment.js","hash":"59a1e03f2b0ce61dd9bd405d3c52d3e07cc10dec","modified":1619147074504},{"_id":"public/img/weixin.jpg","hash":"8dafb22561698d0758fba9ea2a45abf6ad3512a2","modified":1619147074504},{"_id":"public/js/gitalk.js","hash":"a75ead28e6a1fab2a006cc7332ca2d2e868ce8e1","modified":1619147074504},{"_id":"source/.DS_Store","hash":"d719dcb3bcfa75b4e4d513f90b5678d9aa820ea2","modified":1619149967866},{"_id":"themes/tree/README.md","hash":"d86e907c5845e38df019af0eb9869124fc6c22bb","modified":1619158802736},{"_id":"themes/tree/_config.yml","hash":"86aaaa969fec4b5fad0d785816f72d3670f307a6","modified":1619159045270},{"_id":"source/_posts/.DS_Store","hash":"85cb662fd608006f67c6bfe8096bd11210a7e2e4","modified":1619149967867},{"_id":"source/categories/.DS_Store","hash":"af5ebac0ae2a7092d1d70e537899bbda514e438e","modified":1619149931158},{"_id":"source/tags/.DS_Store","hash":"af5ebac0ae2a7092d1d70e537899bbda514e438e","modified":1619149938193},{"_id":"themes/tree/layout/categories.ejs","hash":"d35dab43aefc3d0f798d8676103345e71caf1e23","modified":1619158802737},{"_id":"themes/tree/layout/index.ejs","hash":"101bd5d563884579882c8caaeafbc3139134bab4","modified":1619158802738},{"_id":"themes/tree/layout/layout.ejs","hash":"b1cbe3e0c52a8fa644a5adb97b31b5ecfdec2162","modified":1619158802738},{"_id":"themes/tree/layout/tags.ejs","hash":"973aa7f435ceba2a2f6c1a5ac7d3677fea5a50b7","modified":1619158802738},{"_id":"themes/tree/layout/post.ejs","hash":"eca7b15659367d917b11c627fb30e2fc3a7ebbc0","modified":1619158802738},{"_id":"themes/tree/source/404.html","hash":"cfa1b96d02d143d77c0ba519dab1c23de0cae6ee","modified":1619158802738},{"_id":"themes/tree/source/favicon.ico","hash":"268be0a863caae7c5a732fb8a911a81434f31117","modified":1619158802740},{"_id":"source/_posts/技术/.DS_Store","hash":"350481fb8cf68f9e471307f694b55f6d8259157c","modified":1619149957344},{"_id":"source/_posts/日常/.DS_Store","hash":"262d6896941909b993d5618494e9acd59064a250","modified":1619149962862},{"_id":"themes/tree/layout/_partial/head.ejs","hash":"733fbbd69615f3161101d0eb2f559d22a43ef276","modified":1619158802737},{"_id":"themes/tree/layout/_partial/footer.ejs","hash":"1bbd6ce41784b3f7011b928b168bf84755a0d9b9","modified":1619158802737},{"_id":"themes/tree/layout/_partial/header.ejs","hash":"abb98bf2ae182e4988f65b9a8f0f024dbd97185b","modified":1619158802737},{"_id":"themes/tree/layout/_partial/sidebar.ejs","hash":"ea6c595b9a1b0e743930adf395ee66697ff61ab1","modified":1619158802737},{"_id":"themes/tree/layout/_partial/totop.ejs","hash":"a94c4b350b76d21bb530095981049a2647cc6273","modified":1619158802737},{"_id":"themes/tree/source/css/main.css","hash":"a374eb273c29e0c96cdbb11e3e204051c6a3dfcc","modified":1619158802740},{"_id":"themes/tree/source/js/main.js","hash":"3216da0a4e322b58917a94f9296ddca3823fd3a4","modified":1619158802740},{"_id":"themes/tree/source/lib/jquery.pjax.js","hash":"13485a1e2dc9c8df28267549de1b8af8f39061d9","modified":1619158802743},{"_id":"themes/tree/source/Tree.png","hash":"e5a30d5edf5aa2dc39fb4db7b4761b516826cf5b","modified":1619158802739},{"_id":"themes/tree/source/lib/jquery-3.4.1.min.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1619158802743},{"_id":"themes/tree/source/lib/font-awesome/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1619158802741},{"_id":"themes/tree/source/lib/highlight/darcula.css","hash":"7740224d07375ddc321147dffabbfa83e39f0d8b","modified":1619158802741},{"_id":"themes/tree/source/lib/highlight/highlight.pack.js","hash":"84a7e42dfa8c44a422a8328277d660d9428fcea6","modified":1619158802742},{"_id":"themes/tree/source/lib/valine/Valine-1.3.10-min.js","hash":"e3a340d0c39eee2ae651284fda9d351e752fcbd0","modified":1619158802744},{"_id":"themes/tree/source/lib/busuanzi/2.3/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1619158802741},{"_id":"themes/tree/source/lib/valine/av-3.0.4-min.js","hash":"2577e72b52b736d99649f9e95be8976d58563333","modified":1619158802745}],"Category":[],"Data":[],"Page":[{"title":"categories","date":"2020-05-22T02:41:30.000Z","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2020-05-22 10:41:30\ntype: \"categories\"\nlayout: \"categories\"\n---\n","updated":"2021-04-23T06:32:11.189Z","path":"categories/index.html","_id":"ckntqd2y0000038pw0g626web","comments":1,"content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2020-05-22T02:42:26.000Z","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2020-05-22 10:42:26\ntype: tags\nlayout: \"tags\"\n---\n","updated":"2021-04-23T06:28:54.674Z","path":"tags/index.html","_id":"ckntqd2y2000138pw1o2oduio","comments":1,"content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"_content":"","source":"_posts/plan/8-3.md","raw":"","slug":"plan/8-3","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"plan/8-3","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd2z1000238pw3p98g63g","content":"","site":{"data":{}},"excerpt":"","more":""},{"_content":"","source":"_posts/plan/test.md","raw":"","slug":"plan/test","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"plan/test","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd2z3000338pw7fekdqet","content":"","site":{"data":{}},"excerpt":"","more":""},{"_content":"","source":"_posts/技术/hive/hive20190109.md","raw":"","slug":"技术/hive/hive20190109","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/hive/hive20190109","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd308000438pwadv86e69","content":"","site":{"data":{}},"excerpt":"","more":""},{"_content":"","source":"_posts/技术/program/mutiThread.md","raw":"","slug":"技术/program/mutiThread","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/program/mutiThread","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd309000538pw3r4i05jx","content":"","site":{"data":{}},"excerpt":"","more":""},{"_content":"","source":"_posts/日常/vue/vue.md","raw":"","slug":"日常/vue/vue","published":1,"date":"2021-01-05T02:35:38.000Z","updated":"2021-01-05T02:35:38.000Z","title":"日常/vue/vue","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd309000638pwekh37js8","content":"","site":{"data":{}},"excerpt":"","more":""},{"_content":"","source":"_posts/日常/考试/电子科大2019招生收集.md","raw":"","slug":"日常/考试/电子科大2019招生收集","published":1,"date":"2021-01-05T02:35:38.000Z","updated":"2021-01-05T02:35:38.000Z","title":"日常/考试/电子科大2019招生收集","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd30a000738pwaymn85ge","content":"","site":{"data":{}},"excerpt":"","more":""},{"_content":"# python learn\n\n1.print 函数详细用法\n\nprint (value,...,sep='',end='\\n',file=sys.stdout,flush=False)\n\n\n\n\n","source":"_posts/技术/bash/python_learn.md","raw":"# python learn\n\n1.print 函数详细用法\n\nprint (value,...,sep='',end='\\n',file=sys.stdout,flush=False)\n\n\n\n\n","slug":"技术/bash/python_learn","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/bash/python_learn","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd30r000838pw21dnhuyp","content":"<h1 id=\"python-learn\"><a href=\"#python-learn\" class=\"headerlink\" title=\"python learn\"></a>python learn</h1><p>1.print 函数详细用法</p>\n<p>print (value,…,sep=’’,end=’\\n’,file=sys.stdout,flush=False)</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"python-learn\"><a href=\"#python-learn\" class=\"headerlink\" title=\"python learn\"></a>python learn</h1><p>1.print 函数详细用法</p>\n<p>print (value,…,sep=’’,end=’\\n’,file=sys.stdout,flush=False)</p>\n"},{"_content":"# shell collect\n\n\n/Users/wqkenqing/Library/Preferences/IntelliJIdea2019.3/scratches\n\n","source":"_posts/技术/bash/shell积累.md","raw":"# shell collect\n\n\n/Users/wqkenqing/Library/Preferences/IntelliJIdea2019.3/scratches\n\n","slug":"技术/bash/shell积累","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/bash/shell积累","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd30z000938pwc2exe157","content":"<h1 id=\"shell-collect\"><a href=\"#shell-collect\" class=\"headerlink\" title=\"shell collect\"></a>shell collect</h1><p>/Users/wqkenqing/Library/Preferences/IntelliJIdea2019.3/scratches</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"shell-collect\"><a href=\"#shell-collect\" class=\"headerlink\" title=\"shell collect\"></a>shell collect</h1><p>/Users/wqkenqing/Library/Preferences/IntelliJIdea2019.3/scratches</p>\n"},{"_content":"# hue 使用\n\n## hive 集成\n\n```sql\n\ncreate table theme_click_log (user_id string , theme_id string,item_id string,leaf_cate_id string,cate_leve1_id string,\nclk_cnt int,reach_time date) row format delimited fields terminated by ',';\n\n```\n\n```sql\n\nalter table user_item_purchase_log set serdeproperties('field.delim'=',');\n\n```\n\n\n```sql\n\n\ncreate table theme_click_log (user_id string , theme_id string,item_id string,leaf_cate_id string,cate_leve1_id string,\nclk_cnt int,reach_time date) row format delimited fields terminated by ',';\ncreate table theme_item_pool (theme_id string,item_id string) row format delimited fields terminated by ',';\ncreate table user_item_purchase_log (user_id string,item_id string,paytime string);\ncreate table item_embedding (item_id string,emb string)row format delimited fields terminated by ',';\ncreate table user_embedding (user_id string,emb string) row format delimited fields terminated by ',';\n\n\n```\n\n```sql\n\ncreate table theme_click_log (user_id string , theme_id string,item_id string,leaf_cate_id string,cate_leve1_id string,\nclk_cnt int,reach_time date) row format delimited fields terminated by ',';\ncreate table theme_item_pool (theme_id string,item_id string) row format delimited fields terminated by ',';\ncreate table user_item_purchase_log (user_id string,item_id string,paytime string);\ncreate table item_embedding (item_id string,emb string)row format delimited fields terminated by ',';\ncreate table user_embedding (user_id string,emb string) row format delimited fields terminated by ',';\ndrop table test;\n\n\nload data local inpath '/data/upload/item_embedding.csv' overwrite into table item_embedding;\n\nselect count(*) from theme_click_log;\n```\n\n```\ncreate table theme_click_log (user_id string , theme_id string,item_id string,leaf_cate_id string,cate_leve1_id string,\nclk_cnt int,reach_time date) row format delimited fields terminated by ',';\ncreate table theme_item_pool (theme_id string,item_id string) row format delimited fields terminated by ',';\ncreate table user_item_purchase_log (user_id string,item_id string,paytime string);\ncreate table item_embedding (item_id string,emb string)row format delimited fields terminated by ',';\ncreate table user_embedding (user_id string,emb string) row format delimited fields terminated by ',';\ndrop table test;\n\n\nload data local inpath '/data/upload/item_embedding.csv' overwrite into table item_embedding;\n\nselect count(*) from item_embedding;\n\nload data local inpath '/data/upload/theme_item_pool.csv' overwrite into table theme_item_pool;\nload data local inpath '/data/upload/user_item_purchase_log.csv' overwrite into table user_item_purchase_log;\n\n```\n\n```sql\n\ncreate table flight_info (flight_id int,flight_time string,area string,flight_number int,rise_up_airport int,arrive_in_airport int,rise_up_time string,arrive_in_time string,airplane_id int,airplane_type int,importance double)\nrow format delimited\nfields terminated by '\\t';\n\n```","source":"_posts/技术/cdh/hue.md","raw":"# hue 使用\n\n## hive 集成\n\n```sql\n\ncreate table theme_click_log (user_id string , theme_id string,item_id string,leaf_cate_id string,cate_leve1_id string,\nclk_cnt int,reach_time date) row format delimited fields terminated by ',';\n\n```\n\n```sql\n\nalter table user_item_purchase_log set serdeproperties('field.delim'=',');\n\n```\n\n\n```sql\n\n\ncreate table theme_click_log (user_id string , theme_id string,item_id string,leaf_cate_id string,cate_leve1_id string,\nclk_cnt int,reach_time date) row format delimited fields terminated by ',';\ncreate table theme_item_pool (theme_id string,item_id string) row format delimited fields terminated by ',';\ncreate table user_item_purchase_log (user_id string,item_id string,paytime string);\ncreate table item_embedding (item_id string,emb string)row format delimited fields terminated by ',';\ncreate table user_embedding (user_id string,emb string) row format delimited fields terminated by ',';\n\n\n```\n\n```sql\n\ncreate table theme_click_log (user_id string , theme_id string,item_id string,leaf_cate_id string,cate_leve1_id string,\nclk_cnt int,reach_time date) row format delimited fields terminated by ',';\ncreate table theme_item_pool (theme_id string,item_id string) row format delimited fields terminated by ',';\ncreate table user_item_purchase_log (user_id string,item_id string,paytime string);\ncreate table item_embedding (item_id string,emb string)row format delimited fields terminated by ',';\ncreate table user_embedding (user_id string,emb string) row format delimited fields terminated by ',';\ndrop table test;\n\n\nload data local inpath '/data/upload/item_embedding.csv' overwrite into table item_embedding;\n\nselect count(*) from theme_click_log;\n```\n\n```\ncreate table theme_click_log (user_id string , theme_id string,item_id string,leaf_cate_id string,cate_leve1_id string,\nclk_cnt int,reach_time date) row format delimited fields terminated by ',';\ncreate table theme_item_pool (theme_id string,item_id string) row format delimited fields terminated by ',';\ncreate table user_item_purchase_log (user_id string,item_id string,paytime string);\ncreate table item_embedding (item_id string,emb string)row format delimited fields terminated by ',';\ncreate table user_embedding (user_id string,emb string) row format delimited fields terminated by ',';\ndrop table test;\n\n\nload data local inpath '/data/upload/item_embedding.csv' overwrite into table item_embedding;\n\nselect count(*) from item_embedding;\n\nload data local inpath '/data/upload/theme_item_pool.csv' overwrite into table theme_item_pool;\nload data local inpath '/data/upload/user_item_purchase_log.csv' overwrite into table user_item_purchase_log;\n\n```\n\n```sql\n\ncreate table flight_info (flight_id int,flight_time string,area string,flight_number int,rise_up_airport int,arrive_in_airport int,rise_up_time string,arrive_in_time string,airplane_id int,airplane_type int,importance double)\nrow format delimited\nfields terminated by '\\t';\n\n```","slug":"技术/cdh/hue","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/cdh/hue","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd310000a38pw9tcgd409","content":"<h1 id=\"hue-使用\"><a href=\"#hue-使用\" class=\"headerlink\" title=\"hue 使用\"></a>hue 使用</h1><h2 id=\"hive-集成\"><a href=\"#hive-集成\" class=\"headerlink\" title=\"hive 集成\"></a>hive 集成</h2><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> theme_click_log (user_id <span class=\"keyword\">string</span> , theme_id <span class=\"keyword\">string</span>,item_id <span class=\"keyword\">string</span>,leaf_cate_id <span class=\"keyword\">string</span>,cate_leve1_id <span class=\"keyword\">string</span>,</span><br><span class=\"line\">clk_cnt <span class=\"built_in\">int</span>,reach_time <span class=\"built_in\">date</span>) <span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">alter</span> <span class=\"keyword\">table</span> user_item_purchase_log <span class=\"keyword\">set</span> serdeproperties(<span class=\"string\">'field.delim'</span>=<span class=\"string\">','</span>);</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> theme_click_log (user_id <span class=\"keyword\">string</span> , theme_id <span class=\"keyword\">string</span>,item_id <span class=\"keyword\">string</span>,leaf_cate_id <span class=\"keyword\">string</span>,cate_leve1_id <span class=\"keyword\">string</span>,</span><br><span class=\"line\">clk_cnt <span class=\"built_in\">int</span>,reach_time <span class=\"built_in\">date</span>) <span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> theme_item_pool (theme_id <span class=\"keyword\">string</span>,item_id <span class=\"keyword\">string</span>) <span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> user_item_purchase_log (user_id <span class=\"keyword\">string</span>,item_id <span class=\"keyword\">string</span>,paytime <span class=\"keyword\">string</span>);</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> item_embedding (item_id <span class=\"keyword\">string</span>,emb <span class=\"keyword\">string</span>)<span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> user_embedding (user_id <span class=\"keyword\">string</span>,emb <span class=\"keyword\">string</span>) <span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> theme_click_log (user_id <span class=\"keyword\">string</span> , theme_id <span class=\"keyword\">string</span>,item_id <span class=\"keyword\">string</span>,leaf_cate_id <span class=\"keyword\">string</span>,cate_leve1_id <span class=\"keyword\">string</span>,</span><br><span class=\"line\">clk_cnt <span class=\"built_in\">int</span>,reach_time <span class=\"built_in\">date</span>) <span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> theme_item_pool (theme_id <span class=\"keyword\">string</span>,item_id <span class=\"keyword\">string</span>) <span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> user_item_purchase_log (user_id <span class=\"keyword\">string</span>,item_id <span class=\"keyword\">string</span>,paytime <span class=\"keyword\">string</span>);</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> item_embedding (item_id <span class=\"keyword\">string</span>,emb <span class=\"keyword\">string</span>)<span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> user_embedding (user_id <span class=\"keyword\">string</span>,emb <span class=\"keyword\">string</span>) <span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br><span class=\"line\"><span class=\"keyword\">drop</span> <span class=\"keyword\">table</span> <span class=\"keyword\">test</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">load</span> <span class=\"keyword\">data</span> <span class=\"keyword\">local</span> inpath <span class=\"string\">'/data/upload/item_embedding.csv'</span> overwrite <span class=\"keyword\">into</span> <span class=\"keyword\">table</span> item_embedding;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"keyword\">count</span>(*) <span class=\"keyword\">from</span> theme_click_log;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create table theme_click_log (user_id string , theme_id string,item_id string,leaf_cate_id string,cate_leve1_id string,</span><br><span class=\"line\">clk_cnt int,reach_time date) row format delimited fields terminated by &#39;,&#39;;</span><br><span class=\"line\">create table theme_item_pool (theme_id string,item_id string) row format delimited fields terminated by &#39;,&#39;;</span><br><span class=\"line\">create table user_item_purchase_log (user_id string,item_id string,paytime string);</span><br><span class=\"line\">create table item_embedding (item_id string,emb string)row format delimited fields terminated by &#39;,&#39;;</span><br><span class=\"line\">create table user_embedding (user_id string,emb string) row format delimited fields terminated by &#39;,&#39;;</span><br><span class=\"line\">drop table test;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">load data local inpath &#39;&#x2F;data&#x2F;upload&#x2F;item_embedding.csv&#39; overwrite into table item_embedding;</span><br><span class=\"line\"></span><br><span class=\"line\">select count(*) from item_embedding;</span><br><span class=\"line\"></span><br><span class=\"line\">load data local inpath &#39;&#x2F;data&#x2F;upload&#x2F;theme_item_pool.csv&#39; overwrite into table theme_item_pool;</span><br><span class=\"line\">load data local inpath &#39;&#x2F;data&#x2F;upload&#x2F;user_item_purchase_log.csv&#39; overwrite into table user_item_purchase_log;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> flight_info (flight_id <span class=\"built_in\">int</span>,flight_time <span class=\"keyword\">string</span>,area <span class=\"keyword\">string</span>,flight_number <span class=\"built_in\">int</span>,rise_up_airport <span class=\"built_in\">int</span>,arrive_in_airport <span class=\"built_in\">int</span>,rise_up_time <span class=\"keyword\">string</span>,arrive_in_time <span class=\"keyword\">string</span>,airplane_id <span class=\"built_in\">int</span>,airplane_type <span class=\"built_in\">int</span>,importance <span class=\"keyword\">double</span>)</span><br><span class=\"line\"><span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span></span><br><span class=\"line\"><span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">'\\t'</span>;</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"hue-使用\"><a href=\"#hue-使用\" class=\"headerlink\" title=\"hue 使用\"></a>hue 使用</h1><h2 id=\"hive-集成\"><a href=\"#hive-集成\" class=\"headerlink\" title=\"hive 集成\"></a>hive 集成</h2><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> theme_click_log (user_id <span class=\"keyword\">string</span> , theme_id <span class=\"keyword\">string</span>,item_id <span class=\"keyword\">string</span>,leaf_cate_id <span class=\"keyword\">string</span>,cate_leve1_id <span class=\"keyword\">string</span>,</span><br><span class=\"line\">clk_cnt <span class=\"built_in\">int</span>,reach_time <span class=\"built_in\">date</span>) <span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">alter</span> <span class=\"keyword\">table</span> user_item_purchase_log <span class=\"keyword\">set</span> serdeproperties(<span class=\"string\">'field.delim'</span>=<span class=\"string\">','</span>);</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> theme_click_log (user_id <span class=\"keyword\">string</span> , theme_id <span class=\"keyword\">string</span>,item_id <span class=\"keyword\">string</span>,leaf_cate_id <span class=\"keyword\">string</span>,cate_leve1_id <span class=\"keyword\">string</span>,</span><br><span class=\"line\">clk_cnt <span class=\"built_in\">int</span>,reach_time <span class=\"built_in\">date</span>) <span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> theme_item_pool (theme_id <span class=\"keyword\">string</span>,item_id <span class=\"keyword\">string</span>) <span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> user_item_purchase_log (user_id <span class=\"keyword\">string</span>,item_id <span class=\"keyword\">string</span>,paytime <span class=\"keyword\">string</span>);</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> item_embedding (item_id <span class=\"keyword\">string</span>,emb <span class=\"keyword\">string</span>)<span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> user_embedding (user_id <span class=\"keyword\">string</span>,emb <span class=\"keyword\">string</span>) <span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> theme_click_log (user_id <span class=\"keyword\">string</span> , theme_id <span class=\"keyword\">string</span>,item_id <span class=\"keyword\">string</span>,leaf_cate_id <span class=\"keyword\">string</span>,cate_leve1_id <span class=\"keyword\">string</span>,</span><br><span class=\"line\">clk_cnt <span class=\"built_in\">int</span>,reach_time <span class=\"built_in\">date</span>) <span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> theme_item_pool (theme_id <span class=\"keyword\">string</span>,item_id <span class=\"keyword\">string</span>) <span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> user_item_purchase_log (user_id <span class=\"keyword\">string</span>,item_id <span class=\"keyword\">string</span>,paytime <span class=\"keyword\">string</span>);</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> item_embedding (item_id <span class=\"keyword\">string</span>,emb <span class=\"keyword\">string</span>)<span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> user_embedding (user_id <span class=\"keyword\">string</span>,emb <span class=\"keyword\">string</span>) <span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span> <span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">','</span>;</span><br><span class=\"line\"><span class=\"keyword\">drop</span> <span class=\"keyword\">table</span> <span class=\"keyword\">test</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">load</span> <span class=\"keyword\">data</span> <span class=\"keyword\">local</span> inpath <span class=\"string\">'/data/upload/item_embedding.csv'</span> overwrite <span class=\"keyword\">into</span> <span class=\"keyword\">table</span> item_embedding;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">select</span> <span class=\"keyword\">count</span>(*) <span class=\"keyword\">from</span> theme_click_log;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">create table theme_click_log (user_id string , theme_id string,item_id string,leaf_cate_id string,cate_leve1_id string,</span><br><span class=\"line\">clk_cnt int,reach_time date) row format delimited fields terminated by &#39;,&#39;;</span><br><span class=\"line\">create table theme_item_pool (theme_id string,item_id string) row format delimited fields terminated by &#39;,&#39;;</span><br><span class=\"line\">create table user_item_purchase_log (user_id string,item_id string,paytime string);</span><br><span class=\"line\">create table item_embedding (item_id string,emb string)row format delimited fields terminated by &#39;,&#39;;</span><br><span class=\"line\">create table user_embedding (user_id string,emb string) row format delimited fields terminated by &#39;,&#39;;</span><br><span class=\"line\">drop table test;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">load data local inpath &#39;&#x2F;data&#x2F;upload&#x2F;item_embedding.csv&#39; overwrite into table item_embedding;</span><br><span class=\"line\"></span><br><span class=\"line\">select count(*) from item_embedding;</span><br><span class=\"line\"></span><br><span class=\"line\">load data local inpath &#39;&#x2F;data&#x2F;upload&#x2F;theme_item_pool.csv&#39; overwrite into table theme_item_pool;</span><br><span class=\"line\">load data local inpath &#39;&#x2F;data&#x2F;upload&#x2F;user_item_purchase_log.csv&#39; overwrite into table user_item_purchase_log;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">create</span> <span class=\"keyword\">table</span> flight_info (flight_id <span class=\"built_in\">int</span>,flight_time <span class=\"keyword\">string</span>,area <span class=\"keyword\">string</span>,flight_number <span class=\"built_in\">int</span>,rise_up_airport <span class=\"built_in\">int</span>,arrive_in_airport <span class=\"built_in\">int</span>,rise_up_time <span class=\"keyword\">string</span>,arrive_in_time <span class=\"keyword\">string</span>,airplane_id <span class=\"built_in\">int</span>,airplane_type <span class=\"built_in\">int</span>,importance <span class=\"keyword\">double</span>)</span><br><span class=\"line\"><span class=\"keyword\">row</span> <span class=\"keyword\">format</span> <span class=\"keyword\">delimited</span></span><br><span class=\"line\"><span class=\"keyword\">fields</span> <span class=\"keyword\">terminated</span> <span class=\"keyword\">by</span> <span class=\"string\">'\\t'</span>;</span><br></pre></td></tr></table></figure>"},{"_content":"# Dockerfile基础\n\n```\nDockerfile用于指定docker build 指令时执行时的动作与资源划分等,如指定分配内存大小,cpu个数等.从而构成相应的镜像\n\n```\n\n## 详细指令\n\n![docker详细指令](http://img.wqkenqing.ren/5aaf3a25b3434c7b1ec0cd5b41b2be1a.png)\n\n如上图即docker主要的指令与作用\n\n\nDockerfile由多条指令组成,每条指令在编译镜像时执行相应的程序完成某些功能,指令+参数组成，以逗号分隔，#作为注释起始符，虽说指令不区分大小写，但是一般指令使用大些，参数使用小写.\n\n\n\n### 指令：FROM\n\n功能描述：设置基础镜像\n语法：FROM < image>[:< tag> | @< digest>]\n提示：镜像都是从一个基础镜像（操作系统或其他镜像）生成，可以在一个Dockerfile中添加多条FROM指令，一次生成多个镜像\n注意：如果忽略tag选项，会使用latest镜像\n\n### 指令：MAINTAINER\n\n功能描述：设置镜像作者\n语法：MAINTAINER < name>\n\n\n### 指令：RUN\n\n功能描述：\n语法：RUN < command>\n          RUN [“executable”,”param1”,”param2”]\n提示：RUN指令会生成容器，在容器中执行脚本，容器使用当前镜像，脚本指令完成后，Docker Daemon会将该容器提交为一个中间镜像，供后面的指令使用\n补充：RUN指令第一种方式为shell方式，使用/bin/sh -c < command>运行脚本，可以在其中使用\\将脚本分为多行\n          RUN指令第二种方式为exec方式，镜像中没有/bin/sh或者要使用其他shell时使用该方式，其不会调用shell命令\n例子：RUN source $HOME/.bashrc;\\\n          echo $HOME\n\n          RUN [“/bin/bash”,”-c”,”echo hello”]\n\n          RUN [“sh”,”-c”,”echo”,”$HOME”] 使用第二种方式调用shell读取环境变量\n\n### 指令：CMD\n\n功能描述：设置容器的启动命令\n语法：CMD [“executable”,”param1”,”param2”]\n          CMD [“param1”,”param2”]\n          CMD < command>\n提示：CMD第一种、第三种方式和RUN类似，第二种方式为ENTRYPOINT参数方式，为entrypoint提供参数列表\n注意：**Dockerfile中只能有一条CMD命令，如果写了多条则最后一条生效**\n\n### 指令：LABEL\n功能描述：设置镜像的标签\n延伸：镜像标签可以通过docker inspect查看\n格式：LABEL < key>=< value> < key>=< value> …\n提示：不同标签之间通过空格隔开\n注意：每条指令都会生成一个镜像层，Docker中镜像最多只能有127层，如果超出Docker Daemon就会报错，如LABEL ..=.. <假装这里有个换行> LABEL ..=..合在一起用空格分隔就可以减少镜像层数量，同样，可以使用连接符\\将脚本分为多行镜像会继承基础镜像中的标签，如果存在同名标签则会覆盖\n\n### 指令：EXPOSE\n功能描述：设置镜像暴露端口，记录容器启动时监听哪些端口\n语法：EXPOSE < port> < port> …\n延伸：镜像暴露端口可以通过docker inspect查看\n提示：容器启动时，Docker Daemon会扫描镜像中暴露的端口，如果加入-P参数，Docker Daemon会把镜像中所有暴露端口导出，并为每个暴露端口分配一个随机的主机端口（暴露端口是容器监听端口，主机端口为外部访问容器的端口）\n注意：EXPOSE只设置暴露端口并不导出端口，只有启动容器时使用-P/-p才导出端口，这个时候才能通过外部访问容器提供的服务\n\n### 指令：ENV\n功能描述：设置镜像中的环境变量\n语法：ENV < key>=< value>…|< key> < value>\n注意：环境变量在整个编译周期都有效，第一种方式可设置多个环境变量，第二种方式只设置一个环境变量\n提示：通过${变量名}或者 $变量名使用变量，使用方式${变量名}时可以用${变量名:-default} ${变量名:+cover}设定默认值或者覆盖值 ENV设置的变量值在整个编译过程中总是保持不变的\n\n### 指令：ADD\n功能描述：复制文件到镜像中\n语法：ADD < src>… < dest>|[“< src>”,… “< dest>”]\n注意：当路径中有空格时，需要使用第二种方式\n          当src为文件或目录时，Docker Daemon会从编译目录寻找这些文件或目录，而dest为镜像中的绝对路径或者相对于WORKDIR的路径\n提示：src为目录时，复制目录中所有内容，包括文件系统的元数据，但不包括目录本身\n          src为压缩文件，并且压缩方式为gzip,bzip2或xz时，指令会将其解压为目录\n          如果src为文件，则复制文件和元数据\n          如果dest不存在，指令会自动创建dest和缺失的上级目录\n\n### 指令：COPY\n功能描述：复制文件到镜像中\n语法：COPY < src>… < dest>|[“< src>”,… “< dest>”]\n提示：指令逻辑和ADD十分相似，同样Docker Daemon会从编译目录寻找文件或目录，dest为镜像中的绝对路径或者相对于WORKDIR的路径\n\n### 指令：ENTRYPOINT\n功能描述：设置容器的入口程序\n语法：ENTRYPOINT [“executable”,”param1”,”param2”]\n          ENTRYPOINT command param1 param2（shell方式）\n提示：入口程序是容器启动时执行的程序，docker run中最后的命令将作为参数传递给入口程序\n          入口程序有两种格式：exec、shell，其中shell使用/bin/sh -c运行入口程序，此时入口程序不能接收信号量\n          当Dockerfile有多条ENTRYPOINT时只有最后的ENTRYPOINT指令生效\n          如果使用脚本作为入口程序，需要保证脚本的最后一个程序能够接收信号量，可以在脚本最后使用exec或gosu启动传入脚本的命令\n注意：通过shell方式启动入口程序时，会忽略CMD指令和docker run中的参数\n          为了保证容器能够接受docker stop发送的信号量，需要通过exec启动程序；如果没有加入exec命令，则在启动容器时容器会出现两个进程，并且使用docker stop命令容器无法正常退出（无法接受SIGTERM信号），超时后docker stop发送SIGKILL，强制停止容器\n例子：FROM ubuntu <换行> ENTRYPOINT exec top -b\n\n### 指令：VOLUME\n功能描述：设置容器的挂载点\n语法：VOLUME [“/data”]\n          VOLUME /data1 /data2\n提示：启动容器时，Docker Daemon会新建挂载点，并用镜像中的数据初始化挂载点，可以将主机目录或数据卷容器挂载到这些挂载点\n\n### 指令：USER\n\n功能描述：设置RUN CMD ENTRYPOINT的用户名或UID\n语法：USER < name>\n\n### 指令：WORKDIR\n\n功能描述：设置RUN CMD ENTRYPOINT ADD COPY指令的工作目录\n语法：WORKDIR < Path>\n提示：如果工作目录不存在，则Docker Daemon会自动创建\n          Dockerfile中多个地方都可以调用WORKDIR，如果后面跟的是相对位置，则会跟在上条WORKDIR指定路径后（如WORKDIR /A   WORKDIR B   WORKDIR C，最终路径为/A/B/C\n\n### 指令：ARG\n\n功能描述：设置编译变量\n语法：ARG < name>[=< defaultValue>]\n注意：ARG从定义它的地方开始生效而不是调用的地方，在ARG之前调用编译变量总为空，在编译镜像时，可以通过docker build –build-arg < var>=< value>设置变量，如果var没有通过ARG定义则Daemon会报错\n          可以使用ENV或ARG设置RUN使用的变量，如果同名则ENV定义的值会覆盖ARG定义的值，与ENV不同，ARG的变量值在编译过程中是可变的，会对比使用编译缓存造成影响（ARG值不同则编译过程也不同）\n例子：ARG CONT_IMAG_VER <换行> RUN echo $CONT_IMG_VER\n          ARG CONT_IMAG_VER <换行> RUN echo hello\n          当编译时给ARG变量赋值hello，则两个Dockerfile可以使用相同的中间镜像，如果不为hello，则不能使用同一个中间镜像\n\n\n\n### CMD ENTRYPOINT和RUN的区别\nRUN指令是设置编译镜像时执行的脚本和程序，镜像编译完成后，RUN指令的生命周期结束  \n容器启动时，可以通过CMD和ENTRYPOINT设置启动项，其中CMD叫做容器默认启动命令，如果在docker run命令末尾添加command，则会替换镜像中CMD设置的启动程序；ENRTYPOINT叫做入口程序，不能被docker run命令末尾的command替换，而是将command当作字符串，传递给ENTRYPOINT作为参数\n","source":"_posts/技术/docker/Dockerfile基础&编写.md","raw":"# Dockerfile基础\n\n```\nDockerfile用于指定docker build 指令时执行时的动作与资源划分等,如指定分配内存大小,cpu个数等.从而构成相应的镜像\n\n```\n\n## 详细指令\n\n![docker详细指令](http://img.wqkenqing.ren/5aaf3a25b3434c7b1ec0cd5b41b2be1a.png)\n\n如上图即docker主要的指令与作用\n\n\nDockerfile由多条指令组成,每条指令在编译镜像时执行相应的程序完成某些功能,指令+参数组成，以逗号分隔，#作为注释起始符，虽说指令不区分大小写，但是一般指令使用大些，参数使用小写.\n\n\n\n### 指令：FROM\n\n功能描述：设置基础镜像\n语法：FROM < image>[:< tag> | @< digest>]\n提示：镜像都是从一个基础镜像（操作系统或其他镜像）生成，可以在一个Dockerfile中添加多条FROM指令，一次生成多个镜像\n注意：如果忽略tag选项，会使用latest镜像\n\n### 指令：MAINTAINER\n\n功能描述：设置镜像作者\n语法：MAINTAINER < name>\n\n\n### 指令：RUN\n\n功能描述：\n语法：RUN < command>\n          RUN [“executable”,”param1”,”param2”]\n提示：RUN指令会生成容器，在容器中执行脚本，容器使用当前镜像，脚本指令完成后，Docker Daemon会将该容器提交为一个中间镜像，供后面的指令使用\n补充：RUN指令第一种方式为shell方式，使用/bin/sh -c < command>运行脚本，可以在其中使用\\将脚本分为多行\n          RUN指令第二种方式为exec方式，镜像中没有/bin/sh或者要使用其他shell时使用该方式，其不会调用shell命令\n例子：RUN source $HOME/.bashrc;\\\n          echo $HOME\n\n          RUN [“/bin/bash”,”-c”,”echo hello”]\n\n          RUN [“sh”,”-c”,”echo”,”$HOME”] 使用第二种方式调用shell读取环境变量\n\n### 指令：CMD\n\n功能描述：设置容器的启动命令\n语法：CMD [“executable”,”param1”,”param2”]\n          CMD [“param1”,”param2”]\n          CMD < command>\n提示：CMD第一种、第三种方式和RUN类似，第二种方式为ENTRYPOINT参数方式，为entrypoint提供参数列表\n注意：**Dockerfile中只能有一条CMD命令，如果写了多条则最后一条生效**\n\n### 指令：LABEL\n功能描述：设置镜像的标签\n延伸：镜像标签可以通过docker inspect查看\n格式：LABEL < key>=< value> < key>=< value> …\n提示：不同标签之间通过空格隔开\n注意：每条指令都会生成一个镜像层，Docker中镜像最多只能有127层，如果超出Docker Daemon就会报错，如LABEL ..=.. <假装这里有个换行> LABEL ..=..合在一起用空格分隔就可以减少镜像层数量，同样，可以使用连接符\\将脚本分为多行镜像会继承基础镜像中的标签，如果存在同名标签则会覆盖\n\n### 指令：EXPOSE\n功能描述：设置镜像暴露端口，记录容器启动时监听哪些端口\n语法：EXPOSE < port> < port> …\n延伸：镜像暴露端口可以通过docker inspect查看\n提示：容器启动时，Docker Daemon会扫描镜像中暴露的端口，如果加入-P参数，Docker Daemon会把镜像中所有暴露端口导出，并为每个暴露端口分配一个随机的主机端口（暴露端口是容器监听端口，主机端口为外部访问容器的端口）\n注意：EXPOSE只设置暴露端口并不导出端口，只有启动容器时使用-P/-p才导出端口，这个时候才能通过外部访问容器提供的服务\n\n### 指令：ENV\n功能描述：设置镜像中的环境变量\n语法：ENV < key>=< value>…|< key> < value>\n注意：环境变量在整个编译周期都有效，第一种方式可设置多个环境变量，第二种方式只设置一个环境变量\n提示：通过${变量名}或者 $变量名使用变量，使用方式${变量名}时可以用${变量名:-default} ${变量名:+cover}设定默认值或者覆盖值 ENV设置的变量值在整个编译过程中总是保持不变的\n\n### 指令：ADD\n功能描述：复制文件到镜像中\n语法：ADD < src>… < dest>|[“< src>”,… “< dest>”]\n注意：当路径中有空格时，需要使用第二种方式\n          当src为文件或目录时，Docker Daemon会从编译目录寻找这些文件或目录，而dest为镜像中的绝对路径或者相对于WORKDIR的路径\n提示：src为目录时，复制目录中所有内容，包括文件系统的元数据，但不包括目录本身\n          src为压缩文件，并且压缩方式为gzip,bzip2或xz时，指令会将其解压为目录\n          如果src为文件，则复制文件和元数据\n          如果dest不存在，指令会自动创建dest和缺失的上级目录\n\n### 指令：COPY\n功能描述：复制文件到镜像中\n语法：COPY < src>… < dest>|[“< src>”,… “< dest>”]\n提示：指令逻辑和ADD十分相似，同样Docker Daemon会从编译目录寻找文件或目录，dest为镜像中的绝对路径或者相对于WORKDIR的路径\n\n### 指令：ENTRYPOINT\n功能描述：设置容器的入口程序\n语法：ENTRYPOINT [“executable”,”param1”,”param2”]\n          ENTRYPOINT command param1 param2（shell方式）\n提示：入口程序是容器启动时执行的程序，docker run中最后的命令将作为参数传递给入口程序\n          入口程序有两种格式：exec、shell，其中shell使用/bin/sh -c运行入口程序，此时入口程序不能接收信号量\n          当Dockerfile有多条ENTRYPOINT时只有最后的ENTRYPOINT指令生效\n          如果使用脚本作为入口程序，需要保证脚本的最后一个程序能够接收信号量，可以在脚本最后使用exec或gosu启动传入脚本的命令\n注意：通过shell方式启动入口程序时，会忽略CMD指令和docker run中的参数\n          为了保证容器能够接受docker stop发送的信号量，需要通过exec启动程序；如果没有加入exec命令，则在启动容器时容器会出现两个进程，并且使用docker stop命令容器无法正常退出（无法接受SIGTERM信号），超时后docker stop发送SIGKILL，强制停止容器\n例子：FROM ubuntu <换行> ENTRYPOINT exec top -b\n\n### 指令：VOLUME\n功能描述：设置容器的挂载点\n语法：VOLUME [“/data”]\n          VOLUME /data1 /data2\n提示：启动容器时，Docker Daemon会新建挂载点，并用镜像中的数据初始化挂载点，可以将主机目录或数据卷容器挂载到这些挂载点\n\n### 指令：USER\n\n功能描述：设置RUN CMD ENTRYPOINT的用户名或UID\n语法：USER < name>\n\n### 指令：WORKDIR\n\n功能描述：设置RUN CMD ENTRYPOINT ADD COPY指令的工作目录\n语法：WORKDIR < Path>\n提示：如果工作目录不存在，则Docker Daemon会自动创建\n          Dockerfile中多个地方都可以调用WORKDIR，如果后面跟的是相对位置，则会跟在上条WORKDIR指定路径后（如WORKDIR /A   WORKDIR B   WORKDIR C，最终路径为/A/B/C\n\n### 指令：ARG\n\n功能描述：设置编译变量\n语法：ARG < name>[=< defaultValue>]\n注意：ARG从定义它的地方开始生效而不是调用的地方，在ARG之前调用编译变量总为空，在编译镜像时，可以通过docker build –build-arg < var>=< value>设置变量，如果var没有通过ARG定义则Daemon会报错\n          可以使用ENV或ARG设置RUN使用的变量，如果同名则ENV定义的值会覆盖ARG定义的值，与ENV不同，ARG的变量值在编译过程中是可变的，会对比使用编译缓存造成影响（ARG值不同则编译过程也不同）\n例子：ARG CONT_IMAG_VER <换行> RUN echo $CONT_IMG_VER\n          ARG CONT_IMAG_VER <换行> RUN echo hello\n          当编译时给ARG变量赋值hello，则两个Dockerfile可以使用相同的中间镜像，如果不为hello，则不能使用同一个中间镜像\n\n\n\n### CMD ENTRYPOINT和RUN的区别\nRUN指令是设置编译镜像时执行的脚本和程序，镜像编译完成后，RUN指令的生命周期结束  \n容器启动时，可以通过CMD和ENTRYPOINT设置启动项，其中CMD叫做容器默认启动命令，如果在docker run命令末尾添加command，则会替换镜像中CMD设置的启动程序；ENRTYPOINT叫做入口程序，不能被docker run命令末尾的command替换，而是将command当作字符串，传递给ENTRYPOINT作为参数\n","slug":"技术/docker/Dockerfile基础&编写","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/docker/Dockerfile基础&编写","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd311000b38pw5wrfcfcm","content":"<h1 id=\"Dockerfile基础\"><a href=\"#Dockerfile基础\" class=\"headerlink\" title=\"Dockerfile基础\"></a>Dockerfile基础</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Dockerfile用于指定docker build 指令时执行时的动作与资源划分等,如指定分配内存大小,cpu个数等.从而构成相应的镜像</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"详细指令\"><a href=\"#详细指令\" class=\"headerlink\" title=\"详细指令\"></a>详细指令</h2><p><img src=\"http://img.wqkenqing.ren/5aaf3a25b3434c7b1ec0cd5b41b2be1a.png\" alt=\"docker详细指令\"></p>\n<p>如上图即docker主要的指令与作用</p>\n<p>Dockerfile由多条指令组成,每条指令在编译镜像时执行相应的程序完成某些功能,指令+参数组成，以逗号分隔，#作为注释起始符，虽说指令不区分大小写，但是一般指令使用大些，参数使用小写.</p>\n<h3 id=\"指令：FROM\"><a href=\"#指令：FROM\" class=\"headerlink\" title=\"指令：FROM\"></a>指令：FROM</h3><p>功能描述：设置基础镜像<br>语法：FROM &lt; image&gt;[:&lt; tag&gt; | @&lt; digest&gt;]<br>提示：镜像都是从一个基础镜像（操作系统或其他镜像）生成，可以在一个Dockerfile中添加多条FROM指令，一次生成多个镜像<br>注意：如果忽略tag选项，会使用latest镜像</p>\n<h3 id=\"指令：MAINTAINER\"><a href=\"#指令：MAINTAINER\" class=\"headerlink\" title=\"指令：MAINTAINER\"></a>指令：MAINTAINER</h3><p>功能描述：设置镜像作者<br>语法：MAINTAINER &lt; name&gt;</p>\n<h3 id=\"指令：RUN\"><a href=\"#指令：RUN\" class=\"headerlink\" title=\"指令：RUN\"></a>指令：RUN</h3><p>功能描述：<br>语法：RUN &lt; command&gt;<br>          RUN [“executable”,”param1”,”param2”]<br>提示：RUN指令会生成容器，在容器中执行脚本，容器使用当前镜像，脚本指令完成后，Docker Daemon会将该容器提交为一个中间镜像，供后面的指令使用<br>补充：RUN指令第一种方式为shell方式，使用/bin/sh -c &lt; command&gt;运行脚本，可以在其中使用\\将脚本分为多行<br>          RUN指令第二种方式为exec方式，镜像中没有/bin/sh或者要使用其他shell时使用该方式，其不会调用shell命令<br>例子：RUN source $HOME/.bashrc;<br>          echo $HOME</p>\n<pre><code>RUN [“/bin/bash”,”-c”,”echo hello”]\n\nRUN [“sh”,”-c”,”echo”,”$HOME”] 使用第二种方式调用shell读取环境变量</code></pre><h3 id=\"指令：CMD\"><a href=\"#指令：CMD\" class=\"headerlink\" title=\"指令：CMD\"></a>指令：CMD</h3><p>功能描述：设置容器的启动命令<br>语法：CMD [“executable”,”param1”,”param2”]<br>          CMD [“param1”,”param2”]<br>          CMD &lt; command&gt;<br>提示：CMD第一种、第三种方式和RUN类似，第二种方式为ENTRYPOINT参数方式，为entrypoint提供参数列表<br>注意：<strong>Dockerfile中只能有一条CMD命令，如果写了多条则最后一条生效</strong></p>\n<h3 id=\"指令：LABEL\"><a href=\"#指令：LABEL\" class=\"headerlink\" title=\"指令：LABEL\"></a>指令：LABEL</h3><p>功能描述：设置镜像的标签<br>延伸：镜像标签可以通过docker inspect查看<br>格式：LABEL &lt; key&gt;=&lt; value&gt; &lt; key&gt;=&lt; value&gt; …<br>提示：不同标签之间通过空格隔开<br>注意：每条指令都会生成一个镜像层，Docker中镜像最多只能有127层，如果超出Docker Daemon就会报错，如LABEL ..=.. &lt;假装这里有个换行&gt; LABEL ..=..合在一起用空格分隔就可以减少镜像层数量，同样，可以使用连接符\\将脚本分为多行镜像会继承基础镜像中的标签，如果存在同名标签则会覆盖</p>\n<h3 id=\"指令：EXPOSE\"><a href=\"#指令：EXPOSE\" class=\"headerlink\" title=\"指令：EXPOSE\"></a>指令：EXPOSE</h3><p>功能描述：设置镜像暴露端口，记录容器启动时监听哪些端口<br>语法：EXPOSE &lt; port&gt; &lt; port&gt; …<br>延伸：镜像暴露端口可以通过docker inspect查看<br>提示：容器启动时，Docker Daemon会扫描镜像中暴露的端口，如果加入-P参数，Docker Daemon会把镜像中所有暴露端口导出，并为每个暴露端口分配一个随机的主机端口（暴露端口是容器监听端口，主机端口为外部访问容器的端口）<br>注意：EXPOSE只设置暴露端口并不导出端口，只有启动容器时使用-P/-p才导出端口，这个时候才能通过外部访问容器提供的服务</p>\n<h3 id=\"指令：ENV\"><a href=\"#指令：ENV\" class=\"headerlink\" title=\"指令：ENV\"></a>指令：ENV</h3><p>功能描述：设置镜像中的环境变量<br>语法：ENV &lt; key&gt;=&lt; value&gt;…|&lt; key&gt; &lt; value&gt;<br>注意：环境变量在整个编译周期都有效，第一种方式可设置多个环境变量，第二种方式只设置一个环境变量<br>提示：通过${变量名}或者 $变量名使用变量，使用方式${变量名}时可以用${变量名:-default} ${变量名:+cover}设定默认值或者覆盖值 ENV设置的变量值在整个编译过程中总是保持不变的</p>\n<h3 id=\"指令：ADD\"><a href=\"#指令：ADD\" class=\"headerlink\" title=\"指令：ADD\"></a>指令：ADD</h3><p>功能描述：复制文件到镜像中<br>语法：ADD &lt; src&gt;… &lt; dest&gt;|[“&lt; src&gt;”,… “&lt; dest&gt;”]<br>注意：当路径中有空格时，需要使用第二种方式<br>          当src为文件或目录时，Docker Daemon会从编译目录寻找这些文件或目录，而dest为镜像中的绝对路径或者相对于WORKDIR的路径<br>提示：src为目录时，复制目录中所有内容，包括文件系统的元数据，但不包括目录本身<br>          src为压缩文件，并且压缩方式为gzip,bzip2或xz时，指令会将其解压为目录<br>          如果src为文件，则复制文件和元数据<br>          如果dest不存在，指令会自动创建dest和缺失的上级目录</p>\n<h3 id=\"指令：COPY\"><a href=\"#指令：COPY\" class=\"headerlink\" title=\"指令：COPY\"></a>指令：COPY</h3><p>功能描述：复制文件到镜像中<br>语法：COPY &lt; src&gt;… &lt; dest&gt;|[“&lt; src&gt;”,… “&lt; dest&gt;”]<br>提示：指令逻辑和ADD十分相似，同样Docker Daemon会从编译目录寻找文件或目录，dest为镜像中的绝对路径或者相对于WORKDIR的路径</p>\n<h3 id=\"指令：ENTRYPOINT\"><a href=\"#指令：ENTRYPOINT\" class=\"headerlink\" title=\"指令：ENTRYPOINT\"></a>指令：ENTRYPOINT</h3><p>功能描述：设置容器的入口程序<br>语法：ENTRYPOINT [“executable”,”param1”,”param2”]<br>          ENTRYPOINT command param1 param2（shell方式）<br>提示：入口程序是容器启动时执行的程序，docker run中最后的命令将作为参数传递给入口程序<br>          入口程序有两种格式：exec、shell，其中shell使用/bin/sh -c运行入口程序，此时入口程序不能接收信号量<br>          当Dockerfile有多条ENTRYPOINT时只有最后的ENTRYPOINT指令生效<br>          如果使用脚本作为入口程序，需要保证脚本的最后一个程序能够接收信号量，可以在脚本最后使用exec或gosu启动传入脚本的命令<br>注意：通过shell方式启动入口程序时，会忽略CMD指令和docker run中的参数<br>          为了保证容器能够接受docker stop发送的信号量，需要通过exec启动程序；如果没有加入exec命令，则在启动容器时容器会出现两个进程，并且使用docker stop命令容器无法正常退出（无法接受SIGTERM信号），超时后docker stop发送SIGKILL，强制停止容器<br>例子：FROM ubuntu &lt;换行&gt; ENTRYPOINT exec top -b</p>\n<h3 id=\"指令：VOLUME\"><a href=\"#指令：VOLUME\" class=\"headerlink\" title=\"指令：VOLUME\"></a>指令：VOLUME</h3><p>功能描述：设置容器的挂载点<br>语法：VOLUME [“/data”]<br>          VOLUME /data1 /data2<br>提示：启动容器时，Docker Daemon会新建挂载点，并用镜像中的数据初始化挂载点，可以将主机目录或数据卷容器挂载到这些挂载点</p>\n<h3 id=\"指令：USER\"><a href=\"#指令：USER\" class=\"headerlink\" title=\"指令：USER\"></a>指令：USER</h3><p>功能描述：设置RUN CMD ENTRYPOINT的用户名或UID<br>语法：USER &lt; name&gt;</p>\n<h3 id=\"指令：WORKDIR\"><a href=\"#指令：WORKDIR\" class=\"headerlink\" title=\"指令：WORKDIR\"></a>指令：WORKDIR</h3><p>功能描述：设置RUN CMD ENTRYPOINT ADD COPY指令的工作目录<br>语法：WORKDIR &lt; Path&gt;<br>提示：如果工作目录不存在，则Docker Daemon会自动创建<br>          Dockerfile中多个地方都可以调用WORKDIR，如果后面跟的是相对位置，则会跟在上条WORKDIR指定路径后（如WORKDIR /A   WORKDIR B   WORKDIR C，最终路径为/A/B/C</p>\n<h3 id=\"指令：ARG\"><a href=\"#指令：ARG\" class=\"headerlink\" title=\"指令：ARG\"></a>指令：ARG</h3><p>功能描述：设置编译变量<br>语法：ARG &lt; name&gt;[=&lt; defaultValue&gt;]<br>注意：ARG从定义它的地方开始生效而不是调用的地方，在ARG之前调用编译变量总为空，在编译镜像时，可以通过docker build –build-arg &lt; var&gt;=&lt; value&gt;设置变量，如果var没有通过ARG定义则Daemon会报错<br>          可以使用ENV或ARG设置RUN使用的变量，如果同名则ENV定义的值会覆盖ARG定义的值，与ENV不同，ARG的变量值在编译过程中是可变的，会对比使用编译缓存造成影响（ARG值不同则编译过程也不同）<br>例子：ARG CONT_IMAG_VER &lt;换行&gt; RUN echo $CONT_IMG_VER<br>          ARG CONT_IMAG_VER &lt;换行&gt; RUN echo hello<br>          当编译时给ARG变量赋值hello，则两个Dockerfile可以使用相同的中间镜像，如果不为hello，则不能使用同一个中间镜像</p>\n<h3 id=\"CMD-ENTRYPOINT和RUN的区别\"><a href=\"#CMD-ENTRYPOINT和RUN的区别\" class=\"headerlink\" title=\"CMD ENTRYPOINT和RUN的区别\"></a>CMD ENTRYPOINT和RUN的区别</h3><p>RUN指令是设置编译镜像时执行的脚本和程序，镜像编译完成后，RUN指令的生命周期结束<br>容器启动时，可以通过CMD和ENTRYPOINT设置启动项，其中CMD叫做容器默认启动命令，如果在docker run命令末尾添加command，则会替换镜像中CMD设置的启动程序；ENRTYPOINT叫做入口程序，不能被docker run命令末尾的command替换，而是将command当作字符串，传递给ENTRYPOINT作为参数</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Dockerfile基础\"><a href=\"#Dockerfile基础\" class=\"headerlink\" title=\"Dockerfile基础\"></a>Dockerfile基础</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Dockerfile用于指定docker build 指令时执行时的动作与资源划分等,如指定分配内存大小,cpu个数等.从而构成相应的镜像</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"详细指令\"><a href=\"#详细指令\" class=\"headerlink\" title=\"详细指令\"></a>详细指令</h2><p><img src=\"http://img.wqkenqing.ren/5aaf3a25b3434c7b1ec0cd5b41b2be1a.png\" alt=\"docker详细指令\"></p>\n<p>如上图即docker主要的指令与作用</p>\n<p>Dockerfile由多条指令组成,每条指令在编译镜像时执行相应的程序完成某些功能,指令+参数组成，以逗号分隔，#作为注释起始符，虽说指令不区分大小写，但是一般指令使用大些，参数使用小写.</p>\n<h3 id=\"指令：FROM\"><a href=\"#指令：FROM\" class=\"headerlink\" title=\"指令：FROM\"></a>指令：FROM</h3><p>功能描述：设置基础镜像<br>语法：FROM &lt; image&gt;[:&lt; tag&gt; | @&lt; digest&gt;]<br>提示：镜像都是从一个基础镜像（操作系统或其他镜像）生成，可以在一个Dockerfile中添加多条FROM指令，一次生成多个镜像<br>注意：如果忽略tag选项，会使用latest镜像</p>\n<h3 id=\"指令：MAINTAINER\"><a href=\"#指令：MAINTAINER\" class=\"headerlink\" title=\"指令：MAINTAINER\"></a>指令：MAINTAINER</h3><p>功能描述：设置镜像作者<br>语法：MAINTAINER &lt; name&gt;</p>\n<h3 id=\"指令：RUN\"><a href=\"#指令：RUN\" class=\"headerlink\" title=\"指令：RUN\"></a>指令：RUN</h3><p>功能描述：<br>语法：RUN &lt; command&gt;<br>          RUN [“executable”,”param1”,”param2”]<br>提示：RUN指令会生成容器，在容器中执行脚本，容器使用当前镜像，脚本指令完成后，Docker Daemon会将该容器提交为一个中间镜像，供后面的指令使用<br>补充：RUN指令第一种方式为shell方式，使用/bin/sh -c &lt; command&gt;运行脚本，可以在其中使用\\将脚本分为多行<br>          RUN指令第二种方式为exec方式，镜像中没有/bin/sh或者要使用其他shell时使用该方式，其不会调用shell命令<br>例子：RUN source $HOME/.bashrc;<br>          echo $HOME</p>\n<pre><code>RUN [“/bin/bash”,”-c”,”echo hello”]\n\nRUN [“sh”,”-c”,”echo”,”$HOME”] 使用第二种方式调用shell读取环境变量</code></pre><h3 id=\"指令：CMD\"><a href=\"#指令：CMD\" class=\"headerlink\" title=\"指令：CMD\"></a>指令：CMD</h3><p>功能描述：设置容器的启动命令<br>语法：CMD [“executable”,”param1”,”param2”]<br>          CMD [“param1”,”param2”]<br>          CMD &lt; command&gt;<br>提示：CMD第一种、第三种方式和RUN类似，第二种方式为ENTRYPOINT参数方式，为entrypoint提供参数列表<br>注意：<strong>Dockerfile中只能有一条CMD命令，如果写了多条则最后一条生效</strong></p>\n<h3 id=\"指令：LABEL\"><a href=\"#指令：LABEL\" class=\"headerlink\" title=\"指令：LABEL\"></a>指令：LABEL</h3><p>功能描述：设置镜像的标签<br>延伸：镜像标签可以通过docker inspect查看<br>格式：LABEL &lt; key&gt;=&lt; value&gt; &lt; key&gt;=&lt; value&gt; …<br>提示：不同标签之间通过空格隔开<br>注意：每条指令都会生成一个镜像层，Docker中镜像最多只能有127层，如果超出Docker Daemon就会报错，如LABEL ..=.. &lt;假装这里有个换行&gt; LABEL ..=..合在一起用空格分隔就可以减少镜像层数量，同样，可以使用连接符\\将脚本分为多行镜像会继承基础镜像中的标签，如果存在同名标签则会覆盖</p>\n<h3 id=\"指令：EXPOSE\"><a href=\"#指令：EXPOSE\" class=\"headerlink\" title=\"指令：EXPOSE\"></a>指令：EXPOSE</h3><p>功能描述：设置镜像暴露端口，记录容器启动时监听哪些端口<br>语法：EXPOSE &lt; port&gt; &lt; port&gt; …<br>延伸：镜像暴露端口可以通过docker inspect查看<br>提示：容器启动时，Docker Daemon会扫描镜像中暴露的端口，如果加入-P参数，Docker Daemon会把镜像中所有暴露端口导出，并为每个暴露端口分配一个随机的主机端口（暴露端口是容器监听端口，主机端口为外部访问容器的端口）<br>注意：EXPOSE只设置暴露端口并不导出端口，只有启动容器时使用-P/-p才导出端口，这个时候才能通过外部访问容器提供的服务</p>\n<h3 id=\"指令：ENV\"><a href=\"#指令：ENV\" class=\"headerlink\" title=\"指令：ENV\"></a>指令：ENV</h3><p>功能描述：设置镜像中的环境变量<br>语法：ENV &lt; key&gt;=&lt; value&gt;…|&lt; key&gt; &lt; value&gt;<br>注意：环境变量在整个编译周期都有效，第一种方式可设置多个环境变量，第二种方式只设置一个环境变量<br>提示：通过${变量名}或者 $变量名使用变量，使用方式${变量名}时可以用${变量名:-default} ${变量名:+cover}设定默认值或者覆盖值 ENV设置的变量值在整个编译过程中总是保持不变的</p>\n<h3 id=\"指令：ADD\"><a href=\"#指令：ADD\" class=\"headerlink\" title=\"指令：ADD\"></a>指令：ADD</h3><p>功能描述：复制文件到镜像中<br>语法：ADD &lt; src&gt;… &lt; dest&gt;|[“&lt; src&gt;”,… “&lt; dest&gt;”]<br>注意：当路径中有空格时，需要使用第二种方式<br>          当src为文件或目录时，Docker Daemon会从编译目录寻找这些文件或目录，而dest为镜像中的绝对路径或者相对于WORKDIR的路径<br>提示：src为目录时，复制目录中所有内容，包括文件系统的元数据，但不包括目录本身<br>          src为压缩文件，并且压缩方式为gzip,bzip2或xz时，指令会将其解压为目录<br>          如果src为文件，则复制文件和元数据<br>          如果dest不存在，指令会自动创建dest和缺失的上级目录</p>\n<h3 id=\"指令：COPY\"><a href=\"#指令：COPY\" class=\"headerlink\" title=\"指令：COPY\"></a>指令：COPY</h3><p>功能描述：复制文件到镜像中<br>语法：COPY &lt; src&gt;… &lt; dest&gt;|[“&lt; src&gt;”,… “&lt; dest&gt;”]<br>提示：指令逻辑和ADD十分相似，同样Docker Daemon会从编译目录寻找文件或目录，dest为镜像中的绝对路径或者相对于WORKDIR的路径</p>\n<h3 id=\"指令：ENTRYPOINT\"><a href=\"#指令：ENTRYPOINT\" class=\"headerlink\" title=\"指令：ENTRYPOINT\"></a>指令：ENTRYPOINT</h3><p>功能描述：设置容器的入口程序<br>语法：ENTRYPOINT [“executable”,”param1”,”param2”]<br>          ENTRYPOINT command param1 param2（shell方式）<br>提示：入口程序是容器启动时执行的程序，docker run中最后的命令将作为参数传递给入口程序<br>          入口程序有两种格式：exec、shell，其中shell使用/bin/sh -c运行入口程序，此时入口程序不能接收信号量<br>          当Dockerfile有多条ENTRYPOINT时只有最后的ENTRYPOINT指令生效<br>          如果使用脚本作为入口程序，需要保证脚本的最后一个程序能够接收信号量，可以在脚本最后使用exec或gosu启动传入脚本的命令<br>注意：通过shell方式启动入口程序时，会忽略CMD指令和docker run中的参数<br>          为了保证容器能够接受docker stop发送的信号量，需要通过exec启动程序；如果没有加入exec命令，则在启动容器时容器会出现两个进程，并且使用docker stop命令容器无法正常退出（无法接受SIGTERM信号），超时后docker stop发送SIGKILL，强制停止容器<br>例子：FROM ubuntu &lt;换行&gt; ENTRYPOINT exec top -b</p>\n<h3 id=\"指令：VOLUME\"><a href=\"#指令：VOLUME\" class=\"headerlink\" title=\"指令：VOLUME\"></a>指令：VOLUME</h3><p>功能描述：设置容器的挂载点<br>语法：VOLUME [“/data”]<br>          VOLUME /data1 /data2<br>提示：启动容器时，Docker Daemon会新建挂载点，并用镜像中的数据初始化挂载点，可以将主机目录或数据卷容器挂载到这些挂载点</p>\n<h3 id=\"指令：USER\"><a href=\"#指令：USER\" class=\"headerlink\" title=\"指令：USER\"></a>指令：USER</h3><p>功能描述：设置RUN CMD ENTRYPOINT的用户名或UID<br>语法：USER &lt; name&gt;</p>\n<h3 id=\"指令：WORKDIR\"><a href=\"#指令：WORKDIR\" class=\"headerlink\" title=\"指令：WORKDIR\"></a>指令：WORKDIR</h3><p>功能描述：设置RUN CMD ENTRYPOINT ADD COPY指令的工作目录<br>语法：WORKDIR &lt; Path&gt;<br>提示：如果工作目录不存在，则Docker Daemon会自动创建<br>          Dockerfile中多个地方都可以调用WORKDIR，如果后面跟的是相对位置，则会跟在上条WORKDIR指定路径后（如WORKDIR /A   WORKDIR B   WORKDIR C，最终路径为/A/B/C</p>\n<h3 id=\"指令：ARG\"><a href=\"#指令：ARG\" class=\"headerlink\" title=\"指令：ARG\"></a>指令：ARG</h3><p>功能描述：设置编译变量<br>语法：ARG &lt; name&gt;[=&lt; defaultValue&gt;]<br>注意：ARG从定义它的地方开始生效而不是调用的地方，在ARG之前调用编译变量总为空，在编译镜像时，可以通过docker build –build-arg &lt; var&gt;=&lt; value&gt;设置变量，如果var没有通过ARG定义则Daemon会报错<br>          可以使用ENV或ARG设置RUN使用的变量，如果同名则ENV定义的值会覆盖ARG定义的值，与ENV不同，ARG的变量值在编译过程中是可变的，会对比使用编译缓存造成影响（ARG值不同则编译过程也不同）<br>例子：ARG CONT_IMAG_VER &lt;换行&gt; RUN echo $CONT_IMG_VER<br>          ARG CONT_IMAG_VER &lt;换行&gt; RUN echo hello<br>          当编译时给ARG变量赋值hello，则两个Dockerfile可以使用相同的中间镜像，如果不为hello，则不能使用同一个中间镜像</p>\n<h3 id=\"CMD-ENTRYPOINT和RUN的区别\"><a href=\"#CMD-ENTRYPOINT和RUN的区别\" class=\"headerlink\" title=\"CMD ENTRYPOINT和RUN的区别\"></a>CMD ENTRYPOINT和RUN的区别</h3><p>RUN指令是设置编译镜像时执行的脚本和程序，镜像编译完成后，RUN指令的生命周期结束<br>容器启动时，可以通过CMD和ENTRYPOINT设置启动项，其中CMD叫做容器默认启动命令，如果在docker run命令末尾添加command，则会替换镜像中CMD设置的启动程序；ENRTYPOINT叫做入口程序，不能被docker run命令末尾的command替换，而是将command当作字符串，传递给ENTRYPOINT作为参数</p>\n"},{"_content":"docker run -d --restart=unless-stopped  -v /data/upload/newInfo.txt:/newInfo.txt    registry.lisong.pub:5000/kafka_tmp\ndocker run -d --restart=unless-stopped  -v /Users/wqkenqing/Desktop/newInfo.txt:/newInfo.txt    kafka_tmp\ndocker run -d --restart=unless-stopped  -v /data/upload/newInfo.txt:/newInfo.txt    registry.lisong.pub:5000/kafka_tmp\ndocker run  -d  --restart=unless-stopped -v /data/upload/state_info.txt:/state_info.txt    registry.lisong.pub:5000/kafka_tmpn\ndocker run -d --restart=unless-stopped  -v /data/upload/newInfo.txt:/newInfo.txt    registry.lisong.pub:5000/kafka_tmp\ndocker run -d --restart=unless-stopped  -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka_all /info.txt jzw_toll_island_infonn\n\n\ndocker run -d --restart=unless-stopped  -v /Users/wqkenqing/Desktop/newInfo.txt:/info.txt    kafka_all /info.txt jzw_toll_island_infonn\ndocker run   -v /Users/wqkenqing/Desktop/newInfo.txt:/info.txt    kafka_all /info.txt jzw_toll_island_infonn\ndocker run  -d --restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka_all /info.txt jzw_toll_island_infonn\ndocker run  -d --restart=unless-stopped -v /data/upload/state_info.txt:/info.txt   registry.lisong.pub:5000/kafka_all /info.txt jzw_toll_island_state\ndocker run  -d --restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_infonn\ndocker run  -d -v /Users/wqkenqing/Desktop/newInfo.txt:/info.txt    kafk214 /info.txt jzw_toll_island_infonn\n\n\n214\n\ndocker run  -d --name info  --restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_infonn\n\ndocker run  -d --name info  --restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_info\n\ndocker run  -d --name state --restart=unless-stopped -v /data/upload/state_info.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_state\n","source":"_posts/技术/docker/docker操作.md","raw":"docker run -d --restart=unless-stopped  -v /data/upload/newInfo.txt:/newInfo.txt    registry.lisong.pub:5000/kafka_tmp\ndocker run -d --restart=unless-stopped  -v /Users/wqkenqing/Desktop/newInfo.txt:/newInfo.txt    kafka_tmp\ndocker run -d --restart=unless-stopped  -v /data/upload/newInfo.txt:/newInfo.txt    registry.lisong.pub:5000/kafka_tmp\ndocker run  -d  --restart=unless-stopped -v /data/upload/state_info.txt:/state_info.txt    registry.lisong.pub:5000/kafka_tmpn\ndocker run -d --restart=unless-stopped  -v /data/upload/newInfo.txt:/newInfo.txt    registry.lisong.pub:5000/kafka_tmp\ndocker run -d --restart=unless-stopped  -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka_all /info.txt jzw_toll_island_infonn\n\n\ndocker run -d --restart=unless-stopped  -v /Users/wqkenqing/Desktop/newInfo.txt:/info.txt    kafka_all /info.txt jzw_toll_island_infonn\ndocker run   -v /Users/wqkenqing/Desktop/newInfo.txt:/info.txt    kafka_all /info.txt jzw_toll_island_infonn\ndocker run  -d --restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka_all /info.txt jzw_toll_island_infonn\ndocker run  -d --restart=unless-stopped -v /data/upload/state_info.txt:/info.txt   registry.lisong.pub:5000/kafka_all /info.txt jzw_toll_island_state\ndocker run  -d --restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_infonn\ndocker run  -d -v /Users/wqkenqing/Desktop/newInfo.txt:/info.txt    kafk214 /info.txt jzw_toll_island_infonn\n\n\n214\n\ndocker run  -d --name info  --restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_infonn\n\ndocker run  -d --name info  --restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_info\n\ndocker run  -d --name state --restart=unless-stopped -v /data/upload/state_info.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_state\n","slug":"技术/docker/docker操作","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/docker/docker操作","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd313000c38pwccyjh3kx","content":"<p>docker run -d –restart=unless-stopped  -v /data/upload/newInfo.txt:/newInfo.txt    registry.lisong.pub:5000/kafka_tmp<br>docker run -d –restart=unless-stopped  -v /Users/wqkenqing/Desktop/newInfo.txt:/newInfo.txt    kafka_tmp<br>docker run -d –restart=unless-stopped  -v /data/upload/newInfo.txt:/newInfo.txt    registry.lisong.pub:5000/kafka_tmp<br>docker run  -d  –restart=unless-stopped -v /data/upload/state_info.txt:/state_info.txt    registry.lisong.pub:5000/kafka_tmpn<br>docker run -d –restart=unless-stopped  -v /data/upload/newInfo.txt:/newInfo.txt    registry.lisong.pub:5000/kafka_tmp<br>docker run -d –restart=unless-stopped  -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka_all /info.txt jzw_toll_island_infonn</p>\n<p>docker run -d –restart=unless-stopped  -v /Users/wqkenqing/Desktop/newInfo.txt:/info.txt    kafka_all /info.txt jzw_toll_island_infonn<br>docker run   -v /Users/wqkenqing/Desktop/newInfo.txt:/info.txt    kafka_all /info.txt jzw_toll_island_infonn<br>docker run  -d –restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka_all /info.txt jzw_toll_island_infonn<br>docker run  -d –restart=unless-stopped -v /data/upload/state_info.txt:/info.txt   registry.lisong.pub:5000/kafka_all /info.txt jzw_toll_island_state<br>docker run  -d –restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_infonn<br>docker run  -d -v /Users/wqkenqing/Desktop/newInfo.txt:/info.txt    kafk214 /info.txt jzw_toll_island_infonn</p>\n<p>214</p>\n<p>docker run  -d –name info  –restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_infonn</p>\n<p>docker run  -d –name info  –restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_info</p>\n<p>docker run  -d –name state –restart=unless-stopped -v /data/upload/state_info.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_state</p>\n","site":{"data":{}},"excerpt":"","more":"<p>docker run -d –restart=unless-stopped  -v /data/upload/newInfo.txt:/newInfo.txt    registry.lisong.pub:5000/kafka_tmp<br>docker run -d –restart=unless-stopped  -v /Users/wqkenqing/Desktop/newInfo.txt:/newInfo.txt    kafka_tmp<br>docker run -d –restart=unless-stopped  -v /data/upload/newInfo.txt:/newInfo.txt    registry.lisong.pub:5000/kafka_tmp<br>docker run  -d  –restart=unless-stopped -v /data/upload/state_info.txt:/state_info.txt    registry.lisong.pub:5000/kafka_tmpn<br>docker run -d –restart=unless-stopped  -v /data/upload/newInfo.txt:/newInfo.txt    registry.lisong.pub:5000/kafka_tmp<br>docker run -d –restart=unless-stopped  -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka_all /info.txt jzw_toll_island_infonn</p>\n<p>docker run -d –restart=unless-stopped  -v /Users/wqkenqing/Desktop/newInfo.txt:/info.txt    kafka_all /info.txt jzw_toll_island_infonn<br>docker run   -v /Users/wqkenqing/Desktop/newInfo.txt:/info.txt    kafka_all /info.txt jzw_toll_island_infonn<br>docker run  -d –restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka_all /info.txt jzw_toll_island_infonn<br>docker run  -d –restart=unless-stopped -v /data/upload/state_info.txt:/info.txt   registry.lisong.pub:5000/kafka_all /info.txt jzw_toll_island_state<br>docker run  -d –restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_infonn<br>docker run  -d -v /Users/wqkenqing/Desktop/newInfo.txt:/info.txt    kafk214 /info.txt jzw_toll_island_infonn</p>\n<p>214</p>\n<p>docker run  -d –name info  –restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_infonn</p>\n<p>docker run  -d –name info  –restart=unless-stopped -v /data/upload/newInfo.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_info</p>\n<p>docker run  -d –name state –restart=unless-stopped -v /data/upload/state_info.txt:/info.txt    registry.lisong.pub:5000/kafka214 /info.txt jzw_toll_island_state</p>\n"},{"_content":"# docker 镜像\n\nhttp://hub-mirror.c.163.com\n\nhttps://reg-mirror.qiniu.com\n\nhttps://dockerhub.azk8s.cn\n\n\n## my mirror on aliyun\nregistry.cn-chengdu.aliyuncs.com/kuiq_wang/daily\n\n## company\n\nregistry.lisong.pub:5000\nregistry.lisong.pub:28500\n\n\n{\n  \"registry-mirrors\": [\"http://hub-mirror.c.163.com\"]\n}\n","source":"_posts/技术/docker/docker镜像.md","raw":"# docker 镜像\n\nhttp://hub-mirror.c.163.com\n\nhttps://reg-mirror.qiniu.com\n\nhttps://dockerhub.azk8s.cn\n\n\n## my mirror on aliyun\nregistry.cn-chengdu.aliyuncs.com/kuiq_wang/daily\n\n## company\n\nregistry.lisong.pub:5000\nregistry.lisong.pub:28500\n\n\n{\n  \"registry-mirrors\": [\"http://hub-mirror.c.163.com\"]\n}\n","slug":"技术/docker/docker镜像","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/docker/docker镜像","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd315000d38pwayes4p8n","content":"<h1 id=\"docker-镜像\"><a href=\"#docker-镜像\" class=\"headerlink\" title=\"docker 镜像\"></a>docker 镜像</h1><p><a href=\"http://hub-mirror.c.163.com\" target=\"_blank\" rel=\"noopener\">http://hub-mirror.c.163.com</a></p>\n<p><a href=\"https://reg-mirror.qiniu.com\" target=\"_blank\" rel=\"noopener\">https://reg-mirror.qiniu.com</a></p>\n<p><a href=\"https://dockerhub.azk8s.cn\" target=\"_blank\" rel=\"noopener\">https://dockerhub.azk8s.cn</a></p>\n<h2 id=\"my-mirror-on-aliyun\"><a href=\"#my-mirror-on-aliyun\" class=\"headerlink\" title=\"my mirror on aliyun\"></a>my mirror on aliyun</h2><p>registry.cn-chengdu.aliyuncs.com/kuiq_wang/daily</p>\n<h2 id=\"company\"><a href=\"#company\" class=\"headerlink\" title=\"company\"></a>company</h2><p>registry.lisong.pub:5000<br>registry.lisong.pub:28500</p>\n<p>{<br>  “registry-mirrors”: [“<a href=\"http://hub-mirror.c.163.com&quot;]\">http://hub-mirror.c.163.com&quot;]</a><br>}</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"docker-镜像\"><a href=\"#docker-镜像\" class=\"headerlink\" title=\"docker 镜像\"></a>docker 镜像</h1><p><a href=\"http://hub-mirror.c.163.com\" target=\"_blank\" rel=\"noopener\">http://hub-mirror.c.163.com</a></p>\n<p><a href=\"https://reg-mirror.qiniu.com\" target=\"_blank\" rel=\"noopener\">https://reg-mirror.qiniu.com</a></p>\n<p><a href=\"https://dockerhub.azk8s.cn\" target=\"_blank\" rel=\"noopener\">https://dockerhub.azk8s.cn</a></p>\n<h2 id=\"my-mirror-on-aliyun\"><a href=\"#my-mirror-on-aliyun\" class=\"headerlink\" title=\"my mirror on aliyun\"></a>my mirror on aliyun</h2><p>registry.cn-chengdu.aliyuncs.com/kuiq_wang/daily</p>\n<h2 id=\"company\"><a href=\"#company\" class=\"headerlink\" title=\"company\"></a>company</h2><p>registry.lisong.pub:5000<br>registry.lisong.pub:28500</p>\n<p>{<br>  “registry-mirrors”: [“<a href=\"http://hub-mirror.c.163.com&quot;]\">http://hub-mirror.c.163.com&quot;]</a><br>}</p>\n"},{"_content":"`flink相关记录`\n\n## yarn启动\n\n\n```js\nUsage:\n   Required\n     -n,--container <arg>   Number of YARN container to allocate (=Number of Task Managers)\n   Optional\n     -D <arg>                        Dynamic properties\n     -d,--detached                   Start detached\n     -jm,--jobManagerMemory <arg>    Memory for JobManager Container with optional unit (default: MB)\n     -nm,--name                      Set a custom name for the application on YARN\n     -q,--query                      Display available YARN resources (memory, cores)\n     -qu,--queue <arg>               Specify YARN queue.\n     -s,--slots <arg>                Number of slots per TaskManager\n     -tm,--taskManagerMemory <arg>   Memory per TaskManager Container with optional unit (default: MB)\n     -z,--zookeeperNamespace <arg>   Namespace to create the Zookeeper sub-paths for HA mode\n```\n\nyarn-session.sh -n 2 -jm 1024 -tm 1024 -d\n参数解释：\n```\n//-n 2 表示指定两个容器\n// -jm 1024 表示jobmanager 1024M内存\n// -tm 1024表示taskmanager 1024M内存\n//-d 任务后台运行\n//-nm,--name  YARN上为一个自定义的应用设置一个名字\n//-q,--query  显示yarn中可用的资源 (内存, cpu核数)\n//-z,--zookeeperNamespace <arg>   针对HA模式在zookeeper上创建NameSpace\n//-id,--applicationId <yarnAppId>   YARN集群上的任务id，附着到一个后台运行的yarn session中\n```\n**命令**:\n\n` yarn-session.sh -n 10 -tm 8192 -s 32`\n` yarn-session.sh -n 2 -tm 2048 -s 2`\n` yarn-session.sh -n 1 -tm 1024 -s 2`\n\nyarn-session.sh -n 1 -tm 1024 -s 2\nyarn-session.sh -n 2 -tm 4096 -jm  4096\n\nyarn-session.sh -id application_1563854073510_0006\n\nflink run -m yarn-cluster -yn 2 -yjm 4096 -ytm  4096\n\n\n2020年 01月 13日 星期一 16:39:12\n\nyarn-session.sh -n 2 -tm 4096 -jm  4096\n","source":"_posts/技术/flink/Flink记录.md","raw":"`flink相关记录`\n\n## yarn启动\n\n\n```js\nUsage:\n   Required\n     -n,--container <arg>   Number of YARN container to allocate (=Number of Task Managers)\n   Optional\n     -D <arg>                        Dynamic properties\n     -d,--detached                   Start detached\n     -jm,--jobManagerMemory <arg>    Memory for JobManager Container with optional unit (default: MB)\n     -nm,--name                      Set a custom name for the application on YARN\n     -q,--query                      Display available YARN resources (memory, cores)\n     -qu,--queue <arg>               Specify YARN queue.\n     -s,--slots <arg>                Number of slots per TaskManager\n     -tm,--taskManagerMemory <arg>   Memory per TaskManager Container with optional unit (default: MB)\n     -z,--zookeeperNamespace <arg>   Namespace to create the Zookeeper sub-paths for HA mode\n```\n\nyarn-session.sh -n 2 -jm 1024 -tm 1024 -d\n参数解释：\n```\n//-n 2 表示指定两个容器\n// -jm 1024 表示jobmanager 1024M内存\n// -tm 1024表示taskmanager 1024M内存\n//-d 任务后台运行\n//-nm,--name  YARN上为一个自定义的应用设置一个名字\n//-q,--query  显示yarn中可用的资源 (内存, cpu核数)\n//-z,--zookeeperNamespace <arg>   针对HA模式在zookeeper上创建NameSpace\n//-id,--applicationId <yarnAppId>   YARN集群上的任务id，附着到一个后台运行的yarn session中\n```\n**命令**:\n\n` yarn-session.sh -n 10 -tm 8192 -s 32`\n` yarn-session.sh -n 2 -tm 2048 -s 2`\n` yarn-session.sh -n 1 -tm 1024 -s 2`\n\nyarn-session.sh -n 1 -tm 1024 -s 2\nyarn-session.sh -n 2 -tm 4096 -jm  4096\n\nyarn-session.sh -id application_1563854073510_0006\n\nflink run -m yarn-cluster -yn 2 -yjm 4096 -ytm  4096\n\n\n2020年 01月 13日 星期一 16:39:12\n\nyarn-session.sh -n 2 -tm 4096 -jm  4096\n","slug":"技术/flink/Flink记录","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/flink/Flink记录","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd317000e38pwb0icgyx7","content":"<p><code>flink相关记录</code></p>\n<h2 id=\"yarn启动\"><a href=\"#yarn启动\" class=\"headerlink\" title=\"yarn启动\"></a>yarn启动</h2><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Usage:</span><br><span class=\"line\">   Required</span><br><span class=\"line\">     -n,--container &lt;arg&gt;   <span class=\"built_in\">Number</span> <span class=\"keyword\">of</span> YARN container to allocate (=<span class=\"built_in\">Number</span> <span class=\"keyword\">of</span> Task Managers)</span><br><span class=\"line\">   Optional</span><br><span class=\"line\">     -D &lt;arg&gt;                        Dynamic properties</span><br><span class=\"line\">     -d,--detached                   Start detached</span><br><span class=\"line\">     -jm,--jobManagerMemory &lt;arg&gt;    Memory <span class=\"keyword\">for</span> JobManager Container <span class=\"keyword\">with</span> optional unit (<span class=\"keyword\">default</span>: MB)</span><br><span class=\"line\">     -nm,--name                      <span class=\"built_in\">Set</span> a custom name <span class=\"keyword\">for</span> the application on YARN</span><br><span class=\"line\">     -q,--query                      Display available YARN resources (memory, cores)</span><br><span class=\"line\">     -qu,--queue &lt;arg&gt;               Specify YARN queue.</span><br><span class=\"line\">     -s,--slots &lt;arg&gt;                <span class=\"built_in\">Number</span> <span class=\"keyword\">of</span> slots per TaskManager</span><br><span class=\"line\">     -tm,--taskManagerMemory &lt;arg&gt;   Memory per TaskManager Container <span class=\"keyword\">with</span> optional unit (<span class=\"keyword\">default</span>: MB)</span><br><span class=\"line\">     -z,--zookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper sub-paths <span class=\"keyword\">for</span> HA mode</span><br></pre></td></tr></table></figure>\n\n<p>yarn-session.sh -n 2 -jm 1024 -tm 1024 -d<br>参数解释：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;-n 2 表示指定两个容器</span><br><span class=\"line\">&#x2F;&#x2F; -jm 1024 表示jobmanager 1024M内存</span><br><span class=\"line\">&#x2F;&#x2F; -tm 1024表示taskmanager 1024M内存</span><br><span class=\"line\">&#x2F;&#x2F;-d 任务后台运行</span><br><span class=\"line\">&#x2F;&#x2F;-nm,--name  YARN上为一个自定义的应用设置一个名字</span><br><span class=\"line\">&#x2F;&#x2F;-q,--query  显示yarn中可用的资源 (内存, cpu核数)</span><br><span class=\"line\">&#x2F;&#x2F;-z,--zookeeperNamespace &lt;arg&gt;   针对HA模式在zookeeper上创建NameSpace</span><br><span class=\"line\">&#x2F;&#x2F;-id,--applicationId &lt;yarnAppId&gt;   YARN集群上的任务id，附着到一个后台运行的yarn session中</span><br></pre></td></tr></table></figure>\n<p><strong>命令</strong>:</p>\n<p><code>yarn-session.sh -n 10 -tm 8192 -s 32</code><br><code>yarn-session.sh -n 2 -tm 2048 -s 2</code><br><code>yarn-session.sh -n 1 -tm 1024 -s 2</code></p>\n<p>yarn-session.sh -n 1 -tm 1024 -s 2<br>yarn-session.sh -n 2 -tm 4096 -jm  4096</p>\n<p>yarn-session.sh -id application_1563854073510_0006</p>\n<p>flink run -m yarn-cluster -yn 2 -yjm 4096 -ytm  4096</p>\n<p>2020年 01月 13日 星期一 16:39:12</p>\n<p>yarn-session.sh -n 2 -tm 4096 -jm  4096</p>\n","site":{"data":{}},"excerpt":"","more":"<p><code>flink相关记录</code></p>\n<h2 id=\"yarn启动\"><a href=\"#yarn启动\" class=\"headerlink\" title=\"yarn启动\"></a>yarn启动</h2><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Usage:</span><br><span class=\"line\">   Required</span><br><span class=\"line\">     -n,--container &lt;arg&gt;   <span class=\"built_in\">Number</span> <span class=\"keyword\">of</span> YARN container to allocate (=<span class=\"built_in\">Number</span> <span class=\"keyword\">of</span> Task Managers)</span><br><span class=\"line\">   Optional</span><br><span class=\"line\">     -D &lt;arg&gt;                        Dynamic properties</span><br><span class=\"line\">     -d,--detached                   Start detached</span><br><span class=\"line\">     -jm,--jobManagerMemory &lt;arg&gt;    Memory <span class=\"keyword\">for</span> JobManager Container <span class=\"keyword\">with</span> optional unit (<span class=\"keyword\">default</span>: MB)</span><br><span class=\"line\">     -nm,--name                      <span class=\"built_in\">Set</span> a custom name <span class=\"keyword\">for</span> the application on YARN</span><br><span class=\"line\">     -q,--query                      Display available YARN resources (memory, cores)</span><br><span class=\"line\">     -qu,--queue &lt;arg&gt;               Specify YARN queue.</span><br><span class=\"line\">     -s,--slots &lt;arg&gt;                <span class=\"built_in\">Number</span> <span class=\"keyword\">of</span> slots per TaskManager</span><br><span class=\"line\">     -tm,--taskManagerMemory &lt;arg&gt;   Memory per TaskManager Container <span class=\"keyword\">with</span> optional unit (<span class=\"keyword\">default</span>: MB)</span><br><span class=\"line\">     -z,--zookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper sub-paths <span class=\"keyword\">for</span> HA mode</span><br></pre></td></tr></table></figure>\n\n<p>yarn-session.sh -n 2 -jm 1024 -tm 1024 -d<br>参数解释：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;-n 2 表示指定两个容器</span><br><span class=\"line\">&#x2F;&#x2F; -jm 1024 表示jobmanager 1024M内存</span><br><span class=\"line\">&#x2F;&#x2F; -tm 1024表示taskmanager 1024M内存</span><br><span class=\"line\">&#x2F;&#x2F;-d 任务后台运行</span><br><span class=\"line\">&#x2F;&#x2F;-nm,--name  YARN上为一个自定义的应用设置一个名字</span><br><span class=\"line\">&#x2F;&#x2F;-q,--query  显示yarn中可用的资源 (内存, cpu核数)</span><br><span class=\"line\">&#x2F;&#x2F;-z,--zookeeperNamespace &lt;arg&gt;   针对HA模式在zookeeper上创建NameSpace</span><br><span class=\"line\">&#x2F;&#x2F;-id,--applicationId &lt;yarnAppId&gt;   YARN集群上的任务id，附着到一个后台运行的yarn session中</span><br></pre></td></tr></table></figure>\n<p><strong>命令</strong>:</p>\n<p><code>yarn-session.sh -n 10 -tm 8192 -s 32</code><br><code>yarn-session.sh -n 2 -tm 2048 -s 2</code><br><code>yarn-session.sh -n 1 -tm 1024 -s 2</code></p>\n<p>yarn-session.sh -n 1 -tm 1024 -s 2<br>yarn-session.sh -n 2 -tm 4096 -jm  4096</p>\n<p>yarn-session.sh -id application_1563854073510_0006</p>\n<p>flink run -m yarn-cluster -yn 2 -yjm 4096 -ytm  4096</p>\n<p>2020年 01月 13日 星期一 16:39:12</p>\n<p>yarn-session.sh -n 2 -tm 4096 -jm  4096</p>\n"},{"_content":" ---\n title:  flink学习\n date:\n tags:\n ---\n\n <!--more-->\n\n base of flink\n\n * checkpoint\n * state\n * time\n * window\n\n\n##\n\nflink\n\nenvStream\naddSource\nfunction\naddSink\n\n## addSource\n\n### 基于集合\n\n* fromCollection(Collection)\n* fromCollection(Iterator, Class)\n* fromElements(T …)\n* fromParallelCollection(SplittableIterator, Class)\n* generateSequence(from,to)\n\n`fromCollection`\n\n```java\n\npublic static void fromCollection() {\n      List<String> list = new ArrayList<>();\n      for (int i = 0; i < 5; i++) {\n          list.add(i + \"\");\n      }\n      DataStream<String> dstrem = env.fromCollection(list);\n      dstrem.map(s -> new Tuple2(s, 1)).returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String.class, Integer.class))\n              .print();\n  }\n\n```\n`generateSequence`\n\n```java\n\npublic static void fromGenerateSequence() {\n      DataStreamSource<Long> interStream = env.generateSequence(10, 1000);\n      interStream.print();\n  }\n\n```\n\n### 基于文件\n\n* readFile()\n\n```java\nDataStream<String> text = env.readTextFile(\"/Users/wqkenqing/Desktop/out/keyCount.txt\");\n       FlatMapFunction<String, Tuple2<String, String>> spliter = (String sentence, Collector<Tuple2<String, String>> out) -> {\n           String ss[] = sentence.split(\"\\\\s+\");\n           out.collect(new Tuple2<String, String>(ss[0], ss[1]));\n       };\n       FilterFunction<Tuple2<String, String>> filter = (Tuple2<String, String> message) -> {\n           if (message.f1.contains(\"m\")) {\n               return true;\n           }\n           return false;\n       };\n       ReduceFunction<String> reduceFunction = (m1, m2) -> {\n           return m1 + m2;\n       };\n       text.flatMap(spliter).returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String.class, String.class))\n               .keyBy(0)\n               .reduce(reduceFunction)\n               .print();\n```\n\n\n### 基于socket\n\n\n```java\n\n```\n\n\n\n### 自定义Source\n\n分两部份,一是一些现有的source,但未集成至flink.另一种是纯自己写的source\n\npart one:\n\n* kafkaSource\n* hdfsSource\n\n\n`kafaSource`\n","source":"_posts/技术/flink/flink.md","raw":" ---\n title:  flink学习\n date:\n tags:\n ---\n\n <!--more-->\n\n base of flink\n\n * checkpoint\n * state\n * time\n * window\n\n\n##\n\nflink\n\nenvStream\naddSource\nfunction\naddSink\n\n## addSource\n\n### 基于集合\n\n* fromCollection(Collection)\n* fromCollection(Iterator, Class)\n* fromElements(T …)\n* fromParallelCollection(SplittableIterator, Class)\n* generateSequence(from,to)\n\n`fromCollection`\n\n```java\n\npublic static void fromCollection() {\n      List<String> list = new ArrayList<>();\n      for (int i = 0; i < 5; i++) {\n          list.add(i + \"\");\n      }\n      DataStream<String> dstrem = env.fromCollection(list);\n      dstrem.map(s -> new Tuple2(s, 1)).returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String.class, Integer.class))\n              .print();\n  }\n\n```\n`generateSequence`\n\n```java\n\npublic static void fromGenerateSequence() {\n      DataStreamSource<Long> interStream = env.generateSequence(10, 1000);\n      interStream.print();\n  }\n\n```\n\n### 基于文件\n\n* readFile()\n\n```java\nDataStream<String> text = env.readTextFile(\"/Users/wqkenqing/Desktop/out/keyCount.txt\");\n       FlatMapFunction<String, Tuple2<String, String>> spliter = (String sentence, Collector<Tuple2<String, String>> out) -> {\n           String ss[] = sentence.split(\"\\\\s+\");\n           out.collect(new Tuple2<String, String>(ss[0], ss[1]));\n       };\n       FilterFunction<Tuple2<String, String>> filter = (Tuple2<String, String> message) -> {\n           if (message.f1.contains(\"m\")) {\n               return true;\n           }\n           return false;\n       };\n       ReduceFunction<String> reduceFunction = (m1, m2) -> {\n           return m1 + m2;\n       };\n       text.flatMap(spliter).returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String.class, String.class))\n               .keyBy(0)\n               .reduce(reduceFunction)\n               .print();\n```\n\n\n### 基于socket\n\n\n```java\n\n```\n\n\n\n### 自定义Source\n\n分两部份,一是一些现有的source,但未集成至flink.另一种是纯自己写的source\n\npart one:\n\n* kafkaSource\n* hdfsSource\n\n\n`kafaSource`\n","slug":"技术/flink/flink","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/flink/flink","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31a000f38pw1mek135z","content":"<hr>\n<p> title:  flink学习<br> date:<br> tags:</p>\n<hr>\n <a id=\"more\"></a>\n\n<p> base of flink</p>\n<ul>\n<li>checkpoint</li>\n<li>state</li>\n<li>time</li>\n<li>window</li>\n</ul>\n<p>##</p>\n<p>flink</p>\n<p>envStream<br>addSource<br>function<br>addSink</p>\n<h2 id=\"addSource\"><a href=\"#addSource\" class=\"headerlink\" title=\"addSource\"></a>addSource</h2><h3 id=\"基于集合\"><a href=\"#基于集合\" class=\"headerlink\" title=\"基于集合\"></a>基于集合</h3><ul>\n<li>fromCollection(Collection)</li>\n<li>fromCollection(Iterator, Class)</li>\n<li>fromElements(T …)</li>\n<li>fromParallelCollection(SplittableIterator, Class)</li>\n<li>generateSequence(from,to)</li>\n</ul>\n<p><code>fromCollection</code></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">fromCollection</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">      List&lt;String&gt; list = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</span><br><span class=\"line\">      <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">5</span>; i++) &#123;</span><br><span class=\"line\">          list.add(i + <span class=\"string\">\"\"</span>);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      DataStream&lt;String&gt; dstrem = env.fromCollection(list);</span><br><span class=\"line\">      dstrem.map(s -&gt; <span class=\"keyword\">new</span> Tuple2(s, <span class=\"number\">1</span>)).returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String<span class=\"class\">.<span class=\"keyword\">class</span>, <span class=\"title\">Integer</span>.<span class=\"title\">class</span>))</span></span><br><span class=\"line\"><span class=\"class\">              .<span class=\"title\">print</span>()</span>;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p><code>generateSequence</code></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">fromGenerateSequence</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">      DataStreamSource&lt;Long&gt; interStream = env.generateSequence(<span class=\"number\">10</span>, <span class=\"number\">1000</span>);</span><br><span class=\"line\">      interStream.print();</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"基于文件\"><a href=\"#基于文件\" class=\"headerlink\" title=\"基于文件\"></a>基于文件</h3><ul>\n<li>readFile()</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DataStream&lt;String&gt; text = env.readTextFile(<span class=\"string\">\"/Users/wqkenqing/Desktop/out/keyCount.txt\"</span>);</span><br><span class=\"line\">       FlatMapFunction&lt;String, Tuple2&lt;String, String&gt;&gt; spliter = (String sentence, Collector&lt;Tuple2&lt;String, String&gt;&gt; out) -&gt; &#123;</span><br><span class=\"line\">           String ss[] = sentence.split(<span class=\"string\">\"\\\\s+\"</span>);</span><br><span class=\"line\">           out.collect(<span class=\"keyword\">new</span> Tuple2&lt;String, String&gt;(ss[<span class=\"number\">0</span>], ss[<span class=\"number\">1</span>]));</span><br><span class=\"line\">       &#125;;</span><br><span class=\"line\">       FilterFunction&lt;Tuple2&lt;String, String&gt;&gt; filter = (Tuple2&lt;String, String&gt; message) -&gt; &#123;</span><br><span class=\"line\">           <span class=\"keyword\">if</span> (message.f1.contains(<span class=\"string\">\"m\"</span>)) &#123;</span><br><span class=\"line\">               <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">           <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">       &#125;;</span><br><span class=\"line\">       ReduceFunction&lt;String&gt; reduceFunction = (m1, m2) -&gt; &#123;</span><br><span class=\"line\">           <span class=\"keyword\">return</span> m1 + m2;</span><br><span class=\"line\">       &#125;;</span><br><span class=\"line\">       text.flatMap(spliter).returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String<span class=\"class\">.<span class=\"keyword\">class</span>, <span class=\"title\">String</span>.<span class=\"title\">class</span>))</span></span><br><span class=\"line\"><span class=\"class\">               .<span class=\"title\">keyBy</span>(0)</span></span><br><span class=\"line\"><span class=\"class\">               .<span class=\"title\">reduce</span>(<span class=\"title\">reduceFunction</span>)</span></span><br><span class=\"line\"><span class=\"class\">               .<span class=\"title\">print</span>()</span>;</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"基于socket\"><a href=\"#基于socket\" class=\"headerlink\" title=\"基于socket\"></a>基于socket</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"自定义Source\"><a href=\"#自定义Source\" class=\"headerlink\" title=\"自定义Source\"></a>自定义Source</h3><p>分两部份,一是一些现有的source,但未集成至flink.另一种是纯自己写的source</p>\n<p>part one:</p>\n<ul>\n<li>kafkaSource</li>\n<li>hdfsSource</li>\n</ul>\n<p><code>kafaSource</code></p>\n","site":{"data":{}},"excerpt":"<hr>\n<p> title:  flink学习<br> date:<br> tags:</p>\n<hr>","more":"<p> base of flink</p>\n<ul>\n<li>checkpoint</li>\n<li>state</li>\n<li>time</li>\n<li>window</li>\n</ul>\n<p>##</p>\n<p>flink</p>\n<p>envStream<br>addSource<br>function<br>addSink</p>\n<h2 id=\"addSource\"><a href=\"#addSource\" class=\"headerlink\" title=\"addSource\"></a>addSource</h2><h3 id=\"基于集合\"><a href=\"#基于集合\" class=\"headerlink\" title=\"基于集合\"></a>基于集合</h3><ul>\n<li>fromCollection(Collection)</li>\n<li>fromCollection(Iterator, Class)</li>\n<li>fromElements(T …)</li>\n<li>fromParallelCollection(SplittableIterator, Class)</li>\n<li>generateSequence(from,to)</li>\n</ul>\n<p><code>fromCollection</code></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">fromCollection</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">      List&lt;String&gt; list = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</span><br><span class=\"line\">      <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">5</span>; i++) &#123;</span><br><span class=\"line\">          list.add(i + <span class=\"string\">\"\"</span>);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      DataStream&lt;String&gt; dstrem = env.fromCollection(list);</span><br><span class=\"line\">      dstrem.map(s -&gt; <span class=\"keyword\">new</span> Tuple2(s, <span class=\"number\">1</span>)).returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String<span class=\"class\">.<span class=\"keyword\">class</span>, <span class=\"title\">Integer</span>.<span class=\"title\">class</span>))</span></span><br><span class=\"line\"><span class=\"class\">              .<span class=\"title\">print</span>()</span>;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p><code>generateSequence</code></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">fromGenerateSequence</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">      DataStreamSource&lt;Long&gt; interStream = env.generateSequence(<span class=\"number\">10</span>, <span class=\"number\">1000</span>);</span><br><span class=\"line\">      interStream.print();</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"基于文件\"><a href=\"#基于文件\" class=\"headerlink\" title=\"基于文件\"></a>基于文件</h3><ul>\n<li>readFile()</li>\n</ul>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DataStream&lt;String&gt; text = env.readTextFile(<span class=\"string\">\"/Users/wqkenqing/Desktop/out/keyCount.txt\"</span>);</span><br><span class=\"line\">       FlatMapFunction&lt;String, Tuple2&lt;String, String&gt;&gt; spliter = (String sentence, Collector&lt;Tuple2&lt;String, String&gt;&gt; out) -&gt; &#123;</span><br><span class=\"line\">           String ss[] = sentence.split(<span class=\"string\">\"\\\\s+\"</span>);</span><br><span class=\"line\">           out.collect(<span class=\"keyword\">new</span> Tuple2&lt;String, String&gt;(ss[<span class=\"number\">0</span>], ss[<span class=\"number\">1</span>]));</span><br><span class=\"line\">       &#125;;</span><br><span class=\"line\">       FilterFunction&lt;Tuple2&lt;String, String&gt;&gt; filter = (Tuple2&lt;String, String&gt; message) -&gt; &#123;</span><br><span class=\"line\">           <span class=\"keyword\">if</span> (message.f1.contains(<span class=\"string\">\"m\"</span>)) &#123;</span><br><span class=\"line\">               <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">           <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</span><br><span class=\"line\">       &#125;;</span><br><span class=\"line\">       ReduceFunction&lt;String&gt; reduceFunction = (m1, m2) -&gt; &#123;</span><br><span class=\"line\">           <span class=\"keyword\">return</span> m1 + m2;</span><br><span class=\"line\">       &#125;;</span><br><span class=\"line\">       text.flatMap(spliter).returns((TypeInformation) TupleTypeInfo.getBasicTupleTypeInfo(String<span class=\"class\">.<span class=\"keyword\">class</span>, <span class=\"title\">String</span>.<span class=\"title\">class</span>))</span></span><br><span class=\"line\"><span class=\"class\">               .<span class=\"title\">keyBy</span>(0)</span></span><br><span class=\"line\"><span class=\"class\">               .<span class=\"title\">reduce</span>(<span class=\"title\">reduceFunction</span>)</span></span><br><span class=\"line\"><span class=\"class\">               .<span class=\"title\">print</span>()</span>;</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"基于socket\"><a href=\"#基于socket\" class=\"headerlink\" title=\"基于socket\"></a>基于socket</h3><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"自定义Source\"><a href=\"#自定义Source\" class=\"headerlink\" title=\"自定义Source\"></a>自定义Source</h3><p>分两部份,一是一些现有的source,但未集成至flink.另一种是纯自己写的source</p>\n<p>part one:</p>\n<ul>\n<li>kafkaSource</li>\n<li>hdfsSource</li>\n</ul>\n<p><code>kafaSource</code></p>"},{"_content":"# docker 积累\n\n## 常用操作命令\n\ndocker pull XXXX\n\ndocker search nginx 搜索镜像\n\ndocker pull nginx 拉取镜像\n\ndocker images 查看镜像\n\ndocker save XXXX 导出镜象\n\n## Dockerfile\n\n```\nDockerfile 的组成 与常用编辑形式\n```\n\n","source":"_posts/技术/docker/docker.md","raw":"# docker 积累\n\n## 常用操作命令\n\ndocker pull XXXX\n\ndocker search nginx 搜索镜像\n\ndocker pull nginx 拉取镜像\n\ndocker images 查看镜像\n\ndocker save XXXX 导出镜象\n\n## Dockerfile\n\n```\nDockerfile 的组成 与常用编辑形式\n```\n\n","slug":"技术/docker/docker","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/docker/docker","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31c000g38pw2tz8enic","content":"<h1 id=\"docker-积累\"><a href=\"#docker-积累\" class=\"headerlink\" title=\"docker 积累\"></a>docker 积累</h1><h2 id=\"常用操作命令\"><a href=\"#常用操作命令\" class=\"headerlink\" title=\"常用操作命令\"></a>常用操作命令</h2><p>docker pull XXXX</p>\n<p>docker search nginx 搜索镜像</p>\n<p>docker pull nginx 拉取镜像</p>\n<p>docker images 查看镜像</p>\n<p>docker save XXXX 导出镜象</p>\n<h2 id=\"Dockerfile\"><a href=\"#Dockerfile\" class=\"headerlink\" title=\"Dockerfile\"></a>Dockerfile</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Dockerfile 的组成 与常用编辑形式</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"docker-积累\"><a href=\"#docker-积累\" class=\"headerlink\" title=\"docker 积累\"></a>docker 积累</h1><h2 id=\"常用操作命令\"><a href=\"#常用操作命令\" class=\"headerlink\" title=\"常用操作命令\"></a>常用操作命令</h2><p>docker pull XXXX</p>\n<p>docker search nginx 搜索镜像</p>\n<p>docker pull nginx 拉取镜像</p>\n<p>docker images 查看镜像</p>\n<p>docker save XXXX 导出镜象</p>\n<h2 id=\"Dockerfile\"><a href=\"#Dockerfile\" class=\"headerlink\" title=\"Dockerfile\"></a>Dockerfile</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Dockerfile 的组成 与常用编辑形式</span><br></pre></td></tr></table></figure>\n\n"},{"_content":" ---\n title:  flink_book.md\n date: 2020-8-24\n tags:  [flink,book]\n\n ---\n\n <!--more-->\n\n # flink book\n","source":"_posts/技术/flink/flink_book.md","raw":" ---\n title:  flink_book.md\n date: 2020-8-24\n tags:  [flink,book]\n\n ---\n\n <!--more-->\n\n # flink book\n","slug":"技术/flink/flink_book","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/flink/flink_book","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31e000h38pw4ka4c6re","content":"<hr>\n<p> title:  flink_book.md<br> date: 2020-8-24<br> tags:  [flink,book]</p>\n<hr>\n <a id=\"more\"></a>\n\n<h1 id=\"flink-book\"><a href=\"#flink-book\" class=\"headerlink\" title=\"flink book\"></a>flink book</h1>","site":{"data":{}},"excerpt":"<hr>\n<p> title:  flink_book.md<br> date: 2020-8-24<br> tags:  [flink,book]</p>\n<hr>","more":"<h1 id=\"flink-book\"><a href=\"#flink-book\" class=\"headerlink\" title=\"flink book\"></a>flink book</h1>"},{"title":"flume记录","date":"2019-06-18T16:00:00.000Z","_content":"flume几种source and sink实际操作\n<!--more-->\n\n## soruce\n\nflume自身就支持多种source.\nchanel这里暂用mem\nsink to hdfs\n\n简单测试几种source\n\n\n\n```properties\n\n# 配置Agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# 配置Source\na1.sources.r1.type = exec\na1.sources.r1.channels = c1\na1.sources.r1.deserializer.outputCharset = UTF-8\n\n# 配置需要监控的日志输出目录\na1.sources.r1.command = tail\n\n# 配置Sink\na1.sinks.k1.type = hdfs\na1.sinks.k1.hdfs.useLocalTimeStamp = true\na1.sinks.k1.hdfs.path = hdfs://namenode:9000/flume/events/%Y-%m\na1.sinks.k1.hdfs.filePrefix = %Y-%m-%d-%H\na1.sinks.k1.hdfs.fileSuffix = .log\na1.sinks.k1.hdfs.minBlockReplicas = 1\na1.sinks.k1.hdfs.fileType = DataStream\na1.sinks.k1.hdfs.writeFormat = Text\n#a1.sinks.k1.hdfs.rollInterval = 86400\na1.sinks.k1.hdfs.rollSize =0\na1.sinks.k1.hdfs.rollCount =0\n#a1.sinks.k1.hdfs.idleTimeout = 0\n\n# 配置Channel\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# 将三者连接\na1.sources.r1.channel = c1\na1.sinks.k1.channel = c1\n\n```\n\n## 命令\n\nflume-ng agent -c /etc/flume-ng/conf/ -f kafka.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f demo2.conf -n a1 -Dflume.root.logger=INFO,console\n16228\n\n\n## 解决Flume采集数据时在HDFS上产生大量小文件的问题\n\n以上conf为例\n\na1为agent的名称\ndemo.conf为flume配置文件的名称\n-c指向log4j.properties文件和flume_env.sh文件所在目录。\n--Dflume.root.logger=INFO,console 在终端输出运行日志\n\n查阅flume配置参数，如下：\n\nrollSize\n默认值：1024，当临时文件达到该大小（单位：bytes）时，滚动成目标文件。如果设置成0，则表示不根据临时文件大小来滚动文件。\n\nrollCount\n默认值：10，当events数据达到该数量时候，将临时文件滚动成目标文件，如果设置成0，则表示不根据events数据来滚动文件。\n\nround\n默认值：false，是否启用时间上的”舍弃”，类似于”四舍五入”，如果启用，则会影响除了%t的其他所有时间表达式；\n\nroundValue\n默认值：1，时间上进行“舍弃”的值；\n\nroundUnit\n\n默认值：seconds，时间上进行”舍弃”的单位，包含：second,minute,hour\n\n当设置了round、roundValue、roundUnit参数收，需要在sink指定的HDFS路径上指定按照时间生成的目录的格式，例如有需求，每采集1小时就在HDFS目录上生成一个目录，里面存放这1小时内采集到的数据。\n\n## flume file to avro\n\n```properties\n#########a1 agent#####\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# 配置Source\na1.sources.r1.type = exec\na1.sources.r1.channels = c1\na1.sources.r1.deserializer.outputCharset = UTF-8\na1.sources.r1.command = cat  /data/upload/theme_item_pool.csv\n\n# 配置需要监控的日志输出目录\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = namenode\na1.sinks.k1.port = 4444\na1.sinks.k1.requiredAcks = 1\na1.sinks.k1.channel = c1\n\n```\n\n## flume avro to kafka\n\n```properties\n#########a1 agent#####\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# 配置Source\na1.sources.r1.type = exec\na1.sources.r1.channels = c1\na1.sources.r1.deserializer.outputCharset = UTF-8\na1.sources.r1.command = cat  /data/upload/theme_item_pool.csv\n\n# 配置需要监控的日志输出目录\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = namenode\na1.sinks.k1.port = 4444\na1.sinks.k1.requiredAcks = 1\na1.sinks.k1.channel = c1\n```\n\n\n## avro source\n\n```properties\n#test avro sources\na1.sources=r1\na1.channels=c1\na1.sinks=k1\n\na1.sources.r1.type = avro\na1.sources.r1.channels=c1\na1.sources.r1.bind=localhost\na1.sources.r1.port=55555\n\n#Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity=1000\na1.channels.c1.transactionCapacity = 100\n\n#sink配置\na1.sinks.k1.type = hdfs\na1.sinks.k1.hdfs.useLocalTimeStamp = true\na1.sinks.k1.hdfs.path = hdfs://namenode:8020/flume/events/%Y-%m\na1.sinks.k1.hdfs.filePrefix = %Y-%m-%d-%H\na1.sinks.k1.hdfs.fileSuffix = .log\na1.sinks.k1.hdfs.minBlockReplicas = 1\na1.sinks.k1.hdfs.fileType = DataStream\na1.sinks.k1.hdfs.writeFormat = Text\n#a1.sinks.k1.hdfs.rollInterval = 86400\na1.sinks.k1.hdfs.rollSize =0\na1.sinks.k1.hdfs.rollCount =0\n#a1.sinks.k1.hdfs.idleTimeout = 0\na1.sinks.k1.channel = c1\n\n```\n## avro to file\n\n```properties\n#test avro sources\na1.sources=r1\na1.channels=c1\na1.sinks=k1\n\na1.sources.r1.type = exec\na1.sources.r1.channels = c1\na1.sources.r1.deserializer.outputCharset = UTF-8\na1.sources.r1.command =curl http://192.168.10.104:8088/ws/v1/cluster/apps\n\n#Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity=1000\na1.channels.c1.transactionCapacity = 100\n\n#sink配置\n\na1.sinks.k1.type=file_roll\n\na1.sinks.k1.channel=c1\n\na1.sinks.k1.sink.directory=/data/hadoop/flume-ng/config/log\n\n```\n\n\n```properties\n### 多个sink\n\n#test avro sources\na1.sources=r1\na1.channels=c1 c2\na1.sinks=k1 k2\n\n\na1.sources.r1.type = netcat\na1.sources.r1.bind=192.168.10.101\na1.sources.r1.port=55555\na1.sources.r1.channels = c1 c2\n\n#Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity=1000\na1.channels.c1.transactionCapacity = 100\n\n###channel2\na1.channels.c2.type = memory\na1.channels.c2.capacity=1000\na1.channels.c2.transactionCapacity = 100\n\n\n#sink配置\n\n#a1.sinks.k1.type=logger\na1.sinks.k1.type=file_roll\na1.sinks.k1.channel=c1\na1.sinks.k1.sink.directory=/data/hadoop/flume-ng/config/log\n\n###sink2\na1.sinks.k2.type = org.apache.flume.sink.kafka.KafkaSink\na1.sinks.k2.brokerList = namenode:9092\na1.sinks.k2.topic = carstream\na1.sinks.k2.batchSize = 100\na1.sinks.k2.requiredAcks = 1\na1.sinks.k2.channel=c2\n\n```\n\n## 目录变化\n\n```properties\n\na1.sources=r1\na1.channels=c1\na1.sinks=k1\n\na1.sources.r1.type = spooldir\na1.sources.r1.spoolDir = /data/hadoop/flume-ng/config/log\na1.sources.r1.fileHeader = true\na1.sources.r1.channels = c1\n\n#Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity=1000\na1.channels.c1.transactionCapacity = 100\n\na1.sinks.k1.type=logger\na1.sinks.k1.channel=c1\n\n\n```\n\n## tailDir\n```properties\n\n# in this case called 'a1'\n\na1.sources = s1\na1.channels = c1\na1.sinks = k1\n\n# For each one of the sources, the type is defined\na1.sources.s1.type = org.apache.flume.source.taildir.TaildirSource\na1.sources.s1.positionFile = /opt/cdhmoduels/apache-flume-1.5.0-cdh5.3.6-bin/taidir/dirsource/taildir_position.json\na1.sources.s1.filegroups = f1\na1.sources.s1.filegroups.f1 = /data/hadoop/flume-ng/log/\n\n# The channel can be defined as follows.\na1.sources.s1.channels = c1\na1.sinks.k1.channel = c1\n\n# Each sink's type must be defined\na1.sinks.k1.type = hdfs\na1.sinks.k1.hdfs.path = /flume/event/taildir\na1.sinks.k1.hdfs.filePrefix = hive-log\n#Specify the channel the sink should use\n\n# Each channel's type is defined.\na1.channels.c1.type = memory\n\n# Other config values specific to each type of channel(sink or source)\n# can be defined as well\n# In this case, it specifies the capacity of the memory channel\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 1000\n\n\n```\n\n\n\n\n\n## 操作记录\n\nflume-ng avro-client -c conf -H localhost -p 55555  -F /data/upload/trip_coord.csv\n\n/data/upload/trip_coord.csv\n\nflume-ng agent -c /etc/flume-ng/conf/ -f avro.conf -n a1 -Dflume.root.logger=INFO,console\n\nflume-ng agent -c /etc/flume-ng/conf/ -f avro2.conf -n a1 -Dflume.root.logger=INFO,console\n\nflume-ng agent -c /etc/flume-ng/conf/ -f curl.conf -n a1 -Dflume.root.logger=INFO,console\n\nflume-ng agent -c /etc/flume-ng/conf/ -f self2.conf -n a1 -Dflume.root.logger=INFO,console\n\nflume-ng agent -c /etc/flume-ng/conf/ -f kafka.conf -n a1 -Dflume.root.logger=INFO,console\n\nflume-ng agent -c /etc/flume-ng/conf/ -f tcp_logger.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f avro3.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f tcp_hdfs.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f avro.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f avro2.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f tcp.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f tcp4.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f syslog.conf -n a1 -Dflume.root.logger=INFO,console\n\nflume-ng agent -c /etc/flume-ng/conf/ -f http_logger.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f api_logger.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f api_logger_twosink.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f kafka.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f demo4.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f tcp.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f tcp_two_sink.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f spool.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f taildir.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f udp.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f udp_two_sink.conf -n a1 -Dflume.root.logger=INFO,console\n---\n\n2020-7-22\n\ndocker run --restart=unless-stopped  -d -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\"  --name=\"hankoues\" elasticsearch:7.1.0\n\n\ndocker pull docker.io/cerebro\ndocker pull docker.io/cerebro\n\n---\n","source":"_posts/技术/flume/Flume.md","raw":"---\n\ntitle: flume记录\ndate: 2019-06-19\ntags:\n\n---\nflume几种source and sink实际操作\n<!--more-->\n\n## soruce\n\nflume自身就支持多种source.\nchanel这里暂用mem\nsink to hdfs\n\n简单测试几种source\n\n\n\n```properties\n\n# 配置Agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# 配置Source\na1.sources.r1.type = exec\na1.sources.r1.channels = c1\na1.sources.r1.deserializer.outputCharset = UTF-8\n\n# 配置需要监控的日志输出目录\na1.sources.r1.command = tail\n\n# 配置Sink\na1.sinks.k1.type = hdfs\na1.sinks.k1.hdfs.useLocalTimeStamp = true\na1.sinks.k1.hdfs.path = hdfs://namenode:9000/flume/events/%Y-%m\na1.sinks.k1.hdfs.filePrefix = %Y-%m-%d-%H\na1.sinks.k1.hdfs.fileSuffix = .log\na1.sinks.k1.hdfs.minBlockReplicas = 1\na1.sinks.k1.hdfs.fileType = DataStream\na1.sinks.k1.hdfs.writeFormat = Text\n#a1.sinks.k1.hdfs.rollInterval = 86400\na1.sinks.k1.hdfs.rollSize =0\na1.sinks.k1.hdfs.rollCount =0\n#a1.sinks.k1.hdfs.idleTimeout = 0\n\n# 配置Channel\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# 将三者连接\na1.sources.r1.channel = c1\na1.sinks.k1.channel = c1\n\n```\n\n## 命令\n\nflume-ng agent -c /etc/flume-ng/conf/ -f kafka.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f demo2.conf -n a1 -Dflume.root.logger=INFO,console\n16228\n\n\n## 解决Flume采集数据时在HDFS上产生大量小文件的问题\n\n以上conf为例\n\na1为agent的名称\ndemo.conf为flume配置文件的名称\n-c指向log4j.properties文件和flume_env.sh文件所在目录。\n--Dflume.root.logger=INFO,console 在终端输出运行日志\n\n查阅flume配置参数，如下：\n\nrollSize\n默认值：1024，当临时文件达到该大小（单位：bytes）时，滚动成目标文件。如果设置成0，则表示不根据临时文件大小来滚动文件。\n\nrollCount\n默认值：10，当events数据达到该数量时候，将临时文件滚动成目标文件，如果设置成0，则表示不根据events数据来滚动文件。\n\nround\n默认值：false，是否启用时间上的”舍弃”，类似于”四舍五入”，如果启用，则会影响除了%t的其他所有时间表达式；\n\nroundValue\n默认值：1，时间上进行“舍弃”的值；\n\nroundUnit\n\n默认值：seconds，时间上进行”舍弃”的单位，包含：second,minute,hour\n\n当设置了round、roundValue、roundUnit参数收，需要在sink指定的HDFS路径上指定按照时间生成的目录的格式，例如有需求，每采集1小时就在HDFS目录上生成一个目录，里面存放这1小时内采集到的数据。\n\n## flume file to avro\n\n```properties\n#########a1 agent#####\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# 配置Source\na1.sources.r1.type = exec\na1.sources.r1.channels = c1\na1.sources.r1.deserializer.outputCharset = UTF-8\na1.sources.r1.command = cat  /data/upload/theme_item_pool.csv\n\n# 配置需要监控的日志输出目录\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = namenode\na1.sinks.k1.port = 4444\na1.sinks.k1.requiredAcks = 1\na1.sinks.k1.channel = c1\n\n```\n\n## flume avro to kafka\n\n```properties\n#########a1 agent#####\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# 配置Source\na1.sources.r1.type = exec\na1.sources.r1.channels = c1\na1.sources.r1.deserializer.outputCharset = UTF-8\na1.sources.r1.command = cat  /data/upload/theme_item_pool.csv\n\n# 配置需要监控的日志输出目录\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = namenode\na1.sinks.k1.port = 4444\na1.sinks.k1.requiredAcks = 1\na1.sinks.k1.channel = c1\n```\n\n\n## avro source\n\n```properties\n#test avro sources\na1.sources=r1\na1.channels=c1\na1.sinks=k1\n\na1.sources.r1.type = avro\na1.sources.r1.channels=c1\na1.sources.r1.bind=localhost\na1.sources.r1.port=55555\n\n#Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity=1000\na1.channels.c1.transactionCapacity = 100\n\n#sink配置\na1.sinks.k1.type = hdfs\na1.sinks.k1.hdfs.useLocalTimeStamp = true\na1.sinks.k1.hdfs.path = hdfs://namenode:8020/flume/events/%Y-%m\na1.sinks.k1.hdfs.filePrefix = %Y-%m-%d-%H\na1.sinks.k1.hdfs.fileSuffix = .log\na1.sinks.k1.hdfs.minBlockReplicas = 1\na1.sinks.k1.hdfs.fileType = DataStream\na1.sinks.k1.hdfs.writeFormat = Text\n#a1.sinks.k1.hdfs.rollInterval = 86400\na1.sinks.k1.hdfs.rollSize =0\na1.sinks.k1.hdfs.rollCount =0\n#a1.sinks.k1.hdfs.idleTimeout = 0\na1.sinks.k1.channel = c1\n\n```\n## avro to file\n\n```properties\n#test avro sources\na1.sources=r1\na1.channels=c1\na1.sinks=k1\n\na1.sources.r1.type = exec\na1.sources.r1.channels = c1\na1.sources.r1.deserializer.outputCharset = UTF-8\na1.sources.r1.command =curl http://192.168.10.104:8088/ws/v1/cluster/apps\n\n#Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity=1000\na1.channels.c1.transactionCapacity = 100\n\n#sink配置\n\na1.sinks.k1.type=file_roll\n\na1.sinks.k1.channel=c1\n\na1.sinks.k1.sink.directory=/data/hadoop/flume-ng/config/log\n\n```\n\n\n```properties\n### 多个sink\n\n#test avro sources\na1.sources=r1\na1.channels=c1 c2\na1.sinks=k1 k2\n\n\na1.sources.r1.type = netcat\na1.sources.r1.bind=192.168.10.101\na1.sources.r1.port=55555\na1.sources.r1.channels = c1 c2\n\n#Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity=1000\na1.channels.c1.transactionCapacity = 100\n\n###channel2\na1.channels.c2.type = memory\na1.channels.c2.capacity=1000\na1.channels.c2.transactionCapacity = 100\n\n\n#sink配置\n\n#a1.sinks.k1.type=logger\na1.sinks.k1.type=file_roll\na1.sinks.k1.channel=c1\na1.sinks.k1.sink.directory=/data/hadoop/flume-ng/config/log\n\n###sink2\na1.sinks.k2.type = org.apache.flume.sink.kafka.KafkaSink\na1.sinks.k2.brokerList = namenode:9092\na1.sinks.k2.topic = carstream\na1.sinks.k2.batchSize = 100\na1.sinks.k2.requiredAcks = 1\na1.sinks.k2.channel=c2\n\n```\n\n## 目录变化\n\n```properties\n\na1.sources=r1\na1.channels=c1\na1.sinks=k1\n\na1.sources.r1.type = spooldir\na1.sources.r1.spoolDir = /data/hadoop/flume-ng/config/log\na1.sources.r1.fileHeader = true\na1.sources.r1.channels = c1\n\n#Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity=1000\na1.channels.c1.transactionCapacity = 100\n\na1.sinks.k1.type=logger\na1.sinks.k1.channel=c1\n\n\n```\n\n## tailDir\n```properties\n\n# in this case called 'a1'\n\na1.sources = s1\na1.channels = c1\na1.sinks = k1\n\n# For each one of the sources, the type is defined\na1.sources.s1.type = org.apache.flume.source.taildir.TaildirSource\na1.sources.s1.positionFile = /opt/cdhmoduels/apache-flume-1.5.0-cdh5.3.6-bin/taidir/dirsource/taildir_position.json\na1.sources.s1.filegroups = f1\na1.sources.s1.filegroups.f1 = /data/hadoop/flume-ng/log/\n\n# The channel can be defined as follows.\na1.sources.s1.channels = c1\na1.sinks.k1.channel = c1\n\n# Each sink's type must be defined\na1.sinks.k1.type = hdfs\na1.sinks.k1.hdfs.path = /flume/event/taildir\na1.sinks.k1.hdfs.filePrefix = hive-log\n#Specify the channel the sink should use\n\n# Each channel's type is defined.\na1.channels.c1.type = memory\n\n# Other config values specific to each type of channel(sink or source)\n# can be defined as well\n# In this case, it specifies the capacity of the memory channel\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 1000\n\n\n```\n\n\n\n\n\n## 操作记录\n\nflume-ng avro-client -c conf -H localhost -p 55555  -F /data/upload/trip_coord.csv\n\n/data/upload/trip_coord.csv\n\nflume-ng agent -c /etc/flume-ng/conf/ -f avro.conf -n a1 -Dflume.root.logger=INFO,console\n\nflume-ng agent -c /etc/flume-ng/conf/ -f avro2.conf -n a1 -Dflume.root.logger=INFO,console\n\nflume-ng agent -c /etc/flume-ng/conf/ -f curl.conf -n a1 -Dflume.root.logger=INFO,console\n\nflume-ng agent -c /etc/flume-ng/conf/ -f self2.conf -n a1 -Dflume.root.logger=INFO,console\n\nflume-ng agent -c /etc/flume-ng/conf/ -f kafka.conf -n a1 -Dflume.root.logger=INFO,console\n\nflume-ng agent -c /etc/flume-ng/conf/ -f tcp_logger.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f avro3.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f tcp_hdfs.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f avro.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f avro2.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f tcp.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f tcp4.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f syslog.conf -n a1 -Dflume.root.logger=INFO,console\n\nflume-ng agent -c /etc/flume-ng/conf/ -f http_logger.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f api_logger.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f api_logger_twosink.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f kafka.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f demo4.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f tcp.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f tcp_two_sink.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f spool.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f taildir.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f udp.conf -n a1 -Dflume.root.logger=INFO,console\nflume-ng agent -c /etc/flume-ng/conf/ -f udp_two_sink.conf -n a1 -Dflume.root.logger=INFO,console\n---\n\n2020-7-22\n\ndocker run --restart=unless-stopped  -d -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\"  --name=\"hankoues\" elasticsearch:7.1.0\n\n\ndocker pull docker.io/cerebro\ndocker pull docker.io/cerebro\n\n---\n","slug":"技术/flume/Flume","published":1,"updated":"2021-01-05T02:35:36.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31f000i38pw69vwbrlx","content":"<p>flume几种source and sink实际操作</p>\n<a id=\"more\"></a>\n\n<h2 id=\"soruce\"><a href=\"#soruce\" class=\"headerlink\" title=\"soruce\"></a>soruce</h2><p>flume自身就支持多种source.<br>chanel这里暂用mem<br>sink to hdfs</p>\n<p>简单测试几种source</p>\n<figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置Agent</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span> = <span class=\"string\">r1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span> = <span class=\"string\">k1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置Source</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.type</span> = <span class=\"string\">exec</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.deserializer.outputCharset</span> = <span class=\"string\">UTF-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置需要监控的日志输出目录</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.command</span> = <span class=\"string\">tail</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置Sink</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span> = <span class=\"string\">hdfs</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.useLocalTimeStamp</span> = <span class=\"string\">true</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.path</span> = <span class=\"string\">hdfs://namenode:9000/flume/events/%Y-%m</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.filePrefix</span> = <span class=\"string\">%Y-%m-%d-%H</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.fileSuffix</span> = <span class=\"string\">.log</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.minBlockReplicas</span> = <span class=\"string\">1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.fileType</span> = <span class=\"string\">DataStream</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.writeFormat</span> = <span class=\"string\">Text</span></span><br><span class=\"line\"><span class=\"comment\">#a1.sinks.k1.hdfs.rollInterval = 86400</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.rollSize</span> =<span class=\"string\">0</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.rollCount</span> =<span class=\"string\">0</span></span><br><span class=\"line\"><span class=\"comment\">#a1.sinks.k1.hdfs.idleTimeout = 0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置Channel</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span> = <span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将三者连接</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channel</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span> = <span class=\"string\">c1</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"命令\"><a href=\"#命令\" class=\"headerlink\" title=\"命令\"></a>命令</h2><p>flume-ng agent -c /etc/flume-ng/conf/ -f kafka.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f demo2.conf -n a1 -Dflume.root.logger=INFO,console<br>16228</p>\n<h2 id=\"解决Flume采集数据时在HDFS上产生大量小文件的问题\"><a href=\"#解决Flume采集数据时在HDFS上产生大量小文件的问题\" class=\"headerlink\" title=\"解决Flume采集数据时在HDFS上产生大量小文件的问题\"></a>解决Flume采集数据时在HDFS上产生大量小文件的问题</h2><p>以上conf为例</p>\n<p>a1为agent的名称<br>demo.conf为flume配置文件的名称<br>-c指向log4j.properties文件和flume_env.sh文件所在目录。<br>–Dflume.root.logger=INFO,console 在终端输出运行日志</p>\n<p>查阅flume配置参数，如下：</p>\n<p>rollSize<br>默认值：1024，当临时文件达到该大小（单位：bytes）时，滚动成目标文件。如果设置成0，则表示不根据临时文件大小来滚动文件。</p>\n<p>rollCount<br>默认值：10，当events数据达到该数量时候，将临时文件滚动成目标文件，如果设置成0，则表示不根据events数据来滚动文件。</p>\n<p>round<br>默认值：false，是否启用时间上的”舍弃”，类似于”四舍五入”，如果启用，则会影响除了%t的其他所有时间表达式；</p>\n<p>roundValue<br>默认值：1，时间上进行“舍弃”的值；</p>\n<p>roundUnit</p>\n<p>默认值：seconds，时间上进行”舍弃”的单位，包含：second,minute,hour</p>\n<p>当设置了round、roundValue、roundUnit参数收，需要在sink指定的HDFS路径上指定按照时间生成的目录的格式，例如有需求，每采集1小时就在HDFS目录上生成一个目录，里面存放这1小时内采集到的数据。</p>\n<h2 id=\"flume-file-to-avro\"><a href=\"#flume-file-to-avro\" class=\"headerlink\" title=\"flume file to avro\"></a>flume file to avro</h2><figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#########a1 agent#####</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span> = <span class=\"string\">r1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span> = <span class=\"string\">k1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置Source</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.type</span> = <span class=\"string\">exec</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.deserializer.outputCharset</span> = <span class=\"string\">UTF-8</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.command</span> = <span class=\"string\">cat  /data/upload/theme_item_pool.csv</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置需要监控的日志输出目录</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span> = <span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span> = <span class=\"string\">avro</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hostname</span> = <span class=\"string\">namenode</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.port</span> = <span class=\"string\">4444</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.requiredAcks</span> = <span class=\"string\">1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span> = <span class=\"string\">c1</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"flume-avro-to-kafka\"><a href=\"#flume-avro-to-kafka\" class=\"headerlink\" title=\"flume avro to kafka\"></a>flume avro to kafka</h2><figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#########a1 agent#####</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span> = <span class=\"string\">r1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span> = <span class=\"string\">k1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置Source</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.type</span> = <span class=\"string\">exec</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.deserializer.outputCharset</span> = <span class=\"string\">UTF-8</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.command</span> = <span class=\"string\">cat  /data/upload/theme_item_pool.csv</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置需要监控的日志输出目录</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span> = <span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span> = <span class=\"string\">avro</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hostname</span> = <span class=\"string\">namenode</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.port</span> = <span class=\"string\">4444</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.requiredAcks</span> = <span class=\"string\">1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span> = <span class=\"string\">c1</span></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"avro-source\"><a href=\"#avro-source\" class=\"headerlink\" title=\"avro source\"></a>avro source</h2><figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#test avro sources</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span>=<span class=\"string\">r1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span>=<span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span>=<span class=\"string\">k1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.type</span> = <span class=\"string\">avro</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channels</span>=<span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.bind</span>=<span class=\"string\">localhost</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.port</span>=<span class=\"string\">55555</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Use a channel which buffers events in memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span>=<span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#sink配置</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span> = <span class=\"string\">hdfs</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.useLocalTimeStamp</span> = <span class=\"string\">true</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.path</span> = <span class=\"string\">hdfs://namenode:8020/flume/events/%Y-%m</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.filePrefix</span> = <span class=\"string\">%Y-%m-%d-%H</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.fileSuffix</span> = <span class=\"string\">.log</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.minBlockReplicas</span> = <span class=\"string\">1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.fileType</span> = <span class=\"string\">DataStream</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.writeFormat</span> = <span class=\"string\">Text</span></span><br><span class=\"line\"><span class=\"comment\">#a1.sinks.k1.hdfs.rollInterval = 86400</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.rollSize</span> =<span class=\"string\">0</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.rollCount</span> =<span class=\"string\">0</span></span><br><span class=\"line\"><span class=\"comment\">#a1.sinks.k1.hdfs.idleTimeout = 0</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span> = <span class=\"string\">c1</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"avro-to-file\"><a href=\"#avro-to-file\" class=\"headerlink\" title=\"avro to file\"></a>avro to file</h2><figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#test avro sources</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span>=<span class=\"string\">r1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span>=<span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span>=<span class=\"string\">k1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.type</span> = <span class=\"string\">exec</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.deserializer.outputCharset</span> = <span class=\"string\">UTF-8</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.command</span> =<span class=\"string\">curl http://192.168.10.104:8088/ws/v1/cluster/apps</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Use a channel which buffers events in memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span>=<span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#sink配置</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span>=<span class=\"string\">file_roll</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span>=<span class=\"string\">c1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.sink.directory</span>=<span class=\"string\">/data/hadoop/flume-ng/config/log</span></span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">### 多个sink</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#test avro sources</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span>=<span class=\"string\">r1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span>=<span class=\"string\">c1 c2</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span>=<span class=\"string\">k1 k2</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.type</span> = <span class=\"string\">netcat</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.bind</span>=<span class=\"string\">192.168.10.101</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.port</span>=<span class=\"string\">55555</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channels</span> = <span class=\"string\">c1 c2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Use a channel which buffers events in memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span>=<span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###channel2</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c2.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c2.capacity</span>=<span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c2.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#sink配置</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#a1.sinks.k1.type=logger</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span>=<span class=\"string\">file_roll</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span>=<span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.sink.directory</span>=<span class=\"string\">/data/hadoop/flume-ng/config/log</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###sink2</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k2.type</span> = <span class=\"string\">org.apache.flume.sink.kafka.KafkaSink</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k2.brokerList</span> = <span class=\"string\">namenode:9092</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k2.topic</span> = <span class=\"string\">carstream</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k2.batchSize</span> = <span class=\"string\">100</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k2.requiredAcks</span> = <span class=\"string\">1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k2.channel</span>=<span class=\"string\">c2</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"目录变化\"><a href=\"#目录变化\" class=\"headerlink\" title=\"目录变化\"></a>目录变化</h2><figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span>=<span class=\"string\">r1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span>=<span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span>=<span class=\"string\">k1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.type</span> = <span class=\"string\">spooldir</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.spoolDir</span> = <span class=\"string\">/data/hadoop/flume-ng/config/log</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.fileHeader</span> = <span class=\"string\">true</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Use a channel which buffers events in memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span>=<span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span>=<span class=\"string\">logger</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span>=<span class=\"string\">c1</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"tailDir\"><a href=\"#tailDir\" class=\"headerlink\" title=\"tailDir\"></a>tailDir</h2><figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># in this case called 'a1'</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span> = <span class=\"string\">s1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span> = <span class=\"string\">k1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># For each one of the sources, the type is defined</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.s1.type</span> = <span class=\"string\">org.apache.flume.source.taildir.TaildirSource</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.s1.positionFile</span> = <span class=\"string\">/opt/cdhmoduels/apache-flume-1.5.0-cdh5.3.6-bin/taidir/dirsource/taildir_position.json</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.s1.filegroups</span> = <span class=\"string\">f1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.s1.filegroups.f1</span> = <span class=\"string\">/data/hadoop/flume-ng/log/</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># The channel can be defined as follows.</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.s1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Each sink's type must be defined</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span> = <span class=\"string\">hdfs</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.path</span> = <span class=\"string\">/flume/event/taildir</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.filePrefix</span> = <span class=\"string\">hive-log</span></span><br><span class=\"line\"><span class=\"comment\">#Specify the channel the sink should use</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Each channel's type is defined.</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Other config values specific to each type of channel(sink or source)</span></span><br><span class=\"line\"><span class=\"comment\"># can be defined as well</span></span><br><span class=\"line\"><span class=\"comment\"># In this case, it specifies the capacity of the memory channel</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span> = <span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">1000</span></span><br></pre></td></tr></table></figure>\n\n\n\n\n\n<h2 id=\"操作记录\"><a href=\"#操作记录\" class=\"headerlink\" title=\"操作记录\"></a>操作记录</h2><p>flume-ng avro-client -c conf -H localhost -p 55555  -F /data/upload/trip_coord.csv</p>\n<p>/data/upload/trip_coord.csv</p>\n<p>flume-ng agent -c /etc/flume-ng/conf/ -f avro.conf -n a1 -Dflume.root.logger=INFO,console</p>\n<p>flume-ng agent -c /etc/flume-ng/conf/ -f avro2.conf -n a1 -Dflume.root.logger=INFO,console</p>\n<p>flume-ng agent -c /etc/flume-ng/conf/ -f curl.conf -n a1 -Dflume.root.logger=INFO,console</p>\n<p>flume-ng agent -c /etc/flume-ng/conf/ -f self2.conf -n a1 -Dflume.root.logger=INFO,console</p>\n<p>flume-ng agent -c /etc/flume-ng/conf/ -f kafka.conf -n a1 -Dflume.root.logger=INFO,console</p>\n<p>flume-ng agent -c /etc/flume-ng/conf/ -f tcp_logger.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f avro3.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f tcp_hdfs.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f avro.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f avro2.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f tcp.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f tcp4.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f syslog.conf -n a1 -Dflume.root.logger=INFO,console</p>\n<p>flume-ng agent -c /etc/flume-ng/conf/ -f http_logger.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f api_logger.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f api_logger_twosink.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f kafka.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f demo4.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f tcp.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f tcp_two_sink.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f spool.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f taildir.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f udp.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f udp_two_sink.conf -n a1 -Dflume.root.logger=INFO,console</p>\n<hr>\n<p>2020-7-22</p>\n<p>docker run –restart=unless-stopped  -d -p 9200:9200 -p 9300:9300 -e “discovery.type=single-node”  –name=”hankoues” elasticsearch:7.1.0</p>\n<p>docker pull docker.io/cerebro<br>docker pull docker.io/cerebro</p>\n<hr>\n","site":{"data":{}},"excerpt":"<p>flume几种source and sink实际操作</p>","more":"<h2 id=\"soruce\"><a href=\"#soruce\" class=\"headerlink\" title=\"soruce\"></a>soruce</h2><p>flume自身就支持多种source.<br>chanel这里暂用mem<br>sink to hdfs</p>\n<p>简单测试几种source</p>\n<figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置Agent</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span> = <span class=\"string\">r1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span> = <span class=\"string\">k1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置Source</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.type</span> = <span class=\"string\">exec</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.deserializer.outputCharset</span> = <span class=\"string\">UTF-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置需要监控的日志输出目录</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.command</span> = <span class=\"string\">tail</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置Sink</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span> = <span class=\"string\">hdfs</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.useLocalTimeStamp</span> = <span class=\"string\">true</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.path</span> = <span class=\"string\">hdfs://namenode:9000/flume/events/%Y-%m</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.filePrefix</span> = <span class=\"string\">%Y-%m-%d-%H</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.fileSuffix</span> = <span class=\"string\">.log</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.minBlockReplicas</span> = <span class=\"string\">1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.fileType</span> = <span class=\"string\">DataStream</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.writeFormat</span> = <span class=\"string\">Text</span></span><br><span class=\"line\"><span class=\"comment\">#a1.sinks.k1.hdfs.rollInterval = 86400</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.rollSize</span> =<span class=\"string\">0</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.rollCount</span> =<span class=\"string\">0</span></span><br><span class=\"line\"><span class=\"comment\">#a1.sinks.k1.hdfs.idleTimeout = 0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置Channel</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span> = <span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将三者连接</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channel</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span> = <span class=\"string\">c1</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"命令\"><a href=\"#命令\" class=\"headerlink\" title=\"命令\"></a>命令</h2><p>flume-ng agent -c /etc/flume-ng/conf/ -f kafka.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f demo2.conf -n a1 -Dflume.root.logger=INFO,console<br>16228</p>\n<h2 id=\"解决Flume采集数据时在HDFS上产生大量小文件的问题\"><a href=\"#解决Flume采集数据时在HDFS上产生大量小文件的问题\" class=\"headerlink\" title=\"解决Flume采集数据时在HDFS上产生大量小文件的问题\"></a>解决Flume采集数据时在HDFS上产生大量小文件的问题</h2><p>以上conf为例</p>\n<p>a1为agent的名称<br>demo.conf为flume配置文件的名称<br>-c指向log4j.properties文件和flume_env.sh文件所在目录。<br>–Dflume.root.logger=INFO,console 在终端输出运行日志</p>\n<p>查阅flume配置参数，如下：</p>\n<p>rollSize<br>默认值：1024，当临时文件达到该大小（单位：bytes）时，滚动成目标文件。如果设置成0，则表示不根据临时文件大小来滚动文件。</p>\n<p>rollCount<br>默认值：10，当events数据达到该数量时候，将临时文件滚动成目标文件，如果设置成0，则表示不根据events数据来滚动文件。</p>\n<p>round<br>默认值：false，是否启用时间上的”舍弃”，类似于”四舍五入”，如果启用，则会影响除了%t的其他所有时间表达式；</p>\n<p>roundValue<br>默认值：1，时间上进行“舍弃”的值；</p>\n<p>roundUnit</p>\n<p>默认值：seconds，时间上进行”舍弃”的单位，包含：second,minute,hour</p>\n<p>当设置了round、roundValue、roundUnit参数收，需要在sink指定的HDFS路径上指定按照时间生成的目录的格式，例如有需求，每采集1小时就在HDFS目录上生成一个目录，里面存放这1小时内采集到的数据。</p>\n<h2 id=\"flume-file-to-avro\"><a href=\"#flume-file-to-avro\" class=\"headerlink\" title=\"flume file to avro\"></a>flume file to avro</h2><figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#########a1 agent#####</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span> = <span class=\"string\">r1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span> = <span class=\"string\">k1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置Source</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.type</span> = <span class=\"string\">exec</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.deserializer.outputCharset</span> = <span class=\"string\">UTF-8</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.command</span> = <span class=\"string\">cat  /data/upload/theme_item_pool.csv</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置需要监控的日志输出目录</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span> = <span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span> = <span class=\"string\">avro</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hostname</span> = <span class=\"string\">namenode</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.port</span> = <span class=\"string\">4444</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.requiredAcks</span> = <span class=\"string\">1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span> = <span class=\"string\">c1</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"flume-avro-to-kafka\"><a href=\"#flume-avro-to-kafka\" class=\"headerlink\" title=\"flume avro to kafka\"></a>flume avro to kafka</h2><figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#########a1 agent#####</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span> = <span class=\"string\">r1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span> = <span class=\"string\">k1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置Source</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.type</span> = <span class=\"string\">exec</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.deserializer.outputCharset</span> = <span class=\"string\">UTF-8</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.command</span> = <span class=\"string\">cat  /data/upload/theme_item_pool.csv</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置需要监控的日志输出目录</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span> = <span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span> = <span class=\"string\">avro</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hostname</span> = <span class=\"string\">namenode</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.port</span> = <span class=\"string\">4444</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.requiredAcks</span> = <span class=\"string\">1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span> = <span class=\"string\">c1</span></span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"avro-source\"><a href=\"#avro-source\" class=\"headerlink\" title=\"avro source\"></a>avro source</h2><figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#test avro sources</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span>=<span class=\"string\">r1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span>=<span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span>=<span class=\"string\">k1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.type</span> = <span class=\"string\">avro</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channels</span>=<span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.bind</span>=<span class=\"string\">localhost</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.port</span>=<span class=\"string\">55555</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Use a channel which buffers events in memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span>=<span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#sink配置</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span> = <span class=\"string\">hdfs</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.useLocalTimeStamp</span> = <span class=\"string\">true</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.path</span> = <span class=\"string\">hdfs://namenode:8020/flume/events/%Y-%m</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.filePrefix</span> = <span class=\"string\">%Y-%m-%d-%H</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.fileSuffix</span> = <span class=\"string\">.log</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.minBlockReplicas</span> = <span class=\"string\">1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.fileType</span> = <span class=\"string\">DataStream</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.writeFormat</span> = <span class=\"string\">Text</span></span><br><span class=\"line\"><span class=\"comment\">#a1.sinks.k1.hdfs.rollInterval = 86400</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.rollSize</span> =<span class=\"string\">0</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.rollCount</span> =<span class=\"string\">0</span></span><br><span class=\"line\"><span class=\"comment\">#a1.sinks.k1.hdfs.idleTimeout = 0</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span> = <span class=\"string\">c1</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"avro-to-file\"><a href=\"#avro-to-file\" class=\"headerlink\" title=\"avro to file\"></a>avro to file</h2><figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#test avro sources</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span>=<span class=\"string\">r1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span>=<span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span>=<span class=\"string\">k1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.type</span> = <span class=\"string\">exec</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.deserializer.outputCharset</span> = <span class=\"string\">UTF-8</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.command</span> =<span class=\"string\">curl http://192.168.10.104:8088/ws/v1/cluster/apps</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Use a channel which buffers events in memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span>=<span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#sink配置</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span>=<span class=\"string\">file_roll</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span>=<span class=\"string\">c1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.sink.directory</span>=<span class=\"string\">/data/hadoop/flume-ng/config/log</span></span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">### 多个sink</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#test avro sources</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span>=<span class=\"string\">r1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span>=<span class=\"string\">c1 c2</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span>=<span class=\"string\">k1 k2</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.type</span> = <span class=\"string\">netcat</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.bind</span>=<span class=\"string\">192.168.10.101</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.port</span>=<span class=\"string\">55555</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channels</span> = <span class=\"string\">c1 c2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Use a channel which buffers events in memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span>=<span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###channel2</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c2.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c2.capacity</span>=<span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c2.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#sink配置</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#a1.sinks.k1.type=logger</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span>=<span class=\"string\">file_roll</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span>=<span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.sink.directory</span>=<span class=\"string\">/data/hadoop/flume-ng/config/log</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">###sink2</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k2.type</span> = <span class=\"string\">org.apache.flume.sink.kafka.KafkaSink</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k2.brokerList</span> = <span class=\"string\">namenode:9092</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k2.topic</span> = <span class=\"string\">carstream</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k2.batchSize</span> = <span class=\"string\">100</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k2.requiredAcks</span> = <span class=\"string\">1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k2.channel</span>=<span class=\"string\">c2</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"目录变化\"><a href=\"#目录变化\" class=\"headerlink\" title=\"目录变化\"></a>目录变化</h2><figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span>=<span class=\"string\">r1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span>=<span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span>=<span class=\"string\">k1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.type</span> = <span class=\"string\">spooldir</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.spoolDir</span> = <span class=\"string\">/data/hadoop/flume-ng/config/log</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.fileHeader</span> = <span class=\"string\">true</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.r1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Use a channel which buffers events in memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span>=<span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span>=<span class=\"string\">logger</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span>=<span class=\"string\">c1</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"tailDir\"><a href=\"#tailDir\" class=\"headerlink\" title=\"tailDir\"></a>tailDir</h2><figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># in this case called 'a1'</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">a1.sources</span> = <span class=\"string\">s1</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks</span> = <span class=\"string\">k1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># For each one of the sources, the type is defined</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.s1.type</span> = <span class=\"string\">org.apache.flume.source.taildir.TaildirSource</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.s1.positionFile</span> = <span class=\"string\">/opt/cdhmoduels/apache-flume-1.5.0-cdh5.3.6-bin/taidir/dirsource/taildir_position.json</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.s1.filegroups</span> = <span class=\"string\">f1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.s1.filegroups.f1</span> = <span class=\"string\">/data/hadoop/flume-ng/log/</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># The channel can be defined as follows.</span></span><br><span class=\"line\"><span class=\"meta\">a1.sources.s1.channels</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.channel</span> = <span class=\"string\">c1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Each sink's type must be defined</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.type</span> = <span class=\"string\">hdfs</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.path</span> = <span class=\"string\">/flume/event/taildir</span></span><br><span class=\"line\"><span class=\"meta\">a1.sinks.k1.hdfs.filePrefix</span> = <span class=\"string\">hive-log</span></span><br><span class=\"line\"><span class=\"comment\">#Specify the channel the sink should use</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Each channel's type is defined.</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.type</span> = <span class=\"string\">memory</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Other config values specific to each type of channel(sink or source)</span></span><br><span class=\"line\"><span class=\"comment\"># can be defined as well</span></span><br><span class=\"line\"><span class=\"comment\"># In this case, it specifies the capacity of the memory channel</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.capacity</span> = <span class=\"string\">1000</span></span><br><span class=\"line\"><span class=\"meta\">a1.channels.c1.transactionCapacity</span> = <span class=\"string\">1000</span></span><br></pre></td></tr></table></figure>\n\n\n\n\n\n<h2 id=\"操作记录\"><a href=\"#操作记录\" class=\"headerlink\" title=\"操作记录\"></a>操作记录</h2><p>flume-ng avro-client -c conf -H localhost -p 55555  -F /data/upload/trip_coord.csv</p>\n<p>/data/upload/trip_coord.csv</p>\n<p>flume-ng agent -c /etc/flume-ng/conf/ -f avro.conf -n a1 -Dflume.root.logger=INFO,console</p>\n<p>flume-ng agent -c /etc/flume-ng/conf/ -f avro2.conf -n a1 -Dflume.root.logger=INFO,console</p>\n<p>flume-ng agent -c /etc/flume-ng/conf/ -f curl.conf -n a1 -Dflume.root.logger=INFO,console</p>\n<p>flume-ng agent -c /etc/flume-ng/conf/ -f self2.conf -n a1 -Dflume.root.logger=INFO,console</p>\n<p>flume-ng agent -c /etc/flume-ng/conf/ -f kafka.conf -n a1 -Dflume.root.logger=INFO,console</p>\n<p>flume-ng agent -c /etc/flume-ng/conf/ -f tcp_logger.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f avro3.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f tcp_hdfs.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f avro.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f avro2.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f tcp.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f tcp4.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f syslog.conf -n a1 -Dflume.root.logger=INFO,console</p>\n<p>flume-ng agent -c /etc/flume-ng/conf/ -f http_logger.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f api_logger.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f api_logger_twosink.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f kafka.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f demo4.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f tcp.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f tcp_two_sink.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f spool.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f taildir.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f udp.conf -n a1 -Dflume.root.logger=INFO,console<br>flume-ng agent -c /etc/flume-ng/conf/ -f udp_two_sink.conf -n a1 -Dflume.root.logger=INFO,console</p>\n<hr>\n<p>2020-7-22</p>\n<p>docker run –restart=unless-stopped  -d -p 9200:9200 -p 9300:9300 -e “discovery.type=single-node”  –name=”hankoues” elasticsearch:7.1.0</p>\n<p>docker pull docker.io/cerebro<br>docker pull docker.io/cerebro</p>\n<hr>"},{"_content":"# flume in docker\n\n```\n公司flume的docker镜像,\n```\n","source":"_posts/技术/flume/docker版flume使用.md","raw":"# flume in docker\n\n```\n公司flume的docker镜像,\n```\n","slug":"技术/flume/docker版flume使用","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/flume/docker版flume使用","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31g000j38pw047i8hma","content":"<h1 id=\"flume-in-docker\"><a href=\"#flume-in-docker\" class=\"headerlink\" title=\"flume in docker\"></a>flume in docker</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">公司flume的docker镜像,</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"flume-in-docker\"><a href=\"#flume-in-docker\" class=\"headerlink\" title=\"flume in docker\"></a>flume in docker</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">公司flume的docker镜像,</span><br></pre></td></tr></table></figure>\n"},{"_content":"server {\n  listen 8089;\n  server_name ; // 你的域名或者 ip\n  root /root/H-ui.admin; // 你的克隆到的项目路径\n  index index.html; // 显示首页\n  location ~* ^.+\\.(jpg|jpeg|gif|png|ico|css|js|pdf|txt){\n    root //root/H-ui.admin;\n  } // 静态文件访问\n}\n\n\nserver {\n    listen       9001;\n    server_name  localhost;\n    location / {\n        root   /root/node-project/pm2test;\n        #index  index.html index.htm;\n    }\n}\n\n\n\n# For more information on configuration, see:\n#   * Official English Documentation: http://nginx.org/en/docs/\n#   * Official Russian Documentation: http://nginx.org/ru/docs/\n\nuser root;\nworker_processes auto;\nerror_log /var/log/nginx/error.log;\npid /run/nginx.pid;\n\n# Load dynamic modules. See /usr/share/nginx/README.dynamic.\ninclude /usr/share/nginx/modules/*.conf;\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile            on;\n    tcp_nopush          on;\n    tcp_nodelay         on;\n    keepalive_timeout   65;\n    types_hash_max_size 2048;\n\n    include             /etc/nginx/mime.types;\n    default_type        application/octet-stream;\n\n    # Load modular configuration files from the /etc/nginx/conf.d directory.\n    # See http://nginx.org/en/docs/ngx_core_module.html#include\n    # for more information.\n\n    #include /etc/nginx/conf.d/*.conf;\n\n    server {\n  listen 8089;\n  server_name  116.196.81.123;\n  root /root/H-ui.admin;\n  index index.html;\n  location ~* ^.+\\.(jpg|jpeg|gif|png|ico|css|js|pdf|txt){\n    root //root/H-ui.admin;\n  }\n}\n    include /etc/nginx/default.d/*.conf;\n\n# Settings for a TLS enabled server.\n#\n#    server {\n#        listen       443 ssl http2 default_server;\n#        listen       [::]:443 ssl http2 default_server;\n#        server_name  _;\n#        root         /usr/share/nginx/html;\n#\n#        ssl_certificate \"/etc/pki/nginx/server.crt\";\n#        ssl_certificate_key \"/etc/pki/nginx/private/server.key\";\n#        ssl_session_cache shared:SSL:1m;\n#        ssl_session_timeout  10m;\n#        ssl_ciphers HIGH:!aNULL:!MD5;\n#        ssl_prefer_server_ciphers on;\n#\n#        # Load configuration files for the default server block.\n#        include /etc/nginx/default.d/*.conf;\n#\n#        location / {\n#        }\n#\n#        error_page 404 /404.html;\n#            location = /40x.html {\n#        }\n#\n#        error_page 500 502 503 504 /50x.html;\n#            location = /50x.html {\n#        }\n#    }\n\n}\n","source":"_posts/技术/front_end/nginx部署.md","raw":"server {\n  listen 8089;\n  server_name ; // 你的域名或者 ip\n  root /root/H-ui.admin; // 你的克隆到的项目路径\n  index index.html; // 显示首页\n  location ~* ^.+\\.(jpg|jpeg|gif|png|ico|css|js|pdf|txt){\n    root //root/H-ui.admin;\n  } // 静态文件访问\n}\n\n\nserver {\n    listen       9001;\n    server_name  localhost;\n    location / {\n        root   /root/node-project/pm2test;\n        #index  index.html index.htm;\n    }\n}\n\n\n\n# For more information on configuration, see:\n#   * Official English Documentation: http://nginx.org/en/docs/\n#   * Official Russian Documentation: http://nginx.org/ru/docs/\n\nuser root;\nworker_processes auto;\nerror_log /var/log/nginx/error.log;\npid /run/nginx.pid;\n\n# Load dynamic modules. See /usr/share/nginx/README.dynamic.\ninclude /usr/share/nginx/modules/*.conf;\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile            on;\n    tcp_nopush          on;\n    tcp_nodelay         on;\n    keepalive_timeout   65;\n    types_hash_max_size 2048;\n\n    include             /etc/nginx/mime.types;\n    default_type        application/octet-stream;\n\n    # Load modular configuration files from the /etc/nginx/conf.d directory.\n    # See http://nginx.org/en/docs/ngx_core_module.html#include\n    # for more information.\n\n    #include /etc/nginx/conf.d/*.conf;\n\n    server {\n  listen 8089;\n  server_name  116.196.81.123;\n  root /root/H-ui.admin;\n  index index.html;\n  location ~* ^.+\\.(jpg|jpeg|gif|png|ico|css|js|pdf|txt){\n    root //root/H-ui.admin;\n  }\n}\n    include /etc/nginx/default.d/*.conf;\n\n# Settings for a TLS enabled server.\n#\n#    server {\n#        listen       443 ssl http2 default_server;\n#        listen       [::]:443 ssl http2 default_server;\n#        server_name  _;\n#        root         /usr/share/nginx/html;\n#\n#        ssl_certificate \"/etc/pki/nginx/server.crt\";\n#        ssl_certificate_key \"/etc/pki/nginx/private/server.key\";\n#        ssl_session_cache shared:SSL:1m;\n#        ssl_session_timeout  10m;\n#        ssl_ciphers HIGH:!aNULL:!MD5;\n#        ssl_prefer_server_ciphers on;\n#\n#        # Load configuration files for the default server block.\n#        include /etc/nginx/default.d/*.conf;\n#\n#        location / {\n#        }\n#\n#        error_page 404 /404.html;\n#            location = /40x.html {\n#        }\n#\n#        error_page 500 502 503 504 /50x.html;\n#            location = /50x.html {\n#        }\n#    }\n\n}\n","slug":"技术/front_end/nginx部署","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/front_end/nginx部署","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31g000k38pw882r0wxd","content":"<p>server {<br>  listen 8089;<br>  server_name ; // 你的域名或者 ip<br>  root /root/H-ui.admin; // 你的克隆到的项目路径<br>  index index.html; // 显示首页<br>  location ~* ^.+.(jpg|jpeg|gif|png|ico|css|js|pdf|txt){<br>    root //root/H-ui.admin;<br>  } // 静态文件访问<br>}</p>\n<p>server {<br>    listen       9001;<br>    server_name  localhost;<br>    location / {<br>        root   /root/node-project/pm2test;<br>        #index  index.html index.htm;<br>    }<br>}</p>\n<h1 id=\"For-more-information-on-configuration-see\"><a href=\"#For-more-information-on-configuration-see\" class=\"headerlink\" title=\"For more information on configuration, see:\"></a>For more information on configuration, see:</h1><h1 id=\"Official-English-Documentation-http-nginx-org-en-docs\"><a href=\"#Official-English-Documentation-http-nginx-org-en-docs\" class=\"headerlink\" title=\"* Official English Documentation: http://nginx.org/en/docs/\"></a>* Official English Documentation: <a href=\"http://nginx.org/en/docs/\" target=\"_blank\" rel=\"noopener\">http://nginx.org/en/docs/</a></h1><h1 id=\"Official-Russian-Documentation-http-nginx-org-ru-docs\"><a href=\"#Official-Russian-Documentation-http-nginx-org-ru-docs\" class=\"headerlink\" title=\"* Official Russian Documentation: http://nginx.org/ru/docs/\"></a>* Official Russian Documentation: <a href=\"http://nginx.org/ru/docs/\" target=\"_blank\" rel=\"noopener\">http://nginx.org/ru/docs/</a></h1><p>user root;<br>worker_processes auto;<br>error_log /var/log/nginx/error.log;<br>pid /run/nginx.pid;</p>\n<h1 id=\"Load-dynamic-modules-See-usr-share-nginx-README-dynamic\"><a href=\"#Load-dynamic-modules-See-usr-share-nginx-README-dynamic\" class=\"headerlink\" title=\"Load dynamic modules. See /usr/share/nginx/README.dynamic.\"></a>Load dynamic modules. See /usr/share/nginx/README.dynamic.</h1><p>include /usr/share/nginx/modules/*.conf;</p>\n<p>events {<br>    worker_connections 1024;<br>}</p>\n<p>http {<br>    log_format  main  ‘$remote_addr - $remote_user [$time_local] “$request” ‘<br>                      ‘$status $body_bytes_sent “$http_referer” ‘<br>                      ‘“$http_user_agent” “$http_x_forwarded_for”‘;</p>\n<pre><code>access_log  /var/log/nginx/access.log  main;\n\nsendfile            on;\ntcp_nopush          on;\ntcp_nodelay         on;\nkeepalive_timeout   65;\ntypes_hash_max_size 2048;\n\ninclude             /etc/nginx/mime.types;\ndefault_type        application/octet-stream;\n\n# Load modular configuration files from the /etc/nginx/conf.d directory.\n# See http://nginx.org/en/docs/ngx_core_module.html#include\n# for more information.\n\n#include /etc/nginx/conf.d/*.conf;\n\nserver {</code></pre><p>  listen 8089;<br>  server_name  116.196.81.123;<br>  root /root/H-ui.admin;<br>  index index.html;<br>  location ~* ^.+.(jpg|jpeg|gif|png|ico|css|js|pdf|txt){<br>    root //root/H-ui.admin;<br>  }<br>}<br>    include /etc/nginx/default.d/*.conf;</p>\n<h1 id=\"Settings-for-a-TLS-enabled-server\"><a href=\"#Settings-for-a-TLS-enabled-server\" class=\"headerlink\" title=\"Settings for a TLS enabled server.\"></a>Settings for a TLS enabled server.</h1><p>#</p>\n<h1 id=\"server\"><a href=\"#server\" class=\"headerlink\" title=\"server {\"></a>server {</h1><h1 id=\"listen-443-ssl-http2-default-server\"><a href=\"#listen-443-ssl-http2-default-server\" class=\"headerlink\" title=\"listen       443 ssl http2 default_server;\"></a>listen       443 ssl http2 default_server;</h1><h1 id=\"listen-443-ssl-http2-default-server-1\"><a href=\"#listen-443-ssl-http2-default-server-1\" class=\"headerlink\" title=\"listen       [::]:443 ssl http2 default_server;\"></a>listen       [::]:443 ssl http2 default_server;</h1><h1 id=\"server-name\"><a href=\"#server-name\" class=\"headerlink\" title=\"server_name  _;\"></a>server_name  _;</h1><h1 id=\"root-usr-share-nginx-html\"><a href=\"#root-usr-share-nginx-html\" class=\"headerlink\" title=\"root         /usr/share/nginx/html;\"></a>root         /usr/share/nginx/html;</h1><p>#</p>\n<h1 id=\"ssl-certificate-“-etc-pki-nginx-server-crt”\"><a href=\"#ssl-certificate-“-etc-pki-nginx-server-crt”\" class=\"headerlink\" title=\"ssl_certificate “/etc/pki/nginx/server.crt”;\"></a>ssl_certificate “/etc/pki/nginx/server.crt”;</h1><h1 id=\"ssl-certificate-key-“-etc-pki-nginx-private-server-key”\"><a href=\"#ssl-certificate-key-“-etc-pki-nginx-private-server-key”\" class=\"headerlink\" title=\"ssl_certificate_key “/etc/pki/nginx/private/server.key”;\"></a>ssl_certificate_key “/etc/pki/nginx/private/server.key”;</h1><h1 id=\"ssl-session-cache-shared-SSL-1m\"><a href=\"#ssl-session-cache-shared-SSL-1m\" class=\"headerlink\" title=\"ssl_session_cache shared:SSL:1m;\"></a>ssl_session_cache shared:SSL:1m;</h1><h1 id=\"ssl-session-timeout-10m\"><a href=\"#ssl-session-timeout-10m\" class=\"headerlink\" title=\"ssl_session_timeout  10m;\"></a>ssl_session_timeout  10m;</h1><h1 id=\"ssl-ciphers-HIGH-aNULL-MD5\"><a href=\"#ssl-ciphers-HIGH-aNULL-MD5\" class=\"headerlink\" title=\"ssl_ciphers HIGH:!aNULL:!MD5;\"></a>ssl_ciphers HIGH:!aNULL:!MD5;</h1><h1 id=\"ssl-prefer-server-ciphers-on\"><a href=\"#ssl-prefer-server-ciphers-on\" class=\"headerlink\" title=\"ssl_prefer_server_ciphers on;\"></a>ssl_prefer_server_ciphers on;</h1><p>#</p>\n<h1 id=\"Load-configuration-files-for-the-default-server-block\"><a href=\"#Load-configuration-files-for-the-default-server-block\" class=\"headerlink\" title=\"# Load configuration files for the default server block.\"></a># Load configuration files for the default server block.</h1><h1 id=\"include-etc-nginx-default-d-conf\"><a href=\"#include-etc-nginx-default-d-conf\" class=\"headerlink\" title=\"include /etc/nginx/default.d/*.conf;\"></a>include /etc/nginx/default.d/*.conf;</h1><p>#</p>\n<h1 id=\"location\"><a href=\"#location\" class=\"headerlink\" title=\"location / {\"></a>location / {</h1><h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"}\"></a>}</h1><p>#</p>\n<h1 id=\"error-page-404-404-html\"><a href=\"#error-page-404-404-html\" class=\"headerlink\" title=\"error_page 404 /404.html;\"></a>error_page 404 /404.html;</h1><h1 id=\"location-40x-html\"><a href=\"#location-40x-html\" class=\"headerlink\" title=\"location = /40x.html {\"></a>location = /40x.html {</h1><h1 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"}\"></a>}</h1><p>#</p>\n<h1 id=\"error-page-500-502-503-504-50x-html\"><a href=\"#error-page-500-502-503-504-50x-html\" class=\"headerlink\" title=\"error_page 500 502 503 504 /50x.html;\"></a>error_page 500 502 503 504 /50x.html;</h1><h1 id=\"location-50x-html\"><a href=\"#location-50x-html\" class=\"headerlink\" title=\"location = /50x.html {\"></a>location = /50x.html {</h1><h1 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"}\"></a>}</h1><h1 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"}\"></a>}</h1><p>}</p>\n","site":{"data":{}},"excerpt":"","more":"<p>server {<br>  listen 8089;<br>  server_name ; // 你的域名或者 ip<br>  root /root/H-ui.admin; // 你的克隆到的项目路径<br>  index index.html; // 显示首页<br>  location ~* ^.+.(jpg|jpeg|gif|png|ico|css|js|pdf|txt){<br>    root //root/H-ui.admin;<br>  } // 静态文件访问<br>}</p>\n<p>server {<br>    listen       9001;<br>    server_name  localhost;<br>    location / {<br>        root   /root/node-project/pm2test;<br>        #index  index.html index.htm;<br>    }<br>}</p>\n<h1 id=\"For-more-information-on-configuration-see\"><a href=\"#For-more-information-on-configuration-see\" class=\"headerlink\" title=\"For more information on configuration, see:\"></a>For more information on configuration, see:</h1><h1 id=\"Official-English-Documentation-http-nginx-org-en-docs\"><a href=\"#Official-English-Documentation-http-nginx-org-en-docs\" class=\"headerlink\" title=\"* Official English Documentation: http://nginx.org/en/docs/\"></a>* Official English Documentation: <a href=\"http://nginx.org/en/docs/\" target=\"_blank\" rel=\"noopener\">http://nginx.org/en/docs/</a></h1><h1 id=\"Official-Russian-Documentation-http-nginx-org-ru-docs\"><a href=\"#Official-Russian-Documentation-http-nginx-org-ru-docs\" class=\"headerlink\" title=\"* Official Russian Documentation: http://nginx.org/ru/docs/\"></a>* Official Russian Documentation: <a href=\"http://nginx.org/ru/docs/\" target=\"_blank\" rel=\"noopener\">http://nginx.org/ru/docs/</a></h1><p>user root;<br>worker_processes auto;<br>error_log /var/log/nginx/error.log;<br>pid /run/nginx.pid;</p>\n<h1 id=\"Load-dynamic-modules-See-usr-share-nginx-README-dynamic\"><a href=\"#Load-dynamic-modules-See-usr-share-nginx-README-dynamic\" class=\"headerlink\" title=\"Load dynamic modules. See /usr/share/nginx/README.dynamic.\"></a>Load dynamic modules. See /usr/share/nginx/README.dynamic.</h1><p>include /usr/share/nginx/modules/*.conf;</p>\n<p>events {<br>    worker_connections 1024;<br>}</p>\n<p>http {<br>    log_format  main  ‘$remote_addr - $remote_user [$time_local] “$request” ‘<br>                      ‘$status $body_bytes_sent “$http_referer” ‘<br>                      ‘“$http_user_agent” “$http_x_forwarded_for”‘;</p>\n<pre><code>access_log  /var/log/nginx/access.log  main;\n\nsendfile            on;\ntcp_nopush          on;\ntcp_nodelay         on;\nkeepalive_timeout   65;\ntypes_hash_max_size 2048;\n\ninclude             /etc/nginx/mime.types;\ndefault_type        application/octet-stream;\n\n# Load modular configuration files from the /etc/nginx/conf.d directory.\n# See http://nginx.org/en/docs/ngx_core_module.html#include\n# for more information.\n\n#include /etc/nginx/conf.d/*.conf;\n\nserver {</code></pre><p>  listen 8089;<br>  server_name  116.196.81.123;<br>  root /root/H-ui.admin;<br>  index index.html;<br>  location ~* ^.+.(jpg|jpeg|gif|png|ico|css|js|pdf|txt){<br>    root //root/H-ui.admin;<br>  }<br>}<br>    include /etc/nginx/default.d/*.conf;</p>\n<h1 id=\"Settings-for-a-TLS-enabled-server\"><a href=\"#Settings-for-a-TLS-enabled-server\" class=\"headerlink\" title=\"Settings for a TLS enabled server.\"></a>Settings for a TLS enabled server.</h1><p>#</p>\n<h1 id=\"server\"><a href=\"#server\" class=\"headerlink\" title=\"server {\"></a>server {</h1><h1 id=\"listen-443-ssl-http2-default-server\"><a href=\"#listen-443-ssl-http2-default-server\" class=\"headerlink\" title=\"listen       443 ssl http2 default_server;\"></a>listen       443 ssl http2 default_server;</h1><h1 id=\"listen-443-ssl-http2-default-server-1\"><a href=\"#listen-443-ssl-http2-default-server-1\" class=\"headerlink\" title=\"listen       [::]:443 ssl http2 default_server;\"></a>listen       [::]:443 ssl http2 default_server;</h1><h1 id=\"server-name\"><a href=\"#server-name\" class=\"headerlink\" title=\"server_name  _;\"></a>server_name  _;</h1><h1 id=\"root-usr-share-nginx-html\"><a href=\"#root-usr-share-nginx-html\" class=\"headerlink\" title=\"root         /usr/share/nginx/html;\"></a>root         /usr/share/nginx/html;</h1><p>#</p>\n<h1 id=\"ssl-certificate-“-etc-pki-nginx-server-crt”\"><a href=\"#ssl-certificate-“-etc-pki-nginx-server-crt”\" class=\"headerlink\" title=\"ssl_certificate “/etc/pki/nginx/server.crt”;\"></a>ssl_certificate “/etc/pki/nginx/server.crt”;</h1><h1 id=\"ssl-certificate-key-“-etc-pki-nginx-private-server-key”\"><a href=\"#ssl-certificate-key-“-etc-pki-nginx-private-server-key”\" class=\"headerlink\" title=\"ssl_certificate_key “/etc/pki/nginx/private/server.key”;\"></a>ssl_certificate_key “/etc/pki/nginx/private/server.key”;</h1><h1 id=\"ssl-session-cache-shared-SSL-1m\"><a href=\"#ssl-session-cache-shared-SSL-1m\" class=\"headerlink\" title=\"ssl_session_cache shared:SSL:1m;\"></a>ssl_session_cache shared:SSL:1m;</h1><h1 id=\"ssl-session-timeout-10m\"><a href=\"#ssl-session-timeout-10m\" class=\"headerlink\" title=\"ssl_session_timeout  10m;\"></a>ssl_session_timeout  10m;</h1><h1 id=\"ssl-ciphers-HIGH-aNULL-MD5\"><a href=\"#ssl-ciphers-HIGH-aNULL-MD5\" class=\"headerlink\" title=\"ssl_ciphers HIGH:!aNULL:!MD5;\"></a>ssl_ciphers HIGH:!aNULL:!MD5;</h1><h1 id=\"ssl-prefer-server-ciphers-on\"><a href=\"#ssl-prefer-server-ciphers-on\" class=\"headerlink\" title=\"ssl_prefer_server_ciphers on;\"></a>ssl_prefer_server_ciphers on;</h1><p>#</p>\n<h1 id=\"Load-configuration-files-for-the-default-server-block\"><a href=\"#Load-configuration-files-for-the-default-server-block\" class=\"headerlink\" title=\"# Load configuration files for the default server block.\"></a># Load configuration files for the default server block.</h1><h1 id=\"include-etc-nginx-default-d-conf\"><a href=\"#include-etc-nginx-default-d-conf\" class=\"headerlink\" title=\"include /etc/nginx/default.d/*.conf;\"></a>include /etc/nginx/default.d/*.conf;</h1><p>#</p>\n<h1 id=\"location\"><a href=\"#location\" class=\"headerlink\" title=\"location / {\"></a>location / {</h1><h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"}\"></a>}</h1><p>#</p>\n<h1 id=\"error-page-404-404-html\"><a href=\"#error-page-404-404-html\" class=\"headerlink\" title=\"error_page 404 /404.html;\"></a>error_page 404 /404.html;</h1><h1 id=\"location-40x-html\"><a href=\"#location-40x-html\" class=\"headerlink\" title=\"location = /40x.html {\"></a>location = /40x.html {</h1><h1 id=\"-1\"><a href=\"#-1\" class=\"headerlink\" title=\"}\"></a>}</h1><p>#</p>\n<h1 id=\"error-page-500-502-503-504-50x-html\"><a href=\"#error-page-500-502-503-504-50x-html\" class=\"headerlink\" title=\"error_page 500 502 503 504 /50x.html;\"></a>error_page 500 502 503 504 /50x.html;</h1><h1 id=\"location-50x-html\"><a href=\"#location-50x-html\" class=\"headerlink\" title=\"location = /50x.html {\"></a>location = /50x.html {</h1><h1 id=\"-2\"><a href=\"#-2\" class=\"headerlink\" title=\"}\"></a>}</h1><h1 id=\"-3\"><a href=\"#-3\" class=\"headerlink\" title=\"}\"></a>}</h1><p>}</p>\n"},{"_content":"调研几款好用的前端框架与架构\n\n1. H-ui\n\n早期接触过\n\n\nkafka管理\n","source":"_posts/技术/front_end/调研.md","raw":"调研几款好用的前端框架与架构\n\n1. H-ui\n\n早期接触过\n\n\nkafka管理\n","slug":"技术/front_end/调研","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/front_end/调研","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31h000l38pwblds0jzk","content":"<p>调研几款好用的前端框架与架构</p>\n<ol>\n<li>H-ui</li>\n</ol>\n<p>早期接触过</p>\n<p>kafka管理</p>\n","site":{"data":{}},"excerpt":"","more":"<p>调研几款好用的前端框架与架构</p>\n<ol>\n<li>H-ui</li>\n</ol>\n<p>早期接触过</p>\n<p>kafka管理</p>\n"},{"_content":"```\n针对es的一些操作小结\n\n```\n\nes主要的重点在于查询,所以针对es的一些查询进行小结.\n\nes的查询分类\n\n主要有以下\n\n\n\n\nelasticsearch term 匹配索引值, macth匹配文本内容\n所以对text 类,用term会匹配不到 需要用match or match_phrase;\n\n\nsettings的设置\n\n```\n//静态设置：只能在索引创建时或者在状态为 closed index（闭合的索引）上设置\n\nindex.number_of_shards //主分片数，默认为5.只能在创建索引时设置，不能修改\n\nindex.shard.check_on_startup //是否应在索引打开前检查分片是否损坏，当检查到分片损坏将禁止分片被打开\n   false //默认值\n   checksum //检查物理损坏\n   true //检查物理和逻辑损坏，这将消耗大量内存和CPU\n   fix //检查物理和逻辑损坏。有损坏的分片将被集群自动删除，这可能导致数据丢失\n\nindex.routing_partition_size //自定义路由值可以转发的目的分片数。默认为 1，只能在索引创建时设置。此值必须小于index.number_of_shards\n\nindex.codec //默认使用LZ4压缩方式存储数据，也可以设置为 best_compression，它使用 DEFLATE 方式以牺牲字段存储性能为代价来获得更高的压缩比例。\n\n\n```\n\n```\n\nindex.number_of_replicas //每个主分片的副本数。默认为 1。\n\nindex.auto_expand_replicas //基于可用节点的数量自动分配副本数量,默认为 false（即禁用此功能）\n\nindex.refresh_interval //执行刷新操作的频率，这使得索引的最近更改可以被搜索。默认为 1s。可以设置为 -1 以禁用刷新。\n\nindex.max_result_window //用于索引搜索的 from+size 的最大值。默认为 10000\n\nindex.max_rescore_window // 在搜索此索引中 rescore 的 window_size 的最大值\n\nindex.blocks.read_only //设置为 true 使索引和索引元数据为只读，false 为允许写入和元数据更改。\n\nindex.blocks.read // 设置为 true 可禁用对索引的读取操作\n\nindex.blocks.write //设置为 true 可禁用对索引的写入操作。\n\nindex.blocks.metadata // 设置为 true 可禁用索引元数据的读取和写入。\n\nindex.max_refresh_listeners //索引的每个分片上可用的最大刷新侦听器数\n\n\n```\n","source":"_posts/技术/es/es小结.md","raw":"```\n针对es的一些操作小结\n\n```\n\nes主要的重点在于查询,所以针对es的一些查询进行小结.\n\nes的查询分类\n\n主要有以下\n\n\n\n\nelasticsearch term 匹配索引值, macth匹配文本内容\n所以对text 类,用term会匹配不到 需要用match or match_phrase;\n\n\nsettings的设置\n\n```\n//静态设置：只能在索引创建时或者在状态为 closed index（闭合的索引）上设置\n\nindex.number_of_shards //主分片数，默认为5.只能在创建索引时设置，不能修改\n\nindex.shard.check_on_startup //是否应在索引打开前检查分片是否损坏，当检查到分片损坏将禁止分片被打开\n   false //默认值\n   checksum //检查物理损坏\n   true //检查物理和逻辑损坏，这将消耗大量内存和CPU\n   fix //检查物理和逻辑损坏。有损坏的分片将被集群自动删除，这可能导致数据丢失\n\nindex.routing_partition_size //自定义路由值可以转发的目的分片数。默认为 1，只能在索引创建时设置。此值必须小于index.number_of_shards\n\nindex.codec //默认使用LZ4压缩方式存储数据，也可以设置为 best_compression，它使用 DEFLATE 方式以牺牲字段存储性能为代价来获得更高的压缩比例。\n\n\n```\n\n```\n\nindex.number_of_replicas //每个主分片的副本数。默认为 1。\n\nindex.auto_expand_replicas //基于可用节点的数量自动分配副本数量,默认为 false（即禁用此功能）\n\nindex.refresh_interval //执行刷新操作的频率，这使得索引的最近更改可以被搜索。默认为 1s。可以设置为 -1 以禁用刷新。\n\nindex.max_result_window //用于索引搜索的 from+size 的最大值。默认为 10000\n\nindex.max_rescore_window // 在搜索此索引中 rescore 的 window_size 的最大值\n\nindex.blocks.read_only //设置为 true 使索引和索引元数据为只读，false 为允许写入和元数据更改。\n\nindex.blocks.read // 设置为 true 可禁用对索引的读取操作\n\nindex.blocks.write //设置为 true 可禁用对索引的写入操作。\n\nindex.blocks.metadata // 设置为 true 可禁用索引元数据的读取和写入。\n\nindex.max_refresh_listeners //索引的每个分片上可用的最大刷新侦听器数\n\n\n```\n","slug":"技术/es/es小结","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/es/es小结","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31i000m38pwh3hy16rb","content":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">针对es的一些操作小结</span><br></pre></td></tr></table></figure>\n\n<p>es主要的重点在于查询,所以针对es的一些查询进行小结.</p>\n<p>es的查询分类</p>\n<p>主要有以下</p>\n<p>elasticsearch term 匹配索引值, macth匹配文本内容<br>所以对text 类,用term会匹配不到 需要用match or match_phrase;</p>\n<p>settings的设置</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;静态设置：只能在索引创建时或者在状态为 closed index（闭合的索引）上设置</span><br><span class=\"line\"></span><br><span class=\"line\">index.number_of_shards &#x2F;&#x2F;主分片数，默认为5.只能在创建索引时设置，不能修改</span><br><span class=\"line\"></span><br><span class=\"line\">index.shard.check_on_startup &#x2F;&#x2F;是否应在索引打开前检查分片是否损坏，当检查到分片损坏将禁止分片被打开</span><br><span class=\"line\">   false &#x2F;&#x2F;默认值</span><br><span class=\"line\">   checksum &#x2F;&#x2F;检查物理损坏</span><br><span class=\"line\">   true &#x2F;&#x2F;检查物理和逻辑损坏，这将消耗大量内存和CPU</span><br><span class=\"line\">   fix &#x2F;&#x2F;检查物理和逻辑损坏。有损坏的分片将被集群自动删除，这可能导致数据丢失</span><br><span class=\"line\"></span><br><span class=\"line\">index.routing_partition_size &#x2F;&#x2F;自定义路由值可以转发的目的分片数。默认为 1，只能在索引创建时设置。此值必须小于index.number_of_shards</span><br><span class=\"line\"></span><br><span class=\"line\">index.codec &#x2F;&#x2F;默认使用LZ4压缩方式存储数据，也可以设置为 best_compression，它使用 DEFLATE 方式以牺牲字段存储性能为代价来获得更高的压缩比例。</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">index.number_of_replicas &#x2F;&#x2F;每个主分片的副本数。默认为 1。</span><br><span class=\"line\"></span><br><span class=\"line\">index.auto_expand_replicas &#x2F;&#x2F;基于可用节点的数量自动分配副本数量,默认为 false（即禁用此功能）</span><br><span class=\"line\"></span><br><span class=\"line\">index.refresh_interval &#x2F;&#x2F;执行刷新操作的频率，这使得索引的最近更改可以被搜索。默认为 1s。可以设置为 -1 以禁用刷新。</span><br><span class=\"line\"></span><br><span class=\"line\">index.max_result_window &#x2F;&#x2F;用于索引搜索的 from+size 的最大值。默认为 10000</span><br><span class=\"line\"></span><br><span class=\"line\">index.max_rescore_window &#x2F;&#x2F; 在搜索此索引中 rescore 的 window_size 的最大值</span><br><span class=\"line\"></span><br><span class=\"line\">index.blocks.read_only &#x2F;&#x2F;设置为 true 使索引和索引元数据为只读，false 为允许写入和元数据更改。</span><br><span class=\"line\"></span><br><span class=\"line\">index.blocks.read &#x2F;&#x2F; 设置为 true 可禁用对索引的读取操作</span><br><span class=\"line\"></span><br><span class=\"line\">index.blocks.write &#x2F;&#x2F;设置为 true 可禁用对索引的写入操作。</span><br><span class=\"line\"></span><br><span class=\"line\">index.blocks.metadata &#x2F;&#x2F; 设置为 true 可禁用索引元数据的读取和写入。</span><br><span class=\"line\"></span><br><span class=\"line\">index.max_refresh_listeners &#x2F;&#x2F;索引的每个分片上可用的最大刷新侦听器数</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">针对es的一些操作小结</span><br></pre></td></tr></table></figure>\n\n<p>es主要的重点在于查询,所以针对es的一些查询进行小结.</p>\n<p>es的查询分类</p>\n<p>主要有以下</p>\n<p>elasticsearch term 匹配索引值, macth匹配文本内容<br>所以对text 类,用term会匹配不到 需要用match or match_phrase;</p>\n<p>settings的设置</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x2F;&#x2F;静态设置：只能在索引创建时或者在状态为 closed index（闭合的索引）上设置</span><br><span class=\"line\"></span><br><span class=\"line\">index.number_of_shards &#x2F;&#x2F;主分片数，默认为5.只能在创建索引时设置，不能修改</span><br><span class=\"line\"></span><br><span class=\"line\">index.shard.check_on_startup &#x2F;&#x2F;是否应在索引打开前检查分片是否损坏，当检查到分片损坏将禁止分片被打开</span><br><span class=\"line\">   false &#x2F;&#x2F;默认值</span><br><span class=\"line\">   checksum &#x2F;&#x2F;检查物理损坏</span><br><span class=\"line\">   true &#x2F;&#x2F;检查物理和逻辑损坏，这将消耗大量内存和CPU</span><br><span class=\"line\">   fix &#x2F;&#x2F;检查物理和逻辑损坏。有损坏的分片将被集群自动删除，这可能导致数据丢失</span><br><span class=\"line\"></span><br><span class=\"line\">index.routing_partition_size &#x2F;&#x2F;自定义路由值可以转发的目的分片数。默认为 1，只能在索引创建时设置。此值必须小于index.number_of_shards</span><br><span class=\"line\"></span><br><span class=\"line\">index.codec &#x2F;&#x2F;默认使用LZ4压缩方式存储数据，也可以设置为 best_compression，它使用 DEFLATE 方式以牺牲字段存储性能为代价来获得更高的压缩比例。</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">index.number_of_replicas &#x2F;&#x2F;每个主分片的副本数。默认为 1。</span><br><span class=\"line\"></span><br><span class=\"line\">index.auto_expand_replicas &#x2F;&#x2F;基于可用节点的数量自动分配副本数量,默认为 false（即禁用此功能）</span><br><span class=\"line\"></span><br><span class=\"line\">index.refresh_interval &#x2F;&#x2F;执行刷新操作的频率，这使得索引的最近更改可以被搜索。默认为 1s。可以设置为 -1 以禁用刷新。</span><br><span class=\"line\"></span><br><span class=\"line\">index.max_result_window &#x2F;&#x2F;用于索引搜索的 from+size 的最大值。默认为 10000</span><br><span class=\"line\"></span><br><span class=\"line\">index.max_rescore_window &#x2F;&#x2F; 在搜索此索引中 rescore 的 window_size 的最大值</span><br><span class=\"line\"></span><br><span class=\"line\">index.blocks.read_only &#x2F;&#x2F;设置为 true 使索引和索引元数据为只读，false 为允许写入和元数据更改。</span><br><span class=\"line\"></span><br><span class=\"line\">index.blocks.read &#x2F;&#x2F; 设置为 true 可禁用对索引的读取操作</span><br><span class=\"line\"></span><br><span class=\"line\">index.blocks.write &#x2F;&#x2F;设置为 true 可禁用对索引的写入操作。</span><br><span class=\"line\"></span><br><span class=\"line\">index.blocks.metadata &#x2F;&#x2F; 设置为 true 可禁用索引元数据的读取和写入。</span><br><span class=\"line\"></span><br><span class=\"line\">index.max_refresh_listeners &#x2F;&#x2F;索引的每个分片上可用的最大刷新侦听器数</span><br></pre></td></tr></table></figure>\n"},{"_content":"\n集成的ElasticSearch-Kinaba\n\ndocker pull nshou/elasticsearch-kibana\n\ndocker run -d -p 9200:9200 -p 5601:5601 nshou/elasticsearch-kibana\n\n\ndocker run -d --name elasticsearch --net host -e \"discovery.type=single-node\" elasticsearch:6.7.2\ndocker run -d --name kibana --net host -e \"discovery.type=single-node\"  -e ELASTICSEARCH_URL=http://localhost:9200  kibana:6.7.2\n\n\nelasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.2/elasticsearch-analysis-ik-6.7.2.zip\n","source":"_posts/技术/es/es的docker安装.md","raw":"\n集成的ElasticSearch-Kinaba\n\ndocker pull nshou/elasticsearch-kibana\n\ndocker run -d -p 9200:9200 -p 5601:5601 nshou/elasticsearch-kibana\n\n\ndocker run -d --name elasticsearch --net host -e \"discovery.type=single-node\" elasticsearch:6.7.2\ndocker run -d --name kibana --net host -e \"discovery.type=single-node\"  -e ELASTICSEARCH_URL=http://localhost:9200  kibana:6.7.2\n\n\nelasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.2/elasticsearch-analysis-ik-6.7.2.zip\n","slug":"技术/es/es的docker安装","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/es/es的docker安装","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31i000n38pw864p9o69","content":"<p>集成的ElasticSearch-Kinaba</p>\n<p>docker pull nshou/elasticsearch-kibana</p>\n<p>docker run -d -p 9200:9200 -p 5601:5601 nshou/elasticsearch-kibana</p>\n<p>docker run -d –name elasticsearch –net host -e “discovery.type=single-node” elasticsearch:6.7.2<br>docker run -d –name kibana –net host -e “discovery.type=single-node”  -e ELASTICSEARCH_URL=<a href=\"http://localhost:9200\" target=\"_blank\" rel=\"noopener\">http://localhost:9200</a>  kibana:6.7.2</p>\n<p>elasticsearch-plugin install <a href=\"https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.2/elasticsearch-analysis-ik-6.7.2.zip\" target=\"_blank\" rel=\"noopener\">https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.2/elasticsearch-analysis-ik-6.7.2.zip</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>集成的ElasticSearch-Kinaba</p>\n<p>docker pull nshou/elasticsearch-kibana</p>\n<p>docker run -d -p 9200:9200 -p 5601:5601 nshou/elasticsearch-kibana</p>\n<p>docker run -d –name elasticsearch –net host -e “discovery.type=single-node” elasticsearch:6.7.2<br>docker run -d –name kibana –net host -e “discovery.type=single-node”  -e ELASTICSEARCH_URL=<a href=\"http://localhost:9200\" target=\"_blank\" rel=\"noopener\">http://localhost:9200</a>  kibana:6.7.2</p>\n<p>elasticsearch-plugin install <a href=\"https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.2/elasticsearch-analysis-ik-6.7.2.zip\" target=\"_blank\" rel=\"noopener\">https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.2/elasticsearch-analysis-ik-6.7.2.zip</a></p>\n"},{"title":"es常用命令","date":"2020-02-24T16:00:00.000Z","_content":"\n查看集群是否健康\n\n\n```\nGET /_cluster/healt\n\n```\n\n查看节点列表\n\n\n```\nGET /_cat/nodes?v\n\n加v将表头显示出来\n\n```\n\n\n## 索引\n\n### 查询所有索引\n\n```\nGET /_cat/indices?v\n```\n\n### 查看某个索引的映射\n\n```\nGET /indeName/mapping\n```\n\n### 查看某个索引的设置\n\n```\nGET /indeName/mapping\n\n```\n### 添加一个索引\n\n```\nPUT /indexName\n{\n  \"settings\": {\n     \"number_of_shards\": 3,\n     \"number_of_replicas\": 1\n   },\n   \"mappings\": {\n     \"man\": {\n       \"dynamic\": \"strict\",\n       \"properties\": {\n         \"name\": {\n           \"type\": \"text\"\n         },\n         \"age\": {\n           \"type\": \"integer\"\n         },\n         \"birthday\": {\n           \"type\": \"date\",\n           \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\"\n         },\n         \"address\":{\n           \"dynamic\": \"true\",\n           \"type\": \"object\"\n         }\n       }\n     }\n   }\n  }\n\n```\n\ndynamic关键词说明:\n\"dynamic\":\"strict\"  表示如果遇到陌生field会报错\n\"dynamic\": true  表示如果遇到陌生字段，就进行dynamic mapping\n\"dynamic\": \"false\"   表示如果遇到陌生字段，就忽略\n\n---\n\n### 删除索引\n\n删除单个索引\n\n```\n\nDELETE /indexName\n\n```\n\n删除多个\n\n```\n\nDELETE /indexName1,indexName2\n\n```\n\n### 添加字段映射\n\n\n```\nPUT /indexName/_mapping/Field\n{\n  \"properties\":{\n    \"Field\":{\n      \"type\":\"text\"\n    }\n  }\n}\n```\n\n### 索引的别名\n\n\n#### 创建索引别名\n\n```\nPUT /indeName/_alias/aliasName\n\n```\n\n获取索引别名\n\n```\nGET /indexName/_alias/*\n\n```\n查询别名对应的索引\n\n```\nGET /*/_alias/aliasName\n\n```\n\n\n## 文档\n\n### 向索引中添加文档\n\n1. 自定义ID\n\n```\nPUT /indexName/type/id\n{\n  \"Field1\":\"message\",\n  \"Field2\":\"message\",\n  \"Field3\":\"message\",\n  \"Field4\":\"message\"\n}\n\n```  \n\n2. 随机生成id\n\n```\nPOST /indexName/type\n{\n  \"Field1\":\"message\",\n  \"Field2\":\"message\",\n  \"Field3\":\"message\",\n  \"Field4\":\"message\"\n}\n\n```\n\n后者则会自动生成id字符串\n\n3. 修改文档\n\n全文修改,即所有字段信息都要修改\n\n```\nPUT /indexName/type/id\n{\n  \"Field1\":\"update message\",\n  \"Field2\":\"update message\",\n  \"Field3\":\"message\",\n  \"Field4\":\"message\"\n}\n```\n部份修改\n\n```\nPOST /indexName/type/id/_update\n{\n  \"doc\": {\n    \"FIELD\": \"message\"\n  }\n}\n\n```\n脚本(再深入)\n\n```\n\nPOST /indexName/type/_id/_update\n{\n  \"script\": {\n    \"lang\": \"painless\",\n    \"source\": \"ctx._source.age += 10\"\n  }\n}\n```\n在修改document的时候，如果该文档不存在，则使用upsert操作进行初始化\n\n```\nPOST people/man/1/_update\n{\n  \"script\": \"ctx._source.age += 10\",\n  \"upsert\": {\n    \"age\": 20\n  }\n}\n```\n\n### 删除文档\n\n\n删除单个文档\n\n```\nDELETE /indexName/type/id\n\n```\n\n删除type下所有的文档\n\n```\nPOST /indexName/type/_delete_by_query?conflicts=proceed\n{\n  \"query\":{\n    \"match_all\":{\n\n    }\n  }\n}\n\n```\n\n### 查询文档\n\n查询单个文档\n\n```\nGET /indexName/type/id\n\n```\n\n批量查询文档(待验证)\n\n```\nGET /_mget\n{\n  \"docs\": [\n      {\n        \"_index\": \"index1\",\n        \"_type\": \"type\",\n        \"_id\": 1\n      },\n      {\n        \"_index\": \"index2\",\n        \"_type\": \"type\",\n        \"_id\": 2\n      }\n    ]\n}\n\n```\n```\n\nGET /indexName/type/_mget\n{\n\"docs\":[\n{\n  \"FEILD\":\"value\"\n},\n{\n  \"FEILD2\":\"value\"\n}\n  ]\n}\n\n\n```\n\n\n查询所有文档\n\n简单查询\n\n```\nGET /indexName/_serach\n\n```\n\n法二\n\n```\nPOST /people/_serach\n{\n  \"query\":{\n    \"match_all\":{\n    }\n  }\n}\n\n```\n查询某些字段内容\n```\n\n后面跟了 ?_source=field1,field2\n\nPOST /people/_serach?_source=field1,field2\n{\n  \"query\":{\n    \"match_all\":{\n    }\n  }\n}\n\n```\n\n查询多个索引下的多个type\n\n```\nGET /index1,index2/type1,type2/_search\n```\n\n查询所有索引下的部分type\n```\nGET /_all/type1,type2/_search\n\n```\n\n\n### 模糊查询\n```\n\nPOST /indexName/_search\n{\n  \"query\":{\n    \"match\":{\n      \"field\":\"message\"\n    }\n  }\n  ,\n  \"sort\":[\n  {\n    \"filed\":{\"order\":\"desc\"}\n  }\n  ]\n}\n\n\n```\n**注意**\nmessage将会被拆分进行匹配,若message是中文,则会按切分后的每个字来匹配,若\nmessage是英语,则会是按每个单词来匹配\n\n![](http://img.wqkenqing.ren/51ccc5cad8b8fbbbb6ef55d3106bfe43.png)\n![示列图](http://img.wqkenqing.ren/47a3ffe716f0026c004b155836a56641.png)\n\n\n全文搜索(按准度)\n\n```\nGET indexName/_search\n{\n  \"query\":{\n    \"match\":{\n      \"Field\":{\n        \"query\":\"val1 val2\",\n        \"operator\":\"and\"\n      }\n    }\n  }\n}\n\n```\n\n即Fileld 中必须有val1,val2\n\n\n按匹配度查询\n```\nGET /indexName/_search\n{\n\"query\":{\n  \"match\":{\n    \"Field\":{\n      \"query\":\"val1 val2 val3\"\n      \"minimum_should_match\":\"val\" eg:20%\n    }\n\n  }\n}\n}\n```\n即indexName,按Field中 val1 val2 val3 匹配度达到val即返回查询\n\n\n### 高级查询\n\n简单精准查询\n\n```\nGET /indexName/_search\n{\n  \"query\":{\n    \"match_phrase\":{\n      \"Field\":\"val\"\n    }\n  }\n}\n```\n即查询要完全匹配val,但若val只有一个中文,则会Field只要含有val,就会被查出\n\n\nslop结合\n```\nGET /people/_search\n{\n  \"query\": {\n    \"match_phrase\": {\n      \"name\": {\n        \"query\": \"张三\",\n        \"slop\": 3\n      }\n    }\n  }\n}\n```\n解读：slop是移动次数，上面案例表示“张”、“三”两个字可以经过最多挪动3次查询到！\n\nrescore (重打分）\n\n```\nGET /forum/article/_search\n{\n  \"query\": {\n    \"match\": {\n      \"content\": \"java spark\"\n    }\n  },\n  \"rescore\":{\n    \"window_size\": 50,\n    \"query\": {\n      \"rescore_query\": {\n        \"match_phrase\": {\n          \"content\": {\n            \"query\": \"java spark\",\n            \"slop\": 50\n          }\n        }\n      }\n    }\n  }\n}\n\n```\n\n\n多字段匹配查询\n\n```\nGET /indexName/_search\n{\n  \"query\":{\n    \"multi_match\":{\n      \"query\":\"val\"\n      \"fields\":[\"val1\",\"val2\"]\n    }\n  }\n}\n\n```\n\n在多个字段中,也是模糊查询val\n```\nGET /people/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"(叶良辰 AND 火) OR (赵日天 AND 风)\",\n      \"fields\": [\"name\",\"desc\"]\n    }\n  }\n}\n```\n\n### 字段查询\n\n精准查询\n\n```\nGET /indexName/_search\n{\n\"query\":{\n  \"term\":{\n    \"field\":\"val\"\n  }\n}\n}\n\n\n```\n\n分页查询\n\n```\nGET /indexName/_search\n{\n  \"query\":{\n    \"match_all\":{}\n  },\n  \"from\":num,\n  \"size\":num\n}\n```\n\n### 范围查询\n\n数据值型\n\n```\n\nGET /people/_search\n{\n  \"query\": {\n    \"range\": {\n      \"age\": {\n        \"gt\": 16,\n        \"lte\": 30\n      }\n    }\n  }\n}\n\n```\n日期类型\n\n```\nGET /people/_search\n{\n  \"query\": {\n    \"range\": {\n      \"birthday\": {\n        \"gte\": \"2013-01-01\",\n        \"lte\": \"now\"\n      }\n    }\n  }\n}\n\n```\n\n\n```\n\n\nGET book/_search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"range\": {\n          \"date\": {\n            \"gt\": \"now-1M\"\n          }\n        }\n      },\n      \"boost\": 1.2\n    }\n  }\n}\n\n```\n\n\"gt\": \"now-1M\"表示从今天开始，往前推一个月！\n\n\n### 过滤查询\n\n法一\n\n```\n\n\nPOST /people/man/_search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"range\": {\n          \"age\": {\n            \"gte\": 20,\n            \"lte\": 30\n          }\n        }\n      },\n      \"boost\": 1.2\n    }\n  }\n}\n\n\n```\n\n法二\n\n```\nPOST /people/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"filter\": {\n        \"term\": {\n          \"age\": 18\n        }\n      }\n    }\n  }\n}\n```\n\n\n### 布尔查询\nshould查询\n注意：should相当于 或 ，里面的match也是模糊匹配\n\n```\nPOST /people/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"match\": {\n            \"name\": \"叶良辰\"\n          }\n        },\n        {\n          \"match\": {\n            \"desc\": \"赵日天\"\n          }\n        }\n      ]\n    }\n  }\n}\n\n```\n\nmust查询\n注意：两个条件都要满足，并且这里也会把must里面的“叶良辰”拆分成“叶”、“良”和“辰”进行查询；“赵日天”拆分成“赵”、“日”、和“天”！\n\n\n\n```\n\nPOST /people/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"match\": {\n            \"name\": \"叶良辰\"\n          }\n        },\n        {\n          \"match\": {\n            \"desc\": \"赵日天\"\n          }\n        }\n      ]\n    }\n  }\n}\n\n```\n\nmust与filter相结合\n这里也会把must里面的“叶良辰”拆分成“叶”、“良”和“辰”进行查询\n\n\n\n```\n\nPOST /people/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"match\": {\n            \"name\": \"叶良辰\"\n          }\n        },\n        {\n          \"match\": {\n            \"desc\": \"赵日天\"\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"age\": 18\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\nmust_not\n注意：下面语句是精准匹配\n\n```\nPOST /people/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must_not\": {\n        \"term\": {\n          \"name\": \"叶良辰\"\n        }\n      }\n    }\n  }\n}\n```\n\n### 聚合查询\n\n根据字段类型查询\n\n\n```\n\nGET /people/man/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"group_by_age\": {\n      \"terms\": {\n        \"field\": \"age\"\n      }\n    }\n  }\n}\n```\n\n\n查询总体值\n\n```\nPOST /people/_search\n{\n  \"aggs\": {\n    \"grads_age\": {\n      \"stats\": {\n        \"field\": \"age\"\n      }\n    }\n  }\n}\n\n```\n\n查询最小值\n\n```\nPOST /people/_search\n{\n  \"aggs\": {\n    \"grads_age\": {\n      \"min\": {\n        \"field\": \"age\"\n      }\n    }\n  }\n}\n\n```\n\n根据国家分组，然后计算年龄平均值：\n\n\n\n```\n\nGET /people/man/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"group_by_age\": {\n      \"terms\": {\n        \"field\": \"country\"\n      },\n      \"aggs\": {\n        \"avg_age\": {\n          \"avg\": {\n            \"field\": \"age\"\n          }\n        }\n      }\n    }\n  }\n}\n\n```\n解决：上面的reason里面说的很清楚，将fielddata设置为true就行了：\n\n```\n\nPOST /people/_mapping/man\n{\n  \"properties\": {\n    \"country\": {\n      \"type\": \"text\",\n      \"fielddata\": true\n    }\n  }\n}\n```\n\n\n### 排序查询\n\n```\n排序查询通常没有排到我们想要的结果，因为字段分词后，有很多单词，再排序跟我们想要的结果又出入\n\n解决办法：把需要排序的字段建立两次索引，一个排序，另一个不排序。\n\n如下面的案例：把title.raw的fielddata设置为true，是排序的；而title的fielddata默认是false，可以用来搜索\n\nindex: true 是在title.raw建立索引可以被搜索到，\n\nfielddata: true是让其可以排序\n\nPUT /blog\n{\n  \"mappings\": {\n    \"article\": {\n      \"properties\": {\n        \"auther\": {\n          \"type\": \"text\"\n        },\n        \"title\": {\n          \"type\": \"text\",\n          \"fields\": {\n            \"raw\":{\n              \"type\": \"text\",\n              \"index\": true,\n              \"fielddata\": true\n            }\n          }\n        },\n        \"content\":{\n          \"type\": \"text\",\n          \"analyzer\": \"english\"\n        },\n        \"publishdate\": {\n          \"type\": \"date\"\n        }\n      }\n    }\n  }\n}\n\n```\n\n\n```\nGET /blog/article/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    {\n      \"title.raw\": {\n        \"order\": \"desc\"\n      }\n    }\n  ]\n}\n\n```\n\n\n### scroll查询\n\n\n```\n当搜索量比较大的时候，我们在短时间内不可能一次性搜索完然后展示出来\n\n这个时候，可以使用scroll进行搜索\n\n比如下面的案例，可以先搜索3条数据，然后结果中会有一个_scroll_id，下次搜索就可以直接用这个_scroll_id进行搜索了\n```\n\n\n```\nGET test_index/test_type/_search?scroll=1m\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": \"_doc\",\n  \"size\": 3\n}\n```\n\nstep3 把scroll_id粘贴到下面的命令中再次搜索\n\n```\nGET _search/scroll\n{\n  \"scroll\": \"1m\",\n  \"scroll_id\": \"DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAA6FnZPSl9sbVR4UVVDU1NLb2wxVXJlbWcAAAAAAAAAPhZ2T0pfbG1UeFFVQ1NTS29sMVVyZW1nAAAAAAAAADsWdk9KX2xtVHhRVUNTU0tvbDFVcmVtZwAAAAAAAAA8FnZPSl9sbVR4UVVDU1NLb2wxVXJlbWcAAAAAAAAAPRZ2T0pfbG1UeFFVQ1NTS29sMVVyZW1n\"\n}\n\n```\n","source":"_posts/技术/es/es常用命令.md","raw":"---\n\ntitle: es常用命令\ndate: 2020-02-25\ntags:\n\n---\n\n查看集群是否健康\n\n\n```\nGET /_cluster/healt\n\n```\n\n查看节点列表\n\n\n```\nGET /_cat/nodes?v\n\n加v将表头显示出来\n\n```\n\n\n## 索引\n\n### 查询所有索引\n\n```\nGET /_cat/indices?v\n```\n\n### 查看某个索引的映射\n\n```\nGET /indeName/mapping\n```\n\n### 查看某个索引的设置\n\n```\nGET /indeName/mapping\n\n```\n### 添加一个索引\n\n```\nPUT /indexName\n{\n  \"settings\": {\n     \"number_of_shards\": 3,\n     \"number_of_replicas\": 1\n   },\n   \"mappings\": {\n     \"man\": {\n       \"dynamic\": \"strict\",\n       \"properties\": {\n         \"name\": {\n           \"type\": \"text\"\n         },\n         \"age\": {\n           \"type\": \"integer\"\n         },\n         \"birthday\": {\n           \"type\": \"date\",\n           \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\"\n         },\n         \"address\":{\n           \"dynamic\": \"true\",\n           \"type\": \"object\"\n         }\n       }\n     }\n   }\n  }\n\n```\n\ndynamic关键词说明:\n\"dynamic\":\"strict\"  表示如果遇到陌生field会报错\n\"dynamic\": true  表示如果遇到陌生字段，就进行dynamic mapping\n\"dynamic\": \"false\"   表示如果遇到陌生字段，就忽略\n\n---\n\n### 删除索引\n\n删除单个索引\n\n```\n\nDELETE /indexName\n\n```\n\n删除多个\n\n```\n\nDELETE /indexName1,indexName2\n\n```\n\n### 添加字段映射\n\n\n```\nPUT /indexName/_mapping/Field\n{\n  \"properties\":{\n    \"Field\":{\n      \"type\":\"text\"\n    }\n  }\n}\n```\n\n### 索引的别名\n\n\n#### 创建索引别名\n\n```\nPUT /indeName/_alias/aliasName\n\n```\n\n获取索引别名\n\n```\nGET /indexName/_alias/*\n\n```\n查询别名对应的索引\n\n```\nGET /*/_alias/aliasName\n\n```\n\n\n## 文档\n\n### 向索引中添加文档\n\n1. 自定义ID\n\n```\nPUT /indexName/type/id\n{\n  \"Field1\":\"message\",\n  \"Field2\":\"message\",\n  \"Field3\":\"message\",\n  \"Field4\":\"message\"\n}\n\n```  \n\n2. 随机生成id\n\n```\nPOST /indexName/type\n{\n  \"Field1\":\"message\",\n  \"Field2\":\"message\",\n  \"Field3\":\"message\",\n  \"Field4\":\"message\"\n}\n\n```\n\n后者则会自动生成id字符串\n\n3. 修改文档\n\n全文修改,即所有字段信息都要修改\n\n```\nPUT /indexName/type/id\n{\n  \"Field1\":\"update message\",\n  \"Field2\":\"update message\",\n  \"Field3\":\"message\",\n  \"Field4\":\"message\"\n}\n```\n部份修改\n\n```\nPOST /indexName/type/id/_update\n{\n  \"doc\": {\n    \"FIELD\": \"message\"\n  }\n}\n\n```\n脚本(再深入)\n\n```\n\nPOST /indexName/type/_id/_update\n{\n  \"script\": {\n    \"lang\": \"painless\",\n    \"source\": \"ctx._source.age += 10\"\n  }\n}\n```\n在修改document的时候，如果该文档不存在，则使用upsert操作进行初始化\n\n```\nPOST people/man/1/_update\n{\n  \"script\": \"ctx._source.age += 10\",\n  \"upsert\": {\n    \"age\": 20\n  }\n}\n```\n\n### 删除文档\n\n\n删除单个文档\n\n```\nDELETE /indexName/type/id\n\n```\n\n删除type下所有的文档\n\n```\nPOST /indexName/type/_delete_by_query?conflicts=proceed\n{\n  \"query\":{\n    \"match_all\":{\n\n    }\n  }\n}\n\n```\n\n### 查询文档\n\n查询单个文档\n\n```\nGET /indexName/type/id\n\n```\n\n批量查询文档(待验证)\n\n```\nGET /_mget\n{\n  \"docs\": [\n      {\n        \"_index\": \"index1\",\n        \"_type\": \"type\",\n        \"_id\": 1\n      },\n      {\n        \"_index\": \"index2\",\n        \"_type\": \"type\",\n        \"_id\": 2\n      }\n    ]\n}\n\n```\n```\n\nGET /indexName/type/_mget\n{\n\"docs\":[\n{\n  \"FEILD\":\"value\"\n},\n{\n  \"FEILD2\":\"value\"\n}\n  ]\n}\n\n\n```\n\n\n查询所有文档\n\n简单查询\n\n```\nGET /indexName/_serach\n\n```\n\n法二\n\n```\nPOST /people/_serach\n{\n  \"query\":{\n    \"match_all\":{\n    }\n  }\n}\n\n```\n查询某些字段内容\n```\n\n后面跟了 ?_source=field1,field2\n\nPOST /people/_serach?_source=field1,field2\n{\n  \"query\":{\n    \"match_all\":{\n    }\n  }\n}\n\n```\n\n查询多个索引下的多个type\n\n```\nGET /index1,index2/type1,type2/_search\n```\n\n查询所有索引下的部分type\n```\nGET /_all/type1,type2/_search\n\n```\n\n\n### 模糊查询\n```\n\nPOST /indexName/_search\n{\n  \"query\":{\n    \"match\":{\n      \"field\":\"message\"\n    }\n  }\n  ,\n  \"sort\":[\n  {\n    \"filed\":{\"order\":\"desc\"}\n  }\n  ]\n}\n\n\n```\n**注意**\nmessage将会被拆分进行匹配,若message是中文,则会按切分后的每个字来匹配,若\nmessage是英语,则会是按每个单词来匹配\n\n![](http://img.wqkenqing.ren/51ccc5cad8b8fbbbb6ef55d3106bfe43.png)\n![示列图](http://img.wqkenqing.ren/47a3ffe716f0026c004b155836a56641.png)\n\n\n全文搜索(按准度)\n\n```\nGET indexName/_search\n{\n  \"query\":{\n    \"match\":{\n      \"Field\":{\n        \"query\":\"val1 val2\",\n        \"operator\":\"and\"\n      }\n    }\n  }\n}\n\n```\n\n即Fileld 中必须有val1,val2\n\n\n按匹配度查询\n```\nGET /indexName/_search\n{\n\"query\":{\n  \"match\":{\n    \"Field\":{\n      \"query\":\"val1 val2 val3\"\n      \"minimum_should_match\":\"val\" eg:20%\n    }\n\n  }\n}\n}\n```\n即indexName,按Field中 val1 val2 val3 匹配度达到val即返回查询\n\n\n### 高级查询\n\n简单精准查询\n\n```\nGET /indexName/_search\n{\n  \"query\":{\n    \"match_phrase\":{\n      \"Field\":\"val\"\n    }\n  }\n}\n```\n即查询要完全匹配val,但若val只有一个中文,则会Field只要含有val,就会被查出\n\n\nslop结合\n```\nGET /people/_search\n{\n  \"query\": {\n    \"match_phrase\": {\n      \"name\": {\n        \"query\": \"张三\",\n        \"slop\": 3\n      }\n    }\n  }\n}\n```\n解读：slop是移动次数，上面案例表示“张”、“三”两个字可以经过最多挪动3次查询到！\n\nrescore (重打分）\n\n```\nGET /forum/article/_search\n{\n  \"query\": {\n    \"match\": {\n      \"content\": \"java spark\"\n    }\n  },\n  \"rescore\":{\n    \"window_size\": 50,\n    \"query\": {\n      \"rescore_query\": {\n        \"match_phrase\": {\n          \"content\": {\n            \"query\": \"java spark\",\n            \"slop\": 50\n          }\n        }\n      }\n    }\n  }\n}\n\n```\n\n\n多字段匹配查询\n\n```\nGET /indexName/_search\n{\n  \"query\":{\n    \"multi_match\":{\n      \"query\":\"val\"\n      \"fields\":[\"val1\",\"val2\"]\n    }\n  }\n}\n\n```\n\n在多个字段中,也是模糊查询val\n```\nGET /people/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"(叶良辰 AND 火) OR (赵日天 AND 风)\",\n      \"fields\": [\"name\",\"desc\"]\n    }\n  }\n}\n```\n\n### 字段查询\n\n精准查询\n\n```\nGET /indexName/_search\n{\n\"query\":{\n  \"term\":{\n    \"field\":\"val\"\n  }\n}\n}\n\n\n```\n\n分页查询\n\n```\nGET /indexName/_search\n{\n  \"query\":{\n    \"match_all\":{}\n  },\n  \"from\":num,\n  \"size\":num\n}\n```\n\n### 范围查询\n\n数据值型\n\n```\n\nGET /people/_search\n{\n  \"query\": {\n    \"range\": {\n      \"age\": {\n        \"gt\": 16,\n        \"lte\": 30\n      }\n    }\n  }\n}\n\n```\n日期类型\n\n```\nGET /people/_search\n{\n  \"query\": {\n    \"range\": {\n      \"birthday\": {\n        \"gte\": \"2013-01-01\",\n        \"lte\": \"now\"\n      }\n    }\n  }\n}\n\n```\n\n\n```\n\n\nGET book/_search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"range\": {\n          \"date\": {\n            \"gt\": \"now-1M\"\n          }\n        }\n      },\n      \"boost\": 1.2\n    }\n  }\n}\n\n```\n\n\"gt\": \"now-1M\"表示从今天开始，往前推一个月！\n\n\n### 过滤查询\n\n法一\n\n```\n\n\nPOST /people/man/_search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"range\": {\n          \"age\": {\n            \"gte\": 20,\n            \"lte\": 30\n          }\n        }\n      },\n      \"boost\": 1.2\n    }\n  }\n}\n\n\n```\n\n法二\n\n```\nPOST /people/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"filter\": {\n        \"term\": {\n          \"age\": 18\n        }\n      }\n    }\n  }\n}\n```\n\n\n### 布尔查询\nshould查询\n注意：should相当于 或 ，里面的match也是模糊匹配\n\n```\nPOST /people/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"match\": {\n            \"name\": \"叶良辰\"\n          }\n        },\n        {\n          \"match\": {\n            \"desc\": \"赵日天\"\n          }\n        }\n      ]\n    }\n  }\n}\n\n```\n\nmust查询\n注意：两个条件都要满足，并且这里也会把must里面的“叶良辰”拆分成“叶”、“良”和“辰”进行查询；“赵日天”拆分成“赵”、“日”、和“天”！\n\n\n\n```\n\nPOST /people/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"match\": {\n            \"name\": \"叶良辰\"\n          }\n        },\n        {\n          \"match\": {\n            \"desc\": \"赵日天\"\n          }\n        }\n      ]\n    }\n  }\n}\n\n```\n\nmust与filter相结合\n这里也会把must里面的“叶良辰”拆分成“叶”、“良”和“辰”进行查询\n\n\n\n```\n\nPOST /people/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        {\n          \"match\": {\n            \"name\": \"叶良辰\"\n          }\n        },\n        {\n          \"match\": {\n            \"desc\": \"赵日天\"\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"term\": {\n            \"age\": 18\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\nmust_not\n注意：下面语句是精准匹配\n\n```\nPOST /people/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must_not\": {\n        \"term\": {\n          \"name\": \"叶良辰\"\n        }\n      }\n    }\n  }\n}\n```\n\n### 聚合查询\n\n根据字段类型查询\n\n\n```\n\nGET /people/man/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"group_by_age\": {\n      \"terms\": {\n        \"field\": \"age\"\n      }\n    }\n  }\n}\n```\n\n\n查询总体值\n\n```\nPOST /people/_search\n{\n  \"aggs\": {\n    \"grads_age\": {\n      \"stats\": {\n        \"field\": \"age\"\n      }\n    }\n  }\n}\n\n```\n\n查询最小值\n\n```\nPOST /people/_search\n{\n  \"aggs\": {\n    \"grads_age\": {\n      \"min\": {\n        \"field\": \"age\"\n      }\n    }\n  }\n}\n\n```\n\n根据国家分组，然后计算年龄平均值：\n\n\n\n```\n\nGET /people/man/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"group_by_age\": {\n      \"terms\": {\n        \"field\": \"country\"\n      },\n      \"aggs\": {\n        \"avg_age\": {\n          \"avg\": {\n            \"field\": \"age\"\n          }\n        }\n      }\n    }\n  }\n}\n\n```\n解决：上面的reason里面说的很清楚，将fielddata设置为true就行了：\n\n```\n\nPOST /people/_mapping/man\n{\n  \"properties\": {\n    \"country\": {\n      \"type\": \"text\",\n      \"fielddata\": true\n    }\n  }\n}\n```\n\n\n### 排序查询\n\n```\n排序查询通常没有排到我们想要的结果，因为字段分词后，有很多单词，再排序跟我们想要的结果又出入\n\n解决办法：把需要排序的字段建立两次索引，一个排序，另一个不排序。\n\n如下面的案例：把title.raw的fielddata设置为true，是排序的；而title的fielddata默认是false，可以用来搜索\n\nindex: true 是在title.raw建立索引可以被搜索到，\n\nfielddata: true是让其可以排序\n\nPUT /blog\n{\n  \"mappings\": {\n    \"article\": {\n      \"properties\": {\n        \"auther\": {\n          \"type\": \"text\"\n        },\n        \"title\": {\n          \"type\": \"text\",\n          \"fields\": {\n            \"raw\":{\n              \"type\": \"text\",\n              \"index\": true,\n              \"fielddata\": true\n            }\n          }\n        },\n        \"content\":{\n          \"type\": \"text\",\n          \"analyzer\": \"english\"\n        },\n        \"publishdate\": {\n          \"type\": \"date\"\n        }\n      }\n    }\n  }\n}\n\n```\n\n\n```\nGET /blog/article/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": [\n    {\n      \"title.raw\": {\n        \"order\": \"desc\"\n      }\n    }\n  ]\n}\n\n```\n\n\n### scroll查询\n\n\n```\n当搜索量比较大的时候，我们在短时间内不可能一次性搜索完然后展示出来\n\n这个时候，可以使用scroll进行搜索\n\n比如下面的案例，可以先搜索3条数据，然后结果中会有一个_scroll_id，下次搜索就可以直接用这个_scroll_id进行搜索了\n```\n\n\n```\nGET test_index/test_type/_search?scroll=1m\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"sort\": \"_doc\",\n  \"size\": 3\n}\n```\n\nstep3 把scroll_id粘贴到下面的命令中再次搜索\n\n```\nGET _search/scroll\n{\n  \"scroll\": \"1m\",\n  \"scroll_id\": \"DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAA6FnZPSl9sbVR4UVVDU1NLb2wxVXJlbWcAAAAAAAAAPhZ2T0pfbG1UeFFVQ1NTS29sMVVyZW1nAAAAAAAAADsWdk9KX2xtVHhRVUNTU0tvbDFVcmVtZwAAAAAAAAA8FnZPSl9sbVR4UVVDU1NLb2wxVXJlbWcAAAAAAAAAPRZ2T0pfbG1UeFFVQ1NTS29sMVVyZW1n\"\n}\n\n```\n","slug":"技术/es/es常用命令","published":1,"updated":"2021-01-05T02:35:36.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31i000o38pwf1vt0xir","content":"<p>查看集群是否健康</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;_cluster&#x2F;healt</span><br></pre></td></tr></table></figure>\n\n<p>查看节点列表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;_cat&#x2F;nodes?v</span><br><span class=\"line\"></span><br><span class=\"line\">加v将表头显示出来</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"索引\"><a href=\"#索引\" class=\"headerlink\" title=\"索引\"></a>索引</h2><h3 id=\"查询所有索引\"><a href=\"#查询所有索引\" class=\"headerlink\" title=\"查询所有索引\"></a>查询所有索引</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;_cat&#x2F;indices?v</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"查看某个索引的映射\"><a href=\"#查看某个索引的映射\" class=\"headerlink\" title=\"查看某个索引的映射\"></a>查看某个索引的映射</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indeName&#x2F;mapping</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"查看某个索引的设置\"><a href=\"#查看某个索引的设置\" class=\"headerlink\" title=\"查看某个索引的设置\"></a>查看某个索引的设置</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indeName&#x2F;mapping</span><br></pre></td></tr></table></figure>\n<h3 id=\"添加一个索引\"><a href=\"#添加一个索引\" class=\"headerlink\" title=\"添加一个索引\"></a>添加一个索引</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;indexName</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;settings&quot;: &#123;</span><br><span class=\"line\">     &quot;number_of_shards&quot;: 3,</span><br><span class=\"line\">     &quot;number_of_replicas&quot;: 1</span><br><span class=\"line\">   &#125;,</span><br><span class=\"line\">   &quot;mappings&quot;: &#123;</span><br><span class=\"line\">     &quot;man&quot;: &#123;</span><br><span class=\"line\">       &quot;dynamic&quot;: &quot;strict&quot;,</span><br><span class=\"line\">       &quot;properties&quot;: &#123;</span><br><span class=\"line\">         &quot;name&quot;: &#123;</span><br><span class=\"line\">           &quot;type&quot;: &quot;text&quot;</span><br><span class=\"line\">         &#125;,</span><br><span class=\"line\">         &quot;age&quot;: &#123;</span><br><span class=\"line\">           &quot;type&quot;: &quot;integer&quot;</span><br><span class=\"line\">         &#125;,</span><br><span class=\"line\">         &quot;birthday&quot;: &#123;</span><br><span class=\"line\">           &quot;type&quot;: &quot;date&quot;,</span><br><span class=\"line\">           &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot;</span><br><span class=\"line\">         &#125;,</span><br><span class=\"line\">         &quot;address&quot;:&#123;</span><br><span class=\"line\">           &quot;dynamic&quot;: &quot;true&quot;,</span><br><span class=\"line\">           &quot;type&quot;: &quot;object&quot;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>dynamic关键词说明:<br>“dynamic”:”strict”  表示如果遇到陌生field会报错<br>“dynamic”: true  表示如果遇到陌生字段，就进行dynamic mapping<br>“dynamic”: “false”   表示如果遇到陌生字段，就忽略</p>\n<hr>\n<h3 id=\"删除索引\"><a href=\"#删除索引\" class=\"headerlink\" title=\"删除索引\"></a>删除索引</h3><p>删除单个索引</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">DELETE &#x2F;indexName</span><br></pre></td></tr></table></figure>\n\n<p>删除多个</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">DELETE &#x2F;indexName1,indexName2</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"添加字段映射\"><a href=\"#添加字段映射\" class=\"headerlink\" title=\"添加字段映射\"></a>添加字段映射</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;indexName&#x2F;_mapping&#x2F;Field</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;properties&quot;:&#123;</span><br><span class=\"line\">    &quot;Field&quot;:&#123;</span><br><span class=\"line\">      &quot;type&quot;:&quot;text&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"索引的别名\"><a href=\"#索引的别名\" class=\"headerlink\" title=\"索引的别名\"></a>索引的别名</h3><h4 id=\"创建索引别名\"><a href=\"#创建索引别名\" class=\"headerlink\" title=\"创建索引别名\"></a>创建索引别名</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;indeName&#x2F;_alias&#x2F;aliasName</span><br></pre></td></tr></table></figure>\n\n<p>获取索引别名</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indexName&#x2F;_alias&#x2F;*</span><br></pre></td></tr></table></figure>\n<p>查询别名对应的索引</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;*&#x2F;_alias&#x2F;aliasName</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"文档\"><a href=\"#文档\" class=\"headerlink\" title=\"文档\"></a>文档</h2><h3 id=\"向索引中添加文档\"><a href=\"#向索引中添加文档\" class=\"headerlink\" title=\"向索引中添加文档\"></a>向索引中添加文档</h3><ol>\n<li>自定义ID</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;indexName&#x2F;type&#x2F;id</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;Field1&quot;:&quot;message&quot;,</span><br><span class=\"line\">  &quot;Field2&quot;:&quot;message&quot;,</span><br><span class=\"line\">  &quot;Field3&quot;:&quot;message&quot;,</span><br><span class=\"line\">  &quot;Field4&quot;:&quot;message&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#96;&#96;&#96;  </span><br><span class=\"line\"></span><br><span class=\"line\">2. 随机生成id</span><br></pre></td></tr></table></figure>\n<p>POST /indexName/type<br>{<br>  “Field1”:”message”,<br>  “Field2”:”message”,<br>  “Field3”:”message”,<br>  “Field4”:”message”<br>}</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">后者则会自动生成id字符串</span><br><span class=\"line\"></span><br><span class=\"line\">3. 修改文档</span><br><span class=\"line\"></span><br><span class=\"line\">全文修改,即所有字段信息都要修改</span><br></pre></td></tr></table></figure>\n<p>PUT /indexName/type/id<br>{<br>  “Field1”:”update message”,<br>  “Field2”:”update message”,<br>  “Field3”:”message”,<br>  “Field4”:”message”<br>}</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">部份修改</span><br></pre></td></tr></table></figure>\n<p>POST /indexName/type/id/_update<br>{<br>  “doc”: {<br>    “FIELD”: “message”<br>  }<br>}</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">脚本(再深入)</span><br></pre></td></tr></table></figure>\n\n<p>POST /indexName/type/_id/_update<br>{<br>  “script”: {<br>    “lang”: “painless”,<br>    “source”: “ctx._source.age += 10”<br>  }<br>}</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在修改document的时候，如果该文档不存在，则使用upsert操作进行初始化</span><br></pre></td></tr></table></figure>\n<p>POST people/man/1/_update<br>{<br>  “script”: “ctx._source.age += 10”,<br>  “upsert”: {<br>    “age”: 20<br>  }<br>}</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">### 删除文档</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">删除单个文档</span><br></pre></td></tr></table></figure>\n<p>DELETE /indexName/type/id</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">删除type下所有的文档</span><br></pre></td></tr></table></figure>\n<p>POST /indexName/type/_delete_by_query?conflicts=proceed<br>{<br>  “query”:{<br>    “match_all”:{</p>\n<pre><code>}</code></pre><p>  }<br>}</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">### 查询文档</span><br><span class=\"line\"></span><br><span class=\"line\">查询单个文档</span><br></pre></td></tr></table></figure>\n<p>GET /indexName/type/id</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">批量查询文档(待验证)</span><br></pre></td></tr></table></figure>\n<p>GET /_mget<br>{<br>  “docs”: [<br>      {<br>        “_index”: “index1”,<br>        “_type”: “type”,<br>        “_id”: 1<br>      },<br>      {<br>        “_index”: “index2”,<br>        “_type”: “type”,<br>        “_id”: 2<br>      }<br>    ]<br>}</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#96;&#96;&#96;</span><br><span class=\"line\"></span><br><span class=\"line\">GET &#x2F;indexName&#x2F;type&#x2F;_mget</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">&quot;docs&quot;:[</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;FEILD&quot;:&quot;value&quot;</span><br><span class=\"line\">&#125;,</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;FEILD2&quot;:&quot;value&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<p>查询所有文档</p>\n<p>简单查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indexName&#x2F;_serach</span><br></pre></td></tr></table></figure>\n\n<p>法二</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST &#x2F;people&#x2F;_serach</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;:&#123;</span><br><span class=\"line\">    &quot;match_all&quot;:&#123;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>查询某些字段内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">后面跟了 ?_source&#x3D;field1,field2</span><br><span class=\"line\"></span><br><span class=\"line\">POST &#x2F;people&#x2F;_serach?_source&#x3D;field1,field2</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;:&#123;</span><br><span class=\"line\">    &quot;match_all&quot;:&#123;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>查询多个索引下的多个type</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;index1,index2&#x2F;type1,type2&#x2F;_search</span><br></pre></td></tr></table></figure>\n\n<p>查询所有索引下的部分type</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;_all&#x2F;type1,type2&#x2F;_search</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"模糊查询\"><a href=\"#模糊查询\" class=\"headerlink\" title=\"模糊查询\"></a>模糊查询</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">POST &#x2F;indexName&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;:&#123;</span><br><span class=\"line\">    &quot;match&quot;:&#123;</span><br><span class=\"line\">      &quot;field&quot;:&quot;message&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ,</span><br><span class=\"line\">  &quot;sort&quot;:[</span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    &quot;filed&quot;:&#123;&quot;order&quot;:&quot;desc&quot;&#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>注意</strong><br>message将会被拆分进行匹配,若message是中文,则会按切分后的每个字来匹配,若<br>message是英语,则会是按每个单词来匹配</p>\n<p><img src=\"http://img.wqkenqing.ren/51ccc5cad8b8fbbbb6ef55d3106bfe43.png\" alt=\"\"><br><img src=\"http://img.wqkenqing.ren/47a3ffe716f0026c004b155836a56641.png\" alt=\"示列图\"></p>\n<p>全文搜索(按准度)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET indexName&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;:&#123;</span><br><span class=\"line\">    &quot;match&quot;:&#123;</span><br><span class=\"line\">      &quot;Field&quot;:&#123;</span><br><span class=\"line\">        &quot;query&quot;:&quot;val1 val2&quot;,</span><br><span class=\"line\">        &quot;operator&quot;:&quot;and&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>即Fileld 中必须有val1,val2</p>\n<p>按匹配度查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indexName&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">&quot;query&quot;:&#123;</span><br><span class=\"line\">  &quot;match&quot;:&#123;</span><br><span class=\"line\">    &quot;Field&quot;:&#123;</span><br><span class=\"line\">      &quot;query&quot;:&quot;val1 val2 val3&quot;</span><br><span class=\"line\">      &quot;minimum_should_match&quot;:&quot;val&quot; eg:20%</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>即indexName,按Field中 val1 val2 val3 匹配度达到val即返回查询</p>\n<h3 id=\"高级查询\"><a href=\"#高级查询\" class=\"headerlink\" title=\"高级查询\"></a>高级查询</h3><p>简单精准查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indexName&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;:&#123;</span><br><span class=\"line\">    &quot;match_phrase&quot;:&#123;</span><br><span class=\"line\">      &quot;Field&quot;:&quot;val&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>即查询要完全匹配val,但若val只有一个中文,则会Field只要含有val,就会被查出</p>\n<p>slop结合</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;match_phrase&quot;: &#123;</span><br><span class=\"line\">      &quot;name&quot;: &#123;</span><br><span class=\"line\">        &quot;query&quot;: &quot;张三&quot;,</span><br><span class=\"line\">        &quot;slop&quot;: 3</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>解读：slop是移动次数，上面案例表示“张”、“三”两个字可以经过最多挪动3次查询到！</p>\n<p>rescore (重打分）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;forum&#x2F;article&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;match&quot;: &#123;</span><br><span class=\"line\">      &quot;content&quot;: &quot;java spark&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;rescore&quot;:&#123;</span><br><span class=\"line\">    &quot;window_size&quot;: 50,</span><br><span class=\"line\">    &quot;query&quot;: &#123;</span><br><span class=\"line\">      &quot;rescore_query&quot;: &#123;</span><br><span class=\"line\">        &quot;match_phrase&quot;: &#123;</span><br><span class=\"line\">          &quot;content&quot;: &#123;</span><br><span class=\"line\">            &quot;query&quot;: &quot;java spark&quot;,</span><br><span class=\"line\">            &quot;slop&quot;: 50</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<p>多字段匹配查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indexName&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;:&#123;</span><br><span class=\"line\">    &quot;multi_match&quot;:&#123;</span><br><span class=\"line\">      &quot;query&quot;:&quot;val&quot;</span><br><span class=\"line\">      &quot;fields&quot;:[&quot;val1&quot;,&quot;val2&quot;]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>在多个字段中,也是模糊查询val</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;query_string&quot;: &#123;</span><br><span class=\"line\">      &quot;query&quot;: &quot;(叶良辰 AND 火) OR (赵日天 AND 风)&quot;,</span><br><span class=\"line\">      &quot;fields&quot;: [&quot;name&quot;,&quot;desc&quot;]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"字段查询\"><a href=\"#字段查询\" class=\"headerlink\" title=\"字段查询\"></a>字段查询</h3><p>精准查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indexName&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">&quot;query&quot;:&#123;</span><br><span class=\"line\">  &quot;term&quot;:&#123;</span><br><span class=\"line\">    &quot;field&quot;:&quot;val&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>分页查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indexName&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;:&#123;</span><br><span class=\"line\">    &quot;match_all&quot;:&#123;&#125;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;from&quot;:num,</span><br><span class=\"line\">  &quot;size&quot;:num</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"范围查询\"><a href=\"#范围查询\" class=\"headerlink\" title=\"范围查询\"></a>范围查询</h3><p>数据值型</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">GET &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;range&quot;: &#123;</span><br><span class=\"line\">      &quot;age&quot;: &#123;</span><br><span class=\"line\">        &quot;gt&quot;: 16,</span><br><span class=\"line\">        &quot;lte&quot;: 30</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>日期类型</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;range&quot;: &#123;</span><br><span class=\"line\">      &quot;birthday&quot;: &#123;</span><br><span class=\"line\">        &quot;gte&quot;: &quot;2013-01-01&quot;,</span><br><span class=\"line\">        &quot;lte&quot;: &quot;now&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">GET book&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;constant_score&quot;: &#123;</span><br><span class=\"line\">      &quot;filter&quot;: &#123;</span><br><span class=\"line\">        &quot;range&quot;: &#123;</span><br><span class=\"line\">          &quot;date&quot;: &#123;</span><br><span class=\"line\">            &quot;gt&quot;: &quot;now-1M&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;boost&quot;: 1.2</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>“gt”: “now-1M”表示从今天开始，往前推一个月！</p>\n<h3 id=\"过滤查询\"><a href=\"#过滤查询\" class=\"headerlink\" title=\"过滤查询\"></a>过滤查询</h3><p>法一</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">POST &#x2F;people&#x2F;man&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;constant_score&quot;: &#123;</span><br><span class=\"line\">      &quot;filter&quot;: &#123;</span><br><span class=\"line\">        &quot;range&quot;: &#123;</span><br><span class=\"line\">          &quot;age&quot;: &#123;</span><br><span class=\"line\">            &quot;gte&quot;: 20,</span><br><span class=\"line\">            &quot;lte&quot;: 30</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;boost&quot;: 1.2</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>法二</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;bool&quot;: &#123;</span><br><span class=\"line\">      &quot;filter&quot;: &#123;</span><br><span class=\"line\">        &quot;term&quot;: &#123;</span><br><span class=\"line\">          &quot;age&quot;: 18</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"布尔查询\"><a href=\"#布尔查询\" class=\"headerlink\" title=\"布尔查询\"></a>布尔查询</h3><p>should查询<br>注意：should相当于 或 ，里面的match也是模糊匹配</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;bool&quot;: &#123;</span><br><span class=\"line\">      &quot;should&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;match&quot;: &#123;</span><br><span class=\"line\">            &quot;name&quot;: &quot;叶良辰&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;match&quot;: &#123;</span><br><span class=\"line\">            &quot;desc&quot;: &quot;赵日天&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>must查询<br>注意：两个条件都要满足，并且这里也会把must里面的“叶良辰”拆分成“叶”、“良”和“辰”进行查询；“赵日天”拆分成“赵”、“日”、和“天”！</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">POST &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;bool&quot;: &#123;</span><br><span class=\"line\">      &quot;must&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;match&quot;: &#123;</span><br><span class=\"line\">            &quot;name&quot;: &quot;叶良辰&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;match&quot;: &#123;</span><br><span class=\"line\">            &quot;desc&quot;: &quot;赵日天&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>must与filter相结合<br>这里也会把must里面的“叶良辰”拆分成“叶”、“良”和“辰”进行查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">POST &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;bool&quot;: &#123;</span><br><span class=\"line\">      &quot;must&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;match&quot;: &#123;</span><br><span class=\"line\">            &quot;name&quot;: &quot;叶良辰&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;match&quot;: &#123;</span><br><span class=\"line\">            &quot;desc&quot;: &quot;赵日天&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      ],</span><br><span class=\"line\">      &quot;filter&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;term&quot;: &#123;</span><br><span class=\"line\">            &quot;age&quot;: 18</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>must_not<br>注意：下面语句是精准匹配</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;bool&quot;: &#123;</span><br><span class=\"line\">      &quot;must_not&quot;: &#123;</span><br><span class=\"line\">        &quot;term&quot;: &#123;</span><br><span class=\"line\">          &quot;name&quot;: &quot;叶良辰&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"聚合查询\"><a href=\"#聚合查询\" class=\"headerlink\" title=\"聚合查询\"></a>聚合查询</h3><p>根据字段类型查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">GET &#x2F;people&#x2F;man&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;size&quot;: 0,</span><br><span class=\"line\">  &quot;aggs&quot;: &#123;</span><br><span class=\"line\">    &quot;group_by_age&quot;: &#123;</span><br><span class=\"line\">      &quot;terms&quot;: &#123;</span><br><span class=\"line\">        &quot;field&quot;: &quot;age&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<p>查询总体值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;aggs&quot;: &#123;</span><br><span class=\"line\">    &quot;grads_age&quot;: &#123;</span><br><span class=\"line\">      &quot;stats&quot;: &#123;</span><br><span class=\"line\">        &quot;field&quot;: &quot;age&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>查询最小值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;aggs&quot;: &#123;</span><br><span class=\"line\">    &quot;grads_age&quot;: &#123;</span><br><span class=\"line\">      &quot;min&quot;: &#123;</span><br><span class=\"line\">        &quot;field&quot;: &quot;age&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>根据国家分组，然后计算年龄平均值：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">GET &#x2F;people&#x2F;man&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;size&quot;: 0,</span><br><span class=\"line\">  &quot;aggs&quot;: &#123;</span><br><span class=\"line\">    &quot;group_by_age&quot;: &#123;</span><br><span class=\"line\">      &quot;terms&quot;: &#123;</span><br><span class=\"line\">        &quot;field&quot;: &quot;country&quot;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;avg_age&quot;: &#123;</span><br><span class=\"line\">          &quot;avg&quot;: &#123;</span><br><span class=\"line\">            &quot;field&quot;: &quot;age&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>解决：上面的reason里面说的很清楚，将fielddata设置为true就行了：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">POST &#x2F;people&#x2F;_mapping&#x2F;man</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;properties&quot;: &#123;</span><br><span class=\"line\">    &quot;country&quot;: &#123;</span><br><span class=\"line\">      &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">      &quot;fielddata&quot;: true</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"排序查询\"><a href=\"#排序查询\" class=\"headerlink\" title=\"排序查询\"></a>排序查询</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">排序查询通常没有排到我们想要的结果，因为字段分词后，有很多单词，再排序跟我们想要的结果又出入</span><br><span class=\"line\"></span><br><span class=\"line\">解决办法：把需要排序的字段建立两次索引，一个排序，另一个不排序。</span><br><span class=\"line\"></span><br><span class=\"line\">如下面的案例：把title.raw的fielddata设置为true，是排序的；而title的fielddata默认是false，可以用来搜索</span><br><span class=\"line\"></span><br><span class=\"line\">index: true 是在title.raw建立索引可以被搜索到，</span><br><span class=\"line\"></span><br><span class=\"line\">fielddata: true是让其可以排序</span><br><span class=\"line\"></span><br><span class=\"line\">PUT &#x2F;blog</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;mappings&quot;: &#123;</span><br><span class=\"line\">    &quot;article&quot;: &#123;</span><br><span class=\"line\">      &quot;properties&quot;: &#123;</span><br><span class=\"line\">        &quot;auther&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;text&quot;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;title&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">          &quot;fields&quot;: &#123;</span><br><span class=\"line\">            &quot;raw&quot;:&#123;</span><br><span class=\"line\">              &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">              &quot;index&quot;: true,</span><br><span class=\"line\">              &quot;fielddata&quot;: true</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;content&quot;:&#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">          &quot;analyzer&quot;: &quot;english&quot;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;publishdate&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;date&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;blog&#x2F;article&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;match_all&quot;: &#123;&#125;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;sort&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;title.raw&quot;: &#123;</span><br><span class=\"line\">        &quot;order&quot;: &quot;desc&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"scroll查询\"><a href=\"#scroll查询\" class=\"headerlink\" title=\"scroll查询\"></a>scroll查询</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">当搜索量比较大的时候，我们在短时间内不可能一次性搜索完然后展示出来</span><br><span class=\"line\"></span><br><span class=\"line\">这个时候，可以使用scroll进行搜索</span><br><span class=\"line\"></span><br><span class=\"line\">比如下面的案例，可以先搜索3条数据，然后结果中会有一个_scroll_id，下次搜索就可以直接用这个_scroll_id进行搜索了</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET test_index&#x2F;test_type&#x2F;_search?scroll&#x3D;1m</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;match_all&quot;: &#123;&#125;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;sort&quot;: &quot;_doc&quot;,</span><br><span class=\"line\">  &quot;size&quot;: 3</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>step3 把scroll_id粘贴到下面的命令中再次搜索</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET _search&#x2F;scroll</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;scroll&quot;: &quot;1m&quot;,</span><br><span class=\"line\">  &quot;scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAA6FnZPSl9sbVR4UVVDU1NLb2wxVXJlbWcAAAAAAAAAPhZ2T0pfbG1UeFFVQ1NTS29sMVVyZW1nAAAAAAAAADsWdk9KX2xtVHhRVUNTU0tvbDFVcmVtZwAAAAAAAAA8FnZPSl9sbVR4UVVDU1NLb2wxVXJlbWcAAAAAAAAAPRZ2T0pfbG1UeFFVQ1NTS29sMVVyZW1n&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<p>查看集群是否健康</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;_cluster&#x2F;healt</span><br></pre></td></tr></table></figure>\n\n<p>查看节点列表</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;_cat&#x2F;nodes?v</span><br><span class=\"line\"></span><br><span class=\"line\">加v将表头显示出来</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"索引\"><a href=\"#索引\" class=\"headerlink\" title=\"索引\"></a>索引</h2><h3 id=\"查询所有索引\"><a href=\"#查询所有索引\" class=\"headerlink\" title=\"查询所有索引\"></a>查询所有索引</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;_cat&#x2F;indices?v</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"查看某个索引的映射\"><a href=\"#查看某个索引的映射\" class=\"headerlink\" title=\"查看某个索引的映射\"></a>查看某个索引的映射</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indeName&#x2F;mapping</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"查看某个索引的设置\"><a href=\"#查看某个索引的设置\" class=\"headerlink\" title=\"查看某个索引的设置\"></a>查看某个索引的设置</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indeName&#x2F;mapping</span><br></pre></td></tr></table></figure>\n<h3 id=\"添加一个索引\"><a href=\"#添加一个索引\" class=\"headerlink\" title=\"添加一个索引\"></a>添加一个索引</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;indexName</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;settings&quot;: &#123;</span><br><span class=\"line\">     &quot;number_of_shards&quot;: 3,</span><br><span class=\"line\">     &quot;number_of_replicas&quot;: 1</span><br><span class=\"line\">   &#125;,</span><br><span class=\"line\">   &quot;mappings&quot;: &#123;</span><br><span class=\"line\">     &quot;man&quot;: &#123;</span><br><span class=\"line\">       &quot;dynamic&quot;: &quot;strict&quot;,</span><br><span class=\"line\">       &quot;properties&quot;: &#123;</span><br><span class=\"line\">         &quot;name&quot;: &#123;</span><br><span class=\"line\">           &quot;type&quot;: &quot;text&quot;</span><br><span class=\"line\">         &#125;,</span><br><span class=\"line\">         &quot;age&quot;: &#123;</span><br><span class=\"line\">           &quot;type&quot;: &quot;integer&quot;</span><br><span class=\"line\">         &#125;,</span><br><span class=\"line\">         &quot;birthday&quot;: &#123;</span><br><span class=\"line\">           &quot;type&quot;: &quot;date&quot;,</span><br><span class=\"line\">           &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot;</span><br><span class=\"line\">         &#125;,</span><br><span class=\"line\">         &quot;address&quot;:&#123;</span><br><span class=\"line\">           &quot;dynamic&quot;: &quot;true&quot;,</span><br><span class=\"line\">           &quot;type&quot;: &quot;object&quot;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n\n<p>dynamic关键词说明:<br>“dynamic”:”strict”  表示如果遇到陌生field会报错<br>“dynamic”: true  表示如果遇到陌生字段，就进行dynamic mapping<br>“dynamic”: “false”   表示如果遇到陌生字段，就忽略</p>\n<hr>\n<h3 id=\"删除索引\"><a href=\"#删除索引\" class=\"headerlink\" title=\"删除索引\"></a>删除索引</h3><p>删除单个索引</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">DELETE &#x2F;indexName</span><br></pre></td></tr></table></figure>\n\n<p>删除多个</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">DELETE &#x2F;indexName1,indexName2</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"添加字段映射\"><a href=\"#添加字段映射\" class=\"headerlink\" title=\"添加字段映射\"></a>添加字段映射</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;indexName&#x2F;_mapping&#x2F;Field</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;properties&quot;:&#123;</span><br><span class=\"line\">    &quot;Field&quot;:&#123;</span><br><span class=\"line\">      &quot;type&quot;:&quot;text&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"索引的别名\"><a href=\"#索引的别名\" class=\"headerlink\" title=\"索引的别名\"></a>索引的别名</h3><h4 id=\"创建索引别名\"><a href=\"#创建索引别名\" class=\"headerlink\" title=\"创建索引别名\"></a>创建索引别名</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;indeName&#x2F;_alias&#x2F;aliasName</span><br></pre></td></tr></table></figure>\n\n<p>获取索引别名</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indexName&#x2F;_alias&#x2F;*</span><br></pre></td></tr></table></figure>\n<p>查询别名对应的索引</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;*&#x2F;_alias&#x2F;aliasName</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"文档\"><a href=\"#文档\" class=\"headerlink\" title=\"文档\"></a>文档</h2><h3 id=\"向索引中添加文档\"><a href=\"#向索引中添加文档\" class=\"headerlink\" title=\"向索引中添加文档\"></a>向索引中添加文档</h3><ol>\n<li>自定义ID</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;indexName&#x2F;type&#x2F;id</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;Field1&quot;:&quot;message&quot;,</span><br><span class=\"line\">  &quot;Field2&quot;:&quot;message&quot;,</span><br><span class=\"line\">  &quot;Field3&quot;:&quot;message&quot;,</span><br><span class=\"line\">  &quot;Field4&quot;:&quot;message&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#96;&#96;&#96;  </span><br><span class=\"line\"></span><br><span class=\"line\">2. 随机生成id</span><br></pre></td></tr></table></figure>\n<p>POST /indexName/type<br>{<br>  “Field1”:”message”,<br>  “Field2”:”message”,<br>  “Field3”:”message”,<br>  “Field4”:”message”<br>}</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">后者则会自动生成id字符串</span><br><span class=\"line\"></span><br><span class=\"line\">3. 修改文档</span><br><span class=\"line\"></span><br><span class=\"line\">全文修改,即所有字段信息都要修改</span><br></pre></td></tr></table></figure>\n<p>PUT /indexName/type/id<br>{<br>  “Field1”:”update message”,<br>  “Field2”:”update message”,<br>  “Field3”:”message”,<br>  “Field4”:”message”<br>}</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">部份修改</span><br></pre></td></tr></table></figure>\n<p>POST /indexName/type/id/_update<br>{<br>  “doc”: {<br>    “FIELD”: “message”<br>  }<br>}</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">脚本(再深入)</span><br></pre></td></tr></table></figure>\n\n<p>POST /indexName/type/_id/_update<br>{<br>  “script”: {<br>    “lang”: “painless”,<br>    “source”: “ctx._source.age += 10”<br>  }<br>}</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在修改document的时候，如果该文档不存在，则使用upsert操作进行初始化</span><br></pre></td></tr></table></figure>\n<p>POST people/man/1/_update<br>{<br>  “script”: “ctx._source.age += 10”,<br>  “upsert”: {<br>    “age”: 20<br>  }<br>}</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">### 删除文档</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">删除单个文档</span><br></pre></td></tr></table></figure>\n<p>DELETE /indexName/type/id</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">删除type下所有的文档</span><br></pre></td></tr></table></figure>\n<p>POST /indexName/type/_delete_by_query?conflicts=proceed<br>{<br>  “query”:{<br>    “match_all”:{</p>\n<pre><code>}</code></pre><p>  }<br>}</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">### 查询文档</span><br><span class=\"line\"></span><br><span class=\"line\">查询单个文档</span><br></pre></td></tr></table></figure>\n<p>GET /indexName/type/id</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">批量查询文档(待验证)</span><br></pre></td></tr></table></figure>\n<p>GET /_mget<br>{<br>  “docs”: [<br>      {<br>        “_index”: “index1”,<br>        “_type”: “type”,<br>        “_id”: 1<br>      },<br>      {<br>        “_index”: “index2”,<br>        “_type”: “type”,<br>        “_id”: 2<br>      }<br>    ]<br>}</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#96;&#96;&#96;</span><br><span class=\"line\"></span><br><span class=\"line\">GET &#x2F;indexName&#x2F;type&#x2F;_mget</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">&quot;docs&quot;:[</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;FEILD&quot;:&quot;value&quot;</span><br><span class=\"line\">&#125;,</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;FEILD2&quot;:&quot;value&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<p>查询所有文档</p>\n<p>简单查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indexName&#x2F;_serach</span><br></pre></td></tr></table></figure>\n\n<p>法二</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST &#x2F;people&#x2F;_serach</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;:&#123;</span><br><span class=\"line\">    &quot;match_all&quot;:&#123;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>查询某些字段内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">后面跟了 ?_source&#x3D;field1,field2</span><br><span class=\"line\"></span><br><span class=\"line\">POST &#x2F;people&#x2F;_serach?_source&#x3D;field1,field2</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;:&#123;</span><br><span class=\"line\">    &quot;match_all&quot;:&#123;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>查询多个索引下的多个type</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;index1,index2&#x2F;type1,type2&#x2F;_search</span><br></pre></td></tr></table></figure>\n\n<p>查询所有索引下的部分type</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;_all&#x2F;type1,type2&#x2F;_search</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"模糊查询\"><a href=\"#模糊查询\" class=\"headerlink\" title=\"模糊查询\"></a>模糊查询</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">POST &#x2F;indexName&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;:&#123;</span><br><span class=\"line\">    &quot;match&quot;:&#123;</span><br><span class=\"line\">      &quot;field&quot;:&quot;message&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ,</span><br><span class=\"line\">  &quot;sort&quot;:[</span><br><span class=\"line\">  &#123;</span><br><span class=\"line\">    &quot;filed&quot;:&#123;&quot;order&quot;:&quot;desc&quot;&#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p><strong>注意</strong><br>message将会被拆分进行匹配,若message是中文,则会按切分后的每个字来匹配,若<br>message是英语,则会是按每个单词来匹配</p>\n<p><img src=\"http://img.wqkenqing.ren/51ccc5cad8b8fbbbb6ef55d3106bfe43.png\" alt=\"\"><br><img src=\"http://img.wqkenqing.ren/47a3ffe716f0026c004b155836a56641.png\" alt=\"示列图\"></p>\n<p>全文搜索(按准度)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET indexName&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;:&#123;</span><br><span class=\"line\">    &quot;match&quot;:&#123;</span><br><span class=\"line\">      &quot;Field&quot;:&#123;</span><br><span class=\"line\">        &quot;query&quot;:&quot;val1 val2&quot;,</span><br><span class=\"line\">        &quot;operator&quot;:&quot;and&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>即Fileld 中必须有val1,val2</p>\n<p>按匹配度查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indexName&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">&quot;query&quot;:&#123;</span><br><span class=\"line\">  &quot;match&quot;:&#123;</span><br><span class=\"line\">    &quot;Field&quot;:&#123;</span><br><span class=\"line\">      &quot;query&quot;:&quot;val1 val2 val3&quot;</span><br><span class=\"line\">      &quot;minimum_should_match&quot;:&quot;val&quot; eg:20%</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>即indexName,按Field中 val1 val2 val3 匹配度达到val即返回查询</p>\n<h3 id=\"高级查询\"><a href=\"#高级查询\" class=\"headerlink\" title=\"高级查询\"></a>高级查询</h3><p>简单精准查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indexName&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;:&#123;</span><br><span class=\"line\">    &quot;match_phrase&quot;:&#123;</span><br><span class=\"line\">      &quot;Field&quot;:&quot;val&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>即查询要完全匹配val,但若val只有一个中文,则会Field只要含有val,就会被查出</p>\n<p>slop结合</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;match_phrase&quot;: &#123;</span><br><span class=\"line\">      &quot;name&quot;: &#123;</span><br><span class=\"line\">        &quot;query&quot;: &quot;张三&quot;,</span><br><span class=\"line\">        &quot;slop&quot;: 3</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>解读：slop是移动次数，上面案例表示“张”、“三”两个字可以经过最多挪动3次查询到！</p>\n<p>rescore (重打分）</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;forum&#x2F;article&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;match&quot;: &#123;</span><br><span class=\"line\">      &quot;content&quot;: &quot;java spark&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;rescore&quot;:&#123;</span><br><span class=\"line\">    &quot;window_size&quot;: 50,</span><br><span class=\"line\">    &quot;query&quot;: &#123;</span><br><span class=\"line\">      &quot;rescore_query&quot;: &#123;</span><br><span class=\"line\">        &quot;match_phrase&quot;: &#123;</span><br><span class=\"line\">          &quot;content&quot;: &#123;</span><br><span class=\"line\">            &quot;query&quot;: &quot;java spark&quot;,</span><br><span class=\"line\">            &quot;slop&quot;: 50</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<p>多字段匹配查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indexName&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;:&#123;</span><br><span class=\"line\">    &quot;multi_match&quot;:&#123;</span><br><span class=\"line\">      &quot;query&quot;:&quot;val&quot;</span><br><span class=\"line\">      &quot;fields&quot;:[&quot;val1&quot;,&quot;val2&quot;]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>在多个字段中,也是模糊查询val</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;query_string&quot;: &#123;</span><br><span class=\"line\">      &quot;query&quot;: &quot;(叶良辰 AND 火) OR (赵日天 AND 风)&quot;,</span><br><span class=\"line\">      &quot;fields&quot;: [&quot;name&quot;,&quot;desc&quot;]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"字段查询\"><a href=\"#字段查询\" class=\"headerlink\" title=\"字段查询\"></a>字段查询</h3><p>精准查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indexName&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">&quot;query&quot;:&#123;</span><br><span class=\"line\">  &quot;term&quot;:&#123;</span><br><span class=\"line\">    &quot;field&quot;:&quot;val&quot;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>分页查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;indexName&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;:&#123;</span><br><span class=\"line\">    &quot;match_all&quot;:&#123;&#125;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;from&quot;:num,</span><br><span class=\"line\">  &quot;size&quot;:num</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"范围查询\"><a href=\"#范围查询\" class=\"headerlink\" title=\"范围查询\"></a>范围查询</h3><p>数据值型</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">GET &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;range&quot;: &#123;</span><br><span class=\"line\">      &quot;age&quot;: &#123;</span><br><span class=\"line\">        &quot;gt&quot;: 16,</span><br><span class=\"line\">        &quot;lte&quot;: 30</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>日期类型</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;range&quot;: &#123;</span><br><span class=\"line\">      &quot;birthday&quot;: &#123;</span><br><span class=\"line\">        &quot;gte&quot;: &quot;2013-01-01&quot;,</span><br><span class=\"line\">        &quot;lte&quot;: &quot;now&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">GET book&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;constant_score&quot;: &#123;</span><br><span class=\"line\">      &quot;filter&quot;: &#123;</span><br><span class=\"line\">        &quot;range&quot;: &#123;</span><br><span class=\"line\">          &quot;date&quot;: &#123;</span><br><span class=\"line\">            &quot;gt&quot;: &quot;now-1M&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;boost&quot;: 1.2</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>“gt”: “now-1M”表示从今天开始，往前推一个月！</p>\n<h3 id=\"过滤查询\"><a href=\"#过滤查询\" class=\"headerlink\" title=\"过滤查询\"></a>过滤查询</h3><p>法一</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">POST &#x2F;people&#x2F;man&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;constant_score&quot;: &#123;</span><br><span class=\"line\">      &quot;filter&quot;: &#123;</span><br><span class=\"line\">        &quot;range&quot;: &#123;</span><br><span class=\"line\">          &quot;age&quot;: &#123;</span><br><span class=\"line\">            &quot;gte&quot;: 20,</span><br><span class=\"line\">            &quot;lte&quot;: 30</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;boost&quot;: 1.2</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>法二</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;bool&quot;: &#123;</span><br><span class=\"line\">      &quot;filter&quot;: &#123;</span><br><span class=\"line\">        &quot;term&quot;: &#123;</span><br><span class=\"line\">          &quot;age&quot;: 18</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"布尔查询\"><a href=\"#布尔查询\" class=\"headerlink\" title=\"布尔查询\"></a>布尔查询</h3><p>should查询<br>注意：should相当于 或 ，里面的match也是模糊匹配</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;bool&quot;: &#123;</span><br><span class=\"line\">      &quot;should&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;match&quot;: &#123;</span><br><span class=\"line\">            &quot;name&quot;: &quot;叶良辰&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;match&quot;: &#123;</span><br><span class=\"line\">            &quot;desc&quot;: &quot;赵日天&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>must查询<br>注意：两个条件都要满足，并且这里也会把must里面的“叶良辰”拆分成“叶”、“良”和“辰”进行查询；“赵日天”拆分成“赵”、“日”、和“天”！</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">POST &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;bool&quot;: &#123;</span><br><span class=\"line\">      &quot;must&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;match&quot;: &#123;</span><br><span class=\"line\">            &quot;name&quot;: &quot;叶良辰&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;match&quot;: &#123;</span><br><span class=\"line\">            &quot;desc&quot;: &quot;赵日天&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>must与filter相结合<br>这里也会把must里面的“叶良辰”拆分成“叶”、“良”和“辰”进行查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">POST &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;bool&quot;: &#123;</span><br><span class=\"line\">      &quot;must&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;match&quot;: &#123;</span><br><span class=\"line\">            &quot;name&quot;: &quot;叶良辰&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;match&quot;: &#123;</span><br><span class=\"line\">            &quot;desc&quot;: &quot;赵日天&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      ],</span><br><span class=\"line\">      &quot;filter&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">          &quot;term&quot;: &#123;</span><br><span class=\"line\">            &quot;age&quot;: 18</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      ]</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>must_not<br>注意：下面语句是精准匹配</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;bool&quot;: &#123;</span><br><span class=\"line\">      &quot;must_not&quot;: &#123;</span><br><span class=\"line\">        &quot;term&quot;: &#123;</span><br><span class=\"line\">          &quot;name&quot;: &quot;叶良辰&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"聚合查询\"><a href=\"#聚合查询\" class=\"headerlink\" title=\"聚合查询\"></a>聚合查询</h3><p>根据字段类型查询</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">GET &#x2F;people&#x2F;man&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;size&quot;: 0,</span><br><span class=\"line\">  &quot;aggs&quot;: &#123;</span><br><span class=\"line\">    &quot;group_by_age&quot;: &#123;</span><br><span class=\"line\">      &quot;terms&quot;: &#123;</span><br><span class=\"line\">        &quot;field&quot;: &quot;age&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<p>查询总体值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;aggs&quot;: &#123;</span><br><span class=\"line\">    &quot;grads_age&quot;: &#123;</span><br><span class=\"line\">      &quot;stats&quot;: &#123;</span><br><span class=\"line\">        &quot;field&quot;: &quot;age&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>查询最小值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">POST &#x2F;people&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;aggs&quot;: &#123;</span><br><span class=\"line\">    &quot;grads_age&quot;: &#123;</span><br><span class=\"line\">      &quot;min&quot;: &#123;</span><br><span class=\"line\">        &quot;field&quot;: &quot;age&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>根据国家分组，然后计算年龄平均值：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">GET &#x2F;people&#x2F;man&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;size&quot;: 0,</span><br><span class=\"line\">  &quot;aggs&quot;: &#123;</span><br><span class=\"line\">    &quot;group_by_age&quot;: &#123;</span><br><span class=\"line\">      &quot;terms&quot;: &#123;</span><br><span class=\"line\">        &quot;field&quot;: &quot;country&quot;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;aggs&quot;: &#123;</span><br><span class=\"line\">        &quot;avg_age&quot;: &#123;</span><br><span class=\"line\">          &quot;avg&quot;: &#123;</span><br><span class=\"line\">            &quot;field&quot;: &quot;age&quot;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>解决：上面的reason里面说的很清楚，将fielddata设置为true就行了：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">POST &#x2F;people&#x2F;_mapping&#x2F;man</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;properties&quot;: &#123;</span><br><span class=\"line\">    &quot;country&quot;: &#123;</span><br><span class=\"line\">      &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">      &quot;fielddata&quot;: true</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"排序查询\"><a href=\"#排序查询\" class=\"headerlink\" title=\"排序查询\"></a>排序查询</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">排序查询通常没有排到我们想要的结果，因为字段分词后，有很多单词，再排序跟我们想要的结果又出入</span><br><span class=\"line\"></span><br><span class=\"line\">解决办法：把需要排序的字段建立两次索引，一个排序，另一个不排序。</span><br><span class=\"line\"></span><br><span class=\"line\">如下面的案例：把title.raw的fielddata设置为true，是排序的；而title的fielddata默认是false，可以用来搜索</span><br><span class=\"line\"></span><br><span class=\"line\">index: true 是在title.raw建立索引可以被搜索到，</span><br><span class=\"line\"></span><br><span class=\"line\">fielddata: true是让其可以排序</span><br><span class=\"line\"></span><br><span class=\"line\">PUT &#x2F;blog</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;mappings&quot;: &#123;</span><br><span class=\"line\">    &quot;article&quot;: &#123;</span><br><span class=\"line\">      &quot;properties&quot;: &#123;</span><br><span class=\"line\">        &quot;auther&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;text&quot;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;title&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">          &quot;fields&quot;: &#123;</span><br><span class=\"line\">            &quot;raw&quot;:&#123;</span><br><span class=\"line\">              &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">              &quot;index&quot;: true,</span><br><span class=\"line\">              &quot;fielddata&quot;: true</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;content&quot;:&#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;text&quot;,</span><br><span class=\"line\">          &quot;analyzer&quot;: &quot;english&quot;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;publishdate&quot;: &#123;</span><br><span class=\"line\">          &quot;type&quot;: &quot;date&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET &#x2F;blog&#x2F;article&#x2F;_search</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;match_all&quot;: &#123;&#125;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;sort&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;title.raw&quot;: &#123;</span><br><span class=\"line\">        &quot;order&quot;: &quot;desc&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"scroll查询\"><a href=\"#scroll查询\" class=\"headerlink\" title=\"scroll查询\"></a>scroll查询</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">当搜索量比较大的时候，我们在短时间内不可能一次性搜索完然后展示出来</span><br><span class=\"line\"></span><br><span class=\"line\">这个时候，可以使用scroll进行搜索</span><br><span class=\"line\"></span><br><span class=\"line\">比如下面的案例，可以先搜索3条数据，然后结果中会有一个_scroll_id，下次搜索就可以直接用这个_scroll_id进行搜索了</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET test_index&#x2F;test_type&#x2F;_search?scroll&#x3D;1m</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;query&quot;: &#123;</span><br><span class=\"line\">    &quot;match_all&quot;: &#123;&#125;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;sort&quot;: &quot;_doc&quot;,</span><br><span class=\"line\">  &quot;size&quot;: 3</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>step3 把scroll_id粘贴到下面的命令中再次搜索</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GET _search&#x2F;scroll</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;scroll&quot;: &quot;1m&quot;,</span><br><span class=\"line\">  &quot;scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAA6FnZPSl9sbVR4UVVDU1NLb2wxVXJlbWcAAAAAAAAAPhZ2T0pfbG1UeFFVQ1NTS29sMVVyZW1nAAAAAAAAADsWdk9KX2xtVHhRVUNTU0tvbDFVcmVtZwAAAAAAAAA8FnZPSl9sbVR4UVVDU1NLb2wxVXJlbWcAAAAAAAAAPRZ2T0pfbG1UeFFVQ1NTS29sMVVyZW1n&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n"},{"_content":"redis address: 192.168.10.101\nport :5601\n\n\nLYDSJ_DATA_BASE_TYPE\n\n![基础映射类型](http://img.wqkenqing.ren/ce745f99167b91191debfc65fad693b4.png)\n\nLYDSJ_DATA_SOURCE\n\n![共享数据源](http://img.wqkenqing.ren/215be461e444100be5552e288a00caba.png)\n\n\nLYDSJ_DATA_TYPE\n\n![数据类型](http://img.wqkenqing.ren/f6e0b3711855e7f65380444470913900.png)\n\n\nLYDSJ_GATHER_DATA_SOURCE_MAP\n\n\nLYDSJ_GATHER_DATA_TYPE_MAP\n\n\n所以这里要确定一下\n这里的类目后续需不需要与可视化中心的类目统一\n","source":"_posts/技术/es/共享数据集合.md","raw":"redis address: 192.168.10.101\nport :5601\n\n\nLYDSJ_DATA_BASE_TYPE\n\n![基础映射类型](http://img.wqkenqing.ren/ce745f99167b91191debfc65fad693b4.png)\n\nLYDSJ_DATA_SOURCE\n\n![共享数据源](http://img.wqkenqing.ren/215be461e444100be5552e288a00caba.png)\n\n\nLYDSJ_DATA_TYPE\n\n![数据类型](http://img.wqkenqing.ren/f6e0b3711855e7f65380444470913900.png)\n\n\nLYDSJ_GATHER_DATA_SOURCE_MAP\n\n\nLYDSJ_GATHER_DATA_TYPE_MAP\n\n\n所以这里要确定一下\n这里的类目后续需不需要与可视化中心的类目统一\n","slug":"技术/es/共享数据集合","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/es/共享数据集合","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31j000p38pw05oi1eon","content":"<p>redis address: 192.168.10.101<br>port :5601</p>\n<p>LYDSJ_DATA_BASE_TYPE</p>\n<p><img src=\"http://img.wqkenqing.ren/ce745f99167b91191debfc65fad693b4.png\" alt=\"基础映射类型\"></p>\n<p>LYDSJ_DATA_SOURCE</p>\n<p><img src=\"http://img.wqkenqing.ren/215be461e444100be5552e288a00caba.png\" alt=\"共享数据源\"></p>\n<p>LYDSJ_DATA_TYPE</p>\n<p><img src=\"http://img.wqkenqing.ren/f6e0b3711855e7f65380444470913900.png\" alt=\"数据类型\"></p>\n<p>LYDSJ_GATHER_DATA_SOURCE_MAP</p>\n<p>LYDSJ_GATHER_DATA_TYPE_MAP</p>\n<p>所以这里要确定一下<br>这里的类目后续需不需要与可视化中心的类目统一</p>\n","site":{"data":{}},"excerpt":"","more":"<p>redis address: 192.168.10.101<br>port :5601</p>\n<p>LYDSJ_DATA_BASE_TYPE</p>\n<p><img src=\"http://img.wqkenqing.ren/ce745f99167b91191debfc65fad693b4.png\" alt=\"基础映射类型\"></p>\n<p>LYDSJ_DATA_SOURCE</p>\n<p><img src=\"http://img.wqkenqing.ren/215be461e444100be5552e288a00caba.png\" alt=\"共享数据源\"></p>\n<p>LYDSJ_DATA_TYPE</p>\n<p><img src=\"http://img.wqkenqing.ren/f6e0b3711855e7f65380444470913900.png\" alt=\"数据类型\"></p>\n<p>LYDSJ_GATHER_DATA_SOURCE_MAP</p>\n<p>LYDSJ_GATHER_DATA_TYPE_MAP</p>\n<p>所以这里要确定一下<br>这里的类目后续需不需要与可视化中心的类目统一</p>\n"},{"_content":"`针对es的掌握,总有些地方不通透,想来还是从头从细粒度上再捋一遍`\n\n`总得来讲es的掌握需要分为两个大块,一是es的使用,二是es的运维`\n\n\n\n`默认选择kibana操作`\n\n[TOC]\n\n## 前言\n\n以<<Elasticsearch技术解析与实战>>一书为例.\n\n具体的使用章节内容大致有\n\n第二章部份\n\n第三章\n\n第四章\n\n第五章\n\n第七章\n\n运维相关\n\n第二章部份\n\n第六章\n\n第八章\n\n第九章\n\n\n\n## 索引\n\n### 创建索引\n\n```\nPUT /indexName/\n{\n\"settings\":{\n},\n\"mappings\":{\n}\n}\n```\n\n### 修改索引\n\n```\nPUT /indexName/\n{\n\n}\n```\n\n<font color=red>mappings映射只能添加,不能修改.所以一个index的mapping设计需要事先把握清楚.不然后续治理困难</font>\n\n### 删除索引\n\n### 别名\n\n#### 添加\n\n```\nPUT /alias\n{\n    \"actions\": [\n        {\"add\": {\"index\": \"test1\", \"alias\": \"alias1\"}}\n    ]\n}\n```\n\n#### 删除\n\n```\nPUT /alias\n{\n    \"actions\": [\n        {\"remove\": {\"index\": \"test1\", \"alias\": \"alias1\"}}\n    ]\n}\n```\n\n\n\n<strong>别名只能通过删除后再建立来进行修改</strong>\n\n\n\n#### 添加多个别名\n\n```\n{\n    \"actions\": [\n        {\"add\": {\"index\": \"test1\", \"alias\":\"alias1\"}},\n        {\"add\": {\"index\": \"test2\", \"alias\":\"alias1\"}}\n    ]\n}\n```\n\n\n\n\n\n### 索引配置\n\n常规的replica,shard\n\n然后还能配置analyzer\n\n```\nPUT /indexName/_settings\n{\n\"analysis\":{\n\"analyzer\":{\n\"content\":{\n\"type\":\"\",\n\"tokenizer\":\"\"\n}\n}\n}\n}\n```\n\n\n\n<strong>索引只能在关闭后,再进行一些设置上的调整</strong>\n\n\n\n\n### 映射\n\n#### 字符串数据可接受的参数\n\n\n\n\n\n","source":"_posts/技术/es/es详细.md","raw":"`针对es的掌握,总有些地方不通透,想来还是从头从细粒度上再捋一遍`\n\n`总得来讲es的掌握需要分为两个大块,一是es的使用,二是es的运维`\n\n\n\n`默认选择kibana操作`\n\n[TOC]\n\n## 前言\n\n以<<Elasticsearch技术解析与实战>>一书为例.\n\n具体的使用章节内容大致有\n\n第二章部份\n\n第三章\n\n第四章\n\n第五章\n\n第七章\n\n运维相关\n\n第二章部份\n\n第六章\n\n第八章\n\n第九章\n\n\n\n## 索引\n\n### 创建索引\n\n```\nPUT /indexName/\n{\n\"settings\":{\n},\n\"mappings\":{\n}\n}\n```\n\n### 修改索引\n\n```\nPUT /indexName/\n{\n\n}\n```\n\n<font color=red>mappings映射只能添加,不能修改.所以一个index的mapping设计需要事先把握清楚.不然后续治理困难</font>\n\n### 删除索引\n\n### 别名\n\n#### 添加\n\n```\nPUT /alias\n{\n    \"actions\": [\n        {\"add\": {\"index\": \"test1\", \"alias\": \"alias1\"}}\n    ]\n}\n```\n\n#### 删除\n\n```\nPUT /alias\n{\n    \"actions\": [\n        {\"remove\": {\"index\": \"test1\", \"alias\": \"alias1\"}}\n    ]\n}\n```\n\n\n\n<strong>别名只能通过删除后再建立来进行修改</strong>\n\n\n\n#### 添加多个别名\n\n```\n{\n    \"actions\": [\n        {\"add\": {\"index\": \"test1\", \"alias\":\"alias1\"}},\n        {\"add\": {\"index\": \"test2\", \"alias\":\"alias1\"}}\n    ]\n}\n```\n\n\n\n\n\n### 索引配置\n\n常规的replica,shard\n\n然后还能配置analyzer\n\n```\nPUT /indexName/_settings\n{\n\"analysis\":{\n\"analyzer\":{\n\"content\":{\n\"type\":\"\",\n\"tokenizer\":\"\"\n}\n}\n}\n}\n```\n\n\n\n<strong>索引只能在关闭后,再进行一些设置上的调整</strong>\n\n\n\n\n### 映射\n\n#### 字符串数据可接受的参数\n\n\n\n\n\n","slug":"技术/es/es详细","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/es/es详细","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31j000q38pw7g2yasrq","content":"<p><code>针对es的掌握,总有些地方不通透,想来还是从头从细粒度上再捋一遍</code></p>\n<p><code>总得来讲es的掌握需要分为两个大块,一是es的使用,二是es的运维</code></p>\n<p><code>默认选择kibana操作</code></p>\n<p>[TOC]</p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>以&lt;&lt;Elasticsearch技术解析与实战&gt;&gt;一书为例.</p>\n<p>具体的使用章节内容大致有</p>\n<p>第二章部份</p>\n<p>第三章</p>\n<p>第四章</p>\n<p>第五章</p>\n<p>第七章</p>\n<p>运维相关</p>\n<p>第二章部份</p>\n<p>第六章</p>\n<p>第八章</p>\n<p>第九章</p>\n<h2 id=\"索引\"><a href=\"#索引\" class=\"headerlink\" title=\"索引\"></a>索引</h2><h3 id=\"创建索引\"><a href=\"#创建索引\" class=\"headerlink\" title=\"创建索引\"></a>创建索引</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;indexName&#x2F;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">&quot;settings&quot;:&#123;</span><br><span class=\"line\">&#125;,</span><br><span class=\"line\">&quot;mappings&quot;:&#123;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"修改索引\"><a href=\"#修改索引\" class=\"headerlink\" title=\"修改索引\"></a>修改索引</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;indexName&#x2F;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><font color=red>mappings映射只能添加,不能修改.所以一个index的mapping设计需要事先把握清楚.不然后续治理困难</font></p>\n<h3 id=\"删除索引\"><a href=\"#删除索引\" class=\"headerlink\" title=\"删除索引\"></a>删除索引</h3><h3 id=\"别名\"><a href=\"#别名\" class=\"headerlink\" title=\"别名\"></a>别名</h3><h4 id=\"添加\"><a href=\"#添加\" class=\"headerlink\" title=\"添加\"></a>添加</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;alias</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;actions&quot;: [</span><br><span class=\"line\">        &#123;&quot;add&quot;: &#123;&quot;index&quot;: &quot;test1&quot;, &quot;alias&quot;: &quot;alias1&quot;&#125;&#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"删除\"><a href=\"#删除\" class=\"headerlink\" title=\"删除\"></a>删除</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;alias</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;actions&quot;: [</span><br><span class=\"line\">        &#123;&quot;remove&quot;: &#123;&quot;index&quot;: &quot;test1&quot;, &quot;alias&quot;: &quot;alias1&quot;&#125;&#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<p><strong>别名只能通过删除后再建立来进行修改</strong></p>\n<h4 id=\"添加多个别名\"><a href=\"#添加多个别名\" class=\"headerlink\" title=\"添加多个别名\"></a>添加多个别名</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;actions&quot;: [</span><br><span class=\"line\">        &#123;&quot;add&quot;: &#123;&quot;index&quot;: &quot;test1&quot;, &quot;alias&quot;:&quot;alias1&quot;&#125;&#125;,</span><br><span class=\"line\">        &#123;&quot;add&quot;: &#123;&quot;index&quot;: &quot;test2&quot;, &quot;alias&quot;:&quot;alias1&quot;&#125;&#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n\n\n<h3 id=\"索引配置\"><a href=\"#索引配置\" class=\"headerlink\" title=\"索引配置\"></a>索引配置</h3><p>常规的replica,shard</p>\n<p>然后还能配置analyzer</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;indexName&#x2F;_settings</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">&quot;analysis&quot;:&#123;</span><br><span class=\"line\">&quot;analyzer&quot;:&#123;</span><br><span class=\"line\">&quot;content&quot;:&#123;</span><br><span class=\"line\">&quot;type&quot;:&quot;&quot;,</span><br><span class=\"line\">&quot;tokenizer&quot;:&quot;&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<p><strong>索引只能在关闭后,再进行一些设置上的调整</strong></p>\n<h3 id=\"映射\"><a href=\"#映射\" class=\"headerlink\" title=\"映射\"></a>映射</h3><h4 id=\"字符串数据可接受的参数\"><a href=\"#字符串数据可接受的参数\" class=\"headerlink\" title=\"字符串数据可接受的参数\"></a>字符串数据可接受的参数</h4>","site":{"data":{}},"excerpt":"","more":"<p><code>针对es的掌握,总有些地方不通透,想来还是从头从细粒度上再捋一遍</code></p>\n<p><code>总得来讲es的掌握需要分为两个大块,一是es的使用,二是es的运维</code></p>\n<p><code>默认选择kibana操作</code></p>\n<p>[TOC]</p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>以&lt;&lt;Elasticsearch技术解析与实战&gt;&gt;一书为例.</p>\n<p>具体的使用章节内容大致有</p>\n<p>第二章部份</p>\n<p>第三章</p>\n<p>第四章</p>\n<p>第五章</p>\n<p>第七章</p>\n<p>运维相关</p>\n<p>第二章部份</p>\n<p>第六章</p>\n<p>第八章</p>\n<p>第九章</p>\n<h2 id=\"索引\"><a href=\"#索引\" class=\"headerlink\" title=\"索引\"></a>索引</h2><h3 id=\"创建索引\"><a href=\"#创建索引\" class=\"headerlink\" title=\"创建索引\"></a>创建索引</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;indexName&#x2F;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">&quot;settings&quot;:&#123;</span><br><span class=\"line\">&#125;,</span><br><span class=\"line\">&quot;mappings&quot;:&#123;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"修改索引\"><a href=\"#修改索引\" class=\"headerlink\" title=\"修改索引\"></a>修改索引</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;indexName&#x2F;</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><font color=red>mappings映射只能添加,不能修改.所以一个index的mapping设计需要事先把握清楚.不然后续治理困难</font></p>\n<h3 id=\"删除索引\"><a href=\"#删除索引\" class=\"headerlink\" title=\"删除索引\"></a>删除索引</h3><h3 id=\"别名\"><a href=\"#别名\" class=\"headerlink\" title=\"别名\"></a>别名</h3><h4 id=\"添加\"><a href=\"#添加\" class=\"headerlink\" title=\"添加\"></a>添加</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;alias</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;actions&quot;: [</span><br><span class=\"line\">        &#123;&quot;add&quot;: &#123;&quot;index&quot;: &quot;test1&quot;, &quot;alias&quot;: &quot;alias1&quot;&#125;&#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"删除\"><a href=\"#删除\" class=\"headerlink\" title=\"删除\"></a>删除</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;alias</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;actions&quot;: [</span><br><span class=\"line\">        &#123;&quot;remove&quot;: &#123;&quot;index&quot;: &quot;test1&quot;, &quot;alias&quot;: &quot;alias1&quot;&#125;&#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<p><strong>别名只能通过删除后再建立来进行修改</strong></p>\n<h4 id=\"添加多个别名\"><a href=\"#添加多个别名\" class=\"headerlink\" title=\"添加多个别名\"></a>添加多个别名</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;actions&quot;: [</span><br><span class=\"line\">        &#123;&quot;add&quot;: &#123;&quot;index&quot;: &quot;test1&quot;, &quot;alias&quot;:&quot;alias1&quot;&#125;&#125;,</span><br><span class=\"line\">        &#123;&quot;add&quot;: &#123;&quot;index&quot;: &quot;test2&quot;, &quot;alias&quot;:&quot;alias1&quot;&#125;&#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n\n\n<h3 id=\"索引配置\"><a href=\"#索引配置\" class=\"headerlink\" title=\"索引配置\"></a>索引配置</h3><p>常规的replica,shard</p>\n<p>然后还能配置analyzer</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">PUT &#x2F;indexName&#x2F;_settings</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">&quot;analysis&quot;:&#123;</span><br><span class=\"line\">&quot;analyzer&quot;:&#123;</span><br><span class=\"line\">&quot;content&quot;:&#123;</span><br><span class=\"line\">&quot;type&quot;:&quot;&quot;,</span><br><span class=\"line\">&quot;tokenizer&quot;:&quot;&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<p><strong>索引只能在关闭后,再进行一些设置上的调整</strong></p>\n<h3 id=\"映射\"><a href=\"#映射\" class=\"headerlink\" title=\"映射\"></a>映射</h3><h4 id=\"字符串数据可接受的参数\"><a href=\"#字符串数据可接受的参数\" class=\"headerlink\" title=\"字符串数据可接受的参数\"></a>字符串数据可接受的参数</h4>"},{"_content":"## ES基本术语与组成\n\n### 索引index\n\nES中的索引概念可不是关系型数据库中的“索引”，ES中的索引指的是存储数据的地方，类似关系型数据库中的数据库概念\n\n### 类型type\n\n类型Type\n有的文章指出ES中的类型Type对应的就是关系型数据库中的表，在使用ES中我们会遇到另外一个概念映射（Mapping），也有不少的文章指出Mapping对应的就是关系型数据库中的表。关系型数据库中表与表是物理独立的，即使在两个表中存在相同名称不同类型的列，这在我们的关系型数据库也是极为合理的，但这在ES中就不合理，ES中即使是在同一个索引Index下，如果字段Field存在于不同的类型Type中，即使他们代表不同的含义，但是只要它们的名称相同也必须要求类型相同，在ES中类型Type对应于关系型数据库中表的概念已经名存实亡。实际上在ES中Type作为表的概念在后期版本中越来越被弱化，在未被ES正式移除前，ES后期版本已经不允许一个索引Index创建多个Type，相信在后面的版本会彻底移除类型Type。\n\n（注：ES6已经不允许一个Index创建多个Type，https://github.com/elastic/elasticsearch/pull/24317）\n\n如果在现阶段一定要理解ES中的Type，那么一定要和Mapping结合起来。可以理解为类型Type就是定义一个表，仅仅是定义而已，而映射Mapping定义了表结构（有哪些列，列的类型是什么）\n\n### 文档Document\n\n在非关系型数据库中，有部分被称之为“文档数据库”，对应于关系型数据库中的一行记录。\n\n### 字段Field\n\n对应关系型数据库中的列。\n\n\n### 分片\n","source":"_posts/技术/es/积累.md","raw":"## ES基本术语与组成\n\n### 索引index\n\nES中的索引概念可不是关系型数据库中的“索引”，ES中的索引指的是存储数据的地方，类似关系型数据库中的数据库概念\n\n### 类型type\n\n类型Type\n有的文章指出ES中的类型Type对应的就是关系型数据库中的表，在使用ES中我们会遇到另外一个概念映射（Mapping），也有不少的文章指出Mapping对应的就是关系型数据库中的表。关系型数据库中表与表是物理独立的，即使在两个表中存在相同名称不同类型的列，这在我们的关系型数据库也是极为合理的，但这在ES中就不合理，ES中即使是在同一个索引Index下，如果字段Field存在于不同的类型Type中，即使他们代表不同的含义，但是只要它们的名称相同也必须要求类型相同，在ES中类型Type对应于关系型数据库中表的概念已经名存实亡。实际上在ES中Type作为表的概念在后期版本中越来越被弱化，在未被ES正式移除前，ES后期版本已经不允许一个索引Index创建多个Type，相信在后面的版本会彻底移除类型Type。\n\n（注：ES6已经不允许一个Index创建多个Type，https://github.com/elastic/elasticsearch/pull/24317）\n\n如果在现阶段一定要理解ES中的Type，那么一定要和Mapping结合起来。可以理解为类型Type就是定义一个表，仅仅是定义而已，而映射Mapping定义了表结构（有哪些列，列的类型是什么）\n\n### 文档Document\n\n在非关系型数据库中，有部分被称之为“文档数据库”，对应于关系型数据库中的一行记录。\n\n### 字段Field\n\n对应关系型数据库中的列。\n\n\n### 分片\n","slug":"技术/es/积累","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/es/积累","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31l000r38pw9dv41ler","content":"<h2 id=\"ES基本术语与组成\"><a href=\"#ES基本术语与组成\" class=\"headerlink\" title=\"ES基本术语与组成\"></a>ES基本术语与组成</h2><h3 id=\"索引index\"><a href=\"#索引index\" class=\"headerlink\" title=\"索引index\"></a>索引index</h3><p>ES中的索引概念可不是关系型数据库中的“索引”，ES中的索引指的是存储数据的地方，类似关系型数据库中的数据库概念</p>\n<h3 id=\"类型type\"><a href=\"#类型type\" class=\"headerlink\" title=\"类型type\"></a>类型type</h3><p>类型Type<br>有的文章指出ES中的类型Type对应的就是关系型数据库中的表，在使用ES中我们会遇到另外一个概念映射（Mapping），也有不少的文章指出Mapping对应的就是关系型数据库中的表。关系型数据库中表与表是物理独立的，即使在两个表中存在相同名称不同类型的列，这在我们的关系型数据库也是极为合理的，但这在ES中就不合理，ES中即使是在同一个索引Index下，如果字段Field存在于不同的类型Type中，即使他们代表不同的含义，但是只要它们的名称相同也必须要求类型相同，在ES中类型Type对应于关系型数据库中表的概念已经名存实亡。实际上在ES中Type作为表的概念在后期版本中越来越被弱化，在未被ES正式移除前，ES后期版本已经不允许一个索引Index创建多个Type，相信在后面的版本会彻底移除类型Type。</p>\n<p>（注：ES6已经不允许一个Index创建多个Type，<a href=\"https://github.com/elastic/elasticsearch/pull/24317）\" target=\"_blank\" rel=\"noopener\">https://github.com/elastic/elasticsearch/pull/24317）</a></p>\n<p>如果在现阶段一定要理解ES中的Type，那么一定要和Mapping结合起来。可以理解为类型Type就是定义一个表，仅仅是定义而已，而映射Mapping定义了表结构（有哪些列，列的类型是什么）</p>\n<h3 id=\"文档Document\"><a href=\"#文档Document\" class=\"headerlink\" title=\"文档Document\"></a>文档Document</h3><p>在非关系型数据库中，有部分被称之为“文档数据库”，对应于关系型数据库中的一行记录。</p>\n<h3 id=\"字段Field\"><a href=\"#字段Field\" class=\"headerlink\" title=\"字段Field\"></a>字段Field</h3><p>对应关系型数据库中的列。</p>\n<h3 id=\"分片\"><a href=\"#分片\" class=\"headerlink\" title=\"分片\"></a>分片</h3>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"ES基本术语与组成\"><a href=\"#ES基本术语与组成\" class=\"headerlink\" title=\"ES基本术语与组成\"></a>ES基本术语与组成</h2><h3 id=\"索引index\"><a href=\"#索引index\" class=\"headerlink\" title=\"索引index\"></a>索引index</h3><p>ES中的索引概念可不是关系型数据库中的“索引”，ES中的索引指的是存储数据的地方，类似关系型数据库中的数据库概念</p>\n<h3 id=\"类型type\"><a href=\"#类型type\" class=\"headerlink\" title=\"类型type\"></a>类型type</h3><p>类型Type<br>有的文章指出ES中的类型Type对应的就是关系型数据库中的表，在使用ES中我们会遇到另外一个概念映射（Mapping），也有不少的文章指出Mapping对应的就是关系型数据库中的表。关系型数据库中表与表是物理独立的，即使在两个表中存在相同名称不同类型的列，这在我们的关系型数据库也是极为合理的，但这在ES中就不合理，ES中即使是在同一个索引Index下，如果字段Field存在于不同的类型Type中，即使他们代表不同的含义，但是只要它们的名称相同也必须要求类型相同，在ES中类型Type对应于关系型数据库中表的概念已经名存实亡。实际上在ES中Type作为表的概念在后期版本中越来越被弱化，在未被ES正式移除前，ES后期版本已经不允许一个索引Index创建多个Type，相信在后面的版本会彻底移除类型Type。</p>\n<p>（注：ES6已经不允许一个Index创建多个Type，<a href=\"https://github.com/elastic/elasticsearch/pull/24317）\" target=\"_blank\" rel=\"noopener\">https://github.com/elastic/elasticsearch/pull/24317）</a></p>\n<p>如果在现阶段一定要理解ES中的Type，那么一定要和Mapping结合起来。可以理解为类型Type就是定义一个表，仅仅是定义而已，而映射Mapping定义了表结构（有哪些列，列的类型是什么）</p>\n<h3 id=\"文档Document\"><a href=\"#文档Document\" class=\"headerlink\" title=\"文档Document\"></a>文档Document</h3><p>在非关系型数据库中，有部分被称之为“文档数据库”，对应于关系型数据库中的一行记录。</p>\n<h3 id=\"字段Field\"><a href=\"#字段Field\" class=\"headerlink\" title=\"字段Field\"></a>字段Field</h3><p>对应关系型数据库中的列。</p>\n<h3 id=\"分片\"><a href=\"#分片\" class=\"headerlink\" title=\"分片\"></a>分片</h3>"},{"_content":"# summary of today\n`今日总结`\n  * hive外部表的创建与删除\n  * hbase filter的使用\n  * hbase rowkey 的使用\n\n  1. hive 外部表的创建,主要是 postion_gps_online表\n  2. hbase rowkey的使用,这里涉及到相关逻辑里的实现问题,如轨迹某段时间里所有设备的轨迹,rowkey 若设计成\n  device_id+\"_\"+ts_time 形式,则会出现通过start_row与stop_row的模糊区间匹配未找到现成的方案.所以后\n  续rowkey设计成的是ts_time+\"_\"+device_id的形式,这样可以只通过传时间就可以筛选出这个时间区间里的所有\n  device_id.\n\n\n","source":"_posts/技术/hbase/hbase20190109.md","raw":"# summary of today\n`今日总结`\n  * hive外部表的创建与删除\n  * hbase filter的使用\n  * hbase rowkey 的使用\n\n  1. hive 外部表的创建,主要是 postion_gps_online表\n  2. hbase rowkey的使用,这里涉及到相关逻辑里的实现问题,如轨迹某段时间里所有设备的轨迹,rowkey 若设计成\n  device_id+\"_\"+ts_time 形式,则会出现通过start_row与stop_row的模糊区间匹配未找到现成的方案.所以后\n  续rowkey设计成的是ts_time+\"_\"+device_id的形式,这样可以只通过传时间就可以筛选出这个时间区间里的所有\n  device_id.\n\n\n","slug":"技术/hbase/hbase20190109","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/hbase/hbase20190109","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31m000s38pw5zf37lyi","content":"<h1 id=\"summary-of-today\"><a href=\"#summary-of-today\" class=\"headerlink\" title=\"summary of today\"></a>summary of today</h1><p><code>今日总结</code></p>\n<ul>\n<li>hive外部表的创建与删除</li>\n<li>hbase filter的使用</li>\n<li>hbase rowkey 的使用</li>\n</ul>\n<ol>\n<li>hive 外部表的创建,主要是 postion_gps_online表</li>\n<li>hbase rowkey的使用,这里涉及到相关逻辑里的实现问题,如轨迹某段时间里所有设备的轨迹,rowkey 若设计成<br>device_id+”<em>“+ts_time 形式,则会出现通过start_row与stop_row的模糊区间匹配未找到现成的方案.所以后<br>续rowkey设计成的是ts_time+”</em>“+device_id的形式,这样可以只通过传时间就可以筛选出这个时间区间里的所有<br>device_id.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"summary-of-today\"><a href=\"#summary-of-today\" class=\"headerlink\" title=\"summary of today\"></a>summary of today</h1><p><code>今日总结</code></p>\n<ul>\n<li>hive外部表的创建与删除</li>\n<li>hbase filter的使用</li>\n<li>hbase rowkey 的使用</li>\n</ul>\n<ol>\n<li>hive 外部表的创建,主要是 postion_gps_online表</li>\n<li>hbase rowkey的使用,这里涉及到相关逻辑里的实现问题,如轨迹某段时间里所有设备的轨迹,rowkey 若设计成<br>device_id+”<em>“+ts_time 形式,则会出现通过start_row与stop_row的模糊区间匹配未找到现成的方案.所以后<br>续rowkey设计成的是ts_time+”</em>“+device_id的形式,这样可以只通过传时间就可以筛选出这个时间区间里的所有<br>device_id.</li>\n</ol>\n"},{"_content":"# JT809协议分析\n    809我们\n","source":"_posts/技术/gps/JT809.md","raw":"# JT809协议分析\n    809我们\n","slug":"技术/gps/JT809","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/gps/JT809","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31m000t38pwdn6i05cz","content":"<h1 id=\"JT809协议分析\"><a href=\"#JT809协议分析\" class=\"headerlink\" title=\"JT809协议分析\"></a>JT809协议分析</h1><pre><code>809我们</code></pre>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"JT809协议分析\"><a href=\"#JT809协议分析\" class=\"headerlink\" title=\"JT809协议分析\"></a>JT809协议分析</h1><pre><code>809我们</code></pre>"},{"_content":"```\nkafka细粒度学习与总结\n```\n\n### \n\n[相对干货](Kafka史上最详细原理总结)\n\n[Kafka学习笔记——Kafka原理与使用详解](https://blog.csdn.net/fuyuwei2015/article/details/72943207)\n\n记录点\n\n1. kafka组成部分\n2. 数据流转过程\n3. 备份\n4. 安全性\n5. 持久性\n6. 性能\n7. api储备\n8. 消息传输机制(相关语义)\n9. zookeer的作用\n\n\n\n着重点\n\n1. 备份同步(ISR)\n2. 消费请求处理(处理能力,出现总是的恢复)\n3. **Consumer Rebalance** 触发条件\n   1. consumer的增加或删除\n   2. broker的增加或删除\n4. 分区策略\n   1. Rangeassignor\n   2. RoundRobinAssignor\n   3. StickyAssignor\n   4. 自定义\n5. 再均衡\n   1. 均衡器\n      1. GroupCoordinator\n      2. ConsumerCoordinator\n   2. 原因\n      1. 新的消费者入组\n      2. 消息者宕机失联\n      3. 消息者主动退组(leaveGroupRequest),fg 调用unsubscribe()取消订阅\n      4. 消费组对应分区数量发生变化\n\n6. 分区管理\n   1. 优先副本(分区broker)\n      1. leader副本承担读写服务,分区leader被怼坏意味着该分区不可用.即broker节点中对应的leader副本多少,决定了该节点的负载高低.\n      2. \n\n\n\n\n\n\n\nHigh Level Consumer将从某个Partition读取的最后一条消息的offset存于Zookeeper中（从**0.8.2**开始同时支持将Offset存于Zookeeper中和专用的Kafka Topic中）。\n\n\n\n\n\nQ: 为什么kafka吞吐量高\n\n\n\nA: 同一个topic同个CG下的consumer只有一个消费它.\n\n\n\n\n\nQ:为什么要支持存储于专用的Kafka Topic中？\n\n\n\n\n\nKafkaConsumer 多线程思路:\n\n应该不单是只开启多个consumer线程\n\n\n\n\n\n","source":"_posts/技术/kafka/kafka小结.md","raw":"```\nkafka细粒度学习与总结\n```\n\n### \n\n[相对干货](Kafka史上最详细原理总结)\n\n[Kafka学习笔记——Kafka原理与使用详解](https://blog.csdn.net/fuyuwei2015/article/details/72943207)\n\n记录点\n\n1. kafka组成部分\n2. 数据流转过程\n3. 备份\n4. 安全性\n5. 持久性\n6. 性能\n7. api储备\n8. 消息传输机制(相关语义)\n9. zookeer的作用\n\n\n\n着重点\n\n1. 备份同步(ISR)\n2. 消费请求处理(处理能力,出现总是的恢复)\n3. **Consumer Rebalance** 触发条件\n   1. consumer的增加或删除\n   2. broker的增加或删除\n4. 分区策略\n   1. Rangeassignor\n   2. RoundRobinAssignor\n   3. StickyAssignor\n   4. 自定义\n5. 再均衡\n   1. 均衡器\n      1. GroupCoordinator\n      2. ConsumerCoordinator\n   2. 原因\n      1. 新的消费者入组\n      2. 消息者宕机失联\n      3. 消息者主动退组(leaveGroupRequest),fg 调用unsubscribe()取消订阅\n      4. 消费组对应分区数量发生变化\n\n6. 分区管理\n   1. 优先副本(分区broker)\n      1. leader副本承担读写服务,分区leader被怼坏意味着该分区不可用.即broker节点中对应的leader副本多少,决定了该节点的负载高低.\n      2. \n\n\n\n\n\n\n\nHigh Level Consumer将从某个Partition读取的最后一条消息的offset存于Zookeeper中（从**0.8.2**开始同时支持将Offset存于Zookeeper中和专用的Kafka Topic中）。\n\n\n\n\n\nQ: 为什么kafka吞吐量高\n\n\n\nA: 同一个topic同个CG下的consumer只有一个消费它.\n\n\n\n\n\nQ:为什么要支持存储于专用的Kafka Topic中？\n\n\n\n\n\nKafkaConsumer 多线程思路:\n\n应该不单是只开启多个consumer线程\n\n\n\n\n\n","slug":"技术/kafka/kafka小结","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/kafka/kafka小结","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31n000u38pwen64cnj0","content":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kafka细粒度学习与总结</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><a href=\"Kafka史上最详细原理总结\">相对干货</a></p>\n<p><a href=\"https://blog.csdn.net/fuyuwei2015/article/details/72943207\" target=\"_blank\" rel=\"noopener\">Kafka学习笔记——Kafka原理与使用详解</a></p>\n<p>记录点</p>\n<ol>\n<li>kafka组成部分</li>\n<li>数据流转过程</li>\n<li>备份</li>\n<li>安全性</li>\n<li>持久性</li>\n<li>性能</li>\n<li>api储备</li>\n<li>消息传输机制(相关语义)</li>\n<li>zookeer的作用</li>\n</ol>\n<p>着重点</p>\n<ol>\n<li><p>备份同步(ISR)</p>\n</li>\n<li><p>消费请求处理(处理能力,出现总是的恢复)</p>\n</li>\n<li><p><strong>Consumer Rebalance</strong> 触发条件</p>\n<ol>\n<li>consumer的增加或删除</li>\n<li>broker的增加或删除</li>\n</ol>\n</li>\n<li><p>分区策略</p>\n<ol>\n<li>Rangeassignor</li>\n<li>RoundRobinAssignor</li>\n<li>StickyAssignor</li>\n<li>自定义</li>\n</ol>\n</li>\n<li><p>再均衡</p>\n<ol>\n<li>均衡器<ol>\n<li>GroupCoordinator</li>\n<li>ConsumerCoordinator</li>\n</ol>\n</li>\n<li>原因<ol>\n<li>新的消费者入组</li>\n<li>消息者宕机失联</li>\n<li>消息者主动退组(leaveGroupRequest),fg 调用unsubscribe()取消订阅</li>\n<li>消费组对应分区数量发生变化</li>\n</ol>\n</li>\n</ol>\n</li>\n<li><p>分区管理</p>\n<ol>\n<li>优先副本(分区broker)<ol>\n<li>leader副本承担读写服务,分区leader被怼坏意味着该分区不可用.即broker节点中对应的leader副本多少,决定了该节点的负载高低.</li>\n<li></li>\n</ol>\n</li>\n</ol>\n</li>\n</ol>\n<p>High Level Consumer将从某个Partition读取的最后一条消息的offset存于Zookeeper中（从<strong>0.8.2</strong>开始同时支持将Offset存于Zookeeper中和专用的Kafka Topic中）。</p>\n<p>Q: 为什么kafka吞吐量高</p>\n<p>A: 同一个topic同个CG下的consumer只有一个消费它.</p>\n<p>Q:为什么要支持存储于专用的Kafka Topic中？</p>\n<p>KafkaConsumer 多线程思路:</p>\n<p>应该不单是只开启多个consumer线程</p>\n","site":{"data":{}},"excerpt":"","more":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kafka细粒度学习与总结</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h3><p><a href=\"Kafka史上最详细原理总结\">相对干货</a></p>\n<p><a href=\"https://blog.csdn.net/fuyuwei2015/article/details/72943207\" target=\"_blank\" rel=\"noopener\">Kafka学习笔记——Kafka原理与使用详解</a></p>\n<p>记录点</p>\n<ol>\n<li>kafka组成部分</li>\n<li>数据流转过程</li>\n<li>备份</li>\n<li>安全性</li>\n<li>持久性</li>\n<li>性能</li>\n<li>api储备</li>\n<li>消息传输机制(相关语义)</li>\n<li>zookeer的作用</li>\n</ol>\n<p>着重点</p>\n<ol>\n<li><p>备份同步(ISR)</p>\n</li>\n<li><p>消费请求处理(处理能力,出现总是的恢复)</p>\n</li>\n<li><p><strong>Consumer Rebalance</strong> 触发条件</p>\n<ol>\n<li>consumer的增加或删除</li>\n<li>broker的增加或删除</li>\n</ol>\n</li>\n<li><p>分区策略</p>\n<ol>\n<li>Rangeassignor</li>\n<li>RoundRobinAssignor</li>\n<li>StickyAssignor</li>\n<li>自定义</li>\n</ol>\n</li>\n<li><p>再均衡</p>\n<ol>\n<li>均衡器<ol>\n<li>GroupCoordinator</li>\n<li>ConsumerCoordinator</li>\n</ol>\n</li>\n<li>原因<ol>\n<li>新的消费者入组</li>\n<li>消息者宕机失联</li>\n<li>消息者主动退组(leaveGroupRequest),fg 调用unsubscribe()取消订阅</li>\n<li>消费组对应分区数量发生变化</li>\n</ol>\n</li>\n</ol>\n</li>\n<li><p>分区管理</p>\n<ol>\n<li>优先副本(分区broker)<ol>\n<li>leader副本承担读写服务,分区leader被怼坏意味着该分区不可用.即broker节点中对应的leader副本多少,决定了该节点的负载高低.</li>\n<li></li>\n</ol>\n</li>\n</ol>\n</li>\n</ol>\n<p>High Level Consumer将从某个Partition读取的最后一条消息的offset存于Zookeeper中（从<strong>0.8.2</strong>开始同时支持将Offset存于Zookeeper中和专用的Kafka Topic中）。</p>\n<p>Q: 为什么kafka吞吐量高</p>\n<p>A: 同一个topic同个CG下的consumer只有一个消费它.</p>\n<p>Q:为什么要支持存储于专用的Kafka Topic中？</p>\n<p>KafkaConsumer 多线程思路:</p>\n<p>应该不单是只开启多个consumer线程</p>\n"},{"_content":"# 数据中心平台banner页面\n\n\n## 数据治理平台\n\nhttp://192.168.10.100:8888/hue/oozie/editor/workflow/new/  hue\n![](http://img.wqkenqing.ren/grdQ6x.png)\n\n账号:admin\n密码:admin\n\n\n## 元数据管理\n\nhttp://192.168.10.100:9090/home?zkPath=/hbase/table\n![](http://img.wqkenqing.ren/TYpLzV.png)\n账号:admin\n密码:manager\n\n## 数据资源平台\n\n* http://192.168.10.100:60010/tablesDetailed.jsp\n![](http://img.wqkenqing.ren/b5EgXW.png)\n* http://192.168.10.100:10002/hiveserver2.jsp\n![](http://img.wqkenqing.ren/97aFOC.png)\n* http://192.168.10.100:50070/explorer.html#/\n![](http://img.wqkenqing.ren/kQsXqv.png)\n\n## 资源统一调度平台\n\n* http://192.168.10.100:8088/cluster/apps\n![](http://img.wqkenqing.ren/tL6oeC.png)\n","source":"_posts/技术/platform/platform.md","raw":"# 数据中心平台banner页面\n\n\n## 数据治理平台\n\nhttp://192.168.10.100:8888/hue/oozie/editor/workflow/new/  hue\n![](http://img.wqkenqing.ren/grdQ6x.png)\n\n账号:admin\n密码:admin\n\n\n## 元数据管理\n\nhttp://192.168.10.100:9090/home?zkPath=/hbase/table\n![](http://img.wqkenqing.ren/TYpLzV.png)\n账号:admin\n密码:manager\n\n## 数据资源平台\n\n* http://192.168.10.100:60010/tablesDetailed.jsp\n![](http://img.wqkenqing.ren/b5EgXW.png)\n* http://192.168.10.100:10002/hiveserver2.jsp\n![](http://img.wqkenqing.ren/97aFOC.png)\n* http://192.168.10.100:50070/explorer.html#/\n![](http://img.wqkenqing.ren/kQsXqv.png)\n\n## 资源统一调度平台\n\n* http://192.168.10.100:8088/cluster/apps\n![](http://img.wqkenqing.ren/tL6oeC.png)\n","slug":"技术/platform/platform","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/platform/platform","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31n000v38pw765h2i2j","content":"<h1 id=\"数据中心平台banner页面\"><a href=\"#数据中心平台banner页面\" class=\"headerlink\" title=\"数据中心平台banner页面\"></a>数据中心平台banner页面</h1><h2 id=\"数据治理平台\"><a href=\"#数据治理平台\" class=\"headerlink\" title=\"数据治理平台\"></a>数据治理平台</h2><p><a href=\"http://192.168.10.100:8888/hue/oozie/editor/workflow/new/\" target=\"_blank\" rel=\"noopener\">http://192.168.10.100:8888/hue/oozie/editor/workflow/new/</a>  hue<br><img src=\"http://img.wqkenqing.ren/grdQ6x.png\" alt=\"\"></p>\n<p>账号:admin<br>密码:admin</p>\n<h2 id=\"元数据管理\"><a href=\"#元数据管理\" class=\"headerlink\" title=\"元数据管理\"></a>元数据管理</h2><p><a href=\"http://192.168.10.100:9090/home?zkPath=/hbase/table\" target=\"_blank\" rel=\"noopener\">http://192.168.10.100:9090/home?zkPath=/hbase/table</a><br><img src=\"http://img.wqkenqing.ren/TYpLzV.png\" alt=\"\"><br>账号:admin<br>密码:manager</p>\n<h2 id=\"数据资源平台\"><a href=\"#数据资源平台\" class=\"headerlink\" title=\"数据资源平台\"></a>数据资源平台</h2><ul>\n<li><a href=\"http://192.168.10.100:60010/tablesDetailed.jsp\" target=\"_blank\" rel=\"noopener\">http://192.168.10.100:60010/tablesDetailed.jsp</a><br><img src=\"http://img.wqkenqing.ren/b5EgXW.png\" alt=\"\"></li>\n<li><a href=\"http://192.168.10.100:10002/hiveserver2.jsp\" target=\"_blank\" rel=\"noopener\">http://192.168.10.100:10002/hiveserver2.jsp</a><br><img src=\"http://img.wqkenqing.ren/97aFOC.png\" alt=\"\"></li>\n<li><a href=\"http://192.168.10.100:50070/explorer.html#/\" target=\"_blank\" rel=\"noopener\">http://192.168.10.100:50070/explorer.html#/</a><br><img src=\"http://img.wqkenqing.ren/kQsXqv.png\" alt=\"\"></li>\n</ul>\n<h2 id=\"资源统一调度平台\"><a href=\"#资源统一调度平台\" class=\"headerlink\" title=\"资源统一调度平台\"></a>资源统一调度平台</h2><ul>\n<li><a href=\"http://192.168.10.100:8088/cluster/apps\" target=\"_blank\" rel=\"noopener\">http://192.168.10.100:8088/cluster/apps</a><br><img src=\"http://img.wqkenqing.ren/tL6oeC.png\" alt=\"\"></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"数据中心平台banner页面\"><a href=\"#数据中心平台banner页面\" class=\"headerlink\" title=\"数据中心平台banner页面\"></a>数据中心平台banner页面</h1><h2 id=\"数据治理平台\"><a href=\"#数据治理平台\" class=\"headerlink\" title=\"数据治理平台\"></a>数据治理平台</h2><p><a href=\"http://192.168.10.100:8888/hue/oozie/editor/workflow/new/\" target=\"_blank\" rel=\"noopener\">http://192.168.10.100:8888/hue/oozie/editor/workflow/new/</a>  hue<br><img src=\"http://img.wqkenqing.ren/grdQ6x.png\" alt=\"\"></p>\n<p>账号:admin<br>密码:admin</p>\n<h2 id=\"元数据管理\"><a href=\"#元数据管理\" class=\"headerlink\" title=\"元数据管理\"></a>元数据管理</h2><p><a href=\"http://192.168.10.100:9090/home?zkPath=/hbase/table\" target=\"_blank\" rel=\"noopener\">http://192.168.10.100:9090/home?zkPath=/hbase/table</a><br><img src=\"http://img.wqkenqing.ren/TYpLzV.png\" alt=\"\"><br>账号:admin<br>密码:manager</p>\n<h2 id=\"数据资源平台\"><a href=\"#数据资源平台\" class=\"headerlink\" title=\"数据资源平台\"></a>数据资源平台</h2><ul>\n<li><a href=\"http://192.168.10.100:60010/tablesDetailed.jsp\" target=\"_blank\" rel=\"noopener\">http://192.168.10.100:60010/tablesDetailed.jsp</a><br><img src=\"http://img.wqkenqing.ren/b5EgXW.png\" alt=\"\"></li>\n<li><a href=\"http://192.168.10.100:10002/hiveserver2.jsp\" target=\"_blank\" rel=\"noopener\">http://192.168.10.100:10002/hiveserver2.jsp</a><br><img src=\"http://img.wqkenqing.ren/97aFOC.png\" alt=\"\"></li>\n<li><a href=\"http://192.168.10.100:50070/explorer.html#/\" target=\"_blank\" rel=\"noopener\">http://192.168.10.100:50070/explorer.html#/</a><br><img src=\"http://img.wqkenqing.ren/kQsXqv.png\" alt=\"\"></li>\n</ul>\n<h2 id=\"资源统一调度平台\"><a href=\"#资源统一调度平台\" class=\"headerlink\" title=\"资源统一调度平台\"></a>资源统一调度平台</h2><ul>\n<li><a href=\"http://192.168.10.100:8088/cluster/apps\" target=\"_blank\" rel=\"noopener\">http://192.168.10.100:8088/cluster/apps</a><br><img src=\"http://img.wqkenqing.ren/tL6oeC.png\" alt=\"\"></li>\n</ul>\n"},{"_content":"# kafka疑问点\n\n## consumer api\n\n1. High-level && sample-level的区别\n2. 不同api下offset的维护.\n\n","source":"_posts/技术/kafka/kafka疑问点.md","raw":"# kafka疑问点\n\n## consumer api\n\n1. High-level && sample-level的区别\n2. 不同api下offset的维护.\n\n","slug":"技术/kafka/kafka疑问点","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/kafka/kafka疑问点","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31o000w38pw2b3h7487","content":"<h1 id=\"kafka疑问点\"><a href=\"#kafka疑问点\" class=\"headerlink\" title=\"kafka疑问点\"></a>kafka疑问点</h1><h2 id=\"consumer-api\"><a href=\"#consumer-api\" class=\"headerlink\" title=\"consumer api\"></a>consumer api</h2><ol>\n<li>High-level &amp;&amp; sample-level的区别</li>\n<li>不同api下offset的维护.</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"kafka疑问点\"><a href=\"#kafka疑问点\" class=\"headerlink\" title=\"kafka疑问点\"></a>kafka疑问点</h1><h2 id=\"consumer-api\"><a href=\"#consumer-api\" class=\"headerlink\" title=\"consumer api\"></a>consumer api</h2><ol>\n<li>High-level &amp;&amp; sample-level的区别</li>\n<li>不同api下offset的维护.</li>\n</ol>\n"},{"_content":"# 账号管理\n\n\n| 账号 | 密码     |\n| :------------- | :-------------|\n| home       | home123       |\n| kuiq.wang       |        |\n","source":"_posts/技术/owncloud/账号.md","raw":"# 账号管理\n\n\n| 账号 | 密码     |\n| :------------- | :-------------|\n| home       | home123       |\n| kuiq.wang       |        |\n","slug":"技术/owncloud/账号","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/owncloud/账号","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31o000x38pw8rc54uln","content":"<h1 id=\"账号管理\"><a href=\"#账号管理\" class=\"headerlink\" title=\"账号管理\"></a>账号管理</h1><table>\n<thead>\n<tr>\n<th align=\"left\">账号</th>\n<th align=\"left\">密码</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">home</td>\n<td align=\"left\">home123</td>\n</tr>\n<tr>\n<td align=\"left\">kuiq.wang</td>\n<td align=\"left\"></td>\n</tr>\n</tbody></table>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"账号管理\"><a href=\"#账号管理\" class=\"headerlink\" title=\"账号管理\"></a>账号管理</h1><table>\n<thead>\n<tr>\n<th align=\"left\">账号</th>\n<th align=\"left\">密码</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">home</td>\n<td align=\"left\">home123</td>\n</tr>\n<tr>\n<td align=\"left\">kuiq.wang</td>\n<td align=\"left\"></td>\n</tr>\n</tbody></table>\n"},{"_content":"\n安装：\n\nsudo apt-get install vnc4server xfce4\n\n\nvncserver -geometry 1280x800 -alwaysshared :1\n\n\n vncserver -kill :1\n\n\n #!/bin/sh\n\n# Uncomment the following two lines for normal desktop:\n#unset SESSION_MANAGER\n#exec /etc/X11/xinit/xinitrc\n\n#[ -x /etc/vnc/xstartup ] && exec /etc/vnc/xstartup\n#[ -r $HOME/.Xresources ] && xrdb $HOME/.Xresources\n#xsetroot -solid grey\n#vncconfig -iconic &\n#x-terminal-emulator -geometry 80x24+10+10 -ls -title \"$VNCDESKTOP Desktop\" &\n#x-window-manager &\nunset SESSION_MANAGER\nunset DBUS_SESSION_BUS_ADDRESS\n[ -x /etc/vnc/xstartup ] && exec /etc/vnc/xstartup\n[ -r $HOME/.Xresources ] && xrdb $HOME/.Xresources\nvncconfig -iconic &\n-session &\n\n\n sudo x11vnc -display :2 -auth ~/.vnc/passwd -forever -bg -o /var/log/x11vnc.log -rfbauth /etc/x11vnc.pass -shared -noxdamage -xrandr \"resize\" -rfbport 5900\n\n sudo x11vnc -forever -bg -usepw -cursor arrow -display :2\n","source":"_posts/技术/ubuntu/desk_gui.md","raw":"\n安装：\n\nsudo apt-get install vnc4server xfce4\n\n\nvncserver -geometry 1280x800 -alwaysshared :1\n\n\n vncserver -kill :1\n\n\n #!/bin/sh\n\n# Uncomment the following two lines for normal desktop:\n#unset SESSION_MANAGER\n#exec /etc/X11/xinit/xinitrc\n\n#[ -x /etc/vnc/xstartup ] && exec /etc/vnc/xstartup\n#[ -r $HOME/.Xresources ] && xrdb $HOME/.Xresources\n#xsetroot -solid grey\n#vncconfig -iconic &\n#x-terminal-emulator -geometry 80x24+10+10 -ls -title \"$VNCDESKTOP Desktop\" &\n#x-window-manager &\nunset SESSION_MANAGER\nunset DBUS_SESSION_BUS_ADDRESS\n[ -x /etc/vnc/xstartup ] && exec /etc/vnc/xstartup\n[ -r $HOME/.Xresources ] && xrdb $HOME/.Xresources\nvncconfig -iconic &\n-session &\n\n\n sudo x11vnc -display :2 -auth ~/.vnc/passwd -forever -bg -o /var/log/x11vnc.log -rfbauth /etc/x11vnc.pass -shared -noxdamage -xrandr \"resize\" -rfbport 5900\n\n sudo x11vnc -forever -bg -usepw -cursor arrow -display :2\n","slug":"技术/ubuntu/desk_gui","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/ubuntu/desk_gui","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31p000y38pw7oy58jf1","content":"<p>安装：</p>\n<p>sudo apt-get install vnc4server xfce4</p>\n<p>vncserver -geometry 1280x800 -alwaysshared :1</p>\n<p> vncserver -kill :1</p>\n<p> #!/bin/sh</p>\n<h1 id=\"Uncomment-the-following-two-lines-for-normal-desktop\"><a href=\"#Uncomment-the-following-two-lines-for-normal-desktop\" class=\"headerlink\" title=\"Uncomment the following two lines for normal desktop:\"></a>Uncomment the following two lines for normal desktop:</h1><p>#unset SESSION_MANAGER<br>#exec /etc/X11/xinit/xinitrc</p>\n<p>#[ -x /etc/vnc/xstartup ] &amp;&amp; exec /etc/vnc/xstartup<br>#[ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresources<br>#xsetroot -solid grey<br>#vncconfig -iconic &amp;<br>#x-terminal-emulator -geometry 80x24+10+10 -ls -title “$VNCDESKTOP Desktop” &amp;<br>#x-window-manager &amp;<br>unset SESSION_MANAGER<br>unset DBUS_SESSION_BUS_ADDRESS<br>[ -x /etc/vnc/xstartup ] &amp;&amp; exec /etc/vnc/xstartup<br>[ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresources<br>vncconfig -iconic &amp;<br>-session &amp;</p>\n<p> sudo x11vnc -display :2 -auth ~/.vnc/passwd -forever -bg -o /var/log/x11vnc.log -rfbauth /etc/x11vnc.pass -shared -noxdamage -xrandr “resize” -rfbport 5900</p>\n<p> sudo x11vnc -forever -bg -usepw -cursor arrow -display :2</p>\n","site":{"data":{}},"excerpt":"","more":"<p>安装：</p>\n<p>sudo apt-get install vnc4server xfce4</p>\n<p>vncserver -geometry 1280x800 -alwaysshared :1</p>\n<p> vncserver -kill :1</p>\n<p> #!/bin/sh</p>\n<h1 id=\"Uncomment-the-following-two-lines-for-normal-desktop\"><a href=\"#Uncomment-the-following-two-lines-for-normal-desktop\" class=\"headerlink\" title=\"Uncomment the following two lines for normal desktop:\"></a>Uncomment the following two lines for normal desktop:</h1><p>#unset SESSION_MANAGER<br>#exec /etc/X11/xinit/xinitrc</p>\n<p>#[ -x /etc/vnc/xstartup ] &amp;&amp; exec /etc/vnc/xstartup<br>#[ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresources<br>#xsetroot -solid grey<br>#vncconfig -iconic &amp;<br>#x-terminal-emulator -geometry 80x24+10+10 -ls -title “$VNCDESKTOP Desktop” &amp;<br>#x-window-manager &amp;<br>unset SESSION_MANAGER<br>unset DBUS_SESSION_BUS_ADDRESS<br>[ -x /etc/vnc/xstartup ] &amp;&amp; exec /etc/vnc/xstartup<br>[ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresources<br>vncconfig -iconic &amp;<br>-session &amp;</p>\n<p> sudo x11vnc -display :2 -auth ~/.vnc/passwd -forever -bg -o /var/log/x11vnc.log -rfbauth /etc/x11vnc.pass -shared -noxdamage -xrandr “resize” -rfbport 5900</p>\n<p> sudo x11vnc -forever -bg -usepw -cursor arrow -display :2</p>\n"},{"_content":"`\njust about the params of springboot . and the major content are based on  how to use anntation !\n\n`\n\n# normal request params\n\n","source":"_posts/技术/springboot/how to set parm.md","raw":"`\njust about the params of springboot . and the major content are based on  how to use anntation !\n\n`\n\n# normal request params\n\n","slug":"技术/springboot/how to set parm","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/springboot/how to set parm","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31p000z38pw99tp7meh","content":"<p>`<br>just about the params of springboot . and the major content are based on  how to use anntation !</p>\n<p>`</p>\n<h1 id=\"normal-request-params\"><a href=\"#normal-request-params\" class=\"headerlink\" title=\"normal request params\"></a>normal request params</h1>","site":{"data":{}},"excerpt":"","more":"<p>`<br>just about the params of springboot . and the major content are based on  how to use anntation !</p>\n<p>`</p>\n<h1 id=\"normal-request-params\"><a href=\"#normal-request-params\" class=\"headerlink\" title=\"normal request params\"></a>normal request params</h1>"},{"_content":" ---\n title:  北风spark\n date: 2020-07-22\n tags: [spark,sparksql]\n ---\n\n <!--more-->\n\n## 新特性\n\nQ:spark2.x 与spark1.x相比新增的一些新特性\n\n1. DataFrame 与DataSet进行统一,DataFrame即是DataSet<Row>类型别名\n\n2. SparkSession统一sqlContext与HiveContext 新的上下文入口\n\n3. DataSet的增强聚合api\n\n4. 支持sql 2003标准\n\n5. 支持子查询\n\n6. sparkmlib 转成DataSet api ,rdd api转为维护.\n\n7. structured streaming\n\n8. Dstream 支持 kafka 0.11.0\n\n## 移除部份功能\n\nspark2.X 不支持hadoop2.1之前的版本\n\nspark2.X 慢慢移除java7\n\n\n\n\n## spark sql(spark2.0)\n\nsql&DataSet api\n\n\n### SparkSession\n\n\n### DataFrame\n\n\n#### 创建临时视图\n\n#### 持久化\n\n#### 逻辑执行计划\n\nexplain()\n\n#### Dataset与Dataframe的转换\n\njava code\ndf.as(Encoder encoder)\n","source":"_posts/技术/video/北风spark.md","raw":" ---\n title:  北风spark\n date: 2020-07-22\n tags: [spark,sparksql]\n ---\n\n <!--more-->\n\n## 新特性\n\nQ:spark2.x 与spark1.x相比新增的一些新特性\n\n1. DataFrame 与DataSet进行统一,DataFrame即是DataSet<Row>类型别名\n\n2. SparkSession统一sqlContext与HiveContext 新的上下文入口\n\n3. DataSet的增强聚合api\n\n4. 支持sql 2003标准\n\n5. 支持子查询\n\n6. sparkmlib 转成DataSet api ,rdd api转为维护.\n\n7. structured streaming\n\n8. Dstream 支持 kafka 0.11.0\n\n## 移除部份功能\n\nspark2.X 不支持hadoop2.1之前的版本\n\nspark2.X 慢慢移除java7\n\n\n\n\n## spark sql(spark2.0)\n\nsql&DataSet api\n\n\n### SparkSession\n\n\n### DataFrame\n\n\n#### 创建临时视图\n\n#### 持久化\n\n#### 逻辑执行计划\n\nexplain()\n\n#### Dataset与Dataframe的转换\n\njava code\ndf.as(Encoder encoder)\n","slug":"技术/video/北风spark","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/video/北风spark","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31q001038pw1uwgg7ez","content":"<hr>\n<p> title:  北风spark<br> date: 2020-07-22<br> tags: [spark,sparksql]</p>\n<hr>\n <a id=\"more\"></a>\n\n<h2 id=\"新特性\"><a href=\"#新特性\" class=\"headerlink\" title=\"新特性\"></a>新特性</h2><p>Q:spark2.x 与spark1.x相比新增的一些新特性</p>\n<ol>\n<li><p>DataFrame 与DataSet进行统一,DataFrame即是DataSet<Row>类型别名</p>\n</li>\n<li><p>SparkSession统一sqlContext与HiveContext 新的上下文入口</p>\n</li>\n<li><p>DataSet的增强聚合api</p>\n</li>\n<li><p>支持sql 2003标准</p>\n</li>\n<li><p>支持子查询</p>\n</li>\n<li><p>sparkmlib 转成DataSet api ,rdd api转为维护.</p>\n</li>\n<li><p>structured streaming</p>\n</li>\n<li><p>Dstream 支持 kafka 0.11.0</p>\n</li>\n</ol>\n<h2 id=\"移除部份功能\"><a href=\"#移除部份功能\" class=\"headerlink\" title=\"移除部份功能\"></a>移除部份功能</h2><p>spark2.X 不支持hadoop2.1之前的版本</p>\n<p>spark2.X 慢慢移除java7</p>\n<h2 id=\"spark-sql-spark2-0\"><a href=\"#spark-sql-spark2-0\" class=\"headerlink\" title=\"spark sql(spark2.0)\"></a>spark sql(spark2.0)</h2><p>sql&amp;DataSet api</p>\n<h3 id=\"SparkSession\"><a href=\"#SparkSession\" class=\"headerlink\" title=\"SparkSession\"></a>SparkSession</h3><h3 id=\"DataFrame\"><a href=\"#DataFrame\" class=\"headerlink\" title=\"DataFrame\"></a>DataFrame</h3><h4 id=\"创建临时视图\"><a href=\"#创建临时视图\" class=\"headerlink\" title=\"创建临时视图\"></a>创建临时视图</h4><h4 id=\"持久化\"><a href=\"#持久化\" class=\"headerlink\" title=\"持久化\"></a>持久化</h4><h4 id=\"逻辑执行计划\"><a href=\"#逻辑执行计划\" class=\"headerlink\" title=\"逻辑执行计划\"></a>逻辑执行计划</h4><p>explain()</p>\n<h4 id=\"Dataset与Dataframe的转换\"><a href=\"#Dataset与Dataframe的转换\" class=\"headerlink\" title=\"Dataset与Dataframe的转换\"></a>Dataset与Dataframe的转换</h4><p>java code<br>df.as(Encoder encoder)</p>\n","site":{"data":{}},"excerpt":"<hr>\n<p> title:  北风spark<br> date: 2020-07-22<br> tags: [spark,sparksql]</p>\n<hr>","more":"<h2 id=\"新特性\"><a href=\"#新特性\" class=\"headerlink\" title=\"新特性\"></a>新特性</h2><p>Q:spark2.x 与spark1.x相比新增的一些新特性</p>\n<ol>\n<li><p>DataFrame 与DataSet进行统一,DataFrame即是DataSet<Row>类型别名</p>\n</li>\n<li><p>SparkSession统一sqlContext与HiveContext 新的上下文入口</p>\n</li>\n<li><p>DataSet的增强聚合api</p>\n</li>\n<li><p>支持sql 2003标准</p>\n</li>\n<li><p>支持子查询</p>\n</li>\n<li><p>sparkmlib 转成DataSet api ,rdd api转为维护.</p>\n</li>\n<li><p>structured streaming</p>\n</li>\n<li><p>Dstream 支持 kafka 0.11.0</p>\n</li>\n</ol>\n<h2 id=\"移除部份功能\"><a href=\"#移除部份功能\" class=\"headerlink\" title=\"移除部份功能\"></a>移除部份功能</h2><p>spark2.X 不支持hadoop2.1之前的版本</p>\n<p>spark2.X 慢慢移除java7</p>\n<h2 id=\"spark-sql-spark2-0\"><a href=\"#spark-sql-spark2-0\" class=\"headerlink\" title=\"spark sql(spark2.0)\"></a>spark sql(spark2.0)</h2><p>sql&amp;DataSet api</p>\n<h3 id=\"SparkSession\"><a href=\"#SparkSession\" class=\"headerlink\" title=\"SparkSession\"></a>SparkSession</h3><h3 id=\"DataFrame\"><a href=\"#DataFrame\" class=\"headerlink\" title=\"DataFrame\"></a>DataFrame</h3><h4 id=\"创建临时视图\"><a href=\"#创建临时视图\" class=\"headerlink\" title=\"创建临时视图\"></a>创建临时视图</h4><h4 id=\"持久化\"><a href=\"#持久化\" class=\"headerlink\" title=\"持久化\"></a>持久化</h4><h4 id=\"逻辑执行计划\"><a href=\"#逻辑执行计划\" class=\"headerlink\" title=\"逻辑执行计划\"></a>逻辑执行计划</h4><p>explain()</p>\n<h4 id=\"Dataset与Dataframe的转换\"><a href=\"#Dataset与Dataframe的转换\" class=\"headerlink\" title=\"Dataset与Dataframe的转换\"></a>Dataset与Dataframe的转换</h4><p>java code<br>df.as(Encoder encoder)</p>"},{"_content":"```\nlinux自启服务\n\n```\n\n## ubuntu","source":"_posts/技术/ubuntu/linux自启服务.md","raw":"```\nlinux自启服务\n\n```\n\n## ubuntu","slug":"技术/ubuntu/linux自启服务","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/ubuntu/linux自启服务","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31q001138pwgtd6akxi","content":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">linux自启服务</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"ubuntu\"><a href=\"#ubuntu\" class=\"headerlink\" title=\"ubuntu\"></a>ubuntu</h2>","site":{"data":{}},"excerpt":"","more":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">linux自启服务</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"ubuntu\"><a href=\"#ubuntu\" class=\"headerlink\" title=\"ubuntu\"></a>ubuntu</h2>"},{"_content":"# yarn相关参数提取\n\n## rest web api\nhttp://namenode:8088/ws/v1/cluster/apps\n\n获得json对象\n\n```json\n{\n  \"apps\":{\n    \"app\":[\n      {\n        \"applicationType\":\"MAPREDUCE\",\n        \"finalStatus\":\"SUCCEEDED\",\n        \"trackingUrl\":\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0007\\/\",\n        \"runningContainers\":-1,\n        \"clusterId\":1558964977321,\n        \"vcoreSeconds\":349,\n        \"preemptedResourceVCores\":0,\n        \"numAMContainerPreempted\":0,\n        \"allocatedMB\":-1,\n        \"id\":\"application_1558964977321_0007\",\n        \"state\":\"FINISHED\",\n        \"amHostHttpAddress\":\"datanode2:8042\",\n        \"memorySeconds\":538321,\n        \"preemptedResourceMB\":0,\n        \"applicationTags\":\"\",\n        \"startedTime\":1561963496299,\n        \"trackingUI\":\"History\",\n        \"numNonAMContainerPreempted\":0,\n        \"amContainerLogs\":\"http:\\/\\/datanode2:8042\\/node\\/containerlogs\\/container_1558964977321_0007_01_000001\\/hadoop\",\n        \"allocatedVCores\":-1,\n        \"diagnostics\":\"\",\n        \"name\":\"componet_code-jar-with-dependencies.jar\",\n        \"progress\":100,\n        \"finishedTime\":1561963577074,\n        \"user\":\"hadoop\",\n        \"queue\":\"default\",\n        \"elapsedTime\":80775\n      },\n      {\n        \"applicationType\":\"MAPREDUCE\",\n        \"finalStatus\":\"FAILED\",\n        \"trackingUrl\":\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0006\\/\",\n        \"runningContainers\":-1,\n        \"clusterId\":1558964977321,\n        \"vcoreSeconds\":197,\n        \"preemptedResourceVCores\":0,\n        \"numAMContainerPreempted\":0,\n        \"allocatedMB\":-1,\n        \"id\":\"application_1558964977321_0006\",\n        \"state\":\"FINISHED\",\n        \"amHostHttpAddress\":\"datanode1:8042\",\n        \"memorySeconds\":261004,\n        \"preemptedResourceMB\":0,\n        \"applicationTags\":\"\",\n        \"startedTime\":1559543218059,\n        \"trackingUI\":\"History\",\n        \"numNonAMContainerPreempted\":0,\n        \"amContainerLogs\":\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0006_01_000001\\/hadoop\",\n        \"allocatedVCores\":-1,\n        \"diagnostics\":\"Task failed task_1558964977321_0006_m_000000\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\",\n        \"name\":\"N\\/A\",\n        \"progress\":100,\n        \"finishedTime\":1559543260162,\n        \"user\":\"hadoop\",\n        \"queue\":\"default\",\n        \"elapsedTime\":42103\n      },\n      {\n        \"applicationType\":\"MAPREDUCE\",\n        \"finalStatus\":\"FAILED\",\n        \"trackingUrl\":\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0005\\/\",\n        \"runningContainers\":-1,\n        \"clusterId\":1558964977321,\n        \"vcoreSeconds\":181,\n        \"preemptedResourceVCores\":0,\n        \"numAMContainerPreempted\":0,\n        \"allocatedMB\":-1,\n        \"id\":\"application_1558964977321_0005\",\n        \"state\":\"FINISHED\",\n        \"amHostHttpAddress\":\"datanode1:8042\",\n        \"memorySeconds\":239683,\n        \"preemptedResourceMB\":0,\n        \"applicationTags\":\"\",\n        \"startedTime\":1559543157210,\n        \"trackingUI\":\"History\",\n        \"numNonAMContainerPreempted\":0,\n        \"amContainerLogs\":\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0005_01_000001\\/hadoop\",\n        \"allocatedVCores\":-1,\n        \"diagnostics\":\"Task failed task_1558964977321_0005_m_000004\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\",\n        \"name\":\"N\\/A\",\n        \"progress\":100,\n        \"finishedTime\":1559543203631,\n        \"user\":\"hadoop\",\n        \"queue\":\"default\",\n        \"elapsedTime\":46421\n      },\n      {\n        \"applicationType\":\"MAPREDUCE\",\n        \"finalStatus\":\"FAILED\",\n        \"trackingUrl\":\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0004\\/\",\n        \"runningContainers\":-1,\n        \"clusterId\":1558964977321,\n        \"vcoreSeconds\":206,\n        \"preemptedResourceVCores\":0,\n        \"numAMContainerPreempted\":0,\n        \"allocatedMB\":-1,\n        \"id\":\"application_1558964977321_0004\",\n        \"state\":\"FINISHED\",\n        \"amHostHttpAddress\":\"datanode2:8042\",\n        \"memorySeconds\":269329,\n        \"preemptedResourceMB\":0,\n        \"applicationTags\":\"\",\n        \"startedTime\":1559543112981,\n        \"trackingUI\":\"History\",\n        \"numNonAMContainerPreempted\":0,\n        \"amContainerLogs\":\"http:\\/\\/datanode2:8042\\/node\\/containerlogs\\/container_1558964977321_0004_01_000001\\/hadoop\",\n        \"allocatedVCores\":-1,\n        \"diagnostics\":\"Task failed task_1558964977321_0004_m_000000\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\",\n        \"name\":\"N\\/A\",\n        \"progress\":100,\n        \"finishedTime\":1559543159096,\n        \"user\":\"hadoop\",\n        \"queue\":\"default\",\n        \"elapsedTime\":46115\n      },\n      {\n        \"applicationType\":\"MAPREDUCE\",\n        \"finalStatus\":\"FAILED\",\n        \"trackingUrl\":\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0003\\/\",\n        \"runningContainers\":-1,\n        \"clusterId\":1558964977321,\n        \"vcoreSeconds\":172,\n        \"preemptedResourceVCores\":0,\n        \"numAMContainerPreempted\":0,\n        \"allocatedMB\":-1,\n        \"id\":\"application_1558964977321_0003\",\n        \"state\":\"FINISHED\",\n        \"amHostHttpAddress\":\"datanode1:8042\",\n        \"memorySeconds\":227366,\n        \"preemptedResourceMB\":0,\n        \"applicationTags\":\"\",\n        \"startedTime\":1559543073374,\n        \"trackingUI\":\"History\",\n        \"numNonAMContainerPreempted\":0,\n        \"amContainerLogs\":\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0003_01_000001\\/hadoop\",\n        \"allocatedVCores\":-1,\n        \"diagnostics\":\"Task failed task_1558964977321_0003_m_000004\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\",\n        \"name\":\"N\\/A\",\n        \"progress\":100,\n        \"finishedTime\":1559543111009,\n        \"user\":\"hadoop\",\n        \"queue\":\"default\",\n        \"elapsedTime\":37635\n      },\n      {\n        \"applicationType\":\"MAPREDUCE\",\n        \"finalStatus\":\"FAILED\",\n        \"trackingUrl\":\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0002\\/\",\n        \"runningContainers\":-1,\n        \"clusterId\":1558964977321,\n        \"vcoreSeconds\":206,\n        \"preemptedResourceVCores\":0,\n        \"numAMContainerPreempted\":0,\n        \"allocatedMB\":-1,\n        \"id\":\"application_1558964977321_0002\",\n        \"state\":\"FINISHED\",\n        \"amHostHttpAddress\":\"datanode2:8042\",\n        \"memorySeconds\":267663,\n        \"preemptedResourceMB\":0,\n        \"applicationTags\":\"\",\n        \"startedTime\":1559543001391,\n        \"trackingUI\":\"History\",\n        \"numNonAMContainerPreempted\":0,\n        \"amContainerLogs\":\"http:\\/\\/datanode2:8042\\/node\\/containerlogs\\/container_1558964977321_0002_01_000001\\/hadoop\",\n        \"allocatedVCores\":-1,\n        \"diagnostics\":\"Task failed task_1558964977321_0002_m_000004\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\",\n        \"name\":\"N\\/A\",\n        \"progress\":100,\n        \"finishedTime\":1559543043050,\n        \"user\":\"hadoop\",\n        \"queue\":\"default\",\n        \"elapsedTime\":41659\n      },\n      {\n        \"applicationType\":\"MAPREDUCE\",\n        \"finalStatus\":\"SUCCEEDED\",\n        \"trackingUrl\":\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0001\\/\",\n        \"runningContainers\":-1,\n        \"clusterId\":1558964977321,\n        \"vcoreSeconds\":411,\n        \"preemptedResourceVCores\":0,\n        \"numAMContainerPreempted\":0,\n        \"allocatedMB\":-1,\n        \"id\":\"application_1558964977321_0001\",\n        \"state\":\"FINISHED\",\n        \"amHostHttpAddress\":\"datanode1:8042\",\n        \"memorySeconds\":619025,\n        \"preemptedResourceMB\":0,\n        \"applicationTags\":\"\",\n        \"startedTime\":1559197897433,\n        \"trackingUI\":\"History\",\n        \"numNonAMContainerPreempted\":0,\n        \"amContainerLogs\":\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0001_01_000001\\/hadoop\",\n        \"allocatedVCores\":-1,\n        \"diagnostics\":\"\",\n        \"name\":\"componet_code-jar-with-dependencies.jar\",\n        \"progress\":100,\n        \"finishedTime\":1559197986948,\n        \"user\":\"hadoop\",\n        \"queue\":\"default\",\n        \"elapsedTime\":89515\n      }\n    ]\n  }\n}\n\n\n```\n\n对json对象分解后获得如下字段备用\n\n| 字段名 | 含义 |\n| :--- | :----: |\n| applicationType|计算任务类型(mr spark)|\n| finalStatus|最终状态|\n| runningContainers   | 正在运行的容器    |\n\n![](http://img.wqkenqing.ren/r7lvxr.png)\n","source":"_posts/技术/yarn/YarnState.md","raw":"# yarn相关参数提取\n\n## rest web api\nhttp://namenode:8088/ws/v1/cluster/apps\n\n获得json对象\n\n```json\n{\n  \"apps\":{\n    \"app\":[\n      {\n        \"applicationType\":\"MAPREDUCE\",\n        \"finalStatus\":\"SUCCEEDED\",\n        \"trackingUrl\":\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0007\\/\",\n        \"runningContainers\":-1,\n        \"clusterId\":1558964977321,\n        \"vcoreSeconds\":349,\n        \"preemptedResourceVCores\":0,\n        \"numAMContainerPreempted\":0,\n        \"allocatedMB\":-1,\n        \"id\":\"application_1558964977321_0007\",\n        \"state\":\"FINISHED\",\n        \"amHostHttpAddress\":\"datanode2:8042\",\n        \"memorySeconds\":538321,\n        \"preemptedResourceMB\":0,\n        \"applicationTags\":\"\",\n        \"startedTime\":1561963496299,\n        \"trackingUI\":\"History\",\n        \"numNonAMContainerPreempted\":0,\n        \"amContainerLogs\":\"http:\\/\\/datanode2:8042\\/node\\/containerlogs\\/container_1558964977321_0007_01_000001\\/hadoop\",\n        \"allocatedVCores\":-1,\n        \"diagnostics\":\"\",\n        \"name\":\"componet_code-jar-with-dependencies.jar\",\n        \"progress\":100,\n        \"finishedTime\":1561963577074,\n        \"user\":\"hadoop\",\n        \"queue\":\"default\",\n        \"elapsedTime\":80775\n      },\n      {\n        \"applicationType\":\"MAPREDUCE\",\n        \"finalStatus\":\"FAILED\",\n        \"trackingUrl\":\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0006\\/\",\n        \"runningContainers\":-1,\n        \"clusterId\":1558964977321,\n        \"vcoreSeconds\":197,\n        \"preemptedResourceVCores\":0,\n        \"numAMContainerPreempted\":0,\n        \"allocatedMB\":-1,\n        \"id\":\"application_1558964977321_0006\",\n        \"state\":\"FINISHED\",\n        \"amHostHttpAddress\":\"datanode1:8042\",\n        \"memorySeconds\":261004,\n        \"preemptedResourceMB\":0,\n        \"applicationTags\":\"\",\n        \"startedTime\":1559543218059,\n        \"trackingUI\":\"History\",\n        \"numNonAMContainerPreempted\":0,\n        \"amContainerLogs\":\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0006_01_000001\\/hadoop\",\n        \"allocatedVCores\":-1,\n        \"diagnostics\":\"Task failed task_1558964977321_0006_m_000000\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\",\n        \"name\":\"N\\/A\",\n        \"progress\":100,\n        \"finishedTime\":1559543260162,\n        \"user\":\"hadoop\",\n        \"queue\":\"default\",\n        \"elapsedTime\":42103\n      },\n      {\n        \"applicationType\":\"MAPREDUCE\",\n        \"finalStatus\":\"FAILED\",\n        \"trackingUrl\":\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0005\\/\",\n        \"runningContainers\":-1,\n        \"clusterId\":1558964977321,\n        \"vcoreSeconds\":181,\n        \"preemptedResourceVCores\":0,\n        \"numAMContainerPreempted\":0,\n        \"allocatedMB\":-1,\n        \"id\":\"application_1558964977321_0005\",\n        \"state\":\"FINISHED\",\n        \"amHostHttpAddress\":\"datanode1:8042\",\n        \"memorySeconds\":239683,\n        \"preemptedResourceMB\":0,\n        \"applicationTags\":\"\",\n        \"startedTime\":1559543157210,\n        \"trackingUI\":\"History\",\n        \"numNonAMContainerPreempted\":0,\n        \"amContainerLogs\":\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0005_01_000001\\/hadoop\",\n        \"allocatedVCores\":-1,\n        \"diagnostics\":\"Task failed task_1558964977321_0005_m_000004\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\",\n        \"name\":\"N\\/A\",\n        \"progress\":100,\n        \"finishedTime\":1559543203631,\n        \"user\":\"hadoop\",\n        \"queue\":\"default\",\n        \"elapsedTime\":46421\n      },\n      {\n        \"applicationType\":\"MAPREDUCE\",\n        \"finalStatus\":\"FAILED\",\n        \"trackingUrl\":\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0004\\/\",\n        \"runningContainers\":-1,\n        \"clusterId\":1558964977321,\n        \"vcoreSeconds\":206,\n        \"preemptedResourceVCores\":0,\n        \"numAMContainerPreempted\":0,\n        \"allocatedMB\":-1,\n        \"id\":\"application_1558964977321_0004\",\n        \"state\":\"FINISHED\",\n        \"amHostHttpAddress\":\"datanode2:8042\",\n        \"memorySeconds\":269329,\n        \"preemptedResourceMB\":0,\n        \"applicationTags\":\"\",\n        \"startedTime\":1559543112981,\n        \"trackingUI\":\"History\",\n        \"numNonAMContainerPreempted\":0,\n        \"amContainerLogs\":\"http:\\/\\/datanode2:8042\\/node\\/containerlogs\\/container_1558964977321_0004_01_000001\\/hadoop\",\n        \"allocatedVCores\":-1,\n        \"diagnostics\":\"Task failed task_1558964977321_0004_m_000000\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\",\n        \"name\":\"N\\/A\",\n        \"progress\":100,\n        \"finishedTime\":1559543159096,\n        \"user\":\"hadoop\",\n        \"queue\":\"default\",\n        \"elapsedTime\":46115\n      },\n      {\n        \"applicationType\":\"MAPREDUCE\",\n        \"finalStatus\":\"FAILED\",\n        \"trackingUrl\":\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0003\\/\",\n        \"runningContainers\":-1,\n        \"clusterId\":1558964977321,\n        \"vcoreSeconds\":172,\n        \"preemptedResourceVCores\":0,\n        \"numAMContainerPreempted\":0,\n        \"allocatedMB\":-1,\n        \"id\":\"application_1558964977321_0003\",\n        \"state\":\"FINISHED\",\n        \"amHostHttpAddress\":\"datanode1:8042\",\n        \"memorySeconds\":227366,\n        \"preemptedResourceMB\":0,\n        \"applicationTags\":\"\",\n        \"startedTime\":1559543073374,\n        \"trackingUI\":\"History\",\n        \"numNonAMContainerPreempted\":0,\n        \"amContainerLogs\":\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0003_01_000001\\/hadoop\",\n        \"allocatedVCores\":-1,\n        \"diagnostics\":\"Task failed task_1558964977321_0003_m_000004\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\",\n        \"name\":\"N\\/A\",\n        \"progress\":100,\n        \"finishedTime\":1559543111009,\n        \"user\":\"hadoop\",\n        \"queue\":\"default\",\n        \"elapsedTime\":37635\n      },\n      {\n        \"applicationType\":\"MAPREDUCE\",\n        \"finalStatus\":\"FAILED\",\n        \"trackingUrl\":\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0002\\/\",\n        \"runningContainers\":-1,\n        \"clusterId\":1558964977321,\n        \"vcoreSeconds\":206,\n        \"preemptedResourceVCores\":0,\n        \"numAMContainerPreempted\":0,\n        \"allocatedMB\":-1,\n        \"id\":\"application_1558964977321_0002\",\n        \"state\":\"FINISHED\",\n        \"amHostHttpAddress\":\"datanode2:8042\",\n        \"memorySeconds\":267663,\n        \"preemptedResourceMB\":0,\n        \"applicationTags\":\"\",\n        \"startedTime\":1559543001391,\n        \"trackingUI\":\"History\",\n        \"numNonAMContainerPreempted\":0,\n        \"amContainerLogs\":\"http:\\/\\/datanode2:8042\\/node\\/containerlogs\\/container_1558964977321_0002_01_000001\\/hadoop\",\n        \"allocatedVCores\":-1,\n        \"diagnostics\":\"Task failed task_1558964977321_0002_m_000004\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\",\n        \"name\":\"N\\/A\",\n        \"progress\":100,\n        \"finishedTime\":1559543043050,\n        \"user\":\"hadoop\",\n        \"queue\":\"default\",\n        \"elapsedTime\":41659\n      },\n      {\n        \"applicationType\":\"MAPREDUCE\",\n        \"finalStatus\":\"SUCCEEDED\",\n        \"trackingUrl\":\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0001\\/\",\n        \"runningContainers\":-1,\n        \"clusterId\":1558964977321,\n        \"vcoreSeconds\":411,\n        \"preemptedResourceVCores\":0,\n        \"numAMContainerPreempted\":0,\n        \"allocatedMB\":-1,\n        \"id\":\"application_1558964977321_0001\",\n        \"state\":\"FINISHED\",\n        \"amHostHttpAddress\":\"datanode1:8042\",\n        \"memorySeconds\":619025,\n        \"preemptedResourceMB\":0,\n        \"applicationTags\":\"\",\n        \"startedTime\":1559197897433,\n        \"trackingUI\":\"History\",\n        \"numNonAMContainerPreempted\":0,\n        \"amContainerLogs\":\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0001_01_000001\\/hadoop\",\n        \"allocatedVCores\":-1,\n        \"diagnostics\":\"\",\n        \"name\":\"componet_code-jar-with-dependencies.jar\",\n        \"progress\":100,\n        \"finishedTime\":1559197986948,\n        \"user\":\"hadoop\",\n        \"queue\":\"default\",\n        \"elapsedTime\":89515\n      }\n    ]\n  }\n}\n\n\n```\n\n对json对象分解后获得如下字段备用\n\n| 字段名 | 含义 |\n| :--- | :----: |\n| applicationType|计算任务类型(mr spark)|\n| finalStatus|最终状态|\n| runningContainers   | 正在运行的容器    |\n\n![](http://img.wqkenqing.ren/r7lvxr.png)\n","slug":"技术/yarn/YarnState","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/yarn/YarnState","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31r001238pwccmta66a","content":"<h1 id=\"yarn相关参数提取\"><a href=\"#yarn相关参数提取\" class=\"headerlink\" title=\"yarn相关参数提取\"></a>yarn相关参数提取</h1><h2 id=\"rest-web-api\"><a href=\"#rest-web-api\" class=\"headerlink\" title=\"rest web api\"></a>rest web api</h2><p><a href=\"http://namenode:8088/ws/v1/cluster/apps\" target=\"_blank\" rel=\"noopener\">http://namenode:8088/ws/v1/cluster/apps</a></p>\n<p>获得json对象</p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"attr\">\"apps\"</span>:&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"app\"</span>:[</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"applicationType\"</span>:<span class=\"string\">\"MAPREDUCE\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finalStatus\"</span>:<span class=\"string\">\"SUCCEEDED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUrl\"</span>:<span class=\"string\">\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0007\\/\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"runningContainers\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"clusterId\"</span>:<span class=\"number\">1558964977321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"vcoreSeconds\"</span>:<span class=\"number\">349</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceVCores\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedMB\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"id\"</span>:<span class=\"string\">\"application_1558964977321_0007\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"state\"</span>:<span class=\"string\">\"FINISHED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amHostHttpAddress\"</span>:<span class=\"string\">\"datanode2:8042\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"memorySeconds\"</span>:<span class=\"number\">538321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceMB\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"applicationTags\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"startedTime\"</span>:<span class=\"number\">1561963496299</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUI\"</span>:<span class=\"string\">\"History\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numNonAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amContainerLogs\"</span>:<span class=\"string\">\"http:\\/\\/datanode2:8042\\/node\\/containerlogs\\/container_1558964977321_0007_01_000001\\/hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedVCores\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"diagnostics\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>:<span class=\"string\">\"componet_code-jar-with-dependencies.jar\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"progress\"</span>:<span class=\"number\">100</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finishedTime\"</span>:<span class=\"number\">1561963577074</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"user\"</span>:<span class=\"string\">\"hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"queue\"</span>:<span class=\"string\">\"default\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"elapsedTime\"</span>:<span class=\"number\">80775</span></span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"applicationType\"</span>:<span class=\"string\">\"MAPREDUCE\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finalStatus\"</span>:<span class=\"string\">\"FAILED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUrl\"</span>:<span class=\"string\">\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0006\\/\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"runningContainers\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"clusterId\"</span>:<span class=\"number\">1558964977321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"vcoreSeconds\"</span>:<span class=\"number\">197</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceVCores\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedMB\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"id\"</span>:<span class=\"string\">\"application_1558964977321_0006\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"state\"</span>:<span class=\"string\">\"FINISHED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amHostHttpAddress\"</span>:<span class=\"string\">\"datanode1:8042\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"memorySeconds\"</span>:<span class=\"number\">261004</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceMB\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"applicationTags\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"startedTime\"</span>:<span class=\"number\">1559543218059</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUI\"</span>:<span class=\"string\">\"History\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numNonAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amContainerLogs\"</span>:<span class=\"string\">\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0006_01_000001\\/hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedVCores\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"diagnostics\"</span>:<span class=\"string\">\"Task failed task_1558964977321_0006_m_000000\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>:<span class=\"string\">\"N\\/A\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"progress\"</span>:<span class=\"number\">100</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finishedTime\"</span>:<span class=\"number\">1559543260162</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"user\"</span>:<span class=\"string\">\"hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"queue\"</span>:<span class=\"string\">\"default\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"elapsedTime\"</span>:<span class=\"number\">42103</span></span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"applicationType\"</span>:<span class=\"string\">\"MAPREDUCE\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finalStatus\"</span>:<span class=\"string\">\"FAILED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUrl\"</span>:<span class=\"string\">\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0005\\/\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"runningContainers\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"clusterId\"</span>:<span class=\"number\">1558964977321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"vcoreSeconds\"</span>:<span class=\"number\">181</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceVCores\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedMB\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"id\"</span>:<span class=\"string\">\"application_1558964977321_0005\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"state\"</span>:<span class=\"string\">\"FINISHED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amHostHttpAddress\"</span>:<span class=\"string\">\"datanode1:8042\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"memorySeconds\"</span>:<span class=\"number\">239683</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceMB\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"applicationTags\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"startedTime\"</span>:<span class=\"number\">1559543157210</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUI\"</span>:<span class=\"string\">\"History\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numNonAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amContainerLogs\"</span>:<span class=\"string\">\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0005_01_000001\\/hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedVCores\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"diagnostics\"</span>:<span class=\"string\">\"Task failed task_1558964977321_0005_m_000004\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>:<span class=\"string\">\"N\\/A\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"progress\"</span>:<span class=\"number\">100</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finishedTime\"</span>:<span class=\"number\">1559543203631</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"user\"</span>:<span class=\"string\">\"hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"queue\"</span>:<span class=\"string\">\"default\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"elapsedTime\"</span>:<span class=\"number\">46421</span></span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"applicationType\"</span>:<span class=\"string\">\"MAPREDUCE\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finalStatus\"</span>:<span class=\"string\">\"FAILED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUrl\"</span>:<span class=\"string\">\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0004\\/\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"runningContainers\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"clusterId\"</span>:<span class=\"number\">1558964977321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"vcoreSeconds\"</span>:<span class=\"number\">206</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceVCores\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedMB\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"id\"</span>:<span class=\"string\">\"application_1558964977321_0004\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"state\"</span>:<span class=\"string\">\"FINISHED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amHostHttpAddress\"</span>:<span class=\"string\">\"datanode2:8042\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"memorySeconds\"</span>:<span class=\"number\">269329</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceMB\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"applicationTags\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"startedTime\"</span>:<span class=\"number\">1559543112981</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUI\"</span>:<span class=\"string\">\"History\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numNonAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amContainerLogs\"</span>:<span class=\"string\">\"http:\\/\\/datanode2:8042\\/node\\/containerlogs\\/container_1558964977321_0004_01_000001\\/hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedVCores\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"diagnostics\"</span>:<span class=\"string\">\"Task failed task_1558964977321_0004_m_000000\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>:<span class=\"string\">\"N\\/A\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"progress\"</span>:<span class=\"number\">100</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finishedTime\"</span>:<span class=\"number\">1559543159096</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"user\"</span>:<span class=\"string\">\"hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"queue\"</span>:<span class=\"string\">\"default\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"elapsedTime\"</span>:<span class=\"number\">46115</span></span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"applicationType\"</span>:<span class=\"string\">\"MAPREDUCE\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finalStatus\"</span>:<span class=\"string\">\"FAILED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUrl\"</span>:<span class=\"string\">\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0003\\/\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"runningContainers\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"clusterId\"</span>:<span class=\"number\">1558964977321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"vcoreSeconds\"</span>:<span class=\"number\">172</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceVCores\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedMB\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"id\"</span>:<span class=\"string\">\"application_1558964977321_0003\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"state\"</span>:<span class=\"string\">\"FINISHED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amHostHttpAddress\"</span>:<span class=\"string\">\"datanode1:8042\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"memorySeconds\"</span>:<span class=\"number\">227366</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceMB\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"applicationTags\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"startedTime\"</span>:<span class=\"number\">1559543073374</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUI\"</span>:<span class=\"string\">\"History\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numNonAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amContainerLogs\"</span>:<span class=\"string\">\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0003_01_000001\\/hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedVCores\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"diagnostics\"</span>:<span class=\"string\">\"Task failed task_1558964977321_0003_m_000004\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>:<span class=\"string\">\"N\\/A\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"progress\"</span>:<span class=\"number\">100</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finishedTime\"</span>:<span class=\"number\">1559543111009</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"user\"</span>:<span class=\"string\">\"hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"queue\"</span>:<span class=\"string\">\"default\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"elapsedTime\"</span>:<span class=\"number\">37635</span></span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"applicationType\"</span>:<span class=\"string\">\"MAPREDUCE\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finalStatus\"</span>:<span class=\"string\">\"FAILED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUrl\"</span>:<span class=\"string\">\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0002\\/\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"runningContainers\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"clusterId\"</span>:<span class=\"number\">1558964977321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"vcoreSeconds\"</span>:<span class=\"number\">206</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceVCores\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedMB\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"id\"</span>:<span class=\"string\">\"application_1558964977321_0002\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"state\"</span>:<span class=\"string\">\"FINISHED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amHostHttpAddress\"</span>:<span class=\"string\">\"datanode2:8042\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"memorySeconds\"</span>:<span class=\"number\">267663</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceMB\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"applicationTags\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"startedTime\"</span>:<span class=\"number\">1559543001391</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUI\"</span>:<span class=\"string\">\"History\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numNonAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amContainerLogs\"</span>:<span class=\"string\">\"http:\\/\\/datanode2:8042\\/node\\/containerlogs\\/container_1558964977321_0002_01_000001\\/hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedVCores\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"diagnostics\"</span>:<span class=\"string\">\"Task failed task_1558964977321_0002_m_000004\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>:<span class=\"string\">\"N\\/A\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"progress\"</span>:<span class=\"number\">100</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finishedTime\"</span>:<span class=\"number\">1559543043050</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"user\"</span>:<span class=\"string\">\"hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"queue\"</span>:<span class=\"string\">\"default\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"elapsedTime\"</span>:<span class=\"number\">41659</span></span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"applicationType\"</span>:<span class=\"string\">\"MAPREDUCE\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finalStatus\"</span>:<span class=\"string\">\"SUCCEEDED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUrl\"</span>:<span class=\"string\">\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0001\\/\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"runningContainers\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"clusterId\"</span>:<span class=\"number\">1558964977321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"vcoreSeconds\"</span>:<span class=\"number\">411</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceVCores\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedMB\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"id\"</span>:<span class=\"string\">\"application_1558964977321_0001\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"state\"</span>:<span class=\"string\">\"FINISHED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amHostHttpAddress\"</span>:<span class=\"string\">\"datanode1:8042\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"memorySeconds\"</span>:<span class=\"number\">619025</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceMB\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"applicationTags\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"startedTime\"</span>:<span class=\"number\">1559197897433</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUI\"</span>:<span class=\"string\">\"History\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numNonAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amContainerLogs\"</span>:<span class=\"string\">\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0001_01_000001\\/hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedVCores\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"diagnostics\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>:<span class=\"string\">\"componet_code-jar-with-dependencies.jar\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"progress\"</span>:<span class=\"number\">100</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finishedTime\"</span>:<span class=\"number\">1559197986948</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"user\"</span>:<span class=\"string\">\"hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"queue\"</span>:<span class=\"string\">\"default\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"elapsedTime\"</span>:<span class=\"number\">89515</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>对json对象分解后获得如下字段备用</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">字段名</th>\n<th align=\"center\">含义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">applicationType</td>\n<td align=\"center\">计算任务类型(mr spark)</td>\n</tr>\n<tr>\n<td align=\"left\">finalStatus</td>\n<td align=\"center\">最终状态</td>\n</tr>\n<tr>\n<td align=\"left\">runningContainers</td>\n<td align=\"center\">正在运行的容器</td>\n</tr>\n</tbody></table>\n<p><img src=\"http://img.wqkenqing.ren/r7lvxr.png\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"yarn相关参数提取\"><a href=\"#yarn相关参数提取\" class=\"headerlink\" title=\"yarn相关参数提取\"></a>yarn相关参数提取</h1><h2 id=\"rest-web-api\"><a href=\"#rest-web-api\" class=\"headerlink\" title=\"rest web api\"></a>rest web api</h2><p><a href=\"http://namenode:8088/ws/v1/cluster/apps\" target=\"_blank\" rel=\"noopener\">http://namenode:8088/ws/v1/cluster/apps</a></p>\n<p>获得json对象</p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"attr\">\"apps\"</span>:&#123;</span><br><span class=\"line\">    <span class=\"attr\">\"app\"</span>:[</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"applicationType\"</span>:<span class=\"string\">\"MAPREDUCE\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finalStatus\"</span>:<span class=\"string\">\"SUCCEEDED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUrl\"</span>:<span class=\"string\">\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0007\\/\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"runningContainers\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"clusterId\"</span>:<span class=\"number\">1558964977321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"vcoreSeconds\"</span>:<span class=\"number\">349</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceVCores\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedMB\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"id\"</span>:<span class=\"string\">\"application_1558964977321_0007\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"state\"</span>:<span class=\"string\">\"FINISHED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amHostHttpAddress\"</span>:<span class=\"string\">\"datanode2:8042\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"memorySeconds\"</span>:<span class=\"number\">538321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceMB\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"applicationTags\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"startedTime\"</span>:<span class=\"number\">1561963496299</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUI\"</span>:<span class=\"string\">\"History\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numNonAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amContainerLogs\"</span>:<span class=\"string\">\"http:\\/\\/datanode2:8042\\/node\\/containerlogs\\/container_1558964977321_0007_01_000001\\/hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedVCores\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"diagnostics\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>:<span class=\"string\">\"componet_code-jar-with-dependencies.jar\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"progress\"</span>:<span class=\"number\">100</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finishedTime\"</span>:<span class=\"number\">1561963577074</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"user\"</span>:<span class=\"string\">\"hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"queue\"</span>:<span class=\"string\">\"default\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"elapsedTime\"</span>:<span class=\"number\">80775</span></span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"applicationType\"</span>:<span class=\"string\">\"MAPREDUCE\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finalStatus\"</span>:<span class=\"string\">\"FAILED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUrl\"</span>:<span class=\"string\">\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0006\\/\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"runningContainers\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"clusterId\"</span>:<span class=\"number\">1558964977321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"vcoreSeconds\"</span>:<span class=\"number\">197</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceVCores\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedMB\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"id\"</span>:<span class=\"string\">\"application_1558964977321_0006\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"state\"</span>:<span class=\"string\">\"FINISHED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amHostHttpAddress\"</span>:<span class=\"string\">\"datanode1:8042\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"memorySeconds\"</span>:<span class=\"number\">261004</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceMB\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"applicationTags\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"startedTime\"</span>:<span class=\"number\">1559543218059</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUI\"</span>:<span class=\"string\">\"History\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numNonAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amContainerLogs\"</span>:<span class=\"string\">\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0006_01_000001\\/hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedVCores\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"diagnostics\"</span>:<span class=\"string\">\"Task failed task_1558964977321_0006_m_000000\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>:<span class=\"string\">\"N\\/A\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"progress\"</span>:<span class=\"number\">100</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finishedTime\"</span>:<span class=\"number\">1559543260162</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"user\"</span>:<span class=\"string\">\"hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"queue\"</span>:<span class=\"string\">\"default\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"elapsedTime\"</span>:<span class=\"number\">42103</span></span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"applicationType\"</span>:<span class=\"string\">\"MAPREDUCE\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finalStatus\"</span>:<span class=\"string\">\"FAILED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUrl\"</span>:<span class=\"string\">\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0005\\/\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"runningContainers\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"clusterId\"</span>:<span class=\"number\">1558964977321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"vcoreSeconds\"</span>:<span class=\"number\">181</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceVCores\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedMB\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"id\"</span>:<span class=\"string\">\"application_1558964977321_0005\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"state\"</span>:<span class=\"string\">\"FINISHED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amHostHttpAddress\"</span>:<span class=\"string\">\"datanode1:8042\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"memorySeconds\"</span>:<span class=\"number\">239683</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceMB\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"applicationTags\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"startedTime\"</span>:<span class=\"number\">1559543157210</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUI\"</span>:<span class=\"string\">\"History\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numNonAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amContainerLogs\"</span>:<span class=\"string\">\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0005_01_000001\\/hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedVCores\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"diagnostics\"</span>:<span class=\"string\">\"Task failed task_1558964977321_0005_m_000004\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>:<span class=\"string\">\"N\\/A\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"progress\"</span>:<span class=\"number\">100</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finishedTime\"</span>:<span class=\"number\">1559543203631</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"user\"</span>:<span class=\"string\">\"hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"queue\"</span>:<span class=\"string\">\"default\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"elapsedTime\"</span>:<span class=\"number\">46421</span></span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"applicationType\"</span>:<span class=\"string\">\"MAPREDUCE\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finalStatus\"</span>:<span class=\"string\">\"FAILED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUrl\"</span>:<span class=\"string\">\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0004\\/\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"runningContainers\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"clusterId\"</span>:<span class=\"number\">1558964977321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"vcoreSeconds\"</span>:<span class=\"number\">206</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceVCores\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedMB\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"id\"</span>:<span class=\"string\">\"application_1558964977321_0004\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"state\"</span>:<span class=\"string\">\"FINISHED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amHostHttpAddress\"</span>:<span class=\"string\">\"datanode2:8042\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"memorySeconds\"</span>:<span class=\"number\">269329</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceMB\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"applicationTags\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"startedTime\"</span>:<span class=\"number\">1559543112981</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUI\"</span>:<span class=\"string\">\"History\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numNonAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amContainerLogs\"</span>:<span class=\"string\">\"http:\\/\\/datanode2:8042\\/node\\/containerlogs\\/container_1558964977321_0004_01_000001\\/hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedVCores\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"diagnostics\"</span>:<span class=\"string\">\"Task failed task_1558964977321_0004_m_000000\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>:<span class=\"string\">\"N\\/A\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"progress\"</span>:<span class=\"number\">100</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finishedTime\"</span>:<span class=\"number\">1559543159096</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"user\"</span>:<span class=\"string\">\"hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"queue\"</span>:<span class=\"string\">\"default\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"elapsedTime\"</span>:<span class=\"number\">46115</span></span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"applicationType\"</span>:<span class=\"string\">\"MAPREDUCE\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finalStatus\"</span>:<span class=\"string\">\"FAILED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUrl\"</span>:<span class=\"string\">\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0003\\/\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"runningContainers\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"clusterId\"</span>:<span class=\"number\">1558964977321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"vcoreSeconds\"</span>:<span class=\"number\">172</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceVCores\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedMB\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"id\"</span>:<span class=\"string\">\"application_1558964977321_0003\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"state\"</span>:<span class=\"string\">\"FINISHED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amHostHttpAddress\"</span>:<span class=\"string\">\"datanode1:8042\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"memorySeconds\"</span>:<span class=\"number\">227366</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceMB\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"applicationTags\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"startedTime\"</span>:<span class=\"number\">1559543073374</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUI\"</span>:<span class=\"string\">\"History\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numNonAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amContainerLogs\"</span>:<span class=\"string\">\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0003_01_000001\\/hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedVCores\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"diagnostics\"</span>:<span class=\"string\">\"Task failed task_1558964977321_0003_m_000004\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>:<span class=\"string\">\"N\\/A\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"progress\"</span>:<span class=\"number\">100</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finishedTime\"</span>:<span class=\"number\">1559543111009</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"user\"</span>:<span class=\"string\">\"hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"queue\"</span>:<span class=\"string\">\"default\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"elapsedTime\"</span>:<span class=\"number\">37635</span></span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"applicationType\"</span>:<span class=\"string\">\"MAPREDUCE\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finalStatus\"</span>:<span class=\"string\">\"FAILED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUrl\"</span>:<span class=\"string\">\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0002\\/\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"runningContainers\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"clusterId\"</span>:<span class=\"number\">1558964977321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"vcoreSeconds\"</span>:<span class=\"number\">206</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceVCores\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedMB\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"id\"</span>:<span class=\"string\">\"application_1558964977321_0002\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"state\"</span>:<span class=\"string\">\"FINISHED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amHostHttpAddress\"</span>:<span class=\"string\">\"datanode2:8042\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"memorySeconds\"</span>:<span class=\"number\">267663</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceMB\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"applicationTags\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"startedTime\"</span>:<span class=\"number\">1559543001391</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUI\"</span>:<span class=\"string\">\"History\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numNonAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amContainerLogs\"</span>:<span class=\"string\">\"http:\\/\\/datanode2:8042\\/node\\/containerlogs\\/container_1558964977321_0002_01_000001\\/hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedVCores\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"diagnostics\"</span>:<span class=\"string\">\"Task failed task_1558964977321_0002_m_000004\\nJob failed as tasks failed. failedMaps:1 failedReduces:0\\n\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>:<span class=\"string\">\"N\\/A\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"progress\"</span>:<span class=\"number\">100</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finishedTime\"</span>:<span class=\"number\">1559543043050</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"user\"</span>:<span class=\"string\">\"hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"queue\"</span>:<span class=\"string\">\"default\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"elapsedTime\"</span>:<span class=\"number\">41659</span></span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &#123;</span><br><span class=\"line\">        <span class=\"attr\">\"applicationType\"</span>:<span class=\"string\">\"MAPREDUCE\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finalStatus\"</span>:<span class=\"string\">\"SUCCEEDED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUrl\"</span>:<span class=\"string\">\"http:\\/\\/namenode:8088\\/proxy\\/application_1558964977321_0001\\/\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"runningContainers\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"clusterId\"</span>:<span class=\"number\">1558964977321</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"vcoreSeconds\"</span>:<span class=\"number\">411</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceVCores\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedMB\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"id\"</span>:<span class=\"string\">\"application_1558964977321_0001\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"state\"</span>:<span class=\"string\">\"FINISHED\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amHostHttpAddress\"</span>:<span class=\"string\">\"datanode1:8042\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"memorySeconds\"</span>:<span class=\"number\">619025</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"preemptedResourceMB\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"applicationTags\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"startedTime\"</span>:<span class=\"number\">1559197897433</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"trackingUI\"</span>:<span class=\"string\">\"History\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"numNonAMContainerPreempted\"</span>:<span class=\"number\">0</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"amContainerLogs\"</span>:<span class=\"string\">\"http:\\/\\/datanode1:8042\\/node\\/containerlogs\\/container_1558964977321_0001_01_000001\\/hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"allocatedVCores\"</span>:<span class=\"number\">-1</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"diagnostics\"</span>:<span class=\"string\">\"\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"name\"</span>:<span class=\"string\">\"componet_code-jar-with-dependencies.jar\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"progress\"</span>:<span class=\"number\">100</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"finishedTime\"</span>:<span class=\"number\">1559197986948</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"user\"</span>:<span class=\"string\">\"hadoop\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"queue\"</span>:<span class=\"string\">\"default\"</span>,</span><br><span class=\"line\">        <span class=\"attr\">\"elapsedTime\"</span>:<span class=\"number\">89515</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>对json对象分解后获得如下字段备用</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">字段名</th>\n<th align=\"center\">含义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">applicationType</td>\n<td align=\"center\">计算任务类型(mr spark)</td>\n</tr>\n<tr>\n<td align=\"left\">finalStatus</td>\n<td align=\"center\">最终状态</td>\n</tr>\n<tr>\n<td align=\"left\">runningContainers</td>\n<td align=\"center\">正在运行的容器</td>\n</tr>\n</tbody></table>\n<p><img src=\"http://img.wqkenqing.ren/r7lvxr.png\" alt=\"\"></p>\n"},{"_content":"记录移动云服务器上部署的服务\n\n| 服务名 | 说明     |\n| :------------- | :------------- |\n| sjksh_store_info | 数据可视化模块的存储信息查询服务  |\n","source":"_posts/技术/yd_server_config/README.md","raw":"记录移动云服务器上部署的服务\n\n| 服务名 | 说明     |\n| :------------- | :------------- |\n| sjksh_store_info | 数据可视化模块的存储信息查询服务  |\n","slug":"技术/yd_server_config/README","published":1,"date":"2021-01-05T02:35:38.000Z","updated":"2021-01-05T02:35:38.000Z","title":"技术/yd_server_config/README","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31r001338pw4uzi632o","content":"<p>记录移动云服务器上部署的服务</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">服务名</th>\n<th align=\"left\">说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">sjksh_store_info</td>\n<td align=\"left\">数据可视化模块的存储信息查询服务</td>\n</tr>\n</tbody></table>\n","site":{"data":{}},"excerpt":"","more":"<p>记录移动云服务器上部署的服务</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">服务名</th>\n<th align=\"left\">说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">sjksh_store_info</td>\n<td align=\"left\">数据可视化模块的存储信息查询服务</td>\n</tr>\n</tbody></table>\n"},{"_content":"\nabout two git project\n\none hexo .\nthis project is used to deploy the files;\n\ntwo : dayliy_doc\n\n\nso I just divide the project about these parts\n\n1. pull files from github \n\n2. list the files;\n\n3. choose the files to sync. \n\n4. push and deploy;\n\n","source":"_posts/日常/blog_sync_project/plan.md","raw":"\nabout two git project\n\none hexo .\nthis project is used to deploy the files;\n\ntwo : dayliy_doc\n\n\nso I just divide the project about these parts\n\n1. pull files from github \n\n2. list the files;\n\n3. choose the files to sync. \n\n4. push and deploy;\n\n","slug":"日常/blog_sync_project/plan","published":1,"date":"2021-01-05T02:35:38.000Z","updated":"2021-01-05T02:35:38.000Z","title":"日常/blog_sync_project/plan","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31s001438pwfdwu3ss3","content":"<p>about two git project</p>\n<p>one hexo .<br>this project is used to deploy the files;</p>\n<p>two : dayliy_doc</p>\n<p>so I just divide the project about these parts</p>\n<ol>\n<li><p>pull files from github </p>\n</li>\n<li><p>list the files;</p>\n</li>\n<li><p>choose the files to sync. </p>\n</li>\n<li><p>push and deploy;</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>about two git project</p>\n<p>one hexo .<br>this project is used to deploy the files;</p>\n<p>two : dayliy_doc</p>\n<p>so I just divide the project about these parts</p>\n<ol>\n<li><p>pull files from github </p>\n</li>\n<li><p>list the files;</p>\n</li>\n<li><p>choose the files to sync. </p>\n</li>\n<li><p>push and deploy;</p>\n</li>\n</ol>\n"},{"_content":"account: wqkenqing\npassword:125323wkQ$\n","source":"_posts/日常/运维/about云账号密码.md","raw":"account: wqkenqing\npassword:125323wkQ$\n","slug":"日常/运维/about云账号密码","published":1,"date":"2021-01-05T02:35:38.000Z","updated":"2021-01-05T02:35:38.000Z","title":"日常/运维/about云账号密码","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31s001538pwhwlw368i","content":"<p>account: wqkenqing<br>password:125323wkQ$</p>\n","site":{"data":{}},"excerpt":"","more":"<p>account: wqkenqing<br>password:125323wkQ$</p>\n"},{"_content":"9 @(私有集群的搭建)\n###私有集群搭建\n一、服务器的准备\n* 阿里云实例（115.29.97.126）\n* 华为云实例（114.115.203.81）\n* 阿里云实例2(115.29.35.125）\n* 本机实例（）\n1.1）服务器环境设置\n修改服务器hostname\n* 阿里（wqkenqing）\n* 华为（wqkenqing02）\n* 阿里2(wqkenqing03)\n* 虚拟机（wqkneqing04）\n[hostname修改教程](http://blog.csdn.net/huangxy10/article/details/40213095)\n***\n1.2）配置语言环境（取的默认环境）\n![Alt text](./1475308903359.png)\n1.3）[关闭防火墙](https://my.oschina.net/cjun/blog/344836)\n1.4）创建hadoop用户\n1.5）上传相关文件(jdk1.7、hadoop2.7.2)\n***\n1.6）配置环境变量\n**\nexport JAVA_HOME=/usr/local/jdk1.7\nexport JRE_HOME=${JAVA_HOME}/jre\nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\n\nexport PATH=${JAVA_HOME}/bin:$PATH **\n***\n1.7）配置免密码登陆\n\t1)实现本机ssh免秘钥登录：\n* $ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa\n* $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys\n* chmod 700 ~/.ssh/authorized_keys\n***\n2)实现集群ssh免秘钥登录\n+ 写入每一个集群机器秘钥到主NN 的authorized_keys中 ssh hadoopXXX 'cat /home/hadoop/.ssh/id_dsa.pub' >> ~/.ssh/authorized_keys\n+  覆盖所有集群机器authorized_keys：scp ~/.ssh/authorized_keys hadoopXXX:/home/hadoop/.ssh/authorized_keys\n\n二、hadoop配置\n2.1）配置hadoop环境\nexport HADOOP_HOME=/usr/local/hadoop/hadoop\n\nexport HIVE_HOME=/usr/local/hadoop/hive\n\nexport HBASE_HOME=/usr/local/hadoop/hbase\n\nPATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HBASE_HOME/bin:$HIVE_HOME/bin\n***\nhadoop配置文件配置\n2.1）wqkenqing节点（以后作为name节点）\n**修改 /home/hadoop/etc/hadoop/ core-site.xml**\n在 <configuration> 块儿中添加：\n\n<property>\n\n<name>fs.defaultFS</name>\n\n<value>hdfs://wqkenqing.com:9000</value>\n\n</property>\n\n<property>\n\n<name>hadoop.tmp.dir</name>\n\n<value>file:/home/hadoop/tmp</value>\n\n</property>\n\n<property>\n\n<name>io.file.buffer.size</name>\n\n<value>131702</value>\n\n<!-- 指定zookeeper地址 -->  \n    <property>  \n        <name>ha.zookeeper.quorum</name>  \n        <value>wqkenqing.com:2181,wqkenqing02.com:2181</value>  \n    </property>  \n</property>\n***\n**修改 /home/hadoop/etc/hadoop/ hdfs-site.xml**\n在 <configuration> 块儿中添加：\n\n<property>\n\n<name>dfs.namenode.name.dir</name>\n\n<value>file:/home/hadoop/hdfs/name</value>\n\n</property>\n\n<property>\n\n<name>dfs.datanode.data.dir</name>\n\n<value>file:/home/hadoop/hdfs/data</value>\n\n</property>\n\n<property>\n\n<name>dfs.replication</name>\n\n<value>2</value>\n\n</property>\n\n<property>\n\n<name>dfs.namenode.secondary.http-address</name>\n\n<value>wqkenqing.com:9001</value>\n\n</property>\n\n<property>\n\n<name>dfs.webhdfs.enabled</name>\n\n<value>true</value>\n</property>\n***\n <!--指定hdfs的nameservice为mycluster，需要和core-site.xml中的保持一致 -->  \n    <property>  \n        <name>dfs.nameservices</name>  \n        <value>wqkenqing.com</value>  \n    </property>  \n\n    <!-- mycluster下面有两个NameNode，分别是nn1，nn2 -->  \n    <property>  \n     <name>dfs.ha.namenodes.mycluster</name>  \n        <value>wqkenqing,wqkenqing02</value>  \n    </property>  \n\n    <!-- nn1的RPC通信地址 -->  \n    <property>  \n        <name>dfs.namenode.rpc-address.mycluster.nn1</name>  \n        <value>wqkenqing:9000</value>  \n    </property>  \n\n    <!-- nn1的http通信地址 -->  \n    <property>  \n        <name>dfs.namenode.http-address.mycluster.nn1</name>  \n        <value>h1m1:50070</value>  \n    </property>  \n\n    <!-- nn2的RPC通信地址 -->  \n    <property>  \n        <name>dfs.namenode.rpc-address.mycluster.nn2</name>  \n        <value>wqkenqing02:9000</value>  \n    </property>  \n\n    <!-- nn2的http通信地址 -->  \n    <property>  \n        <name>dfs.namenode.http-address.mycluster.nn2</name>  \n        <value>wqkenqing02:50070</value>  \n    </property>  \n\n    <!-- 指定NameNode的元数据在JournalNode上的存放位置 -->  \n    <property>  \n        <name>dfs.namenode.shared.edits.dir</name>  \n        <value>qjournal://wqkenqing:8485;wqkenqing02:8485/mycluster</value>  \n    </property>  \n\n    <!-- 指定JournalNode在本地磁盘存放数据的位置 -->  \n    <property>  \n        <name>dfs.journalnode.edits.dir</name>  \n        <value>/usr/lib/hadoop/journal</value>  \n    </property>  \n\n    <!-- 开启NameNode失败自动切换 -->  \n    <property>  \n        <name>dfs.ha.automatic-failover.enabled</name>  \n        <value>true</value>  \n    </property>  \n\n    <!-- 配置失败自动切换实现方式 -->  \n    <property>  \n        <name>dfs.client.failover.proxy.provider.mycluster</name>  \n        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>  \n    </property>  \n\n    <!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行-->  \n    <property>  \n        <name>dfs.ha.fencing.methods</name>  \n        <value>  \n            sshfence  \n            shell(/bin/true)  \n        </value>  \n    </property>  \n\n    <!-- 使用sshfence隔离机制时需要ssh免登陆 -->  \n    <property>  \n        <name>dfs.ha.fencing.ssh.private-key-files</name>  \n        <value>/home/hadoop/.ssh/id_rsa</value>  \n    </property>  \n\n    <!-- 配置sshfence隔离机制超时时间 -->  \n    <property>  \n        <name>dfs.ha.fencing.ssh.connect-timeout</name>  \n        <value>30000</value>  \n    </property>\n***\n**修改 /home/hadoop/etc/hadoop/ mapred-site.xml**\n这个文件默认不存在，需要从 mapred-site.xml.template 复制过来\n在 <configuration> 块儿中添加：\n\n<property>\n\n<name>mapreduce.framework.name</name>\n\n<value>yarn</value>\n\n</property>\n\n<property>\n\n<name>mapreduce.jobhistory.address</name>\n\n<value>wqkenqing:10020</value>\n\n</property>\n\n<property>\n\n<name>mapreduce.jobhistory.webapp.address</name>\n\n<value>wqkenqing:19888</value>\n\n</property>\n***\n**修改 /home/hadoop/etc/hadoop/ yarn-site.xml**\n在 <configuration> 块儿中添加：\n\n<property>\n\n<name>yarn.nodemanager.aux-services</name>\n\n<value>mapreduce_shuffle</value>\n\n</property>\n\n<property>\n\n<name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>\n\n<value>org.apache.hadoop.mapred.ShuffleHandler</value>\n\n</property>\n\n<property>\n\n<name>yarn.resourcemanager.address</name>\n\n<value>wqkenqing:8032</value>\n\n</property>\n\n<property>\n\n<name>yarn.resourcemanager.scheduler.address</name>\n\n<value>wqkenqing:8030</value>\n\n</property>\n\n<property>\n\n<name>yarn.resourcemanager.resource-tracker.address</name>\n\n<value>wqkenqing:8031</value>\n\n</property>\n\n<property>\n\n<name>yarn.resourcemanager.admin.address</name>\n\n<value>wqkenqing:8033</value>\n\n</property>\n\n<property>\n\n<name>yarn.resourcemanager.webapp.address</name>\n\n<value>wqkenqing:8088</value>\n\n</property>\n***\n**修改 /home/hadoop/etc/hadoop/ slaves**\n\n删除已有内容，添加：\nwqkenqing02（data节点）\n\n***\nhttp://115.29.97.126:50070/\n***\n**启动 hadoop**\n在master启动hadoop，从节点会自动启动\n**初始化**\n\thadoop-daemon.sh start journalnode\n$ hdfs namenode -format\n$ hadoop-daemon.sh start namenode\n$ hadoop-daemon.sh start datanode\n$ start-dfs.sh\n$ start-yarn.sh\n***\n启动报错\n**java.io.IOException: All specified directories are failed to load.**\n解决方式：配置文件中的地址书写有问题\n***\n至此hadoop集群中的关键功能已经实现\n* hdfs\n* mapreudce\n[集群配置参考文献](http://www.tuicool.com/articles/uAnyEfj)\n***\nhadoop220集群运行状态：\n![Alt text](./1475419804382.png)\n\n[同步时钟参考文献](http://f.dataguru.cn/thread-475940-1-1.html)\n###hbase集成(尚未完成)\n* [hadoop集群搭建参考文献](http://www.linuxidc.com/Linux/2013-06/86347p4.htm)\n*\n\n***\n###hbase集群启动指令\n* start-hbase.sh\n* [hbase中的zookeeper使用教程](http://blog.csdn.net/korder/article/details/47403801)\n\n###集群重置\n* hadoop配置完成(ali1+ali2)\n* 添加hbase\n","source":"_posts/技术/集群/私有集群.md","raw":"9 @(私有集群的搭建)\n###私有集群搭建\n一、服务器的准备\n* 阿里云实例（115.29.97.126）\n* 华为云实例（114.115.203.81）\n* 阿里云实例2(115.29.35.125）\n* 本机实例（）\n1.1）服务器环境设置\n修改服务器hostname\n* 阿里（wqkenqing）\n* 华为（wqkenqing02）\n* 阿里2(wqkenqing03)\n* 虚拟机（wqkneqing04）\n[hostname修改教程](http://blog.csdn.net/huangxy10/article/details/40213095)\n***\n1.2）配置语言环境（取的默认环境）\n![Alt text](./1475308903359.png)\n1.3）[关闭防火墙](https://my.oschina.net/cjun/blog/344836)\n1.4）创建hadoop用户\n1.5）上传相关文件(jdk1.7、hadoop2.7.2)\n***\n1.6）配置环境变量\n**\nexport JAVA_HOME=/usr/local/jdk1.7\nexport JRE_HOME=${JAVA_HOME}/jre\nexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib\n\nexport PATH=${JAVA_HOME}/bin:$PATH **\n***\n1.7）配置免密码登陆\n\t1)实现本机ssh免秘钥登录：\n* $ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa\n* $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys\n* chmod 700 ~/.ssh/authorized_keys\n***\n2)实现集群ssh免秘钥登录\n+ 写入每一个集群机器秘钥到主NN 的authorized_keys中 ssh hadoopXXX 'cat /home/hadoop/.ssh/id_dsa.pub' >> ~/.ssh/authorized_keys\n+  覆盖所有集群机器authorized_keys：scp ~/.ssh/authorized_keys hadoopXXX:/home/hadoop/.ssh/authorized_keys\n\n二、hadoop配置\n2.1）配置hadoop环境\nexport HADOOP_HOME=/usr/local/hadoop/hadoop\n\nexport HIVE_HOME=/usr/local/hadoop/hive\n\nexport HBASE_HOME=/usr/local/hadoop/hbase\n\nPATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HBASE_HOME/bin:$HIVE_HOME/bin\n***\nhadoop配置文件配置\n2.1）wqkenqing节点（以后作为name节点）\n**修改 /home/hadoop/etc/hadoop/ core-site.xml**\n在 <configuration> 块儿中添加：\n\n<property>\n\n<name>fs.defaultFS</name>\n\n<value>hdfs://wqkenqing.com:9000</value>\n\n</property>\n\n<property>\n\n<name>hadoop.tmp.dir</name>\n\n<value>file:/home/hadoop/tmp</value>\n\n</property>\n\n<property>\n\n<name>io.file.buffer.size</name>\n\n<value>131702</value>\n\n<!-- 指定zookeeper地址 -->  \n    <property>  \n        <name>ha.zookeeper.quorum</name>  \n        <value>wqkenqing.com:2181,wqkenqing02.com:2181</value>  \n    </property>  \n</property>\n***\n**修改 /home/hadoop/etc/hadoop/ hdfs-site.xml**\n在 <configuration> 块儿中添加：\n\n<property>\n\n<name>dfs.namenode.name.dir</name>\n\n<value>file:/home/hadoop/hdfs/name</value>\n\n</property>\n\n<property>\n\n<name>dfs.datanode.data.dir</name>\n\n<value>file:/home/hadoop/hdfs/data</value>\n\n</property>\n\n<property>\n\n<name>dfs.replication</name>\n\n<value>2</value>\n\n</property>\n\n<property>\n\n<name>dfs.namenode.secondary.http-address</name>\n\n<value>wqkenqing.com:9001</value>\n\n</property>\n\n<property>\n\n<name>dfs.webhdfs.enabled</name>\n\n<value>true</value>\n</property>\n***\n <!--指定hdfs的nameservice为mycluster，需要和core-site.xml中的保持一致 -->  \n    <property>  \n        <name>dfs.nameservices</name>  \n        <value>wqkenqing.com</value>  \n    </property>  \n\n    <!-- mycluster下面有两个NameNode，分别是nn1，nn2 -->  \n    <property>  \n     <name>dfs.ha.namenodes.mycluster</name>  \n        <value>wqkenqing,wqkenqing02</value>  \n    </property>  \n\n    <!-- nn1的RPC通信地址 -->  \n    <property>  \n        <name>dfs.namenode.rpc-address.mycluster.nn1</name>  \n        <value>wqkenqing:9000</value>  \n    </property>  \n\n    <!-- nn1的http通信地址 -->  \n    <property>  \n        <name>dfs.namenode.http-address.mycluster.nn1</name>  \n        <value>h1m1:50070</value>  \n    </property>  \n\n    <!-- nn2的RPC通信地址 -->  \n    <property>  \n        <name>dfs.namenode.rpc-address.mycluster.nn2</name>  \n        <value>wqkenqing02:9000</value>  \n    </property>  \n\n    <!-- nn2的http通信地址 -->  \n    <property>  \n        <name>dfs.namenode.http-address.mycluster.nn2</name>  \n        <value>wqkenqing02:50070</value>  \n    </property>  \n\n    <!-- 指定NameNode的元数据在JournalNode上的存放位置 -->  \n    <property>  \n        <name>dfs.namenode.shared.edits.dir</name>  \n        <value>qjournal://wqkenqing:8485;wqkenqing02:8485/mycluster</value>  \n    </property>  \n\n    <!-- 指定JournalNode在本地磁盘存放数据的位置 -->  \n    <property>  \n        <name>dfs.journalnode.edits.dir</name>  \n        <value>/usr/lib/hadoop/journal</value>  \n    </property>  \n\n    <!-- 开启NameNode失败自动切换 -->  \n    <property>  \n        <name>dfs.ha.automatic-failover.enabled</name>  \n        <value>true</value>  \n    </property>  \n\n    <!-- 配置失败自动切换实现方式 -->  \n    <property>  \n        <name>dfs.client.failover.proxy.provider.mycluster</name>  \n        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>  \n    </property>  \n\n    <!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行-->  \n    <property>  \n        <name>dfs.ha.fencing.methods</name>  \n        <value>  \n            sshfence  \n            shell(/bin/true)  \n        </value>  \n    </property>  \n\n    <!-- 使用sshfence隔离机制时需要ssh免登陆 -->  \n    <property>  \n        <name>dfs.ha.fencing.ssh.private-key-files</name>  \n        <value>/home/hadoop/.ssh/id_rsa</value>  \n    </property>  \n\n    <!-- 配置sshfence隔离机制超时时间 -->  \n    <property>  \n        <name>dfs.ha.fencing.ssh.connect-timeout</name>  \n        <value>30000</value>  \n    </property>\n***\n**修改 /home/hadoop/etc/hadoop/ mapred-site.xml**\n这个文件默认不存在，需要从 mapred-site.xml.template 复制过来\n在 <configuration> 块儿中添加：\n\n<property>\n\n<name>mapreduce.framework.name</name>\n\n<value>yarn</value>\n\n</property>\n\n<property>\n\n<name>mapreduce.jobhistory.address</name>\n\n<value>wqkenqing:10020</value>\n\n</property>\n\n<property>\n\n<name>mapreduce.jobhistory.webapp.address</name>\n\n<value>wqkenqing:19888</value>\n\n</property>\n***\n**修改 /home/hadoop/etc/hadoop/ yarn-site.xml**\n在 <configuration> 块儿中添加：\n\n<property>\n\n<name>yarn.nodemanager.aux-services</name>\n\n<value>mapreduce_shuffle</value>\n\n</property>\n\n<property>\n\n<name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>\n\n<value>org.apache.hadoop.mapred.ShuffleHandler</value>\n\n</property>\n\n<property>\n\n<name>yarn.resourcemanager.address</name>\n\n<value>wqkenqing:8032</value>\n\n</property>\n\n<property>\n\n<name>yarn.resourcemanager.scheduler.address</name>\n\n<value>wqkenqing:8030</value>\n\n</property>\n\n<property>\n\n<name>yarn.resourcemanager.resource-tracker.address</name>\n\n<value>wqkenqing:8031</value>\n\n</property>\n\n<property>\n\n<name>yarn.resourcemanager.admin.address</name>\n\n<value>wqkenqing:8033</value>\n\n</property>\n\n<property>\n\n<name>yarn.resourcemanager.webapp.address</name>\n\n<value>wqkenqing:8088</value>\n\n</property>\n***\n**修改 /home/hadoop/etc/hadoop/ slaves**\n\n删除已有内容，添加：\nwqkenqing02（data节点）\n\n***\nhttp://115.29.97.126:50070/\n***\n**启动 hadoop**\n在master启动hadoop，从节点会自动启动\n**初始化**\n\thadoop-daemon.sh start journalnode\n$ hdfs namenode -format\n$ hadoop-daemon.sh start namenode\n$ hadoop-daemon.sh start datanode\n$ start-dfs.sh\n$ start-yarn.sh\n***\n启动报错\n**java.io.IOException: All specified directories are failed to load.**\n解决方式：配置文件中的地址书写有问题\n***\n至此hadoop集群中的关键功能已经实现\n* hdfs\n* mapreudce\n[集群配置参考文献](http://www.tuicool.com/articles/uAnyEfj)\n***\nhadoop220集群运行状态：\n![Alt text](./1475419804382.png)\n\n[同步时钟参考文献](http://f.dataguru.cn/thread-475940-1-1.html)\n###hbase集成(尚未完成)\n* [hadoop集群搭建参考文献](http://www.linuxidc.com/Linux/2013-06/86347p4.htm)\n*\n\n***\n###hbase集群启动指令\n* start-hbase.sh\n* [hbase中的zookeeper使用教程](http://blog.csdn.net/korder/article/details/47403801)\n\n###集群重置\n* hadoop配置完成(ali1+ali2)\n* 添加hbase\n","slug":"技术/集群/私有集群","published":1,"date":"2021-01-05T02:35:38.000Z","updated":"2021-01-05T02:35:38.000Z","title":"技术/集群/私有集群","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31s001638pwgxiy8bib","content":"<p>9 @(私有集群的搭建)<br>###私有集群搭建<br>一、服务器的准备</p>\n<ul>\n<li>阿里云实例（115.29.97.126）</li>\n<li>华为云实例（114.115.203.81）</li>\n<li>阿里云实例2(115.29.35.125）</li>\n<li>本机实例（）</li>\n</ul>\n<p>1.1）服务器环境设置<br>修改服务器hostname</p>\n<ul>\n<li>阿里（wqkenqing）</li>\n<li>华为（wqkenqing02）</li>\n<li>阿里2(wqkenqing03)</li>\n<li>虚拟机（wqkneqing04）<br><a href=\"http://blog.csdn.net/huangxy10/article/details/40213095\" target=\"_blank\" rel=\"noopener\">hostname修改教程</a></li>\n</ul>\n<hr>\n<p>1.2）配置语言环境（取的默认环境）<br><img src=\"./1475308903359.png\" alt=\"Alt text\"><br>1.3）<a href=\"https://my.oschina.net/cjun/blog/344836\" target=\"_blank\" rel=\"noopener\">关闭防火墙</a><br>1.4）创建hadoop用户<br>1.5）上传相关文件(jdk1.7、hadoop2.7.2)</p>\n<hr>\n<p>1.6）配置环境变量<br>**<br>export JAVA_HOME=/usr/local/jdk1.7<br>export JRE_HOME=${JAVA_HOME}/jre<br>export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib</p>\n<p>export PATH=${JAVA_HOME}/bin:$PATH **</p>\n<hr>\n<p>1.7）配置免密码登陆<br>    1)实现本机ssh免秘钥登录：</p>\n<ul>\n<li>$ ssh-keygen -t dsa -P ‘’ -f ~/.ssh/id_dsa</li>\n<li>$ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</li>\n<li>chmod 700 ~/.ssh/authorized_keys</li>\n</ul>\n<hr>\n<p>2)实现集群ssh免秘钥登录</p>\n<ul>\n<li>写入每一个集群机器秘钥到主NN 的authorized_keys中 ssh hadoopXXX ‘cat /home/hadoop/.ssh/id_dsa.pub’ &gt;&gt; ~/.ssh/authorized_keys</li>\n<li>覆盖所有集群机器authorized_keys：scp ~/.ssh/authorized_keys hadoopXXX:/home/hadoop/.ssh/authorized_keys</li>\n</ul>\n<p>二、hadoop配置<br>2.1）配置hadoop环境<br>export HADOOP_HOME=/usr/local/hadoop/hadoop</p>\n<p>export HIVE_HOME=/usr/local/hadoop/hive</p>\n<p>export HBASE_HOME=/usr/local/hadoop/hbase</p>\n<p>PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HBASE_HOME/bin:$HIVE_HOME/bin</p>\n<hr>\n<p>hadoop配置文件配置<br>2.1）wqkenqing节点（以后作为name节点）<br><strong>修改 /home/hadoop/etc/hadoop/ core-site.xml</strong><br>在 <configuration> 块儿中添加：</p>\n<property>\n\n<p><name>fs.defaultFS</name></p>\n<p><value>hdfs://wqkenqing.com:9000</value></p>\n</property>\n\n<property>\n\n<p><name>hadoop.tmp.dir</name></p>\n<p><value>file:/home/hadoop/tmp</value></p>\n</property>\n\n<property>\n\n<p><name>io.file.buffer.size</name></p>\n<p><value>131702</value></p>\n<!-- 指定zookeeper地址 -->  \n<pre><code>&lt;property&gt;  \n    &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;  \n    &lt;value&gt;wqkenqing.com:2181,wqkenqing02.com:2181&lt;/value&gt;  \n&lt;/property&gt;  </code></pre></property>\n***\n**修改 /home/hadoop/etc/hadoop/ hdfs-site.xml**\n在 <configuration> 块儿中添加：\n\n<property>\n\n<p><name>dfs.namenode.name.dir</name></p>\n<p><value>file:/home/hadoop/hdfs/name</value></p>\n</property>\n\n<property>\n\n<p><name>dfs.datanode.data.dir</name></p>\n<p><value>file:/home/hadoop/hdfs/data</value></p>\n</property>\n\n<property>\n\n<p><name>dfs.replication</name></p>\n<p><value>2</value></p>\n</property>\n\n<property>\n\n<p><name>dfs.namenode.secondary.http-address</name></p>\n<p><value>wqkenqing.com:9001</value></p>\n</property>\n\n<property>\n\n<p><name>dfs.webhdfs.enabled</name></p>\n<p><value>true</value><br></property></p>\n<hr>\n <!--指定hdfs的nameservice为mycluster，需要和core-site.xml中的保持一致 -->  \n<pre><code>&lt;property&gt;  \n    &lt;name&gt;dfs.nameservices&lt;/name&gt;  \n    &lt;value&gt;wqkenqing.com&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- mycluster下面有两个NameNode，分别是nn1，nn2 --&gt;  \n&lt;property&gt;  \n &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;  \n    &lt;value&gt;wqkenqing,wqkenqing02&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- nn1的RPC通信地址 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;  \n    &lt;value&gt;wqkenqing:9000&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- nn1的http通信地址 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;  \n    &lt;value&gt;h1m1:50070&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- nn2的RPC通信地址 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;  \n    &lt;value&gt;wqkenqing02:9000&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- nn2的http通信地址 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;  \n    &lt;value&gt;wqkenqing02:50070&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;  \n    &lt;value&gt;qjournal://wqkenqing:8485;wqkenqing02:8485/mycluster&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;  \n    &lt;value&gt;/usr/lib/hadoop/journal&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- 开启NameNode失败自动切换 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;  \n    &lt;value&gt;true&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- 配置失败自动切换实现方式 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;  \n    &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;  \n    &lt;value&gt;  \n        sshfence  \n        shell(/bin/true)  \n    &lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;  \n    &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- 配置sshfence隔离机制超时时间 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;  \n    &lt;value&gt;30000&lt;/value&gt;  \n&lt;/property&gt;</code></pre><hr>\n<p><strong>修改 /home/hadoop/etc/hadoop/ mapred-site.xml</strong><br>这个文件默认不存在，需要从 mapred-site.xml.template 复制过来<br>在 <configuration> 块儿中添加：</p>\n<property>\n\n<p><name>mapreduce.framework.name</name></p>\n<p><value>yarn</value></p>\n</property>\n\n<property>\n\n<p><name>mapreduce.jobhistory.address</name></p>\n<p><value>wqkenqing:10020</value></p>\n</property>\n\n<property>\n\n<p><name>mapreduce.jobhistory.webapp.address</name></p>\n<p><value>wqkenqing:19888</value></p>\n</property>\n***\n**修改 /home/hadoop/etc/hadoop/ yarn-site.xml**\n在 <configuration> 块儿中添加：\n\n<property>\n\n<p><name>yarn.nodemanager.aux-services</name></p>\n<p><value>mapreduce_shuffle</value></p>\n</property>\n\n<property>\n\n<p><name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name></p>\n<p><value>org.apache.hadoop.mapred.ShuffleHandler</value></p>\n</property>\n\n<property>\n\n<p><name>yarn.resourcemanager.address</name></p>\n<p><value>wqkenqing:8032</value></p>\n</property>\n\n<property>\n\n<p><name>yarn.resourcemanager.scheduler.address</name></p>\n<p><value>wqkenqing:8030</value></p>\n</property>\n\n<property>\n\n<p><name>yarn.resourcemanager.resource-tracker.address</name></p>\n<p><value>wqkenqing:8031</value></p>\n</property>\n\n<property>\n\n<p><name>yarn.resourcemanager.admin.address</name></p>\n<p><value>wqkenqing:8033</value></p>\n</property>\n\n<property>\n\n<p><name>yarn.resourcemanager.webapp.address</name></p>\n<p><value>wqkenqing:8088</value></p>\n</property>\n***\n**修改 /home/hadoop/etc/hadoop/ slaves**\n\n<p>删除已有内容，添加：<br>wqkenqing02（data节点）</p>\n<hr>\n<p><a href=\"http://115.29.97.126:50070/\" target=\"_blank\" rel=\"noopener\">http://115.29.97.126:50070/</a></p>\n<hr>\n<p><strong>启动 hadoop</strong><br>在master启动hadoop，从节点会自动启动<br><strong>初始化</strong><br>    hadoop-daemon.sh start journalnode<br>$ hdfs namenode -format<br>$ hadoop-daemon.sh start namenode<br>$ hadoop-daemon.sh start datanode<br>$ start-dfs.sh<br>$ start-yarn.sh</p>\n<hr>\n<p>启动报错<br><strong>java.io.IOException: All specified directories are failed to load.</strong><br>解决方式：配置文件中的地址书写有问题</p>\n<hr>\n<p>至此hadoop集群中的关键功能已经实现</p>\n<ul>\n<li>hdfs</li>\n<li>mapreudce<br><a href=\"http://www.tuicool.com/articles/uAnyEfj\" target=\"_blank\" rel=\"noopener\">集群配置参考文献</a></li>\n</ul>\n<hr>\n<p>hadoop220集群运行状态：<br><img src=\"./1475419804382.png\" alt=\"Alt text\"></p>\n<p><a href=\"http://f.dataguru.cn/thread-475940-1-1.html\" target=\"_blank\" rel=\"noopener\">同步时钟参考文献</a><br>###hbase集成(尚未完成)</p>\n<ul>\n<li><a href=\"http://www.linuxidc.com/Linux/2013-06/86347p4.htm\" target=\"_blank\" rel=\"noopener\">hadoop集群搭建参考文献</a></li>\n<li></li>\n</ul>\n<hr>\n<p>###hbase集群启动指令</p>\n<ul>\n<li>start-hbase.sh</li>\n<li><a href=\"http://blog.csdn.net/korder/article/details/47403801\" target=\"_blank\" rel=\"noopener\">hbase中的zookeeper使用教程</a></li>\n</ul>\n<p>###集群重置</p>\n<ul>\n<li>hadoop配置完成(ali1+ali2)</li>\n<li>添加hbase</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>9 @(私有集群的搭建)<br>###私有集群搭建<br>一、服务器的准备</p>\n<ul>\n<li>阿里云实例（115.29.97.126）</li>\n<li>华为云实例（114.115.203.81）</li>\n<li>阿里云实例2(115.29.35.125）</li>\n<li>本机实例（）</li>\n</ul>\n<p>1.1）服务器环境设置<br>修改服务器hostname</p>\n<ul>\n<li>阿里（wqkenqing）</li>\n<li>华为（wqkenqing02）</li>\n<li>阿里2(wqkenqing03)</li>\n<li>虚拟机（wqkneqing04）<br><a href=\"http://blog.csdn.net/huangxy10/article/details/40213095\" target=\"_blank\" rel=\"noopener\">hostname修改教程</a></li>\n</ul>\n<hr>\n<p>1.2）配置语言环境（取的默认环境）<br><img src=\"./1475308903359.png\" alt=\"Alt text\"><br>1.3）<a href=\"https://my.oschina.net/cjun/blog/344836\" target=\"_blank\" rel=\"noopener\">关闭防火墙</a><br>1.4）创建hadoop用户<br>1.5）上传相关文件(jdk1.7、hadoop2.7.2)</p>\n<hr>\n<p>1.6）配置环境变量<br>**<br>export JAVA_HOME=/usr/local/jdk1.7<br>export JRE_HOME=${JAVA_HOME}/jre<br>export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib</p>\n<p>export PATH=${JAVA_HOME}/bin:$PATH **</p>\n<hr>\n<p>1.7）配置免密码登陆<br>    1)实现本机ssh免秘钥登录：</p>\n<ul>\n<li>$ ssh-keygen -t dsa -P ‘’ -f ~/.ssh/id_dsa</li>\n<li>$ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</li>\n<li>chmod 700 ~/.ssh/authorized_keys</li>\n</ul>\n<hr>\n<p>2)实现集群ssh免秘钥登录</p>\n<ul>\n<li>写入每一个集群机器秘钥到主NN 的authorized_keys中 ssh hadoopXXX ‘cat /home/hadoop/.ssh/id_dsa.pub’ &gt;&gt; ~/.ssh/authorized_keys</li>\n<li>覆盖所有集群机器authorized_keys：scp ~/.ssh/authorized_keys hadoopXXX:/home/hadoop/.ssh/authorized_keys</li>\n</ul>\n<p>二、hadoop配置<br>2.1）配置hadoop环境<br>export HADOOP_HOME=/usr/local/hadoop/hadoop</p>\n<p>export HIVE_HOME=/usr/local/hadoop/hive</p>\n<p>export HBASE_HOME=/usr/local/hadoop/hbase</p>\n<p>PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HBASE_HOME/bin:$HIVE_HOME/bin</p>\n<hr>\n<p>hadoop配置文件配置<br>2.1）wqkenqing节点（以后作为name节点）<br><strong>修改 /home/hadoop/etc/hadoop/ core-site.xml</strong><br>在 <configuration> 块儿中添加：</p>\n<property>\n\n<p><name>fs.defaultFS</name></p>\n<p><value>hdfs://wqkenqing.com:9000</value></p>\n</property>\n\n<property>\n\n<p><name>hadoop.tmp.dir</name></p>\n<p><value>file:/home/hadoop/tmp</value></p>\n</property>\n\n<property>\n\n<p><name>io.file.buffer.size</name></p>\n<p><value>131702</value></p>\n<!-- 指定zookeeper地址 -->  \n<pre><code>&lt;property&gt;  \n    &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;  \n    &lt;value&gt;wqkenqing.com:2181,wqkenqing02.com:2181&lt;/value&gt;  \n&lt;/property&gt;  </code></pre></property>\n***\n**修改 /home/hadoop/etc/hadoop/ hdfs-site.xml**\n在 <configuration> 块儿中添加：\n\n<property>\n\n<p><name>dfs.namenode.name.dir</name></p>\n<p><value>file:/home/hadoop/hdfs/name</value></p>\n</property>\n\n<property>\n\n<p><name>dfs.datanode.data.dir</name></p>\n<p><value>file:/home/hadoop/hdfs/data</value></p>\n</property>\n\n<property>\n\n<p><name>dfs.replication</name></p>\n<p><value>2</value></p>\n</property>\n\n<property>\n\n<p><name>dfs.namenode.secondary.http-address</name></p>\n<p><value>wqkenqing.com:9001</value></p>\n</property>\n\n<property>\n\n<p><name>dfs.webhdfs.enabled</name></p>\n<p><value>true</value><br></property></p>\n<hr>\n <!--指定hdfs的nameservice为mycluster，需要和core-site.xml中的保持一致 -->  \n<pre><code>&lt;property&gt;  \n    &lt;name&gt;dfs.nameservices&lt;/name&gt;  \n    &lt;value&gt;wqkenqing.com&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- mycluster下面有两个NameNode，分别是nn1，nn2 --&gt;  \n&lt;property&gt;  \n &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;  \n    &lt;value&gt;wqkenqing,wqkenqing02&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- nn1的RPC通信地址 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;  \n    &lt;value&gt;wqkenqing:9000&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- nn1的http通信地址 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;  \n    &lt;value&gt;h1m1:50070&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- nn2的RPC通信地址 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;  \n    &lt;value&gt;wqkenqing02:9000&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- nn2的http通信地址 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;  \n    &lt;value&gt;wqkenqing02:50070&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;  \n    &lt;value&gt;qjournal://wqkenqing:8485;wqkenqing02:8485/mycluster&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;  \n    &lt;value&gt;/usr/lib/hadoop/journal&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- 开启NameNode失败自动切换 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;  \n    &lt;value&gt;true&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- 配置失败自动切换实现方式 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;  \n    &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;  \n    &lt;value&gt;  \n        sshfence  \n        shell(/bin/true)  \n    &lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;  \n    &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;  \n&lt;/property&gt;  \n\n&lt;!-- 配置sshfence隔离机制超时时间 --&gt;  \n&lt;property&gt;  \n    &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;  \n    &lt;value&gt;30000&lt;/value&gt;  \n&lt;/property&gt;</code></pre><hr>\n<p><strong>修改 /home/hadoop/etc/hadoop/ mapred-site.xml</strong><br>这个文件默认不存在，需要从 mapred-site.xml.template 复制过来<br>在 <configuration> 块儿中添加：</p>\n<property>\n\n<p><name>mapreduce.framework.name</name></p>\n<p><value>yarn</value></p>\n</property>\n\n<property>\n\n<p><name>mapreduce.jobhistory.address</name></p>\n<p><value>wqkenqing:10020</value></p>\n</property>\n\n<property>\n\n<p><name>mapreduce.jobhistory.webapp.address</name></p>\n<p><value>wqkenqing:19888</value></p>\n</property>\n***\n**修改 /home/hadoop/etc/hadoop/ yarn-site.xml**\n在 <configuration> 块儿中添加：\n\n<property>\n\n<p><name>yarn.nodemanager.aux-services</name></p>\n<p><value>mapreduce_shuffle</value></p>\n</property>\n\n<property>\n\n<p><name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name></p>\n<p><value>org.apache.hadoop.mapred.ShuffleHandler</value></p>\n</property>\n\n<property>\n\n<p><name>yarn.resourcemanager.address</name></p>\n<p><value>wqkenqing:8032</value></p>\n</property>\n\n<property>\n\n<p><name>yarn.resourcemanager.scheduler.address</name></p>\n<p><value>wqkenqing:8030</value></p>\n</property>\n\n<property>\n\n<p><name>yarn.resourcemanager.resource-tracker.address</name></p>\n<p><value>wqkenqing:8031</value></p>\n</property>\n\n<property>\n\n<p><name>yarn.resourcemanager.admin.address</name></p>\n<p><value>wqkenqing:8033</value></p>\n</property>\n\n<property>\n\n<p><name>yarn.resourcemanager.webapp.address</name></p>\n<p><value>wqkenqing:8088</value></p>\n</property>\n***\n**修改 /home/hadoop/etc/hadoop/ slaves**\n\n<p>删除已有内容，添加：<br>wqkenqing02（data节点）</p>\n<hr>\n<p><a href=\"http://115.29.97.126:50070/\" target=\"_blank\" rel=\"noopener\">http://115.29.97.126:50070/</a></p>\n<hr>\n<p><strong>启动 hadoop</strong><br>在master启动hadoop，从节点会自动启动<br><strong>初始化</strong><br>    hadoop-daemon.sh start journalnode<br>$ hdfs namenode -format<br>$ hadoop-daemon.sh start namenode<br>$ hadoop-daemon.sh start datanode<br>$ start-dfs.sh<br>$ start-yarn.sh</p>\n<hr>\n<p>启动报错<br><strong>java.io.IOException: All specified directories are failed to load.</strong><br>解决方式：配置文件中的地址书写有问题</p>\n<hr>\n<p>至此hadoop集群中的关键功能已经实现</p>\n<ul>\n<li>hdfs</li>\n<li>mapreudce<br><a href=\"http://www.tuicool.com/articles/uAnyEfj\" target=\"_blank\" rel=\"noopener\">集群配置参考文献</a></li>\n</ul>\n<hr>\n<p>hadoop220集群运行状态：<br><img src=\"./1475419804382.png\" alt=\"Alt text\"></p>\n<p><a href=\"http://f.dataguru.cn/thread-475940-1-1.html\" target=\"_blank\" rel=\"noopener\">同步时钟参考文献</a><br>###hbase集成(尚未完成)</p>\n<ul>\n<li><a href=\"http://www.linuxidc.com/Linux/2013-06/86347p4.htm\" target=\"_blank\" rel=\"noopener\">hadoop集群搭建参考文献</a></li>\n<li></li>\n</ul>\n<hr>\n<p>###hbase集群启动指令</p>\n<ul>\n<li>start-hbase.sh</li>\n<li><a href=\"http://blog.csdn.net/korder/article/details/47403801\" target=\"_blank\" rel=\"noopener\">hbase中的zookeeper使用教程</a></li>\n</ul>\n<p>###集群重置</p>\n<ul>\n<li>hadoop配置完成(ali1+ali2)</li>\n<li>添加hbase</li>\n</ul>\n"},{"_content":"```\n采集数据,留作备用\n\n```\n\n\n所涉组件\n\n采集\n\n+ flume\n+ logstash\n+ beats\n+ elasticsearch-dump\n\n传输\n\n+ kafka\n+ flume\n\n存储\n\n+ elasticsearch\n+ hbase\n+ hdfs\n\n\nstep one : 采集数据到elasticsearch\n","source":"_posts/日常/prepare/数据储备.md","raw":"```\n采集数据,留作备用\n\n```\n\n\n所涉组件\n\n采集\n\n+ flume\n+ logstash\n+ beats\n+ elasticsearch-dump\n\n传输\n\n+ kafka\n+ flume\n\n存储\n\n+ elasticsearch\n+ hbase\n+ hdfs\n\n\nstep one : 采集数据到elasticsearch\n","slug":"日常/prepare/数据储备","published":1,"date":"2021-01-05T02:35:38.000Z","updated":"2021-01-05T02:35:38.000Z","title":"日常/prepare/数据储备","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31t001738pwbgitdgmf","content":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">采集数据,留作备用</span><br></pre></td></tr></table></figure>\n\n\n<p>所涉组件</p>\n<p>采集</p>\n<ul>\n<li>flume</li>\n<li>logstash</li>\n<li>beats</li>\n<li>elasticsearch-dump</li>\n</ul>\n<p>传输</p>\n<ul>\n<li>kafka</li>\n<li>flume</li>\n</ul>\n<p>存储</p>\n<ul>\n<li>elasticsearch</li>\n<li>hbase</li>\n<li>hdfs</li>\n</ul>\n<p>step one : 采集数据到elasticsearch</p>\n","site":{"data":{}},"excerpt":"","more":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">采集数据,留作备用</span><br></pre></td></tr></table></figure>\n\n\n<p>所涉组件</p>\n<p>采集</p>\n<ul>\n<li>flume</li>\n<li>logstash</li>\n<li>beats</li>\n<li>elasticsearch-dump</li>\n</ul>\n<p>传输</p>\n<ul>\n<li>kafka</li>\n<li>flume</li>\n</ul>\n<p>存储</p>\n<ul>\n<li>elasticsearch</li>\n<li>hbase</li>\n<li>hdfs</li>\n</ul>\n<p>step one : 采集数据到elasticsearch</p>\n"},{"_content":" ---\n title:  jumpserver体验\n date: 2020-07-28\n tags: [jumpserver,资产管理]\n ---\n\n <!--more-->\n\n \\# jumpserver体验\n\n \\#\\# 一体化安装\n\n\nmysql -uroot -e \"create database jumpserver default charset 'utf8'; grant all on jumpserver.\\* to 'jumpserver'@'127.0.0.1' identified by '$DB\\_PASSWORD'; flush privileges;\"\n\n\n\nm6GgXgemmBw5B7om9uYqFSz3erVo0JCLO7Trxpo8S2bxazXk2Y\n\nVv42qLmxsEPUO4Kf\n\n\nxLnAOS5Mc8cqRZ8QDxUzVQvZgxIRnJl73c0rE56WCSq8tYZQFG\n\nETNhIk9zu3fgN2pG\n\n\n","source":"_posts/日常/运维/jumpserver.md","raw":" ---\n title:  jumpserver体验\n date: 2020-07-28\n tags: [jumpserver,资产管理]\n ---\n\n <!--more-->\n\n \\# jumpserver体验\n\n \\#\\# 一体化安装\n\n\nmysql -uroot -e \"create database jumpserver default charset 'utf8'; grant all on jumpserver.\\* to 'jumpserver'@'127.0.0.1' identified by '$DB\\_PASSWORD'; flush privileges;\"\n\n\n\nm6GgXgemmBw5B7om9uYqFSz3erVo0JCLO7Trxpo8S2bxazXk2Y\n\nVv42qLmxsEPUO4Kf\n\n\nxLnAOS5Mc8cqRZ8QDxUzVQvZgxIRnJl73c0rE56WCSq8tYZQFG\n\nETNhIk9zu3fgN2pG\n\n\n","slug":"日常/运维/jumpserver","published":1,"date":"2021-01-05T02:35:38.000Z","updated":"2021-01-05T02:35:38.000Z","title":"日常/运维/jumpserver","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31t001838pw4uaz7ku0","content":"<hr>\n<p> title:  jumpserver体验<br> date: 2020-07-28<br> tags: [jumpserver,资产管理]</p>\n<hr>\n <a id=\"more\"></a>\n\n<p> # jumpserver体验</p>\n<p> ## 一体化安装</p>\n<p>mysql -uroot -e “create database jumpserver default charset ‘utf8’; grant all on jumpserver.* to ‘jumpserver’@’127.0.0.1’ identified by ‘$DB_PASSWORD’; flush privileges;”</p>\n<p>m6GgXgemmBw5B7om9uYqFSz3erVo0JCLO7Trxpo8S2bxazXk2Y</p>\n<p>Vv42qLmxsEPUO4Kf</p>\n<p>xLnAOS5Mc8cqRZ8QDxUzVQvZgxIRnJl73c0rE56WCSq8tYZQFG</p>\n<p>ETNhIk9zu3fgN2pG</p>\n","site":{"data":{}},"excerpt":"<hr>\n<p> title:  jumpserver体验<br> date: 2020-07-28<br> tags: [jumpserver,资产管理]</p>\n<hr>","more":"<p> # jumpserver体验</p>\n<p> ## 一体化安装</p>\n<p>mysql -uroot -e “create database jumpserver default charset ‘utf8’; grant all on jumpserver.* to ‘jumpserver’@’127.0.0.1’ identified by ‘$DB_PASSWORD’; flush privileges;”</p>\n<p>m6GgXgemmBw5B7om9uYqFSz3erVo0JCLO7Trxpo8S2bxazXk2Y</p>\n<p>Vv42qLmxsEPUO4Kf</p>\n<p>xLnAOS5Mc8cqRZ8QDxUzVQvZgxIRnJl73c0rE56WCSq8tYZQFG</p>\n<p>ETNhIk9zu3fgN2pG</p>"},{"_content":"## 使用tmux\n\nmac os 通过 brew install tmux\n\n```\ntmux 接收快捷键的指令是 ^ + B 即mac中的control + B\n```\n\n窗格的操作\n\n```\n这些操作都是通过 ^+b 来接收\n\n```\n\n| 操作符 | 作用     |\n| :------------- | :------------- |\n| %      |     左右创建两个窗格  |\n| ''    |     左右创建两个窗格  |\n| x    |    关闭当前窗格  |\n| {    |    前移当前窗格  |\n| }    |    后移当前窗格  |\n| ;    |    选择上次用的窗格  |\n| o    |    选择下一个窗格  |\n| space   |    切换窗格布局  |\n| z   |     放大窗格 |\n| q   |    显示序号 |\n","source":"_posts/日常/运维/tmux上手.md","raw":"## 使用tmux\n\nmac os 通过 brew install tmux\n\n```\ntmux 接收快捷键的指令是 ^ + B 即mac中的control + B\n```\n\n窗格的操作\n\n```\n这些操作都是通过 ^+b 来接收\n\n```\n\n| 操作符 | 作用     |\n| :------------- | :------------- |\n| %      |     左右创建两个窗格  |\n| ''    |     左右创建两个窗格  |\n| x    |    关闭当前窗格  |\n| {    |    前移当前窗格  |\n| }    |    后移当前窗格  |\n| ;    |    选择上次用的窗格  |\n| o    |    选择下一个窗格  |\n| space   |    切换窗格布局  |\n| z   |     放大窗格 |\n| q   |    显示序号 |\n","slug":"日常/运维/tmux上手","published":1,"date":"2021-01-05T02:35:38.000Z","updated":"2021-01-05T02:35:38.000Z","title":"日常/运维/tmux上手","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31u001938pwg3oo9pk7","content":"<h2 id=\"使用tmux\"><a href=\"#使用tmux\" class=\"headerlink\" title=\"使用tmux\"></a>使用tmux</h2><p>mac os 通过 brew install tmux</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tmux 接收快捷键的指令是 ^ + B 即mac中的control + B</span><br></pre></td></tr></table></figure>\n\n<p>窗格的操作</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">这些操作都是通过 ^+b 来接收</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th align=\"left\">操作符</th>\n<th align=\"left\">作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">%</td>\n<td align=\"left\">左右创建两个窗格</td>\n</tr>\n<tr>\n<td align=\"left\">‘’</td>\n<td align=\"left\">左右创建两个窗格</td>\n</tr>\n<tr>\n<td align=\"left\">x</td>\n<td align=\"left\">关闭当前窗格</td>\n</tr>\n<tr>\n<td align=\"left\">{</td>\n<td align=\"left\">前移当前窗格</td>\n</tr>\n<tr>\n<td align=\"left\">}</td>\n<td align=\"left\">后移当前窗格</td>\n</tr>\n<tr>\n<td align=\"left\">;</td>\n<td align=\"left\">选择上次用的窗格</td>\n</tr>\n<tr>\n<td align=\"left\">o</td>\n<td align=\"left\">选择下一个窗格</td>\n</tr>\n<tr>\n<td align=\"left\">space</td>\n<td align=\"left\">切换窗格布局</td>\n</tr>\n<tr>\n<td align=\"left\">z</td>\n<td align=\"left\">放大窗格</td>\n</tr>\n<tr>\n<td align=\"left\">q</td>\n<td align=\"left\">显示序号</td>\n</tr>\n</tbody></table>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"使用tmux\"><a href=\"#使用tmux\" class=\"headerlink\" title=\"使用tmux\"></a>使用tmux</h2><p>mac os 通过 brew install tmux</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tmux 接收快捷键的指令是 ^ + B 即mac中的control + B</span><br></pre></td></tr></table></figure>\n\n<p>窗格的操作</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">这些操作都是通过 ^+b 来接收</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th align=\"left\">操作符</th>\n<th align=\"left\">作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">%</td>\n<td align=\"left\">左右创建两个窗格</td>\n</tr>\n<tr>\n<td align=\"left\">‘’</td>\n<td align=\"left\">左右创建两个窗格</td>\n</tr>\n<tr>\n<td align=\"left\">x</td>\n<td align=\"left\">关闭当前窗格</td>\n</tr>\n<tr>\n<td align=\"left\">{</td>\n<td align=\"left\">前移当前窗格</td>\n</tr>\n<tr>\n<td align=\"left\">}</td>\n<td align=\"left\">后移当前窗格</td>\n</tr>\n<tr>\n<td align=\"left\">;</td>\n<td align=\"left\">选择上次用的窗格</td>\n</tr>\n<tr>\n<td align=\"left\">o</td>\n<td align=\"left\">选择下一个窗格</td>\n</tr>\n<tr>\n<td align=\"left\">space</td>\n<td align=\"left\">切换窗格布局</td>\n</tr>\n<tr>\n<td align=\"left\">z</td>\n<td align=\"left\">放大窗格</td>\n</tr>\n<tr>\n<td align=\"left\">q</td>\n<td align=\"left\">显示序号</td>\n</tr>\n</tbody></table>\n"},{"_content":"114.67.107.246\nwangkuiqing\neExfeTmKkgRJ255SRRZq\n","source":"_posts/日常/运维/京东云.md","raw":"114.67.107.246\nwangkuiqing\neExfeTmKkgRJ255SRRZq\n","slug":"日常/运维/京东云","published":1,"date":"2021-01-05T02:35:38.000Z","updated":"2021-01-05T02:35:38.000Z","title":"日常/运维/京东云","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31u001a38pwcmltbxj4","content":"<p>114.67.107.246<br>wangkuiqing<br>eExfeTmKkgRJ255SRRZq</p>\n","site":{"data":{}},"excerpt":"","more":"<p>114.67.107.246<br>wangkuiqing<br>eExfeTmKkgRJ255SRRZq</p>\n"},{"_content":"```\n这里的背景是针对博客不同项目生成不同主题类目的方案\n\n```\n\nstep one \n\n 更换wqkenqing.github.io下的github项目\n\n \n","source":"_posts/日常/运维/博客不同主题切换.md","raw":"```\n这里的背景是针对博客不同项目生成不同主题类目的方案\n\n```\n\nstep one \n\n 更换wqkenqing.github.io下的github项目\n\n \n","slug":"日常/运维/博客不同主题切换","published":1,"date":"2021-01-05T02:35:38.000Z","updated":"2021-01-05T02:35:38.000Z","title":"日常/运维/博客不同主题切换","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31v001b38pw96y5c5gs","content":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">这里的背景是针对博客不同项目生成不同主题类目的方案</span><br></pre></td></tr></table></figure>\n\n<p>step one </p>\n<p> 更换wqkenqing.github.io下的github项目</p>\n","site":{"data":{}},"excerpt":"","more":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">这里的背景是针对博客不同项目生成不同主题类目的方案</span><br></pre></td></tr></table></figure>\n\n<p>step one </p>\n<p> 更换wqkenqing.github.io下的github项目</p>\n"},{"title":"mac python版本替换","date":"2020-05-24T16:00:00.000Z","_content":"\n\n<!--more-->\n\n```\nmac python版本替换\n```\n\n\n","source":"_posts/日常/运维/mac python版本替换.md","raw":"title: mac python版本替换\ndate: 2020-05-25\ntags: mac python2 python3\n\n---\n\n\n<!--more-->\n\n```\nmac python版本替换\n```\n\n\n","slug":"日常/运维/mac python版本替换","published":1,"updated":"2021-01-05T02:35:38.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd31v001c38pw3i7vg1xr","content":"<a id=\"more\"></a>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mac python版本替换</span><br></pre></td></tr></table></figure>\n\n\n","site":{"data":{}},"excerpt":"","more":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mac python版本替换</span><br></pre></td></tr></table></figure>"},{"_content":"## Q: 更换ubuntu安装源\ncp /source/apt/source.plist /source/apt/source.list.bak\n\nvim /source/apt/source.plist\n\n添加国内源.\n\nQ: mac 查看目前哪些进程占用哪些端口\n\nlsof -nP  | grep TCP | grep LISTEN\n\nlsof -i :TCP\n\n![查看mac pid进程的端口](http://img.wqkenqing.ren/759a2c6c1c06fc81b9c400060913e93b.png)\n","source":"_posts/日常/运维/运维操作.md","raw":"## Q: 更换ubuntu安装源\ncp /source/apt/source.plist /source/apt/source.list.bak\n\nvim /source/apt/source.plist\n\n添加国内源.\n\nQ: mac 查看目前哪些进程占用哪些端口\n\nlsof -nP  | grep TCP | grep LISTEN\n\nlsof -i :TCP\n\n![查看mac pid进程的端口](http://img.wqkenqing.ren/759a2c6c1c06fc81b9c400060913e93b.png)\n","slug":"日常/运维/运维操作","published":1,"date":"2021-01-05T02:35:38.000Z","updated":"2021-01-05T02:35:38.000Z","title":"日常/运维/运维操作","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd326001f38pw2iud0r1r","content":"<h2 id=\"Q-更换ubuntu安装源\"><a href=\"#Q-更换ubuntu安装源\" class=\"headerlink\" title=\"Q: 更换ubuntu安装源\"></a>Q: 更换ubuntu安装源</h2><p>cp /source/apt/source.plist /source/apt/source.list.bak</p>\n<p>vim /source/apt/source.plist</p>\n<p>添加国内源.</p>\n<p>Q: mac 查看目前哪些进程占用哪些端口</p>\n<p>lsof -nP  | grep TCP | grep LISTEN</p>\n<p>lsof -i :TCP</p>\n<p><img src=\"http://img.wqkenqing.ren/759a2c6c1c06fc81b9c400060913e93b.png\" alt=\"查看mac pid进程的端口\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Q-更换ubuntu安装源\"><a href=\"#Q-更换ubuntu安装源\" class=\"headerlink\" title=\"Q: 更换ubuntu安装源\"></a>Q: 更换ubuntu安装源</h2><p>cp /source/apt/source.plist /source/apt/source.list.bak</p>\n<p>vim /source/apt/source.plist</p>\n<p>添加国内源.</p>\n<p>Q: mac 查看目前哪些进程占用哪些端口</p>\n<p>lsof -nP  | grep TCP | grep LISTEN</p>\n<p>lsof -i :TCP</p>\n<p><img src=\"http://img.wqkenqing.ren/759a2c6c1c06fc81b9c400060913e93b.png\" alt=\"查看mac pid进程的端口\"></p>\n"},{"_content":"","source":"_posts/技术/hexo/Thread/多线程.md","raw":"","slug":"技术/hexo/Thread/多线程","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/hexo/Thread/多线程","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd328001g38pwdxt6c6zv","content":"","site":{"data":{}},"excerpt":"","more":""},{"_content":"\n##  kafka\n\n添加topic\n/data/kafka/add_topic\n\n\n查询toipic\n\n/data/kafka/select_topic\n\n删除topic(待定)\n\n/data/kafka/delete_topic\n\n工具类\n\n删除 topic List;\n\n\n## redis\n","source":"_posts/技术/front_end/接口规划/data-manager.md","raw":"\n##  kafka\n\n添加topic\n/data/kafka/add_topic\n\n\n查询toipic\n\n/data/kafka/select_topic\n\n删除topic(待定)\n\n/data/kafka/delete_topic\n\n工具类\n\n删除 topic List;\n\n\n## redis\n","slug":"技术/front_end/接口规划/data-manager","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/front_end/接口规划/data-manager","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd32l001h38pw00yx5tkt","content":"<h2 id=\"kafka\"><a href=\"#kafka\" class=\"headerlink\" title=\"kafka\"></a>kafka</h2><p>添加topic<br>/data/kafka/add_topic</p>\n<p>查询toipic</p>\n<p>/data/kafka/select_topic</p>\n<p>删除topic(待定)</p>\n<p>/data/kafka/delete_topic</p>\n<p>工具类</p>\n<p>删除 topic List;</p>\n<h2 id=\"redis\"><a href=\"#redis\" class=\"headerlink\" title=\"redis\"></a>redis</h2>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"kafka\"><a href=\"#kafka\" class=\"headerlink\" title=\"kafka\"></a>kafka</h2><p>添加topic<br>/data/kafka/add_topic</p>\n<p>查询toipic</p>\n<p>/data/kafka/select_topic</p>\n<p>删除topic(待定)</p>\n<p>/data/kafka/delete_topic</p>\n<p>工具类</p>\n<p>删除 topic List;</p>\n<h2 id=\"redis\"><a href=\"#redis\" class=\"headerlink\" title=\"redis\"></a>redis</h2>"},{"_content":"```\n适合针对格式化文本操作\n\n```\n\n## 内置变量\n\n* FS(Field Separator)：输入字段分隔符， 默认为空白字符\n* OFS(Out of Field Separator)：输出字段分隔符， 默认为空白字符\n* RS(Record Separator)：输入记录分隔符(输入换行符)， 指定输入时的换行符\n* ORS(Output Record Separate)：输出记录分隔符（输出换行符），输出时用指定符号代替换行符\n* NF(Number for Field)：当前行的字段的个数(即当前行被分割成了几列)\n* NR(Number of Record)：行号，当前处理的文本行的行号。\n* FNR：各文件分别计数的行号\n* ARGC：命令行参数的个数\n* ARGV：数组，保存的是命令行所给定的各参数\n\nawk 其实相当是一门脚本语言,所以用法比较丰富.但一般来讲,一定程度的掌握就已经能满足我们日常使用了.至于是否需要更深入,则根据实际情况.\n\n\n\n常规:\n\nawk '{print $1 , $2}' file.txt\n输出file的1,2列\n\n`说明: awk 命令内容'' `\n\n## 过滤记录\n\nawk '$1>2 'file.txt\n\n\n$0\t 当前记录（这个变量中存放着整个行的内容）\n$1~$n\t 当前记录的第n个字段，字段间由FS分隔\nFS\t输入字段分隔符 默认是空格或Tab\nNF\t当前记录中的字段个数，就是有多少列\nNR\t已经读出的记录数，就是行号，从1开始，如果有多个文件话，这个值也是不断累加中。\nFNR\t当前记录数，与NR不同的是，这个值会是各个文件自己的行号\nRS\t输入的记录分隔符， 默认为换行符\nOFS\t输出字段分隔符， 默认也是空格\nORS\t输出的记录分隔符，默认为换行符\nFILENAME\t当前输入文件的名字\n\n\n### 指定分隔符\n awk  'BEGIN{FS=\":\"} {print $1,$3,$6}' /etc/passwd\n \n亦可用 awk -F \n指定多个分隔符\n\nawk -F '[;:]'\n\n### 字符串匹配\n\nawk '$6 ~ /FIN/ || NR==1 {print NR,$4,$5,$6}' OFS=\"\\t\" netstat.txt\n\nawk '$6 ~ /WAIT/ || NR==1 {print NR,$4,$5,$6}' OFS=\"\\t\" netstat.txt\n\n\nawk '/LISTEN/' netstat.txt\nawk '$6 ~ /FIN|TIME/ || NR==1 {print NR,$4,$5,$6}' OFS=\"\\t\" netstat.txt\n\nawk '$6 !~ /WAIT/ || NR==1 {print NR,$4,$5,$6}' OFS=\"\\t\" netstat.txt\n\n#### if else \n\nawk 'NR!=1{if($6 ~ /TIME|ESTABLISHED/) print > \"1.txt\";\nelse if($6 ~ /LISTEN/) print > \"2.txt\";\nelse print > \"3.txt\" }' netstat.txt\n \n\n\n### 脚本\n\n","source":"_posts/技术/bash/awk/awk.md","raw":"```\n适合针对格式化文本操作\n\n```\n\n## 内置变量\n\n* FS(Field Separator)：输入字段分隔符， 默认为空白字符\n* OFS(Out of Field Separator)：输出字段分隔符， 默认为空白字符\n* RS(Record Separator)：输入记录分隔符(输入换行符)， 指定输入时的换行符\n* ORS(Output Record Separate)：输出记录分隔符（输出换行符），输出时用指定符号代替换行符\n* NF(Number for Field)：当前行的字段的个数(即当前行被分割成了几列)\n* NR(Number of Record)：行号，当前处理的文本行的行号。\n* FNR：各文件分别计数的行号\n* ARGC：命令行参数的个数\n* ARGV：数组，保存的是命令行所给定的各参数\n\nawk 其实相当是一门脚本语言,所以用法比较丰富.但一般来讲,一定程度的掌握就已经能满足我们日常使用了.至于是否需要更深入,则根据实际情况.\n\n\n\n常规:\n\nawk '{print $1 , $2}' file.txt\n输出file的1,2列\n\n`说明: awk 命令内容'' `\n\n## 过滤记录\n\nawk '$1>2 'file.txt\n\n\n$0\t 当前记录（这个变量中存放着整个行的内容）\n$1~$n\t 当前记录的第n个字段，字段间由FS分隔\nFS\t输入字段分隔符 默认是空格或Tab\nNF\t当前记录中的字段个数，就是有多少列\nNR\t已经读出的记录数，就是行号，从1开始，如果有多个文件话，这个值也是不断累加中。\nFNR\t当前记录数，与NR不同的是，这个值会是各个文件自己的行号\nRS\t输入的记录分隔符， 默认为换行符\nOFS\t输出字段分隔符， 默认也是空格\nORS\t输出的记录分隔符，默认为换行符\nFILENAME\t当前输入文件的名字\n\n\n### 指定分隔符\n awk  'BEGIN{FS=\":\"} {print $1,$3,$6}' /etc/passwd\n \n亦可用 awk -F \n指定多个分隔符\n\nawk -F '[;:]'\n\n### 字符串匹配\n\nawk '$6 ~ /FIN/ || NR==1 {print NR,$4,$5,$6}' OFS=\"\\t\" netstat.txt\n\nawk '$6 ~ /WAIT/ || NR==1 {print NR,$4,$5,$6}' OFS=\"\\t\" netstat.txt\n\n\nawk '/LISTEN/' netstat.txt\nawk '$6 ~ /FIN|TIME/ || NR==1 {print NR,$4,$5,$6}' OFS=\"\\t\" netstat.txt\n\nawk '$6 !~ /WAIT/ || NR==1 {print NR,$4,$5,$6}' OFS=\"\\t\" netstat.txt\n\n#### if else \n\nawk 'NR!=1{if($6 ~ /TIME|ESTABLISHED/) print > \"1.txt\";\nelse if($6 ~ /LISTEN/) print > \"2.txt\";\nelse print > \"3.txt\" }' netstat.txt\n \n\n\n### 脚本\n\n","slug":"技术/bash/awk/awk","published":1,"date":"2021-01-05T02:35:36.000Z","updated":"2021-01-05T02:35:36.000Z","title":"技术/bash/awk/awk","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd32q001i38pw8l6ngsjv","content":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">适合针对格式化文本操作</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"内置变量\"><a href=\"#内置变量\" class=\"headerlink\" title=\"内置变量\"></a>内置变量</h2><ul>\n<li>FS(Field Separator)：输入字段分隔符， 默认为空白字符</li>\n<li>OFS(Out of Field Separator)：输出字段分隔符， 默认为空白字符</li>\n<li>RS(Record Separator)：输入记录分隔符(输入换行符)， 指定输入时的换行符</li>\n<li>ORS(Output Record Separate)：输出记录分隔符（输出换行符），输出时用指定符号代替换行符</li>\n<li>NF(Number for Field)：当前行的字段的个数(即当前行被分割成了几列)</li>\n<li>NR(Number of Record)：行号，当前处理的文本行的行号。</li>\n<li>FNR：各文件分别计数的行号</li>\n<li>ARGC：命令行参数的个数</li>\n<li>ARGV：数组，保存的是命令行所给定的各参数</li>\n</ul>\n<p>awk 其实相当是一门脚本语言,所以用法比较丰富.但一般来讲,一定程度的掌握就已经能满足我们日常使用了.至于是否需要更深入,则根据实际情况.</p>\n<p>常规:</p>\n<p>awk ‘{print $1 , $2}’ file.txt<br>输出file的1,2列</p>\n<p><code>说明: awk 命令内容&#39;&#39;</code></p>\n<h2 id=\"过滤记录\"><a href=\"#过滤记录\" class=\"headerlink\" title=\"过滤记录\"></a>过滤记录</h2><p>awk ‘$1&gt;2 ‘file.txt</p>\n<p>$0     当前记录（这个变量中存放着整个行的内容）<br>$1~$n     当前记录的第n个字段，字段间由FS分隔<br>FS    输入字段分隔符 默认是空格或Tab<br>NF    当前记录中的字段个数，就是有多少列<br>NR    已经读出的记录数，就是行号，从1开始，如果有多个文件话，这个值也是不断累加中。<br>FNR    当前记录数，与NR不同的是，这个值会是各个文件自己的行号<br>RS    输入的记录分隔符， 默认为换行符<br>OFS    输出字段分隔符， 默认也是空格<br>ORS    输出的记录分隔符，默认为换行符<br>FILENAME    当前输入文件的名字</p>\n<h3 id=\"指定分隔符\"><a href=\"#指定分隔符\" class=\"headerlink\" title=\"指定分隔符\"></a>指定分隔符</h3><p> awk  ‘BEGIN{FS=”:”} {print $1,$3,$6}’ /etc/passwd</p>\n<p>亦可用 awk -F<br>指定多个分隔符</p>\n<p>awk -F ‘[;:]’</p>\n<h3 id=\"字符串匹配\"><a href=\"#字符串匹配\" class=\"headerlink\" title=\"字符串匹配\"></a>字符串匹配</h3><p>awk ‘$6 ~ /FIN/ || NR==1 {print NR,$4,$5,$6}’ OFS=”\\t” netstat.txt</p>\n<p>awk ‘$6 ~ /WAIT/ || NR==1 {print NR,$4,$5,$6}’ OFS=”\\t” netstat.txt</p>\n<p>awk ‘/LISTEN/‘ netstat.txt<br>awk ‘$6 ~ /FIN|TIME/ || NR==1 {print NR,$4,$5,$6}’ OFS=”\\t” netstat.txt</p>\n<p>awk ‘$6 !~ /WAIT/ || NR==1 {print NR,$4,$5,$6}’ OFS=”\\t” netstat.txt</p>\n<h4 id=\"if-else\"><a href=\"#if-else\" class=\"headerlink\" title=\"if else\"></a>if else</h4><p>awk ‘NR!=1{if($6 ~ /TIME|ESTABLISHED/) print &gt; “1.txt”;<br>else if($6 ~ /LISTEN/) print &gt; “2.txt”;<br>else print &gt; “3.txt” }’ netstat.txt</p>\n<h3 id=\"脚本\"><a href=\"#脚本\" class=\"headerlink\" title=\"脚本\"></a>脚本</h3>","site":{"data":{}},"excerpt":"","more":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">适合针对格式化文本操作</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"内置变量\"><a href=\"#内置变量\" class=\"headerlink\" title=\"内置变量\"></a>内置变量</h2><ul>\n<li>FS(Field Separator)：输入字段分隔符， 默认为空白字符</li>\n<li>OFS(Out of Field Separator)：输出字段分隔符， 默认为空白字符</li>\n<li>RS(Record Separator)：输入记录分隔符(输入换行符)， 指定输入时的换行符</li>\n<li>ORS(Output Record Separate)：输出记录分隔符（输出换行符），输出时用指定符号代替换行符</li>\n<li>NF(Number for Field)：当前行的字段的个数(即当前行被分割成了几列)</li>\n<li>NR(Number of Record)：行号，当前处理的文本行的行号。</li>\n<li>FNR：各文件分别计数的行号</li>\n<li>ARGC：命令行参数的个数</li>\n<li>ARGV：数组，保存的是命令行所给定的各参数</li>\n</ul>\n<p>awk 其实相当是一门脚本语言,所以用法比较丰富.但一般来讲,一定程度的掌握就已经能满足我们日常使用了.至于是否需要更深入,则根据实际情况.</p>\n<p>常规:</p>\n<p>awk ‘{print $1 , $2}’ file.txt<br>输出file的1,2列</p>\n<p><code>说明: awk 命令内容&#39;&#39;</code></p>\n<h2 id=\"过滤记录\"><a href=\"#过滤记录\" class=\"headerlink\" title=\"过滤记录\"></a>过滤记录</h2><p>awk ‘$1&gt;2 ‘file.txt</p>\n<p>$0     当前记录（这个变量中存放着整个行的内容）<br>$1~$n     当前记录的第n个字段，字段间由FS分隔<br>FS    输入字段分隔符 默认是空格或Tab<br>NF    当前记录中的字段个数，就是有多少列<br>NR    已经读出的记录数，就是行号，从1开始，如果有多个文件话，这个值也是不断累加中。<br>FNR    当前记录数，与NR不同的是，这个值会是各个文件自己的行号<br>RS    输入的记录分隔符， 默认为换行符<br>OFS    输出字段分隔符， 默认也是空格<br>ORS    输出的记录分隔符，默认为换行符<br>FILENAME    当前输入文件的名字</p>\n<h3 id=\"指定分隔符\"><a href=\"#指定分隔符\" class=\"headerlink\" title=\"指定分隔符\"></a>指定分隔符</h3><p> awk  ‘BEGIN{FS=”:”} {print $1,$3,$6}’ /etc/passwd</p>\n<p>亦可用 awk -F<br>指定多个分隔符</p>\n<p>awk -F ‘[;:]’</p>\n<h3 id=\"字符串匹配\"><a href=\"#字符串匹配\" class=\"headerlink\" title=\"字符串匹配\"></a>字符串匹配</h3><p>awk ‘$6 ~ /FIN/ || NR==1 {print NR,$4,$5,$6}’ OFS=”\\t” netstat.txt</p>\n<p>awk ‘$6 ~ /WAIT/ || NR==1 {print NR,$4,$5,$6}’ OFS=”\\t” netstat.txt</p>\n<p>awk ‘/LISTEN/‘ netstat.txt<br>awk ‘$6 ~ /FIN|TIME/ || NR==1 {print NR,$4,$5,$6}’ OFS=”\\t” netstat.txt</p>\n<p>awk ‘$6 !~ /WAIT/ || NR==1 {print NR,$4,$5,$6}’ OFS=”\\t” netstat.txt</p>\n<h4 id=\"if-else\"><a href=\"#if-else\" class=\"headerlink\" title=\"if else\"></a>if else</h4><p>awk ‘NR!=1{if($6 ~ /TIME|ESTABLISHED/) print &gt; “1.txt”;<br>else if($6 ~ /LISTEN/) print &gt; “2.txt”;<br>else print &gt; “3.txt” }’ netstat.txt</p>\n<h3 id=\"脚本\"><a href=\"#脚本\" class=\"headerlink\" title=\"脚本\"></a>脚本</h3>"},{"title":"flink学习","date":"2019-07-30T16:00:00.000Z","_content":"\nflink内容记录\n\n<!--more-->\n\n## 搭建\n\n## 创建maven项目\n\n```\nmvn archetype:generate \\\n    -DarchetypeGroupId=org.apache.flink \\\n    -DarchetypeArtifactId=flink-quickstart-java \\\n    -DarchetypeVersion=1.6.1 \\\n    -DgroupId=my-flink-project \\\n    -DartifactId=my-flink-project \\\n    -Dversion=0.1 \\\n    -Dpackage=myflink \\\n    -DinteractiveMode=false\n\n```\n\n```\n\nmvn clean package -Dmaven.test.skip=true\n\n```\n\n```\nflink run -c myflink.demo.SocketTextStreamWordCount my-flink-project-0.1.jar 127.0.0.1 9000\n```\n\n## DataStream API\nflink程序工作解剖图\n![](http://img.wqkenqing.ren/FIyXNI.png)\n\n### 执行环境\n\nflink支持\n- 获取已经存在的flink环境\n- 创建一个本地环境\n- 创建一个远程环境\n\n### DataSource\n\n#### 预置source\n\nSocket-based\n\n- socketTextStream();\n\nFile-based\n\n### Transfomations\n\n- map\n- flatMap\n- filter\n- keyBy\n- reduce\n- fold\n\n合计\n\n- min\n- max\n- sum\n\n窗口\n\n\n\n\n","source":"_posts/技术/hexo/flink/Flink.md","raw":"---\n\ntitle: flink学习\ndate: 2019-07-31\ntags: flink\n\n---\n\nflink内容记录\n\n<!--more-->\n\n## 搭建\n\n## 创建maven项目\n\n```\nmvn archetype:generate \\\n    -DarchetypeGroupId=org.apache.flink \\\n    -DarchetypeArtifactId=flink-quickstart-java \\\n    -DarchetypeVersion=1.6.1 \\\n    -DgroupId=my-flink-project \\\n    -DartifactId=my-flink-project \\\n    -Dversion=0.1 \\\n    -Dpackage=myflink \\\n    -DinteractiveMode=false\n\n```\n\n```\n\nmvn clean package -Dmaven.test.skip=true\n\n```\n\n```\nflink run -c myflink.demo.SocketTextStreamWordCount my-flink-project-0.1.jar 127.0.0.1 9000\n```\n\n## DataStream API\nflink程序工作解剖图\n![](http://img.wqkenqing.ren/FIyXNI.png)\n\n### 执行环境\n\nflink支持\n- 获取已经存在的flink环境\n- 创建一个本地环境\n- 创建一个远程环境\n\n### DataSource\n\n#### 预置source\n\nSocket-based\n\n- socketTextStream();\n\nFile-based\n\n### Transfomations\n\n- map\n- flatMap\n- filter\n- keyBy\n- reduce\n- fold\n\n合计\n\n- min\n- max\n- sum\n\n窗口\n\n\n\n\n","slug":"技术/hexo/flink/Flink","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd32r001j38pwaggr3hh8","content":"<p>flink内容记录</p>\n<a id=\"more\"></a>\n\n<h2 id=\"搭建\"><a href=\"#搭建\" class=\"headerlink\" title=\"搭建\"></a>搭建</h2><h2 id=\"创建maven项目\"><a href=\"#创建maven项目\" class=\"headerlink\" title=\"创建maven项目\"></a>创建maven项目</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn archetype:generate \\</span><br><span class=\"line\">    -DarchetypeGroupId&#x3D;org.apache.flink \\</span><br><span class=\"line\">    -DarchetypeArtifactId&#x3D;flink-quickstart-java \\</span><br><span class=\"line\">    -DarchetypeVersion&#x3D;1.6.1 \\</span><br><span class=\"line\">    -DgroupId&#x3D;my-flink-project \\</span><br><span class=\"line\">    -DartifactId&#x3D;my-flink-project \\</span><br><span class=\"line\">    -Dversion&#x3D;0.1 \\</span><br><span class=\"line\">    -Dpackage&#x3D;myflink \\</span><br><span class=\"line\">    -DinteractiveMode&#x3D;false</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">mvn clean package -Dmaven.test.skip&#x3D;true</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">flink run -c myflink.demo.SocketTextStreamWordCount my-flink-project-0.1.jar 127.0.0.1 9000</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"DataStream-API\"><a href=\"#DataStream-API\" class=\"headerlink\" title=\"DataStream API\"></a>DataStream API</h2><p>flink程序工作解剖图<br><img src=\"http://img.wqkenqing.ren/FIyXNI.png\" alt=\"\"></p>\n<h3 id=\"执行环境\"><a href=\"#执行环境\" class=\"headerlink\" title=\"执行环境\"></a>执行环境</h3><p>flink支持</p>\n<ul>\n<li>获取已经存在的flink环境</li>\n<li>创建一个本地环境</li>\n<li>创建一个远程环境</li>\n</ul>\n<h3 id=\"DataSource\"><a href=\"#DataSource\" class=\"headerlink\" title=\"DataSource\"></a>DataSource</h3><h4 id=\"预置source\"><a href=\"#预置source\" class=\"headerlink\" title=\"预置source\"></a>预置source</h4><p>Socket-based</p>\n<ul>\n<li>socketTextStream();</li>\n</ul>\n<p>File-based</p>\n<h3 id=\"Transfomations\"><a href=\"#Transfomations\" class=\"headerlink\" title=\"Transfomations\"></a>Transfomations</h3><ul>\n<li>map</li>\n<li>flatMap</li>\n<li>filter</li>\n<li>keyBy</li>\n<li>reduce</li>\n<li>fold</li>\n</ul>\n<p>合计</p>\n<ul>\n<li>min</li>\n<li>max</li>\n<li>sum</li>\n</ul>\n<p>窗口</p>\n","site":{"data":{}},"excerpt":"<p>flink内容记录</p>","more":"<h2 id=\"搭建\"><a href=\"#搭建\" class=\"headerlink\" title=\"搭建\"></a>搭建</h2><h2 id=\"创建maven项目\"><a href=\"#创建maven项目\" class=\"headerlink\" title=\"创建maven项目\"></a>创建maven项目</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn archetype:generate \\</span><br><span class=\"line\">    -DarchetypeGroupId&#x3D;org.apache.flink \\</span><br><span class=\"line\">    -DarchetypeArtifactId&#x3D;flink-quickstart-java \\</span><br><span class=\"line\">    -DarchetypeVersion&#x3D;1.6.1 \\</span><br><span class=\"line\">    -DgroupId&#x3D;my-flink-project \\</span><br><span class=\"line\">    -DartifactId&#x3D;my-flink-project \\</span><br><span class=\"line\">    -Dversion&#x3D;0.1 \\</span><br><span class=\"line\">    -Dpackage&#x3D;myflink \\</span><br><span class=\"line\">    -DinteractiveMode&#x3D;false</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">mvn clean package -Dmaven.test.skip&#x3D;true</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">flink run -c myflink.demo.SocketTextStreamWordCount my-flink-project-0.1.jar 127.0.0.1 9000</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"DataStream-API\"><a href=\"#DataStream-API\" class=\"headerlink\" title=\"DataStream API\"></a>DataStream API</h2><p>flink程序工作解剖图<br><img src=\"http://img.wqkenqing.ren/FIyXNI.png\" alt=\"\"></p>\n<h3 id=\"执行环境\"><a href=\"#执行环境\" class=\"headerlink\" title=\"执行环境\"></a>执行环境</h3><p>flink支持</p>\n<ul>\n<li>获取已经存在的flink环境</li>\n<li>创建一个本地环境</li>\n<li>创建一个远程环境</li>\n</ul>\n<h3 id=\"DataSource\"><a href=\"#DataSource\" class=\"headerlink\" title=\"DataSource\"></a>DataSource</h3><h4 id=\"预置source\"><a href=\"#预置source\" class=\"headerlink\" title=\"预置source\"></a>预置source</h4><p>Socket-based</p>\n<ul>\n<li>socketTextStream();</li>\n</ul>\n<p>File-based</p>\n<h3 id=\"Transfomations\"><a href=\"#Transfomations\" class=\"headerlink\" title=\"Transfomations\"></a>Transfomations</h3><ul>\n<li>map</li>\n<li>flatMap</li>\n<li>filter</li>\n<li>keyBy</li>\n<li>reduce</li>\n<li>fold</li>\n</ul>\n<p>合计</p>\n<ul>\n<li>min</li>\n<li>max</li>\n<li>sum</li>\n</ul>\n<p>窗口</p>"},{"title":"CDH搭建细节","date":"2019-05-26T16:00:00.000Z","_content":".....\n<!--more-->\n\n## CDH安装准备\n\n![](http://img.wqkenqing.ren/Ibp691.png)\n\n\nubuntu ulimit","source":"_posts/技术/hexo/CDH/搭建.md","raw":"---\n\ntitle: CDH搭建细节\ndate: 2019-05-27\ntags: \n\n---\n.....\n<!--more-->\n\n## CDH安装准备\n\n![](http://img.wqkenqing.ren/Ibp691.png)\n\n\nubuntu ulimit","slug":"技术/hexo/CDH/搭建","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd32r001k38pwckauf8bb","content":"<p>…..</p>\n<a id=\"more\"></a>\n\n<h2 id=\"CDH安装准备\"><a href=\"#CDH安装准备\" class=\"headerlink\" title=\"CDH安装准备\"></a>CDH安装准备</h2><p><img src=\"http://img.wqkenqing.ren/Ibp691.png\" alt=\"\"></p>\n<p>ubuntu ulimit</p>\n","site":{"data":{}},"excerpt":"<p>…..</p>","more":"<h2 id=\"CDH安装准备\"><a href=\"#CDH安装准备\" class=\"headerlink\" title=\"CDH安装准备\"></a>CDH安装准备</h2><p><img src=\"http://img.wqkenqing.ren/Ibp691.png\" alt=\"\"></p>\n<p>ubuntu ulimit</p>"},{"title":"hadoop高可用模式搭建","date":"2019-05-15T16:00:00.000Z","_content":"\n发现对hadoop的相关版本的组件,进程还有些模糊,借着针对hadoopHA模式搭建的过程,对hadoop\n进行一次细统的回顾.\n\n<!--more-->\n\n# hadoop HA搭建与总结\n\n## 什么是HA\n\nHA即高可用\n\n## HA相关配置\n\n### core-site.xml\n\n基本一致\n\n### hdfs-site.xml\n\n这里有明显差别\nhadoop2.X与hadoop1.X的高可能中的明显差异就是从这里开始的.\n2.x 引入了nameservice. 该nameservice可支持最大两个namenode.\n1.x img 和edits统一放置在namenode上.\n2.x 则通过journalnodes来共享edits日志.\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<!--\n            Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n\n<!-- Put site-specific property overrides in this file. -->\n<configuration>\n <!-- 为namenode集群定义一个services name -->\n  <property>\n    <name>dfs.nameservices</name>\n    <value>ns1</value>\n  </property>\n  \n    <!-- nameservice 包含哪些namenode，为各个namenode起名 -->\n    <property>\n      <name>dfs.ha.namenodes.ns1</name>\n      <value>namenode,datanode1</value>\n    </property>\n  \n   <!-- 名为master188的namenode的rpc地址和端口号，rpc用来和datanode通讯 -->\n    <property>\n      <name>dfs.namenode.rpc-address.ns1.namenode</name>\n      <value>namenode:9000</value>\n    </property>\n  \n    <!-- 名为master189的namenode的rpc地址和端口号，rpc用来和datanode通讯 -->\n    <property>\n      <name>dfs.namenode.rpc-address.ns1.datanode1</name>\n      <value>datanode1:9000</value>\n    </property>\n\n  <!--名为master188的namenode的http地址和端口号，用来和web客户端通讯 -->\n  <property>\n    <name>dfs.namenode.http-address.ns1.namenode</name>\n    <value>namenode:50070</value>\n  </property>\n\n  <!-- 名为master189的namenode的http地址和端口号，用来和web客户端通讯 -->\n  <property>\n    <name>dfs.namenode.http-address.ns1.datanode1</name>\n    <value>datanode1:50070</value>\n  </property>\n\n <!-- namenode间用于共享编辑日志的journal节点列表 -->\n  <property>\n    <name>dfs.namenode.shared.edits.dir</name>\n    <value>qjournal://namenode:8485;datanode1:8485;datanode2:8485/ns1</value>\n  </property>\n  \n    <!-- 指定该集群出现故障时，是否自动切换到另一台namenode -->\n    <property>\n      <name>dfs.ha.automatic-failover.enabled.ns1</name>\n      <value>true</value>\n    </property>\n    \n    \n      <!-- journalnode 上用于存放edits日志的目录 -->\n      <property>\n        <name>dfs.journalnode.edits.dir</name>\n        <value>/home/hadoop/hadoop_store/dfs/data/dfs/journalnode</value>\n      </property>\n  \n  <!-- 客户端连接可用状态的NameNode所用的代理类 -->\n    <property>\n      <name>dfs.client.failover.proxy.provider.ns1</name>\n      <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n    </property>\n  \n    <!-- 一旦需要NameNode切换，使用ssh方式进行操作 -->\n    <property>\n      <name>dfs.ha.fencing.methods</name>\n      <value>sshfence</value>\n    </property>\n  \n    <!-- 如果使用ssh进行故障切换，使用ssh通信时用的密钥存储的位置 -->\n    <property>\n      <name>dfs.ha.fencing.ssh.private-key-files</name>\n      <value>/home/hadoop/.ssh/id_rsa</value>\n    </property>\n  \n    <!-- connect-timeout超时时间 -->\n    <property>\n      <name>dfs.ha.fencing.ssh.connect-timeout</name>\n      <value>30000</value>\n    </property>\n       <property>\n                <name>dfs.name.dir</name>\n                <value>/home/hadoop/hadoop_store/dfs/name</value>\n        </property>\n        <property>\n                <name>dfs.data.dir</name>\n                <value>/home/hadoop/hadoop_store/dfs/data</value>\n        </property>\n        <property>\n                <name>dfs.permissions</name>\n                <value>false</value>\n        </property>\n<property>\n\n<name>dfs.replication</name>\n\n<value>3</value>\n\n</property>\n</configuration>\n\n```\n\n### mapreduce-site.xml\n变动不大\n### yarn-site.xml\n\n```xml\n\n<?xml version=\"1.0\"?>\n<!--\n            Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n<configuration>\n    \n    \n\n       <!-- 启用HA高可用性 -->\n        <property>\n          <name>yarn.resourcemanager.ha.enabled</name>\n          <value>true</value>\n        </property>\n      \n        <!-- 指定resourcemanager的名字 -->\n        <property>\n          <name>yarn.resourcemanager.cluster-id</name>\n          <value>yrc</value>\n        </property>\n      \n        <!-- 使用了2个resourcemanager,分别指定Resourcemanager的地址 -->\n        <property>\n          <name>yarn.resourcemanager.ha.rm-ids</name>\n          <value>rm1,rm2</value>\n        </property>\n        \n        <!-- 指定rm1的地址 -->\n        <property>\n          <name>yarn.resourcemanager.hostname.rm1</name>\n          <value>namenode</value>\n        </property>\n        \n        <!-- 指定rm2的地址  -->\n        <property>\n          <name>yarn.resourcemanager.hostname.rm2</name>\n          <value>datanode1</value>\n        </property>\n        \n        <!-- 指定当前机器master188作为rm1 -->\n        <property>\n          <name>yarn.resourcemanager.ha.id</name>\n          <value>rm1</value>\n        </property>\n        \n        <!-- 指定zookeeper集群机器 -->\n        <property>\n          <name>yarn.resourcemanager.zk-address</name>\n          <value>namenode:2181,datanode1:2181,datanode2:2181</value>\n        </property>\n        \n        <!-- NodeManager上运行的附属服务，默认是mapreduce_shuffle -->\n        <property>\n          <name>yarn.nodemanager.aux-services</name>\n          <value>mapreduce_shuffle</value>\n        </property>\n      \n        <property>\n                <name>yarn.resourcemanager.hostname</name>\n                <value>kuiqwang</value>\n        </property>\n        <property>\n                <name>yarn.nodemanager.aux-services</name>\n                <value>mapreduce_shuffle</value>\n        </property>\n  <property>\n    <name>yarn.log.server.url</name>\n    <value>http://namenode:19888/tmp/logs/hadoop/logs/</value>\n  </property>\n\n                <property>\n                <name>yarn.nodemanager.local-dirs</name>\n                <value>/home/hadoop/hadoop_store/logs/yarn</value>\n        </property>\n           <property>\n                <name>yarn.nodemanager.log-dirs</name>\n                <value>/home/hadoop/hadoop_store/logs/userlogs</value>\n        </property>\n<!--内存,核数大小配置 -->\n\n        <property>\n      <name>yarn.nodemanager.resource.memory-mb</name>\n      <value>4096</value>\n  </property>\n\n  <property>\n      <name>yarn.scheduler.minimum-allocation-mb</name>\n      <value>1024</value>\n  </property>\n  <property>\n      <name>yarn.scheduler.maximum-allocation-mb</name>\n      <value>3072</value>\n  </property>\n  <property>\n      <name>yarn.app.mapreduce.am.resource.mb</name>\n      <value>3072</value>\n  </property>\n  <property>\n      <name>yarn.app.mapreduce.am.command-opts</name>\n      <value>-Xmx3276m</value>\n  </property>\n    <property>\n      <name>yarn.nodemanager.resource.cpu-vcores</name>\n      <value>2</value>\n  </property>\n  <property>\n      <name>yarn.scheduler.maximum-allocation-vcores</name>\n      <value>3</value>\n  </property>\n</configuration>\n\n\n```\n\n### HA过程中主要用到的操作命令\n\n当配置文件完成后,先启动journalnode,以助namenode 和standby node 共享edits文件\nhadoop-daemon.sh\n![](http://img.wqkenqing.ren/urHEX6.png)\n\n然后再进行namdnode格式化,hadoop namenode -format\n进行namenode格式化\n当namenode格式化完成后可以先启动该节点的namenode\nhadoop-daemon.sh start namenode\n然后再在另一namdnode节点执行\nhdfs namenode -bootstrapStandby\n到这可以将之前的journalnode停用,然后start-dfs.sh\n\n因为要用到zookeeper协助同步配置文件与操作日志,所以这里可以先对zookeeper进行hdfs内容的格式化\nhdfs zkfc –formatZK\n然后启动FailOver进程\nhadoop-daemon.sh start zkfc\n![](http://img.wqkenqing.ren/lvuo9N.png)\n至此则是这些进程\n然后启用yarn.\n即\nstart-yarn.sh\n到这里HA过程中用到的一些常用指令大致总结完成\n\n---\n至此 hadoop HA的常规总结完成.后续再补充一些细节,如standy 节点切的,与切换机制.HA背后的运作机制,与效果\n\n\n","source":"_posts/技术/hexo/hadoop/hadoopHA搭建.md","raw":"---\n\ntitle: hadoop高可用模式搭建\ndate: 2019-05-16\ntags: \n\n---\n\n发现对hadoop的相关版本的组件,进程还有些模糊,借着针对hadoopHA模式搭建的过程,对hadoop\n进行一次细统的回顾.\n\n<!--more-->\n\n# hadoop HA搭建与总结\n\n## 什么是HA\n\nHA即高可用\n\n## HA相关配置\n\n### core-site.xml\n\n基本一致\n\n### hdfs-site.xml\n\n这里有明显差别\nhadoop2.X与hadoop1.X的高可能中的明显差异就是从这里开始的.\n2.x 引入了nameservice. 该nameservice可支持最大两个namenode.\n1.x img 和edits统一放置在namenode上.\n2.x 则通过journalnodes来共享edits日志.\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<!--\n            Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n\n<!-- Put site-specific property overrides in this file. -->\n<configuration>\n <!-- 为namenode集群定义一个services name -->\n  <property>\n    <name>dfs.nameservices</name>\n    <value>ns1</value>\n  </property>\n  \n    <!-- nameservice 包含哪些namenode，为各个namenode起名 -->\n    <property>\n      <name>dfs.ha.namenodes.ns1</name>\n      <value>namenode,datanode1</value>\n    </property>\n  \n   <!-- 名为master188的namenode的rpc地址和端口号，rpc用来和datanode通讯 -->\n    <property>\n      <name>dfs.namenode.rpc-address.ns1.namenode</name>\n      <value>namenode:9000</value>\n    </property>\n  \n    <!-- 名为master189的namenode的rpc地址和端口号，rpc用来和datanode通讯 -->\n    <property>\n      <name>dfs.namenode.rpc-address.ns1.datanode1</name>\n      <value>datanode1:9000</value>\n    </property>\n\n  <!--名为master188的namenode的http地址和端口号，用来和web客户端通讯 -->\n  <property>\n    <name>dfs.namenode.http-address.ns1.namenode</name>\n    <value>namenode:50070</value>\n  </property>\n\n  <!-- 名为master189的namenode的http地址和端口号，用来和web客户端通讯 -->\n  <property>\n    <name>dfs.namenode.http-address.ns1.datanode1</name>\n    <value>datanode1:50070</value>\n  </property>\n\n <!-- namenode间用于共享编辑日志的journal节点列表 -->\n  <property>\n    <name>dfs.namenode.shared.edits.dir</name>\n    <value>qjournal://namenode:8485;datanode1:8485;datanode2:8485/ns1</value>\n  </property>\n  \n    <!-- 指定该集群出现故障时，是否自动切换到另一台namenode -->\n    <property>\n      <name>dfs.ha.automatic-failover.enabled.ns1</name>\n      <value>true</value>\n    </property>\n    \n    \n      <!-- journalnode 上用于存放edits日志的目录 -->\n      <property>\n        <name>dfs.journalnode.edits.dir</name>\n        <value>/home/hadoop/hadoop_store/dfs/data/dfs/journalnode</value>\n      </property>\n  \n  <!-- 客户端连接可用状态的NameNode所用的代理类 -->\n    <property>\n      <name>dfs.client.failover.proxy.provider.ns1</name>\n      <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n    </property>\n  \n    <!-- 一旦需要NameNode切换，使用ssh方式进行操作 -->\n    <property>\n      <name>dfs.ha.fencing.methods</name>\n      <value>sshfence</value>\n    </property>\n  \n    <!-- 如果使用ssh进行故障切换，使用ssh通信时用的密钥存储的位置 -->\n    <property>\n      <name>dfs.ha.fencing.ssh.private-key-files</name>\n      <value>/home/hadoop/.ssh/id_rsa</value>\n    </property>\n  \n    <!-- connect-timeout超时时间 -->\n    <property>\n      <name>dfs.ha.fencing.ssh.connect-timeout</name>\n      <value>30000</value>\n    </property>\n       <property>\n                <name>dfs.name.dir</name>\n                <value>/home/hadoop/hadoop_store/dfs/name</value>\n        </property>\n        <property>\n                <name>dfs.data.dir</name>\n                <value>/home/hadoop/hadoop_store/dfs/data</value>\n        </property>\n        <property>\n                <name>dfs.permissions</name>\n                <value>false</value>\n        </property>\n<property>\n\n<name>dfs.replication</name>\n\n<value>3</value>\n\n</property>\n</configuration>\n\n```\n\n### mapreduce-site.xml\n变动不大\n### yarn-site.xml\n\n```xml\n\n<?xml version=\"1.0\"?>\n<!--\n            Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n<configuration>\n    \n    \n\n       <!-- 启用HA高可用性 -->\n        <property>\n          <name>yarn.resourcemanager.ha.enabled</name>\n          <value>true</value>\n        </property>\n      \n        <!-- 指定resourcemanager的名字 -->\n        <property>\n          <name>yarn.resourcemanager.cluster-id</name>\n          <value>yrc</value>\n        </property>\n      \n        <!-- 使用了2个resourcemanager,分别指定Resourcemanager的地址 -->\n        <property>\n          <name>yarn.resourcemanager.ha.rm-ids</name>\n          <value>rm1,rm2</value>\n        </property>\n        \n        <!-- 指定rm1的地址 -->\n        <property>\n          <name>yarn.resourcemanager.hostname.rm1</name>\n          <value>namenode</value>\n        </property>\n        \n        <!-- 指定rm2的地址  -->\n        <property>\n          <name>yarn.resourcemanager.hostname.rm2</name>\n          <value>datanode1</value>\n        </property>\n        \n        <!-- 指定当前机器master188作为rm1 -->\n        <property>\n          <name>yarn.resourcemanager.ha.id</name>\n          <value>rm1</value>\n        </property>\n        \n        <!-- 指定zookeeper集群机器 -->\n        <property>\n          <name>yarn.resourcemanager.zk-address</name>\n          <value>namenode:2181,datanode1:2181,datanode2:2181</value>\n        </property>\n        \n        <!-- NodeManager上运行的附属服务，默认是mapreduce_shuffle -->\n        <property>\n          <name>yarn.nodemanager.aux-services</name>\n          <value>mapreduce_shuffle</value>\n        </property>\n      \n        <property>\n                <name>yarn.resourcemanager.hostname</name>\n                <value>kuiqwang</value>\n        </property>\n        <property>\n                <name>yarn.nodemanager.aux-services</name>\n                <value>mapreduce_shuffle</value>\n        </property>\n  <property>\n    <name>yarn.log.server.url</name>\n    <value>http://namenode:19888/tmp/logs/hadoop/logs/</value>\n  </property>\n\n                <property>\n                <name>yarn.nodemanager.local-dirs</name>\n                <value>/home/hadoop/hadoop_store/logs/yarn</value>\n        </property>\n           <property>\n                <name>yarn.nodemanager.log-dirs</name>\n                <value>/home/hadoop/hadoop_store/logs/userlogs</value>\n        </property>\n<!--内存,核数大小配置 -->\n\n        <property>\n      <name>yarn.nodemanager.resource.memory-mb</name>\n      <value>4096</value>\n  </property>\n\n  <property>\n      <name>yarn.scheduler.minimum-allocation-mb</name>\n      <value>1024</value>\n  </property>\n  <property>\n      <name>yarn.scheduler.maximum-allocation-mb</name>\n      <value>3072</value>\n  </property>\n  <property>\n      <name>yarn.app.mapreduce.am.resource.mb</name>\n      <value>3072</value>\n  </property>\n  <property>\n      <name>yarn.app.mapreduce.am.command-opts</name>\n      <value>-Xmx3276m</value>\n  </property>\n    <property>\n      <name>yarn.nodemanager.resource.cpu-vcores</name>\n      <value>2</value>\n  </property>\n  <property>\n      <name>yarn.scheduler.maximum-allocation-vcores</name>\n      <value>3</value>\n  </property>\n</configuration>\n\n\n```\n\n### HA过程中主要用到的操作命令\n\n当配置文件完成后,先启动journalnode,以助namenode 和standby node 共享edits文件\nhadoop-daemon.sh\n![](http://img.wqkenqing.ren/urHEX6.png)\n\n然后再进行namdnode格式化,hadoop namenode -format\n进行namenode格式化\n当namenode格式化完成后可以先启动该节点的namenode\nhadoop-daemon.sh start namenode\n然后再在另一namdnode节点执行\nhdfs namenode -bootstrapStandby\n到这可以将之前的journalnode停用,然后start-dfs.sh\n\n因为要用到zookeeper协助同步配置文件与操作日志,所以这里可以先对zookeeper进行hdfs内容的格式化\nhdfs zkfc –formatZK\n然后启动FailOver进程\nhadoop-daemon.sh start zkfc\n![](http://img.wqkenqing.ren/lvuo9N.png)\n至此则是这些进程\n然后启用yarn.\n即\nstart-yarn.sh\n到这里HA过程中用到的一些常用指令大致总结完成\n\n---\n至此 hadoop HA的常规总结完成.后续再补充一些细节,如standy 节点切的,与切换机制.HA背后的运作机制,与效果\n\n\n","slug":"技术/hexo/hadoop/hadoopHA搭建","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd32s001m38pw82jt91yc","content":"<p>发现对hadoop的相关版本的组件,进程还有些模糊,借着针对hadoopHA模式搭建的过程,对hadoop<br>进行一次细统的回顾.</p>\n<a id=\"more\"></a>\n\n<h1 id=\"hadoop-HA搭建与总结\"><a href=\"#hadoop-HA搭建与总结\" class=\"headerlink\" title=\"hadoop HA搭建与总结\"></a>hadoop HA搭建与总结</h1><h2 id=\"什么是HA\"><a href=\"#什么是HA\" class=\"headerlink\" title=\"什么是HA\"></a>什么是HA</h2><p>HA即高可用</p>\n<h2 id=\"HA相关配置\"><a href=\"#HA相关配置\" class=\"headerlink\" title=\"HA相关配置\"></a>HA相关配置</h2><h3 id=\"core-site-xml\"><a href=\"#core-site-xml\" class=\"headerlink\" title=\"core-site.xml\"></a>core-site.xml</h3><p>基本一致</p>\n<h3 id=\"hdfs-site-xml\"><a href=\"#hdfs-site-xml\" class=\"headerlink\" title=\"hdfs-site.xml\"></a>hdfs-site.xml</h3><p>这里有明显差别<br>hadoop2.X与hadoop1.X的高可能中的明显差异就是从这里开始的.<br>2.x 引入了nameservice. 该nameservice可支持最大两个namenode.<br>1.x img 和edits统一放置在namenode上.<br>2.x 则通过journalnodes来共享edits日志.</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;</span></span><br><span class=\"line\"><span class=\"meta\">&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;!--</span></span><br><span class=\"line\"><span class=\"comment\">            Licensed under the Apache License, Version 2.0 (the \"License\");</span></span><br><span class=\"line\"><span class=\"comment\">  you may not use this file except in compliance with the License.</span></span><br><span class=\"line\"><span class=\"comment\">  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\">  distributed under the License is distributed on an \"AS IS\" BASIS,</span></span><br><span class=\"line\"><span class=\"comment\">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\">  See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class=\"line\"><span class=\"comment\">--&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\"> <span class=\"comment\">&lt;!-- 为namenode集群定义一个services name --&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.nameservices<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>ns1<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">&lt;!-- nameservice 包含哪些namenode，为各个namenode起名 --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.ha.namenodes.ns1<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>namenode,datanode1<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">   <span class=\"comment\">&lt;!-- 名为master188的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.namenode<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>namenode:9000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">&lt;!-- 名为master189的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.datanode1<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>datanode1:9000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">&lt;!--名为master188的namenode的http地址和端口号，用来和web客户端通讯 --&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.http-address.ns1.namenode<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>namenode:50070<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">&lt;!-- 名为master189的namenode的http地址和端口号，用来和web客户端通讯 --&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.http-address.ns1.datanode1<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>datanode1:50070<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"comment\">&lt;!-- namenode间用于共享编辑日志的journal节点列表 --&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>qjournal://namenode:8485;datanode1:8485;datanode2:8485/ns1<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">&lt;!-- 指定该集群出现故障时，是否自动切换到另一台namenode --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.ha.automatic-failover.enabled.ns1<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\">      <span class=\"comment\">&lt;!-- journalnode 上用于存放edits日志的目录 --&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.journalnode.edits.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/hadoop/hadoop_store/dfs/data/dfs/journalnode<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"comment\">&lt;!-- 客户端连接可用状态的NameNode所用的代理类 --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns1<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">&lt;!-- 一旦需要NameNode切换，使用ssh方式进行操作 --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.ha.fencing.methods<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>sshfence<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">&lt;!-- 如果使用ssh进行故障切换，使用ssh通信时用的密钥存储的位置 --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/hadoop/.ssh/id_rsa<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">&lt;!-- connect-timeout超时时间 --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>30000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.name.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/hadoop/hadoop_store/dfs/name<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.data.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/hadoop/hadoop_store/dfs/data<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.permissions<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.replication<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"mapreduce-site-xml\"><a href=\"#mapreduce-site-xml\" class=\"headerlink\" title=\"mapreduce-site.xml\"></a>mapreduce-site.xml</h3><p>变动不大</p>\n<h3 id=\"yarn-site-xml\"><a href=\"#yarn-site-xml\" class=\"headerlink\" title=\"yarn-site.xml\"></a>yarn-site.xml</h3><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">&lt;?xml version=\"1.0\"?&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;!--</span></span><br><span class=\"line\"><span class=\"comment\">            Licensed under the Apache License, Version 2.0 (the \"License\");</span></span><br><span class=\"line\"><span class=\"comment\">  you may not use this file except in compliance with the License.</span></span><br><span class=\"line\"><span class=\"comment\">  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\">  distributed under the License is distributed on an \"AS IS\" BASIS,</span></span><br><span class=\"line\"><span class=\"comment\">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\">  See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class=\"line\"><span class=\"comment\">--&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\"></span><br><span class=\"line\">       <span class=\"comment\">&lt;!-- 启用HA高可用性 --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      </span><br><span class=\"line\">        <span class=\"comment\">&lt;!-- 指定resourcemanager的名字 --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>yrc<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      </span><br><span class=\"line\">        <span class=\"comment\">&lt;!-- 使用了2个resourcemanager,分别指定Resourcemanager的地址 --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>rm1,rm2<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">&lt;!-- 指定rm1的地址 --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>namenode<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">&lt;!-- 指定rm2的地址  --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>datanode1<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">&lt;!-- 指定当前机器master188作为rm1 --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.ha.id<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>rm1<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">&lt;!-- 指定zookeeper集群机器 --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>namenode:2181,datanode1:2181,datanode2:2181<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">&lt;!-- NodeManager上运行的附属服务，默认是mapreduce_shuffle --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce_shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      </span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.hostname<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>kuiqwang<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce_shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.log.server.url<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>http://namenode:19888/tmp/logs/hadoop/logs/<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.local-dirs<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/hadoop/hadoop_store/logs/yarn<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">           <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.log-dirs<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/hadoop/hadoop_store/logs/userlogs<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;!--内存,核数大小配置 --&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>4096<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1024<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3072<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.app.mapreduce.am.resource.mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3072<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.app.mapreduce.am.command-opts<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>-Xmx3276m<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"HA过程中主要用到的操作命令\"><a href=\"#HA过程中主要用到的操作命令\" class=\"headerlink\" title=\"HA过程中主要用到的操作命令\"></a>HA过程中主要用到的操作命令</h3><p>当配置文件完成后,先启动journalnode,以助namenode 和standby node 共享edits文件<br>hadoop-daemon.sh<br><img src=\"http://img.wqkenqing.ren/urHEX6.png\" alt=\"\"></p>\n<p>然后再进行namdnode格式化,hadoop namenode -format<br>进行namenode格式化<br>当namenode格式化完成后可以先启动该节点的namenode<br>hadoop-daemon.sh start namenode<br>然后再在另一namdnode节点执行<br>hdfs namenode -bootstrapStandby<br>到这可以将之前的journalnode停用,然后start-dfs.sh</p>\n<p>因为要用到zookeeper协助同步配置文件与操作日志,所以这里可以先对zookeeper进行hdfs内容的格式化<br>hdfs zkfc –formatZK<br>然后启动FailOver进程<br>hadoop-daemon.sh start zkfc<br><img src=\"http://img.wqkenqing.ren/lvuo9N.png\" alt=\"\"><br>至此则是这些进程<br>然后启用yarn.<br>即<br>start-yarn.sh<br>到这里HA过程中用到的一些常用指令大致总结完成</p>\n<hr>\n<p>至此 hadoop HA的常规总结完成.后续再补充一些细节,如standy 节点切的,与切换机制.HA背后的运作机制,与效果</p>\n","site":{"data":{}},"excerpt":"<p>发现对hadoop的相关版本的组件,进程还有些模糊,借着针对hadoopHA模式搭建的过程,对hadoop<br>进行一次细统的回顾.</p>","more":"<h1 id=\"hadoop-HA搭建与总结\"><a href=\"#hadoop-HA搭建与总结\" class=\"headerlink\" title=\"hadoop HA搭建与总结\"></a>hadoop HA搭建与总结</h1><h2 id=\"什么是HA\"><a href=\"#什么是HA\" class=\"headerlink\" title=\"什么是HA\"></a>什么是HA</h2><p>HA即高可用</p>\n<h2 id=\"HA相关配置\"><a href=\"#HA相关配置\" class=\"headerlink\" title=\"HA相关配置\"></a>HA相关配置</h2><h3 id=\"core-site-xml\"><a href=\"#core-site-xml\" class=\"headerlink\" title=\"core-site.xml\"></a>core-site.xml</h3><p>基本一致</p>\n<h3 id=\"hdfs-site-xml\"><a href=\"#hdfs-site-xml\" class=\"headerlink\" title=\"hdfs-site.xml\"></a>hdfs-site.xml</h3><p>这里有明显差别<br>hadoop2.X与hadoop1.X的高可能中的明显差异就是从这里开始的.<br>2.x 引入了nameservice. 该nameservice可支持最大两个namenode.<br>1.x img 和edits统一放置在namenode上.<br>2.x 则通过journalnodes来共享edits日志.</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;</span></span><br><span class=\"line\"><span class=\"meta\">&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;!--</span></span><br><span class=\"line\"><span class=\"comment\">            Licensed under the Apache License, Version 2.0 (the \"License\");</span></span><br><span class=\"line\"><span class=\"comment\">  you may not use this file except in compliance with the License.</span></span><br><span class=\"line\"><span class=\"comment\">  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\">  distributed under the License is distributed on an \"AS IS\" BASIS,</span></span><br><span class=\"line\"><span class=\"comment\">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\">  See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class=\"line\"><span class=\"comment\">--&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\"> <span class=\"comment\">&lt;!-- 为namenode集群定义一个services name --&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.nameservices<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>ns1<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">&lt;!-- nameservice 包含哪些namenode，为各个namenode起名 --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.ha.namenodes.ns1<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>namenode,datanode1<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">   <span class=\"comment\">&lt;!-- 名为master188的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.namenode<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>namenode:9000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">&lt;!-- 名为master189的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.datanode1<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>datanode1:9000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">&lt;!--名为master188的namenode的http地址和端口号，用来和web客户端通讯 --&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.http-address.ns1.namenode<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>namenode:50070<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">&lt;!-- 名为master189的namenode的http地址和端口号，用来和web客户端通讯 --&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.http-address.ns1.datanode1<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>datanode1:50070<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"> <span class=\"comment\">&lt;!-- namenode间用于共享编辑日志的journal节点列表 --&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>qjournal://namenode:8485;datanode1:8485;datanode2:8485/ns1<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">&lt;!-- 指定该集群出现故障时，是否自动切换到另一台namenode --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.ha.automatic-failover.enabled.ns1<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\">      <span class=\"comment\">&lt;!-- journalnode 上用于存放edits日志的目录 --&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.journalnode.edits.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/hadoop/hadoop_store/dfs/data/dfs/journalnode<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  <span class=\"comment\">&lt;!-- 客户端连接可用状态的NameNode所用的代理类 --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns1<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">&lt;!-- 一旦需要NameNode切换，使用ssh方式进行操作 --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.ha.fencing.methods<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>sshfence<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">&lt;!-- 如果使用ssh进行故障切换，使用ssh通信时用的密钥存储的位置 --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/hadoop/.ssh/id_rsa<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">&lt;!-- connect-timeout超时时间 --&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>30000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">       <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.name.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/hadoop/hadoop_store/dfs/name<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.data.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/hadoop/hadoop_store/dfs/data<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.permissions<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.replication<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"mapreduce-site-xml\"><a href=\"#mapreduce-site-xml\" class=\"headerlink\" title=\"mapreduce-site.xml\"></a>mapreduce-site.xml</h3><p>变动不大</p>\n<h3 id=\"yarn-site-xml\"><a href=\"#yarn-site-xml\" class=\"headerlink\" title=\"yarn-site.xml\"></a>yarn-site.xml</h3><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">&lt;?xml version=\"1.0\"?&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;!--</span></span><br><span class=\"line\"><span class=\"comment\">            Licensed under the Apache License, Version 2.0 (the \"License\");</span></span><br><span class=\"line\"><span class=\"comment\">  you may not use this file except in compliance with the License.</span></span><br><span class=\"line\"><span class=\"comment\">  You may obtain a copy of the License at</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class=\"line\"><span class=\"comment\"></span></span><br><span class=\"line\"><span class=\"comment\">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class=\"line\"><span class=\"comment\">  distributed under the License is distributed on an \"AS IS\" BASIS,</span></span><br><span class=\"line\"><span class=\"comment\">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class=\"line\"><span class=\"comment\">  See the License for the specific language governing permissions and</span></span><br><span class=\"line\"><span class=\"comment\">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class=\"line\"><span class=\"comment\">--&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\"></span><br><span class=\"line\">       <span class=\"comment\">&lt;!-- 启用HA高可用性 --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      </span><br><span class=\"line\">        <span class=\"comment\">&lt;!-- 指定resourcemanager的名字 --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>yrc<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      </span><br><span class=\"line\">        <span class=\"comment\">&lt;!-- 使用了2个resourcemanager,分别指定Resourcemanager的地址 --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>rm1,rm2<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">&lt;!-- 指定rm1的地址 --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>namenode<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">&lt;!-- 指定rm2的地址  --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>datanode1<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">&lt;!-- 指定当前机器master188作为rm1 --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.ha.id<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>rm1<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">&lt;!-- 指定zookeeper集群机器 --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>namenode:2181,datanode1:2181,datanode2:2181<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">&lt;!-- NodeManager上运行的附属服务，默认是mapreduce_shuffle --&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce_shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      </span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.hostname<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>kuiqwang<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce_shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.log.server.url<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>http://namenode:19888/tmp/logs/hadoop/logs/<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.local-dirs<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/hadoop/hadoop_store/logs/yarn<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">           <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.log-dirs<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/hadoop/hadoop_store/logs/userlogs<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">        <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"comment\">&lt;!--内存,核数大小配置 --&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>4096<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1024<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3072<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.app.mapreduce.am.resource.mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3072<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.app.mapreduce.am.command-opts<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>-Xmx3276m<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>3<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"HA过程中主要用到的操作命令\"><a href=\"#HA过程中主要用到的操作命令\" class=\"headerlink\" title=\"HA过程中主要用到的操作命令\"></a>HA过程中主要用到的操作命令</h3><p>当配置文件完成后,先启动journalnode,以助namenode 和standby node 共享edits文件<br>hadoop-daemon.sh<br><img src=\"http://img.wqkenqing.ren/urHEX6.png\" alt=\"\"></p>\n<p>然后再进行namdnode格式化,hadoop namenode -format<br>进行namenode格式化<br>当namenode格式化完成后可以先启动该节点的namenode<br>hadoop-daemon.sh start namenode<br>然后再在另一namdnode节点执行<br>hdfs namenode -bootstrapStandby<br>到这可以将之前的journalnode停用,然后start-dfs.sh</p>\n<p>因为要用到zookeeper协助同步配置文件与操作日志,所以这里可以先对zookeeper进行hdfs内容的格式化<br>hdfs zkfc –formatZK<br>然后启动FailOver进程<br>hadoop-daemon.sh start zkfc<br><img src=\"http://img.wqkenqing.ren/lvuo9N.png\" alt=\"\"><br>至此则是这些进程<br>然后启用yarn.<br>即<br>start-yarn.sh<br>到这里HA过程中用到的一些常用指令大致总结完成</p>\n<hr>\n<p>至此 hadoop HA的常规总结完成.后续再补充一些细节,如standy 节点切的,与切换机制.HA背后的运作机制,与效果</p>"},{"title":"kafka学习","date":"2021-01-05T02:35:37.000Z","abstract":"kafka high level","_content":"\n[ x ]  Consumer Group里只会被某一个Consumer消费 ,Kafka还允许不同Consumer Group同时消费同一条消息，这一特性可以为消息的多元化处理提供支持。\n<!--more-->\n## kafka 发送模式\n通过producer.type设置,可以设置producer的发送模式,具体参数据有\nproducer.type=false即同步(默认就是同步),设置为true为异步,即以batch形式像broker发送信息.(这里的batch可以设置)\n还有一种oneway.即通过对ack的设置即可实现,ack=0时,即为oneway,只管发,不管是否接收成功.-1则是全部副本接收成功才算成功.\n\n## kakfa消费模式\n1. at last one\n2. at most one\n3. exactly one\n\n\n","source":"_posts/技术/hexo/kafka/kafka实现.md","raw":"---\ntitle: kafka学习\ndate: \ntags: kafka\nabstract: kafka high level\n---\n\n[ x ]  Consumer Group里只会被某一个Consumer消费 ,Kafka还允许不同Consumer Group同时消费同一条消息，这一特性可以为消息的多元化处理提供支持。\n<!--more-->\n## kafka 发送模式\n通过producer.type设置,可以设置producer的发送模式,具体参数据有\nproducer.type=false即同步(默认就是同步),设置为true为异步,即以batch形式像broker发送信息.(这里的batch可以设置)\n还有一种oneway.即通过对ack的设置即可实现,ack=0时,即为oneway,只管发,不管是否接收成功.-1则是全部副本接收成功才算成功.\n\n## kakfa消费模式\n1. at last one\n2. at most one\n3. exactly one\n\n\n","slug":"技术/hexo/kafka/kafka实现","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd32t001n38pwdw0586at","content":"<p>[ x ]  Consumer Group里只会被某一个Consumer消费 ,Kafka还允许不同Consumer Group同时消费同一条消息，这一特性可以为消息的多元化处理提供支持。</p>\n<a id=\"more\"></a>\n<h2 id=\"kafka-发送模式\"><a href=\"#kafka-发送模式\" class=\"headerlink\" title=\"kafka 发送模式\"></a>kafka 发送模式</h2><p>通过producer.type设置,可以设置producer的发送模式,具体参数据有<br>producer.type=false即同步(默认就是同步),设置为true为异步,即以batch形式像broker发送信息.(这里的batch可以设置)<br>还有一种oneway.即通过对ack的设置即可实现,ack=0时,即为oneway,只管发,不管是否接收成功.-1则是全部副本接收成功才算成功.</p>\n<h2 id=\"kakfa消费模式\"><a href=\"#kakfa消费模式\" class=\"headerlink\" title=\"kakfa消费模式\"></a>kakfa消费模式</h2><ol>\n<li>at last one</li>\n<li>at most one</li>\n<li>exactly one</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>[ x ]  Consumer Group里只会被某一个Consumer消费 ,Kafka还允许不同Consumer Group同时消费同一条消息，这一特性可以为消息的多元化处理提供支持。</p>","more":"<h2 id=\"kafka-发送模式\"><a href=\"#kafka-发送模式\" class=\"headerlink\" title=\"kafka 发送模式\"></a>kafka 发送模式</h2><p>通过producer.type设置,可以设置producer的发送模式,具体参数据有<br>producer.type=false即同步(默认就是同步),设置为true为异步,即以batch形式像broker发送信息.(这里的batch可以设置)<br>还有一种oneway.即通过对ack的设置即可实现,ack=0时,即为oneway,只管发,不管是否接收成功.-1则是全部副本接收成功才算成功.</p>\n<h2 id=\"kakfa消费模式\"><a href=\"#kakfa消费模式\" class=\"headerlink\" title=\"kakfa消费模式\"></a>kakfa消费模式</h2><ol>\n<li>at last one</li>\n<li>at most one</li>\n<li>exactly one</li>\n</ol>"},{"title":"Lambda&Stream.md","abstract":"Lambda积累","_content":"\n# Lambda&Stream积累\n\n<!-- more -->\n## Lambda\nLambda主要是一个类语法长糖,尽量为java引入函数编程等实现,细节后续再补充\n\n## Stream\njava8提拱的新特性之一就有stream.stream主要是针对集合的处理类.提供了一系列集合处理方式.\n配合使用lambda写出简介优美的代码\n\n### Stream的使用\n\n通过如\n``` java\n\nList<Integer> list = new ArrayList<>();\nlist.stream();//即可以开启串行流;\nlist.parallelStream().filter(a -> {\n            return a > 20;\n        });//开启并行流\n```\n串行流即内部单线程顺序执行,并行则是启用多线程执行.\n后者并不一定效率就比前者高.因为并行执行启用分配线程资源时同样要消耗时间和资源,在一定量级下,前者的执行效率一度要高过后者.        \n\n我这里对三种对集合的处理形式的比较,可以简单参考一下\n1. stream 串行流\n2. parallelStream 并行流\n3. 常规循环式\n\n``` java\n\n  List<Integer> list = new ArrayList<>();\n        for (int i = 0; i < 100000; i++) {\n            list.add(getRandomNum());\n        }\n\n        DateUtil.setBegin();\n        list.stream().filter(a -> {\n            return a > 20;\n        });\n\n        DateUtil.setStop();\n        System.out.println(\"串行耗时\"+DateUtil.calCostTime());\n\n        DateUtil.setBegin();\n        list.parallelStream().filter(a -> {\n            return a > 20;\n        });\n        DateUtil.setStop();\n        System.out.println(\"并行耗时\"+DateUtil.calCostTime());\n        int count = 0;\n\n\n        DateUtil.setBegin();\n        for (int l : list) {\n            if (l > 20) {\n                count++;\n            }\n        }\n        DateUtil.setStop();\n        System.out.println(\"循环耗时\"+DateUtil.calCostTime());\n\n```\n经由相当量次的测试后,我觉得如果要对集合中的数据进行遍历操作,根据量级的不同,\n建议低量级还是采用普通循环,量级特别大,可考虑用并行流.书写方便,又不是大批量数据处理操作\n可以直接采用串行流\n\n### Stream的操作分类\n1. Intermediate\n2. Terminal\n3. Short-circuiting\n","source":"_posts/技术/hexo/old/Lamda积累.md","raw":"---\ntitle: Lambda&Stream.md\ntags: 日常总结\nabstract: Lambda积累\n---\n\n# Lambda&Stream积累\n\n<!-- more -->\n## Lambda\nLambda主要是一个类语法长糖,尽量为java引入函数编程等实现,细节后续再补充\n\n## Stream\njava8提拱的新特性之一就有stream.stream主要是针对集合的处理类.提供了一系列集合处理方式.\n配合使用lambda写出简介优美的代码\n\n### Stream的使用\n\n通过如\n``` java\n\nList<Integer> list = new ArrayList<>();\nlist.stream();//即可以开启串行流;\nlist.parallelStream().filter(a -> {\n            return a > 20;\n        });//开启并行流\n```\n串行流即内部单线程顺序执行,并行则是启用多线程执行.\n后者并不一定效率就比前者高.因为并行执行启用分配线程资源时同样要消耗时间和资源,在一定量级下,前者的执行效率一度要高过后者.        \n\n我这里对三种对集合的处理形式的比较,可以简单参考一下\n1. stream 串行流\n2. parallelStream 并行流\n3. 常规循环式\n\n``` java\n\n  List<Integer> list = new ArrayList<>();\n        for (int i = 0; i < 100000; i++) {\n            list.add(getRandomNum());\n        }\n\n        DateUtil.setBegin();\n        list.stream().filter(a -> {\n            return a > 20;\n        });\n\n        DateUtil.setStop();\n        System.out.println(\"串行耗时\"+DateUtil.calCostTime());\n\n        DateUtil.setBegin();\n        list.parallelStream().filter(a -> {\n            return a > 20;\n        });\n        DateUtil.setStop();\n        System.out.println(\"并行耗时\"+DateUtil.calCostTime());\n        int count = 0;\n\n\n        DateUtil.setBegin();\n        for (int l : list) {\n            if (l > 20) {\n                count++;\n            }\n        }\n        DateUtil.setStop();\n        System.out.println(\"循环耗时\"+DateUtil.calCostTime());\n\n```\n经由相当量次的测试后,我觉得如果要对集合中的数据进行遍历操作,根据量级的不同,\n建议低量级还是采用普通循环,量级特别大,可考虑用并行流.书写方便,又不是大批量数据处理操作\n可以直接采用串行流\n\n### Stream的操作分类\n1. Intermediate\n2. Terminal\n3. Short-circuiting\n","slug":"技术/hexo/old/Lamda积累","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd32t001o38pwe6yy17em","content":"<h1 id=\"Lambda-amp-Stream积累\"><a href=\"#Lambda-amp-Stream积累\" class=\"headerlink\" title=\"Lambda&amp;Stream积累\"></a>Lambda&amp;Stream积累</h1><a id=\"more\"></a>\n<h2 id=\"Lambda\"><a href=\"#Lambda\" class=\"headerlink\" title=\"Lambda\"></a>Lambda</h2><p>Lambda主要是一个类语法长糖,尽量为java引入函数编程等实现,细节后续再补充</p>\n<h2 id=\"Stream\"><a href=\"#Stream\" class=\"headerlink\" title=\"Stream\"></a>Stream</h2><p>java8提拱的新特性之一就有stream.stream主要是针对集合的处理类.提供了一系列集合处理方式.<br>配合使用lambda写出简介优美的代码</p>\n<h3 id=\"Stream的使用\"><a href=\"#Stream的使用\" class=\"headerlink\" title=\"Stream的使用\"></a>Stream的使用</h3><p>通过如</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">List&lt;Integer&gt; list = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</span><br><span class=\"line\">list.stream();<span class=\"comment\">//即可以开启串行流;</span></span><br><span class=\"line\">list.parallelStream().filter(a -&gt; &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> a &gt; <span class=\"number\">20</span>;</span><br><span class=\"line\">        &#125;);<span class=\"comment\">//开启并行流</span></span><br></pre></td></tr></table></figure>\n<p>串行流即内部单线程顺序执行,并行则是启用多线程执行.<br>后者并不一定效率就比前者高.因为并行执行启用分配线程资源时同样要消耗时间和资源,在一定量级下,前者的执行效率一度要高过后者.        </p>\n<p>我这里对三种对集合的处理形式的比较,可以简单参考一下</p>\n<ol>\n<li>stream 串行流</li>\n<li>parallelStream 并行流</li>\n<li>常规循环式</li>\n</ol>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">List&lt;Integer&gt; list = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</span><br><span class=\"line\">      <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">100000</span>; i++) &#123;</span><br><span class=\"line\">          list.add(getRandomNum());</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">      DateUtil.setBegin();</span><br><span class=\"line\">      list.stream().filter(a -&gt; &#123;</span><br><span class=\"line\">          <span class=\"keyword\">return</span> a &gt; <span class=\"number\">20</span>;</span><br><span class=\"line\">      &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">      DateUtil.setStop();</span><br><span class=\"line\">      System.out.println(<span class=\"string\">\"串行耗时\"</span>+DateUtil.calCostTime());</span><br><span class=\"line\"></span><br><span class=\"line\">      DateUtil.setBegin();</span><br><span class=\"line\">      list.parallelStream().filter(a -&gt; &#123;</span><br><span class=\"line\">          <span class=\"keyword\">return</span> a &gt; <span class=\"number\">20</span>;</span><br><span class=\"line\">      &#125;);</span><br><span class=\"line\">      DateUtil.setStop();</span><br><span class=\"line\">      System.out.println(<span class=\"string\">\"并行耗时\"</span>+DateUtil.calCostTime());</span><br><span class=\"line\">      <span class=\"keyword\">int</span> count = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">      DateUtil.setBegin();</span><br><span class=\"line\">      <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> l : list) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (l &gt; <span class=\"number\">20</span>) &#123;</span><br><span class=\"line\">              count++;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      DateUtil.setStop();</span><br><span class=\"line\">      System.out.println(<span class=\"string\">\"循环耗时\"</span>+DateUtil.calCostTime());</span><br></pre></td></tr></table></figure>\n<p>经由相当量次的测试后,我觉得如果要对集合中的数据进行遍历操作,根据量级的不同,<br>建议低量级还是采用普通循环,量级特别大,可考虑用并行流.书写方便,又不是大批量数据处理操作<br>可以直接采用串行流</p>\n<h3 id=\"Stream的操作分类\"><a href=\"#Stream的操作分类\" class=\"headerlink\" title=\"Stream的操作分类\"></a>Stream的操作分类</h3><ol>\n<li>Intermediate</li>\n<li>Terminal</li>\n<li>Short-circuiting</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h1 id=\"Lambda-amp-Stream积累\"><a href=\"#Lambda-amp-Stream积累\" class=\"headerlink\" title=\"Lambda&amp;Stream积累\"></a>Lambda&amp;Stream积累</h1>","more":"<h2 id=\"Lambda\"><a href=\"#Lambda\" class=\"headerlink\" title=\"Lambda\"></a>Lambda</h2><p>Lambda主要是一个类语法长糖,尽量为java引入函数编程等实现,细节后续再补充</p>\n<h2 id=\"Stream\"><a href=\"#Stream\" class=\"headerlink\" title=\"Stream\"></a>Stream</h2><p>java8提拱的新特性之一就有stream.stream主要是针对集合的处理类.提供了一系列集合处理方式.<br>配合使用lambda写出简介优美的代码</p>\n<h3 id=\"Stream的使用\"><a href=\"#Stream的使用\" class=\"headerlink\" title=\"Stream的使用\"></a>Stream的使用</h3><p>通过如</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">List&lt;Integer&gt; list = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</span><br><span class=\"line\">list.stream();<span class=\"comment\">//即可以开启串行流;</span></span><br><span class=\"line\">list.parallelStream().filter(a -&gt; &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span> a &gt; <span class=\"number\">20</span>;</span><br><span class=\"line\">        &#125;);<span class=\"comment\">//开启并行流</span></span><br></pre></td></tr></table></figure>\n<p>串行流即内部单线程顺序执行,并行则是启用多线程执行.<br>后者并不一定效率就比前者高.因为并行执行启用分配线程资源时同样要消耗时间和资源,在一定量级下,前者的执行效率一度要高过后者.        </p>\n<p>我这里对三种对集合的处理形式的比较,可以简单参考一下</p>\n<ol>\n<li>stream 串行流</li>\n<li>parallelStream 并行流</li>\n<li>常规循环式</li>\n</ol>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">List&lt;Integer&gt; list = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</span><br><span class=\"line\">      <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">100000</span>; i++) &#123;</span><br><span class=\"line\">          list.add(getRandomNum());</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">      DateUtil.setBegin();</span><br><span class=\"line\">      list.stream().filter(a -&gt; &#123;</span><br><span class=\"line\">          <span class=\"keyword\">return</span> a &gt; <span class=\"number\">20</span>;</span><br><span class=\"line\">      &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">      DateUtil.setStop();</span><br><span class=\"line\">      System.out.println(<span class=\"string\">\"串行耗时\"</span>+DateUtil.calCostTime());</span><br><span class=\"line\"></span><br><span class=\"line\">      DateUtil.setBegin();</span><br><span class=\"line\">      list.parallelStream().filter(a -&gt; &#123;</span><br><span class=\"line\">          <span class=\"keyword\">return</span> a &gt; <span class=\"number\">20</span>;</span><br><span class=\"line\">      &#125;);</span><br><span class=\"line\">      DateUtil.setStop();</span><br><span class=\"line\">      System.out.println(<span class=\"string\">\"并行耗时\"</span>+DateUtil.calCostTime());</span><br><span class=\"line\">      <span class=\"keyword\">int</span> count = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">      DateUtil.setBegin();</span><br><span class=\"line\">      <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> l : list) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (l &gt; <span class=\"number\">20</span>) &#123;</span><br><span class=\"line\">              count++;</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      DateUtil.setStop();</span><br><span class=\"line\">      System.out.println(<span class=\"string\">\"循环耗时\"</span>+DateUtil.calCostTime());</span><br></pre></td></tr></table></figure>\n<p>经由相当量次的测试后,我觉得如果要对集合中的数据进行遍历操作,根据量级的不同,<br>建议低量级还是采用普通循环,量级特别大,可考虑用并行流.书写方便,又不是大批量数据处理操作<br>可以直接采用串行流</p>\n<h3 id=\"Stream的操作分类\"><a href=\"#Stream的操作分类\" class=\"headerlink\" title=\"Stream的操作分类\"></a>Stream的操作分类</h3><ol>\n<li>Intermediate</li>\n<li>Terminal</li>\n<li>Short-circuiting</li>\n</ol>"},{"title":"Yarn配置细节","date":"2019-06-12T16:00:00.000Z","_content":"此处简介\n\n<!--more-->\n# Yarn配置细节\n##内存,核数设置\n<!-- more -->\n\n```\n\n        <property>\n      <name>yarn.nodemanager.resource.memory-mb</name>\n      <value>4096</value>\n  </property>\n\n  <property>\n      <name>yarn.scheduler.minimum-allocation-mb</name>\n      <value>1024</value>\n  </property>\n  <property>\n      <name>yarn.scheduler.maximum-allocation-mb</name>\n      <value>3072</value>\n  </property>\n  <!--该配置用于配置任务请求时的资源. -->\n  <property>\n      <name>yarn.app.mapreduce.am.resource.mb</name>\n      <value>2048</value>\n  </property>\n\n  <property>\n      <name>yarn.app.mapreduce.am.command-opts</name>\n      <value>-Xmx3276m</value>\n  </property>\n    <property>\n      <name>yarn.nodemanager.resource.cpu-vcores</name>\n      <value>2</value>\n  </property>\n  <property>\n      <name>yarn.scheduler.maximum-allocation-vcores</name>\n      <value>3</value>\n  </property>\n\n\n  ```\n  ","source":"_posts/技术/hexo/old/Yarn配置.md","raw":"---\n\ntitle: Yarn配置细节\ndate: 2019-06-13\ntags: \n\n---\n此处简介\n\n<!--more-->\n# Yarn配置细节\n##内存,核数设置\n<!-- more -->\n\n```\n\n        <property>\n      <name>yarn.nodemanager.resource.memory-mb</name>\n      <value>4096</value>\n  </property>\n\n  <property>\n      <name>yarn.scheduler.minimum-allocation-mb</name>\n      <value>1024</value>\n  </property>\n  <property>\n      <name>yarn.scheduler.maximum-allocation-mb</name>\n      <value>3072</value>\n  </property>\n  <!--该配置用于配置任务请求时的资源. -->\n  <property>\n      <name>yarn.app.mapreduce.am.resource.mb</name>\n      <value>2048</value>\n  </property>\n\n  <property>\n      <name>yarn.app.mapreduce.am.command-opts</name>\n      <value>-Xmx3276m</value>\n  </property>\n    <property>\n      <name>yarn.nodemanager.resource.cpu-vcores</name>\n      <value>2</value>\n  </property>\n  <property>\n      <name>yarn.scheduler.maximum-allocation-vcores</name>\n      <value>3</value>\n  </property>\n\n\n  ```\n  ","slug":"技术/hexo/old/Yarn配置","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd32u001r38pwctjt0bvm","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n<h1 id=\"Yarn配置细节\"><a href=\"#Yarn配置细节\" class=\"headerlink\" title=\"Yarn配置细节\"></a>Yarn配置细节</h1><p>##内存,核数设置</p>\n<!-- more -->\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">      &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;&#x2F;name&gt;</span><br><span class=\"line\">    &lt;value&gt;4096&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;&#x2F;name&gt;</span><br><span class=\"line\">    &lt;value&gt;1024&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;&#x2F;name&gt;</span><br><span class=\"line\">    &lt;value&gt;3072&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\">&lt;!--该配置用于配置任务请求时的资源. --&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;&#x2F;name&gt;</span><br><span class=\"line\">    &lt;value&gt;2048&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;&#x2F;name&gt;</span><br><span class=\"line\">    &lt;value&gt;-Xmx3276m&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;&#x2F;name&gt;</span><br><span class=\"line\">    &lt;value&gt;2&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;&#x2F;name&gt;</span><br><span class=\"line\">    &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h1 id=\"Yarn配置细节\"><a href=\"#Yarn配置细节\" class=\"headerlink\" title=\"Yarn配置细节\"></a>Yarn配置细节</h1><p>##内存,核数设置</p>\n<!-- more -->\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">      &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;&#x2F;name&gt;</span><br><span class=\"line\">    &lt;value&gt;4096&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;&#x2F;name&gt;</span><br><span class=\"line\">    &lt;value&gt;1024&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;&#x2F;name&gt;</span><br><span class=\"line\">    &lt;value&gt;3072&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\">&lt;!--该配置用于配置任务请求时的资源. --&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;&#x2F;name&gt;</span><br><span class=\"line\">    &lt;value&gt;2048&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;&#x2F;name&gt;</span><br><span class=\"line\">    &lt;value&gt;-Xmx3276m&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;&#x2F;name&gt;</span><br><span class=\"line\">    &lt;value&gt;2&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;&#x2F;name&gt;</span><br><span class=\"line\">    &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>"},{"title":"kafka","_content":"kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic sparkstreaming\nkafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic flumetest\n<!-- more -->\n\nkafka-console-producer.sh --broker-list localhost:9092 --topic flumetest :创建生产者\n\nkafka-console-consumer.sh --bootstrap-server namenode:9092  --topic  flume-ng\n\n# Kafka相关小结\n\n##  kafka 相关指令\nkafka-server-start.sh config/server.properties & 启动\nkafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic topic_name  :创建topic\nkafka-console-producer.sh --broker-list localhost:9092 --topic topic_name :创建生产者\n\nkafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic_name :创建消费者\n\nkafka-console-producer.sh --broker-list namenode:9092 --topic sparkstreaming\n\n删除group\n\nkafka-consumer-groups --bootstrap-server 192.168.10.100:9092,192.168.10.101:9092,192.168.10.102:9092  —group traffic_history —delete\n\n\n## kafka java api\nkafka 虽然搭建较为简单,但想要对针它编程体验还是有些问题.初步使用下来明显感觉对版本的强约束性.以我线上版本\n\n![2019-03-12-11-02-43](http://img.wqkenqing.ren/2019-03-12-11-02-43.png)为例,我java项目对应的版本则是\n``` java\n\n<dependency>\n            <groupId>org.apache.kafka</groupId>\n            <artifactId>kafka_2.10</artifactId>\n            <version>0.8.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.kafka</groupId>\n            <artifactId>kafka-clients</artifactId>\n            <version>0.8.2.1</version>\n        </dependency>\n```\n\n以上版本搭配经由我亲测通过\n","source":"_posts/技术/hexo/old/kafka.md","raw":"---\ntitle: kafka \ntags: kafka\n---\nkafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic sparkstreaming\nkafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic flumetest\n<!-- more -->\n\nkafka-console-producer.sh --broker-list localhost:9092 --topic flumetest :创建生产者\n\nkafka-console-consumer.sh --bootstrap-server namenode:9092  --topic  flume-ng\n\n# Kafka相关小结\n\n##  kafka 相关指令\nkafka-server-start.sh config/server.properties & 启动\nkafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic topic_name  :创建topic\nkafka-console-producer.sh --broker-list localhost:9092 --topic topic_name :创建生产者\n\nkafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic_name :创建消费者\n\nkafka-console-producer.sh --broker-list namenode:9092 --topic sparkstreaming\n\n删除group\n\nkafka-consumer-groups --bootstrap-server 192.168.10.100:9092,192.168.10.101:9092,192.168.10.102:9092  —group traffic_history —delete\n\n\n## kafka java api\nkafka 虽然搭建较为简单,但想要对针它编程体验还是有些问题.初步使用下来明显感觉对版本的强约束性.以我线上版本\n\n![2019-03-12-11-02-43](http://img.wqkenqing.ren/2019-03-12-11-02-43.png)为例,我java项目对应的版本则是\n``` java\n\n<dependency>\n            <groupId>org.apache.kafka</groupId>\n            <artifactId>kafka_2.10</artifactId>\n            <version>0.8.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.kafka</groupId>\n            <artifactId>kafka-clients</artifactId>\n            <version>0.8.2.1</version>\n        </dependency>\n```\n\n以上版本搭配经由我亲测通过\n","slug":"技术/hexo/old/kafka","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd32v001s38pwfyefe7d4","content":"<p>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic sparkstreaming<br>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic flumetest</p>\n<a id=\"more\"></a>\n\n<p>kafka-console-producer.sh –broker-list localhost:9092 –topic flumetest :创建生产者</p>\n<p>kafka-console-consumer.sh –bootstrap-server namenode:9092  –topic  flume-ng</p>\n<h1 id=\"Kafka相关小结\"><a href=\"#Kafka相关小结\" class=\"headerlink\" title=\"Kafka相关小结\"></a>Kafka相关小结</h1><h2 id=\"kafka-相关指令\"><a href=\"#kafka-相关指令\" class=\"headerlink\" title=\"kafka 相关指令\"></a>kafka 相关指令</h2><p>kafka-server-start.sh config/server.properties &amp; 启动<br>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic topic_name  :创建topic<br>kafka-console-producer.sh –broker-list localhost:9092 –topic topic_name :创建生产者</p>\n<p>kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic topic_name :创建消费者</p>\n<p>kafka-console-producer.sh –broker-list namenode:9092 –topic sparkstreaming</p>\n<p>删除group</p>\n<p>kafka-consumer-groups –bootstrap-server 192.168.10.100:9092,192.168.10.101:9092,192.168.10.102:9092  —group traffic_history —delete</p>\n<h2 id=\"kafka-java-api\"><a href=\"#kafka-java-api\" class=\"headerlink\" title=\"kafka java api\"></a>kafka java api</h2><p>kafka 虽然搭建较为简单,但想要对针它编程体验还是有些问题.初步使用下来明显感觉对版本的强约束性.以我线上版本</p>\n<p><img src=\"http://img.wqkenqing.ren/2019-03-12-11-02-43.png\" alt=\"2019-03-12-11-02-43\">为例,我java项目对应的版本则是</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;0.8.1&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;0.8.2.1&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n\n<p>以上版本搭配经由我亲测通过</p>\n","site":{"data":{}},"excerpt":"<p>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic sparkstreaming<br>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic flumetest</p>","more":"<p>kafka-console-producer.sh –broker-list localhost:9092 –topic flumetest :创建生产者</p>\n<p>kafka-console-consumer.sh –bootstrap-server namenode:9092  –topic  flume-ng</p>\n<h1 id=\"Kafka相关小结\"><a href=\"#Kafka相关小结\" class=\"headerlink\" title=\"Kafka相关小结\"></a>Kafka相关小结</h1><h2 id=\"kafka-相关指令\"><a href=\"#kafka-相关指令\" class=\"headerlink\" title=\"kafka 相关指令\"></a>kafka 相关指令</h2><p>kafka-server-start.sh config/server.properties &amp; 启动<br>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic topic_name  :创建topic<br>kafka-console-producer.sh –broker-list localhost:9092 –topic topic_name :创建生产者</p>\n<p>kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic topic_name :创建消费者</p>\n<p>kafka-console-producer.sh –broker-list namenode:9092 –topic sparkstreaming</p>\n<p>删除group</p>\n<p>kafka-consumer-groups –bootstrap-server 192.168.10.100:9092,192.168.10.101:9092,192.168.10.102:9092  —group traffic_history —delete</p>\n<h2 id=\"kafka-java-api\"><a href=\"#kafka-java-api\" class=\"headerlink\" title=\"kafka java api\"></a>kafka java api</h2><p>kafka 虽然搭建较为简单,但想要对针它编程体验还是有些问题.初步使用下来明显感觉对版本的强约束性.以我线上版本</p>\n<p><img src=\"http://img.wqkenqing.ren/2019-03-12-11-02-43.png\" alt=\"2019-03-12-11-02-43\">为例,我java项目对应的版本则是</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;0.8.1&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br><span class=\"line\">        &lt;dependency&gt;</span><br><span class=\"line\">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class=\"line\">            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class=\"line\">            &lt;version&gt;0.8.2.1&lt;/version&gt;</span><br><span class=\"line\">        &lt;/dependency&gt;</span><br></pre></td></tr></table></figure>\n\n<p>以上版本搭配经由我亲测通过</p>"},{"title":"hbase积累.md","date":"2018-06-04T02:54:48.000Z","abstract":"hbase细节总结","_content":"# hbase积累\n<!-- more -->\n\n## 细节点\n\n### 1.Rowkey设计原则\n<!-- more -->\n1.1 **长度原则** rowkey 在hbase以二进制码流,可以是任意字符串,\n\n* **最大长度是64kb**,实际应用主要是100~100bytes\n\n* 长度尽量为8的整数倍,因为现在的系统主要是64位,内存8字节对齐.控制在16字节,符合操作系统特性\n\n1.2 **散列原则**:因为hbase是分布式存储,rowkey的高位尽量是散列字段,散列性弱的尽量放在低位段.如Time AND Device_id的组合,相对而言device_id 应该量级较小,散列性高.而TIME散列性低,如果TIME放在高位,可能造成数据在某个RegeionServer上堆积的情况.所以较合理的rowkey组合应是\ndevice_id+time.\n\n1.3 **RowKey唯一原则**：必须在设计上保证其唯一性.\nhbase 中以KeyValue形式存储,key若重复,行内容则会被覆盖.\n\n---\n### 2.Hbase的Regeion热点问题解决\n\n`\n因为在创建表是没有提前预分区,创建的表默认就只会有一个region,这个region的rowkey是没有边界的,即没有startkey与stopkey.数据在写入时,都会写入到这个region.随着数据的不断增加,达到某个阈值时,才会split成2个region.在这个过程中就会产生所有数据囤积在一个regionServer上,出现热点问题.另在split时,会占用集群的I/O资源.通过预分区可以解决该问题\n`\n\n#### 2.1 预分区\n\n预分区,\"预\"字是核心.我们在建表时,预先对表中要存放的数据形式和可能的量级,心中必然会有所估量,即这里应**预**估数据量.若数据量较大,则在建表时又应该预分区.即根据数据形式,量级,事先预设好一定量的region,后面数据写入时,则会写入到相应的分区.从而避免热点,减少split.\n\n2.1.2 salting(加盐)\nhbase rowkey设计,避免热点,常会用到该操作,这里的加盐本身不是加密操作,而是在原数据前加入一些随机数据,从而起到分散不同region的作用.\n\n2.1.3 预习区具体方案\n\nhbase预分区的相关操作,如shell形式,可直接在hbase shell\n操作.如\n\n[https://blog.csdn.net/xiao_jun_0820/article/details/24419793](Hbase shell 预分区操作.)\n\njava形式\n[https://blog.csdn.net/qq_20641565/article/details/56482407](Hbase 预分区 java API形式)\n\n以上操作形式有个问题就是rowkey是随机生成的,虽起到了散列存储,避免了热点堆积,但因为加盐的缘故,想要直接的获取某行数据较为困难.若针对的是高频使用的数据,则会出现问题.\n\n2.1.4 hash分区\n\n在原先预分区的基础上,通过相关规则将原数据hash,从而获得这个原数据对应在哪个分区,使当拿到相关原数据,就能推演出相关rowkey.从而能准确的get数据.\n\n### hbase优化\n\n#### 确定优化目标\n沟通交流后，业务方更看重降低成本。数据量梳理后略有降低，保证吞吐，无长期请求堆积前提下可以放宽延时要求。为了更快的进行优化，放宽稳定性可以要求接受短期波动。\n另外，该分组的RegionServer之前存在不稳定的问题，这次优化也一并解决。\n\n---\n","source":"_posts/技术/hexo/old/hbase积累.md","raw":"---\ntitle: hbase积累.md\ndate: 2018-06-04 10:54:48\ntags: 日常总结\nabstract: hbase细节总结\n---\n# hbase积累\n<!-- more -->\n\n## 细节点\n\n### 1.Rowkey设计原则\n<!-- more -->\n1.1 **长度原则** rowkey 在hbase以二进制码流,可以是任意字符串,\n\n* **最大长度是64kb**,实际应用主要是100~100bytes\n\n* 长度尽量为8的整数倍,因为现在的系统主要是64位,内存8字节对齐.控制在16字节,符合操作系统特性\n\n1.2 **散列原则**:因为hbase是分布式存储,rowkey的高位尽量是散列字段,散列性弱的尽量放在低位段.如Time AND Device_id的组合,相对而言device_id 应该量级较小,散列性高.而TIME散列性低,如果TIME放在高位,可能造成数据在某个RegeionServer上堆积的情况.所以较合理的rowkey组合应是\ndevice_id+time.\n\n1.3 **RowKey唯一原则**：必须在设计上保证其唯一性.\nhbase 中以KeyValue形式存储,key若重复,行内容则会被覆盖.\n\n---\n### 2.Hbase的Regeion热点问题解决\n\n`\n因为在创建表是没有提前预分区,创建的表默认就只会有一个region,这个region的rowkey是没有边界的,即没有startkey与stopkey.数据在写入时,都会写入到这个region.随着数据的不断增加,达到某个阈值时,才会split成2个region.在这个过程中就会产生所有数据囤积在一个regionServer上,出现热点问题.另在split时,会占用集群的I/O资源.通过预分区可以解决该问题\n`\n\n#### 2.1 预分区\n\n预分区,\"预\"字是核心.我们在建表时,预先对表中要存放的数据形式和可能的量级,心中必然会有所估量,即这里应**预**估数据量.若数据量较大,则在建表时又应该预分区.即根据数据形式,量级,事先预设好一定量的region,后面数据写入时,则会写入到相应的分区.从而避免热点,减少split.\n\n2.1.2 salting(加盐)\nhbase rowkey设计,避免热点,常会用到该操作,这里的加盐本身不是加密操作,而是在原数据前加入一些随机数据,从而起到分散不同region的作用.\n\n2.1.3 预习区具体方案\n\nhbase预分区的相关操作,如shell形式,可直接在hbase shell\n操作.如\n\n[https://blog.csdn.net/xiao_jun_0820/article/details/24419793](Hbase shell 预分区操作.)\n\njava形式\n[https://blog.csdn.net/qq_20641565/article/details/56482407](Hbase 预分区 java API形式)\n\n以上操作形式有个问题就是rowkey是随机生成的,虽起到了散列存储,避免了热点堆积,但因为加盐的缘故,想要直接的获取某行数据较为困难.若针对的是高频使用的数据,则会出现问题.\n\n2.1.4 hash分区\n\n在原先预分区的基础上,通过相关规则将原数据hash,从而获得这个原数据对应在哪个分区,使当拿到相关原数据,就能推演出相关rowkey.从而能准确的get数据.\n\n### hbase优化\n\n#### 确定优化目标\n沟通交流后，业务方更看重降低成本。数据量梳理后略有降低，保证吞吐，无长期请求堆积前提下可以放宽延时要求。为了更快的进行优化，放宽稳定性可以要求接受短期波动。\n另外，该分组的RegionServer之前存在不稳定的问题，这次优化也一并解决。\n\n---\n","slug":"技术/hexo/old/hbase积累","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd32v001t38pw8xlycxn8","content":"<h1 id=\"hbase积累\"><a href=\"#hbase积累\" class=\"headerlink\" title=\"hbase积累\"></a>hbase积累</h1><a id=\"more\"></a>\n\n<h2 id=\"细节点\"><a href=\"#细节点\" class=\"headerlink\" title=\"细节点\"></a>细节点</h2><h3 id=\"1-Rowkey设计原则\"><a href=\"#1-Rowkey设计原则\" class=\"headerlink\" title=\"1.Rowkey设计原则\"></a>1.Rowkey设计原则</h3><!-- more -->\n<p>1.1 <strong>长度原则</strong> rowkey 在hbase以二进制码流,可以是任意字符串,</p>\n<ul>\n<li><p><strong>最大长度是64kb</strong>,实际应用主要是100~100bytes</p>\n</li>\n<li><p>长度尽量为8的整数倍,因为现在的系统主要是64位,内存8字节对齐.控制在16字节,符合操作系统特性</p>\n</li>\n</ul>\n<p>1.2 <strong>散列原则</strong>:因为hbase是分布式存储,rowkey的高位尽量是散列字段,散列性弱的尽量放在低位段.如Time AND Device_id的组合,相对而言device_id 应该量级较小,散列性高.而TIME散列性低,如果TIME放在高位,可能造成数据在某个RegeionServer上堆积的情况.所以较合理的rowkey组合应是<br>device_id+time.</p>\n<p>1.3 <strong>RowKey唯一原则</strong>：必须在设计上保证其唯一性.<br>hbase 中以KeyValue形式存储,key若重复,行内容则会被覆盖.</p>\n<hr>\n<h3 id=\"2-Hbase的Regeion热点问题解决\"><a href=\"#2-Hbase的Regeion热点问题解决\" class=\"headerlink\" title=\"2.Hbase的Regeion热点问题解决\"></a>2.Hbase的Regeion热点问题解决</h3><p><code>因为在创建表是没有提前预分区,创建的表默认就只会有一个region,这个region的rowkey是没有边界的,即没有startkey与stopkey.数据在写入时,都会写入到这个region.随着数据的不断增加,达到某个阈值时,才会split成2个region.在这个过程中就会产生所有数据囤积在一个regionServer上,出现热点问题.另在split时,会占用集群的I/O资源.通过预分区可以解决该问题</code></p>\n<h4 id=\"2-1-预分区\"><a href=\"#2-1-预分区\" class=\"headerlink\" title=\"2.1 预分区\"></a>2.1 预分区</h4><p>预分区,”预”字是核心.我们在建表时,预先对表中要存放的数据形式和可能的量级,心中必然会有所估量,即这里应<strong>预</strong>估数据量.若数据量较大,则在建表时又应该预分区.即根据数据形式,量级,事先预设好一定量的region,后面数据写入时,则会写入到相应的分区.从而避免热点,减少split.</p>\n<p>2.1.2 salting(加盐)<br>hbase rowkey设计,避免热点,常会用到该操作,这里的加盐本身不是加密操作,而是在原数据前加入一些随机数据,从而起到分散不同region的作用.</p>\n<p>2.1.3 预习区具体方案</p>\n<p>hbase预分区的相关操作,如shell形式,可直接在hbase shell<br>操作.如</p>\n<p>[<a href=\"https://blog.csdn.net/xiao_jun_0820/article/details/24419793]\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/xiao_jun_0820/article/details/24419793]</a>(Hbase shell 预分区操作.)</p>\n<p>java形式<br>[<a href=\"https://blog.csdn.net/qq_20641565/article/details/56482407]\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/qq_20641565/article/details/56482407]</a>(Hbase 预分区 java API形式)</p>\n<p>以上操作形式有个问题就是rowkey是随机生成的,虽起到了散列存储,避免了热点堆积,但因为加盐的缘故,想要直接的获取某行数据较为困难.若针对的是高频使用的数据,则会出现问题.</p>\n<p>2.1.4 hash分区</p>\n<p>在原先预分区的基础上,通过相关规则将原数据hash,从而获得这个原数据对应在哪个分区,使当拿到相关原数据,就能推演出相关rowkey.从而能准确的get数据.</p>\n<h3 id=\"hbase优化\"><a href=\"#hbase优化\" class=\"headerlink\" title=\"hbase优化\"></a>hbase优化</h3><h4 id=\"确定优化目标\"><a href=\"#确定优化目标\" class=\"headerlink\" title=\"确定优化目标\"></a>确定优化目标</h4><p>沟通交流后，业务方更看重降低成本。数据量梳理后略有降低，保证吞吐，无长期请求堆积前提下可以放宽延时要求。为了更快的进行优化，放宽稳定性可以要求接受短期波动。<br>另外，该分组的RegionServer之前存在不稳定的问题，这次优化也一并解决。</p>\n<hr>\n","site":{"data":{}},"excerpt":"<h1 id=\"hbase积累\"><a href=\"#hbase积累\" class=\"headerlink\" title=\"hbase积累\"></a>hbase积累</h1>","more":"<h2 id=\"细节点\"><a href=\"#细节点\" class=\"headerlink\" title=\"细节点\"></a>细节点</h2><h3 id=\"1-Rowkey设计原则\"><a href=\"#1-Rowkey设计原则\" class=\"headerlink\" title=\"1.Rowkey设计原则\"></a>1.Rowkey设计原则</h3><!-- more -->\n<p>1.1 <strong>长度原则</strong> rowkey 在hbase以二进制码流,可以是任意字符串,</p>\n<ul>\n<li><p><strong>最大长度是64kb</strong>,实际应用主要是100~100bytes</p>\n</li>\n<li><p>长度尽量为8的整数倍,因为现在的系统主要是64位,内存8字节对齐.控制在16字节,符合操作系统特性</p>\n</li>\n</ul>\n<p>1.2 <strong>散列原则</strong>:因为hbase是分布式存储,rowkey的高位尽量是散列字段,散列性弱的尽量放在低位段.如Time AND Device_id的组合,相对而言device_id 应该量级较小,散列性高.而TIME散列性低,如果TIME放在高位,可能造成数据在某个RegeionServer上堆积的情况.所以较合理的rowkey组合应是<br>device_id+time.</p>\n<p>1.3 <strong>RowKey唯一原则</strong>：必须在设计上保证其唯一性.<br>hbase 中以KeyValue形式存储,key若重复,行内容则会被覆盖.</p>\n<hr>\n<h3 id=\"2-Hbase的Regeion热点问题解决\"><a href=\"#2-Hbase的Regeion热点问题解决\" class=\"headerlink\" title=\"2.Hbase的Regeion热点问题解决\"></a>2.Hbase的Regeion热点问题解决</h3><p><code>因为在创建表是没有提前预分区,创建的表默认就只会有一个region,这个region的rowkey是没有边界的,即没有startkey与stopkey.数据在写入时,都会写入到这个region.随着数据的不断增加,达到某个阈值时,才会split成2个region.在这个过程中就会产生所有数据囤积在一个regionServer上,出现热点问题.另在split时,会占用集群的I/O资源.通过预分区可以解决该问题</code></p>\n<h4 id=\"2-1-预分区\"><a href=\"#2-1-预分区\" class=\"headerlink\" title=\"2.1 预分区\"></a>2.1 预分区</h4><p>预分区,”预”字是核心.我们在建表时,预先对表中要存放的数据形式和可能的量级,心中必然会有所估量,即这里应<strong>预</strong>估数据量.若数据量较大,则在建表时又应该预分区.即根据数据形式,量级,事先预设好一定量的region,后面数据写入时,则会写入到相应的分区.从而避免热点,减少split.</p>\n<p>2.1.2 salting(加盐)<br>hbase rowkey设计,避免热点,常会用到该操作,这里的加盐本身不是加密操作,而是在原数据前加入一些随机数据,从而起到分散不同region的作用.</p>\n<p>2.1.3 预习区具体方案</p>\n<p>hbase预分区的相关操作,如shell形式,可直接在hbase shell<br>操作.如</p>\n<p>[<a href=\"https://blog.csdn.net/xiao_jun_0820/article/details/24419793]\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/xiao_jun_0820/article/details/24419793]</a>(Hbase shell 预分区操作.)</p>\n<p>java形式<br>[<a href=\"https://blog.csdn.net/qq_20641565/article/details/56482407]\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/qq_20641565/article/details/56482407]</a>(Hbase 预分区 java API形式)</p>\n<p>以上操作形式有个问题就是rowkey是随机生成的,虽起到了散列存储,避免了热点堆积,但因为加盐的缘故,想要直接的获取某行数据较为困难.若针对的是高频使用的数据,则会出现问题.</p>\n<p>2.1.4 hash分区</p>\n<p>在原先预分区的基础上,通过相关规则将原数据hash,从而获得这个原数据对应在哪个分区,使当拿到相关原数据,就能推演出相关rowkey.从而能准确的get数据.</p>\n<h3 id=\"hbase优化\"><a href=\"#hbase优化\" class=\"headerlink\" title=\"hbase优化\"></a>hbase优化</h3><h4 id=\"确定优化目标\"><a href=\"#确定优化目标\" class=\"headerlink\" title=\"确定优化目标\"></a>确定优化目标</h4><p>沟通交流后，业务方更看重降低成本。数据量梳理后略有降低，保证吞吐，无长期请求堆积前提下可以放宽延时要求。为了更快的进行优化，放宽稳定性可以要求接受短期波动。<br>另外，该分组的RegionServer之前存在不稳定的问题，这次优化也一并解决。</p>\n<hr>"},{"title":"mapreduce组件总结","_content":"\n\n# mapreduce组件总结\n\n<!-- more -->\n相关组件大致有\n1. Inputformat\n2. Inputsplit\n3. ReadRecorder\n4. mapper\n5. Combiner\n6. Partioner\n7. Reduce\n8. GroupComparator\n9. Reduce\n\n# shuffle\n\n![2019-03-19-15-39-59](http://img.wqkenqing.ren/2019-03-19-15-39-59.png)\n![2019-03-19-16-46-06](http://img.wqkenqing.ren/2019-03-19-16-46-06.png)\n\n```\nshuffle 被称为mapreduce的核心,一个真正让奇迹发生的地方.但它到底是什么呢?简练的讲,它就是 map out 到 reduce in 这段过程中对数据的处理过程.\n```\nshuffle过程中主要发生的操作有,Partion,Sort,spill,merge,copy,sort,merge.(还有可能有combine操作)\n\n具体流程是\nmap out后,Collector 对out后的数据进行处理. 数据将会写入到内存缓冲区,该内存缓冲区的数据达到80%后,会开启一个溢写线程,在磁盘本地创建一个文件.如果reduce设置了多个分区,写入buffer区的数据,会被打上一个分区标记.通过sortAndSpill()方法进行指对数据按分区号,key排序.最后溢出的文件是分区的,按key有序的文件.若buffer区中的20%一直未被填满,buffer写入进程不会断.但若达到100%,Buffer写入进程则会阻塞.并在buffer区中的数据全部spill完后才会再开启. (buffer区的内存默认是100M),spill过程中,若设置过combiner.则会对数据先进行combiner逻辑处理,再将处理后的数据写出\n\nspill完成后则会对本地的spill后的文件进行Merge.即把多个spill后的文件进行合并,并排序.最后会行成一个有序文件\n\n当1个Map Task 完成后,reduce 就会开启copy进程(默认是5个线程).这个过程中会通过http请求去各taskTracker(nodemanager),拉取相应的spill&merge后的文件.\n当copy完成后,则又会对数据进行merge.这个过程中同样有个类似map shuffle 中的buffer 溢写的阶段. 这个过程同样会触发combiner组件.这里的merge数据源有三种\n1. memory to memory\n2. memory to disk\n3. disk   to disk \n默认1是不开启的.\n\ncopy phase 完成后,是reduceTask 中的 sort phase\n即对merge 中的文件继续进行sort and group .\n\n当sort phase 完成.则开启reduce phase .到此shuffle正式完成.\n\n##二次排序\n```\n```\nmapreduce 常见的辅助排序\n1. partitioner\n2. key的比较Comparator\n3. 分组函数Grouping Comparator\n\n## join \nmap join ,semi join ,reduce join\n## \n","source":"_posts/技术/hexo/old/mapreduce组件总结.md","raw":"---\ntitle:  mapreduce组件总结\ntags: bigdata\n---\n\n\n# mapreduce组件总结\n\n<!-- more -->\n相关组件大致有\n1. Inputformat\n2. Inputsplit\n3. ReadRecorder\n4. mapper\n5. Combiner\n6. Partioner\n7. Reduce\n8. GroupComparator\n9. Reduce\n\n# shuffle\n\n![2019-03-19-15-39-59](http://img.wqkenqing.ren/2019-03-19-15-39-59.png)\n![2019-03-19-16-46-06](http://img.wqkenqing.ren/2019-03-19-16-46-06.png)\n\n```\nshuffle 被称为mapreduce的核心,一个真正让奇迹发生的地方.但它到底是什么呢?简练的讲,它就是 map out 到 reduce in 这段过程中对数据的处理过程.\n```\nshuffle过程中主要发生的操作有,Partion,Sort,spill,merge,copy,sort,merge.(还有可能有combine操作)\n\n具体流程是\nmap out后,Collector 对out后的数据进行处理. 数据将会写入到内存缓冲区,该内存缓冲区的数据达到80%后,会开启一个溢写线程,在磁盘本地创建一个文件.如果reduce设置了多个分区,写入buffer区的数据,会被打上一个分区标记.通过sortAndSpill()方法进行指对数据按分区号,key排序.最后溢出的文件是分区的,按key有序的文件.若buffer区中的20%一直未被填满,buffer写入进程不会断.但若达到100%,Buffer写入进程则会阻塞.并在buffer区中的数据全部spill完后才会再开启. (buffer区的内存默认是100M),spill过程中,若设置过combiner.则会对数据先进行combiner逻辑处理,再将处理后的数据写出\n\nspill完成后则会对本地的spill后的文件进行Merge.即把多个spill后的文件进行合并,并排序.最后会行成一个有序文件\n\n当1个Map Task 完成后,reduce 就会开启copy进程(默认是5个线程).这个过程中会通过http请求去各taskTracker(nodemanager),拉取相应的spill&merge后的文件.\n当copy完成后,则又会对数据进行merge.这个过程中同样有个类似map shuffle 中的buffer 溢写的阶段. 这个过程同样会触发combiner组件.这里的merge数据源有三种\n1. memory to memory\n2. memory to disk\n3. disk   to disk \n默认1是不开启的.\n\ncopy phase 完成后,是reduceTask 中的 sort phase\n即对merge 中的文件继续进行sort and group .\n\n当sort phase 完成.则开启reduce phase .到此shuffle正式完成.\n\n##二次排序\n```\n```\nmapreduce 常见的辅助排序\n1. partitioner\n2. key的比较Comparator\n3. 分组函数Grouping Comparator\n\n## join \nmap join ,semi join ,reduce join\n## \n","slug":"技术/hexo/old/mapreduce组件总结","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd32w001w38pw4qxgh2hj","content":"<h1 id=\"mapreduce组件总结\"><a href=\"#mapreduce组件总结\" class=\"headerlink\" title=\"mapreduce组件总结\"></a>mapreduce组件总结</h1><a id=\"more\"></a>\n<p>相关组件大致有</p>\n<ol>\n<li>Inputformat</li>\n<li>Inputsplit</li>\n<li>ReadRecorder</li>\n<li>mapper</li>\n<li>Combiner</li>\n<li>Partioner</li>\n<li>Reduce</li>\n<li>GroupComparator</li>\n<li>Reduce</li>\n</ol>\n<h1 id=\"shuffle\"><a href=\"#shuffle\" class=\"headerlink\" title=\"shuffle\"></a>shuffle</h1><p><img src=\"http://img.wqkenqing.ren/2019-03-19-15-39-59.png\" alt=\"2019-03-19-15-39-59\"><br><img src=\"http://img.wqkenqing.ren/2019-03-19-16-46-06.png\" alt=\"2019-03-19-16-46-06\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">shuffle 被称为mapreduce的核心,一个真正让奇迹发生的地方.但它到底是什么呢?简练的讲,它就是 map out 到 reduce in 这段过程中对数据的处理过程.</span><br></pre></td></tr></table></figure>\n<p>shuffle过程中主要发生的操作有,Partion,Sort,spill,merge,copy,sort,merge.(还有可能有combine操作)</p>\n<p>具体流程是<br>map out后,Collector 对out后的数据进行处理. 数据将会写入到内存缓冲区,该内存缓冲区的数据达到80%后,会开启一个溢写线程,在磁盘本地创建一个文件.如果reduce设置了多个分区,写入buffer区的数据,会被打上一个分区标记.通过sortAndSpill()方法进行指对数据按分区号,key排序.最后溢出的文件是分区的,按key有序的文件.若buffer区中的20%一直未被填满,buffer写入进程不会断.但若达到100%,Buffer写入进程则会阻塞.并在buffer区中的数据全部spill完后才会再开启. (buffer区的内存默认是100M),spill过程中,若设置过combiner.则会对数据先进行combiner逻辑处理,再将处理后的数据写出</p>\n<p>spill完成后则会对本地的spill后的文件进行Merge.即把多个spill后的文件进行合并,并排序.最后会行成一个有序文件</p>\n<p>当1个Map Task 完成后,reduce 就会开启copy进程(默认是5个线程).这个过程中会通过http请求去各taskTracker(nodemanager),拉取相应的spill&amp;merge后的文件.<br>当copy完成后,则又会对数据进行merge.这个过程中同样有个类似map shuffle 中的buffer 溢写的阶段. 这个过程同样会触发combiner组件.这里的merge数据源有三种</p>\n<ol>\n<li>memory to memory</li>\n<li>memory to disk</li>\n<li>disk   to disk<br>默认1是不开启的.</li>\n</ol>\n<p>copy phase 完成后,是reduceTask 中的 sort phase<br>即对merge 中的文件继续进行sort and group .</p>\n<p>当sort phase 完成.则开启reduce phase .到此shuffle正式完成.</p>\n<p>##二次排序</p>\n<pre><code></code></pre><p>mapreduce 常见的辅助排序</p>\n<ol>\n<li>partitioner</li>\n<li>key的比较Comparator</li>\n<li>分组函数Grouping Comparator</li>\n</ol>\n<h2 id=\"join\"><a href=\"#join\" class=\"headerlink\" title=\"join\"></a>join</h2><p>map join ,semi join ,reduce join</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2>","site":{"data":{}},"excerpt":"<h1 id=\"mapreduce组件总结\"><a href=\"#mapreduce组件总结\" class=\"headerlink\" title=\"mapreduce组件总结\"></a>mapreduce组件总结</h1>","more":"<p>相关组件大致有</p>\n<ol>\n<li>Inputformat</li>\n<li>Inputsplit</li>\n<li>ReadRecorder</li>\n<li>mapper</li>\n<li>Combiner</li>\n<li>Partioner</li>\n<li>Reduce</li>\n<li>GroupComparator</li>\n<li>Reduce</li>\n</ol>\n<h1 id=\"shuffle\"><a href=\"#shuffle\" class=\"headerlink\" title=\"shuffle\"></a>shuffle</h1><p><img src=\"http://img.wqkenqing.ren/2019-03-19-15-39-59.png\" alt=\"2019-03-19-15-39-59\"><br><img src=\"http://img.wqkenqing.ren/2019-03-19-16-46-06.png\" alt=\"2019-03-19-16-46-06\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">shuffle 被称为mapreduce的核心,一个真正让奇迹发生的地方.但它到底是什么呢?简练的讲,它就是 map out 到 reduce in 这段过程中对数据的处理过程.</span><br></pre></td></tr></table></figure>\n<p>shuffle过程中主要发生的操作有,Partion,Sort,spill,merge,copy,sort,merge.(还有可能有combine操作)</p>\n<p>具体流程是<br>map out后,Collector 对out后的数据进行处理. 数据将会写入到内存缓冲区,该内存缓冲区的数据达到80%后,会开启一个溢写线程,在磁盘本地创建一个文件.如果reduce设置了多个分区,写入buffer区的数据,会被打上一个分区标记.通过sortAndSpill()方法进行指对数据按分区号,key排序.最后溢出的文件是分区的,按key有序的文件.若buffer区中的20%一直未被填满,buffer写入进程不会断.但若达到100%,Buffer写入进程则会阻塞.并在buffer区中的数据全部spill完后才会再开启. (buffer区的内存默认是100M),spill过程中,若设置过combiner.则会对数据先进行combiner逻辑处理,再将处理后的数据写出</p>\n<p>spill完成后则会对本地的spill后的文件进行Merge.即把多个spill后的文件进行合并,并排序.最后会行成一个有序文件</p>\n<p>当1个Map Task 完成后,reduce 就会开启copy进程(默认是5个线程).这个过程中会通过http请求去各taskTracker(nodemanager),拉取相应的spill&amp;merge后的文件.<br>当copy完成后,则又会对数据进行merge.这个过程中同样有个类似map shuffle 中的buffer 溢写的阶段. 这个过程同样会触发combiner组件.这里的merge数据源有三种</p>\n<ol>\n<li>memory to memory</li>\n<li>memory to disk</li>\n<li>disk   to disk<br>默认1是不开启的.</li>\n</ol>\n<p>copy phase 完成后,是reduceTask 中的 sort phase<br>即对merge 中的文件继续进行sort and group .</p>\n<p>当sort phase 完成.则开启reduce phase .到此shuffle正式完成.</p>\n<p>##二次排序</p>\n<pre><code></code></pre><p>mapreduce 常见的辅助排序</p>\n<ol>\n<li>partitioner</li>\n<li>key的比较Comparator</li>\n<li>分组函数Grouping Comparator</li>\n</ol>\n<h2 id=\"join\"><a href=\"#join\" class=\"headerlink\" title=\"join\"></a>join</h2><p>map join ,semi join ,reduce join</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2>"},{"title":"flume记录","date":"2019-06-12T16:00:00.000Z","_content":"此处简介\n<!--more-->\n# flume记录\n\n## from kafka\n<!-- more -->\n```\na1.sources = source1\n\na1.sources.source1.type = org.apache.flume.source.kafka.KafkaSource\n\na1.sources.source1.channels = c1\n\na1.sources.source1.batchSize = 5000\n\na1.sources.source1.batchDurationMillis = 2000\na1.sources.source1.zookeeperConnect = localhost:2181\n\n#a1.sources.source1.kafka.brokerList = localhost:9092\na1.sources.source1.kafka.bootstrap.servers = localhost:9092\na1.sources.source1.topic = flumetest\na1.sources.source1.kafka.consumer.group.id = custom.g.id\n\n\n\na1.channels = c1\n\na1.channels.c1.type = memory\n\na1.channels.c1.capacity = 10000\n\na1.channels.c1.transactionCapacity = 10000\n\na1.channels.c1.byteCapacityBufferPercentage = 20\n\na1.channels.c1.byteCapacity = 800000\n\n\n\na1.sinks = k1\n\na1.sinks.k1.type = file_roll\n\na1.sinks.k1.channel = c1\n\na1.sinks.k1.sink.directory = /home/hadoop/testfile/flume\n```\n\n这里也有版本匹配的问题.经过多番尝试,这里的组合版本是flume1.6+kafka_2.11-2.2.0.tgz \n其它版本可能会有request header 问题.\n另外还遇到了指定topic 和 zookeeper的问题.\n\n执行语句:flume-ng agent -n a1 -c conf -f kafka.properties -Dflume.root.logger=INFO,console\n\n## flume 采集到kafka\n\n\n\n\n```\nagent.sources=r1\nagent.sinks=k1\nagent.channels=c1\n\nagent.sources.r1.type=exec\nagent.sources.r1.command=tail /root/tomcat/logs/catalina.out\nagent.sources.r1.restart=true\nagent.sources.r1.batchSize=1000\nagent.sources.r1.batchTimeout=3000\nagent.sources.r1.channels=c1\n\nagent.channels.c1.type=memory\nagent.channels.c1.capacity=102400\nagent.channels.c1.transactionCapacity=1000\n\nagent.channels.c1.byteCapacity=134217728\nagent.channels.c1.byteCapacityBufferPercentage=80\n\nagent.sinks.k1.channel=c1\nagent.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink\nagent.sinks.k1.kafka.topic=sparkstreaming\nagent.sinks.k1.kafka.zookeeperConnect=47.102.199.215:2181\n#agent.sinks.k1.kafka.bootstrap.servers=47.102.199.215:9092\nagent.sinks.k1.kafka.brokerList =47.102.199.215:9092\nagent.sinks.k1.serializer.class=kafka.serializer.StringEncoder\nagent.sinks.k1.flumeBatchSize=1000\nagent.sinks.k1.useFlumeEventFormat=true\n\n\n```","source":"_posts/技术/hexo/old/flume记录.md","raw":"---\n\ntitle: flume记录\ndate: 2019-06-13\ntags: \n\n---\n此处简介\n<!--more-->\n# flume记录\n\n## from kafka\n<!-- more -->\n```\na1.sources = source1\n\na1.sources.source1.type = org.apache.flume.source.kafka.KafkaSource\n\na1.sources.source1.channels = c1\n\na1.sources.source1.batchSize = 5000\n\na1.sources.source1.batchDurationMillis = 2000\na1.sources.source1.zookeeperConnect = localhost:2181\n\n#a1.sources.source1.kafka.brokerList = localhost:9092\na1.sources.source1.kafka.bootstrap.servers = localhost:9092\na1.sources.source1.topic = flumetest\na1.sources.source1.kafka.consumer.group.id = custom.g.id\n\n\n\na1.channels = c1\n\na1.channels.c1.type = memory\n\na1.channels.c1.capacity = 10000\n\na1.channels.c1.transactionCapacity = 10000\n\na1.channels.c1.byteCapacityBufferPercentage = 20\n\na1.channels.c1.byteCapacity = 800000\n\n\n\na1.sinks = k1\n\na1.sinks.k1.type = file_roll\n\na1.sinks.k1.channel = c1\n\na1.sinks.k1.sink.directory = /home/hadoop/testfile/flume\n```\n\n这里也有版本匹配的问题.经过多番尝试,这里的组合版本是flume1.6+kafka_2.11-2.2.0.tgz \n其它版本可能会有request header 问题.\n另外还遇到了指定topic 和 zookeeper的问题.\n\n执行语句:flume-ng agent -n a1 -c conf -f kafka.properties -Dflume.root.logger=INFO,console\n\n## flume 采集到kafka\n\n\n\n\n```\nagent.sources=r1\nagent.sinks=k1\nagent.channels=c1\n\nagent.sources.r1.type=exec\nagent.sources.r1.command=tail /root/tomcat/logs/catalina.out\nagent.sources.r1.restart=true\nagent.sources.r1.batchSize=1000\nagent.sources.r1.batchTimeout=3000\nagent.sources.r1.channels=c1\n\nagent.channels.c1.type=memory\nagent.channels.c1.capacity=102400\nagent.channels.c1.transactionCapacity=1000\n\nagent.channels.c1.byteCapacity=134217728\nagent.channels.c1.byteCapacityBufferPercentage=80\n\nagent.sinks.k1.channel=c1\nagent.sinks.k1.type=org.apache.flume.sink.kafka.KafkaSink\nagent.sinks.k1.kafka.topic=sparkstreaming\nagent.sinks.k1.kafka.zookeeperConnect=47.102.199.215:2181\n#agent.sinks.k1.kafka.bootstrap.servers=47.102.199.215:9092\nagent.sinks.k1.kafka.brokerList =47.102.199.215:9092\nagent.sinks.k1.serializer.class=kafka.serializer.StringEncoder\nagent.sinks.k1.flumeBatchSize=1000\nagent.sinks.k1.useFlumeEventFormat=true\n\n\n```","slug":"技术/hexo/old/flume记录","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd32x001y38pw4cbu7vec","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n<h1 id=\"flume记录\"><a href=\"#flume记录\" class=\"headerlink\" title=\"flume记录\"></a>flume记录</h1><h2 id=\"from-kafka\"><a href=\"#from-kafka\" class=\"headerlink\" title=\"from kafka\"></a>from kafka</h2><!-- more -->\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a1.sources &#x3D; source1</span><br><span class=\"line\"></span><br><span class=\"line\">a1.sources.source1.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class=\"line\"></span><br><span class=\"line\">a1.sources.source1.channels &#x3D; c1</span><br><span class=\"line\"></span><br><span class=\"line\">a1.sources.source1.batchSize &#x3D; 5000</span><br><span class=\"line\"></span><br><span class=\"line\">a1.sources.source1.batchDurationMillis &#x3D; 2000</span><br><span class=\"line\">a1.sources.source1.zookeeperConnect &#x3D; localhost:2181</span><br><span class=\"line\"></span><br><span class=\"line\">#a1.sources.source1.kafka.brokerList &#x3D; localhost:9092</span><br><span class=\"line\">a1.sources.source1.kafka.bootstrap.servers &#x3D; localhost:9092</span><br><span class=\"line\">a1.sources.source1.topic &#x3D; flumetest</span><br><span class=\"line\">a1.sources.source1.kafka.consumer.group.id &#x3D; custom.g.id</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">a1.channels &#x3D; c1</span><br><span class=\"line\"></span><br><span class=\"line\">a1.channels.c1.type &#x3D; memory</span><br><span class=\"line\"></span><br><span class=\"line\">a1.channels.c1.capacity &#x3D; 10000</span><br><span class=\"line\"></span><br><span class=\"line\">a1.channels.c1.transactionCapacity &#x3D; 10000</span><br><span class=\"line\"></span><br><span class=\"line\">a1.channels.c1.byteCapacityBufferPercentage &#x3D; 20</span><br><span class=\"line\"></span><br><span class=\"line\">a1.channels.c1.byteCapacity &#x3D; 800000</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">a1.sinks &#x3D; k1</span><br><span class=\"line\"></span><br><span class=\"line\">a1.sinks.k1.type &#x3D; file_roll</span><br><span class=\"line\"></span><br><span class=\"line\">a1.sinks.k1.channel &#x3D; c1</span><br><span class=\"line\"></span><br><span class=\"line\">a1.sinks.k1.sink.directory &#x3D; &#x2F;home&#x2F;hadoop&#x2F;testfile&#x2F;flume</span><br></pre></td></tr></table></figure>\n\n<p>这里也有版本匹配的问题.经过多番尝试,这里的组合版本是flume1.6+kafka_2.11-2.2.0.tgz<br>其它版本可能会有request header 问题.<br>另外还遇到了指定topic 和 zookeeper的问题.</p>\n<p>执行语句:flume-ng agent -n a1 -c conf -f kafka.properties -Dflume.root.logger=INFO,console</p>\n<h2 id=\"flume-采集到kafka\"><a href=\"#flume-采集到kafka\" class=\"headerlink\" title=\"flume 采集到kafka\"></a>flume 采集到kafka</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">agent.sources&#x3D;r1</span><br><span class=\"line\">agent.sinks&#x3D;k1</span><br><span class=\"line\">agent.channels&#x3D;c1</span><br><span class=\"line\"></span><br><span class=\"line\">agent.sources.r1.type&#x3D;exec</span><br><span class=\"line\">agent.sources.r1.command&#x3D;tail &#x2F;root&#x2F;tomcat&#x2F;logs&#x2F;catalina.out</span><br><span class=\"line\">agent.sources.r1.restart&#x3D;true</span><br><span class=\"line\">agent.sources.r1.batchSize&#x3D;1000</span><br><span class=\"line\">agent.sources.r1.batchTimeout&#x3D;3000</span><br><span class=\"line\">agent.sources.r1.channels&#x3D;c1</span><br><span class=\"line\"></span><br><span class=\"line\">agent.channels.c1.type&#x3D;memory</span><br><span class=\"line\">agent.channels.c1.capacity&#x3D;102400</span><br><span class=\"line\">agent.channels.c1.transactionCapacity&#x3D;1000</span><br><span class=\"line\"></span><br><span class=\"line\">agent.channels.c1.byteCapacity&#x3D;134217728</span><br><span class=\"line\">agent.channels.c1.byteCapacityBufferPercentage&#x3D;80</span><br><span class=\"line\"></span><br><span class=\"line\">agent.sinks.k1.channel&#x3D;c1</span><br><span class=\"line\">agent.sinks.k1.type&#x3D;org.apache.flume.sink.kafka.KafkaSink</span><br><span class=\"line\">agent.sinks.k1.kafka.topic&#x3D;sparkstreaming</span><br><span class=\"line\">agent.sinks.k1.kafka.zookeeperConnect&#x3D;47.102.199.215:2181</span><br><span class=\"line\">#agent.sinks.k1.kafka.bootstrap.servers&#x3D;47.102.199.215:9092</span><br><span class=\"line\">agent.sinks.k1.kafka.brokerList &#x3D;47.102.199.215:9092</span><br><span class=\"line\">agent.sinks.k1.serializer.class&#x3D;kafka.serializer.StringEncoder</span><br><span class=\"line\">agent.sinks.k1.flumeBatchSize&#x3D;1000</span><br><span class=\"line\">agent.sinks.k1.useFlumeEventFormat&#x3D;true</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h1 id=\"flume记录\"><a href=\"#flume记录\" class=\"headerlink\" title=\"flume记录\"></a>flume记录</h1><h2 id=\"from-kafka\"><a href=\"#from-kafka\" class=\"headerlink\" title=\"from kafka\"></a>from kafka</h2><!-- more -->\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a1.sources &#x3D; source1</span><br><span class=\"line\"></span><br><span class=\"line\">a1.sources.source1.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class=\"line\"></span><br><span class=\"line\">a1.sources.source1.channels &#x3D; c1</span><br><span class=\"line\"></span><br><span class=\"line\">a1.sources.source1.batchSize &#x3D; 5000</span><br><span class=\"line\"></span><br><span class=\"line\">a1.sources.source1.batchDurationMillis &#x3D; 2000</span><br><span class=\"line\">a1.sources.source1.zookeeperConnect &#x3D; localhost:2181</span><br><span class=\"line\"></span><br><span class=\"line\">#a1.sources.source1.kafka.brokerList &#x3D; localhost:9092</span><br><span class=\"line\">a1.sources.source1.kafka.bootstrap.servers &#x3D; localhost:9092</span><br><span class=\"line\">a1.sources.source1.topic &#x3D; flumetest</span><br><span class=\"line\">a1.sources.source1.kafka.consumer.group.id &#x3D; custom.g.id</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">a1.channels &#x3D; c1</span><br><span class=\"line\"></span><br><span class=\"line\">a1.channels.c1.type &#x3D; memory</span><br><span class=\"line\"></span><br><span class=\"line\">a1.channels.c1.capacity &#x3D; 10000</span><br><span class=\"line\"></span><br><span class=\"line\">a1.channels.c1.transactionCapacity &#x3D; 10000</span><br><span class=\"line\"></span><br><span class=\"line\">a1.channels.c1.byteCapacityBufferPercentage &#x3D; 20</span><br><span class=\"line\"></span><br><span class=\"line\">a1.channels.c1.byteCapacity &#x3D; 800000</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">a1.sinks &#x3D; k1</span><br><span class=\"line\"></span><br><span class=\"line\">a1.sinks.k1.type &#x3D; file_roll</span><br><span class=\"line\"></span><br><span class=\"line\">a1.sinks.k1.channel &#x3D; c1</span><br><span class=\"line\"></span><br><span class=\"line\">a1.sinks.k1.sink.directory &#x3D; &#x2F;home&#x2F;hadoop&#x2F;testfile&#x2F;flume</span><br></pre></td></tr></table></figure>\n\n<p>这里也有版本匹配的问题.经过多番尝试,这里的组合版本是flume1.6+kafka_2.11-2.2.0.tgz<br>其它版本可能会有request header 问题.<br>另外还遇到了指定topic 和 zookeeper的问题.</p>\n<p>执行语句:flume-ng agent -n a1 -c conf -f kafka.properties -Dflume.root.logger=INFO,console</p>\n<h2 id=\"flume-采集到kafka\"><a href=\"#flume-采集到kafka\" class=\"headerlink\" title=\"flume 采集到kafka\"></a>flume 采集到kafka</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">agent.sources&#x3D;r1</span><br><span class=\"line\">agent.sinks&#x3D;k1</span><br><span class=\"line\">agent.channels&#x3D;c1</span><br><span class=\"line\"></span><br><span class=\"line\">agent.sources.r1.type&#x3D;exec</span><br><span class=\"line\">agent.sources.r1.command&#x3D;tail &#x2F;root&#x2F;tomcat&#x2F;logs&#x2F;catalina.out</span><br><span class=\"line\">agent.sources.r1.restart&#x3D;true</span><br><span class=\"line\">agent.sources.r1.batchSize&#x3D;1000</span><br><span class=\"line\">agent.sources.r1.batchTimeout&#x3D;3000</span><br><span class=\"line\">agent.sources.r1.channels&#x3D;c1</span><br><span class=\"line\"></span><br><span class=\"line\">agent.channels.c1.type&#x3D;memory</span><br><span class=\"line\">agent.channels.c1.capacity&#x3D;102400</span><br><span class=\"line\">agent.channels.c1.transactionCapacity&#x3D;1000</span><br><span class=\"line\"></span><br><span class=\"line\">agent.channels.c1.byteCapacity&#x3D;134217728</span><br><span class=\"line\">agent.channels.c1.byteCapacityBufferPercentage&#x3D;80</span><br><span class=\"line\"></span><br><span class=\"line\">agent.sinks.k1.channel&#x3D;c1</span><br><span class=\"line\">agent.sinks.k1.type&#x3D;org.apache.flume.sink.kafka.KafkaSink</span><br><span class=\"line\">agent.sinks.k1.kafka.topic&#x3D;sparkstreaming</span><br><span class=\"line\">agent.sinks.k1.kafka.zookeeperConnect&#x3D;47.102.199.215:2181</span><br><span class=\"line\">#agent.sinks.k1.kafka.bootstrap.servers&#x3D;47.102.199.215:9092</span><br><span class=\"line\">agent.sinks.k1.kafka.brokerList &#x3D;47.102.199.215:9092</span><br><span class=\"line\">agent.sinks.k1.serializer.class&#x3D;kafka.serializer.StringEncoder</span><br><span class=\"line\">agent.sinks.k1.flumeBatchSize&#x3D;1000</span><br><span class=\"line\">agent.sinks.k1.useFlumeEventFormat&#x3D;true</span><br></pre></td></tr></table></figure>"},{"title":"spark学习2","date":"2018-03-04T03:12:58.000Z","_content":"# spark学习2\n## spark 运行的四种模式\n<!-- more -->\n\n### 本地模式\n如\n```\n./bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[1] ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100\n```\n\n### standlone模式\n\n#### client \n./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://spark001:7077 --executor-memory 1G --total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100\n#### cluster\n./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://spark001:7077 --deploy-mode cluster --supervise --executor-memory 1G --total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.7.0.jar 100\n\n### Yarn模式\n#### client模式\n```\nclient模式：\n结果xshell可见：\n./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --executor-memory 1G --num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.7.0.jar 100\n```\n#### cluster模式\n./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster --executor-memory 1G --num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100\n\n## spark sql\n\n\n","source":"_posts/技术/hexo/old/spark学习2.md","raw":"---\ntitle: spark学习2\ndate: 2018-03-04 11:12:58\ntags: 学习spark2\n---\n# spark学习2\n## spark 运行的四种模式\n<!-- more -->\n\n### 本地模式\n如\n```\n./bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[1] ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100\n```\n\n### standlone模式\n\n#### client \n./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://spark001:7077 --executor-memory 1G --total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100\n#### cluster\n./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://spark001:7077 --deploy-mode cluster --supervise --executor-memory 1G --total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.7.0.jar 100\n\n### Yarn模式\n#### client模式\n```\nclient模式：\n结果xshell可见：\n./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --executor-memory 1G --num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.7.0.jar 100\n```\n#### cluster模式\n./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster --executor-memory 1G --num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100\n\n## spark sql\n\n\n","slug":"技术/hexo/old/spark学习2","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd32z002038pw0aub85xl","content":"<h1 id=\"spark学习2\"><a href=\"#spark学习2\" class=\"headerlink\" title=\"spark学习2\"></a>spark学习2</h1><h2 id=\"spark-运行的四种模式\"><a href=\"#spark-运行的四种模式\" class=\"headerlink\" title=\"spark 运行的四种模式\"></a>spark 运行的四种模式</h2><a id=\"more\"></a>\n\n<h3 id=\"本地模式\"><a href=\"#本地模式\" class=\"headerlink\" title=\"本地模式\"></a>本地模式</h3><p>如</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.&#x2F;bin&#x2F;spark-submit --class org.apache.spark.examples.SparkPi --master local[1] .&#x2F;lib&#x2F;spark-examples-1.3.1-hadoop2.4.0.jar 100</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"standlone模式\"><a href=\"#standlone模式\" class=\"headerlink\" title=\"standlone模式\"></a>standlone模式</h3><h4 id=\"client\"><a href=\"#client\" class=\"headerlink\" title=\"client\"></a>client</h4><p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master spark://spark001:7077 –executor-memory 1G –total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100</p>\n<h4 id=\"cluster\"><a href=\"#cluster\" class=\"headerlink\" title=\"cluster\"></a>cluster</h4><p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master spark://spark001:7077 –deploy-mode cluster –supervise –executor-memory 1G –total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.7.0.jar 100</p>\n<h3 id=\"Yarn模式\"><a href=\"#Yarn模式\" class=\"headerlink\" title=\"Yarn模式\"></a>Yarn模式</h3><h4 id=\"client模式\"><a href=\"#client模式\" class=\"headerlink\" title=\"client模式\"></a>client模式</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">client模式：</span><br><span class=\"line\">结果xshell可见：</span><br><span class=\"line\">.&#x2F;bin&#x2F;spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --executor-memory 1G --num-executors 1 .&#x2F;lib&#x2F;spark-examples-1.3.1-hadoop2.7.0.jar 100</span><br></pre></td></tr></table></figure>\n<h4 id=\"cluster模式\"><a href=\"#cluster模式\" class=\"headerlink\" title=\"cluster模式\"></a>cluster模式</h4><p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master yarn-cluster –executor-memory 1G –num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100</p>\n<h2 id=\"spark-sql\"><a href=\"#spark-sql\" class=\"headerlink\" title=\"spark sql\"></a>spark sql</h2>","site":{"data":{}},"excerpt":"<h1 id=\"spark学习2\"><a href=\"#spark学习2\" class=\"headerlink\" title=\"spark学习2\"></a>spark学习2</h1><h2 id=\"spark-运行的四种模式\"><a href=\"#spark-运行的四种模式\" class=\"headerlink\" title=\"spark 运行的四种模式\"></a>spark 运行的四种模式</h2>","more":"<h3 id=\"本地模式\"><a href=\"#本地模式\" class=\"headerlink\" title=\"本地模式\"></a>本地模式</h3><p>如</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.&#x2F;bin&#x2F;spark-submit --class org.apache.spark.examples.SparkPi --master local[1] .&#x2F;lib&#x2F;spark-examples-1.3.1-hadoop2.4.0.jar 100</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"standlone模式\"><a href=\"#standlone模式\" class=\"headerlink\" title=\"standlone模式\"></a>standlone模式</h3><h4 id=\"client\"><a href=\"#client\" class=\"headerlink\" title=\"client\"></a>client</h4><p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master spark://spark001:7077 –executor-memory 1G –total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100</p>\n<h4 id=\"cluster\"><a href=\"#cluster\" class=\"headerlink\" title=\"cluster\"></a>cluster</h4><p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master spark://spark001:7077 –deploy-mode cluster –supervise –executor-memory 1G –total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.7.0.jar 100</p>\n<h3 id=\"Yarn模式\"><a href=\"#Yarn模式\" class=\"headerlink\" title=\"Yarn模式\"></a>Yarn模式</h3><h4 id=\"client模式\"><a href=\"#client模式\" class=\"headerlink\" title=\"client模式\"></a>client模式</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">client模式：</span><br><span class=\"line\">结果xshell可见：</span><br><span class=\"line\">.&#x2F;bin&#x2F;spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --executor-memory 1G --num-executors 1 .&#x2F;lib&#x2F;spark-examples-1.3.1-hadoop2.7.0.jar 100</span><br></pre></td></tr></table></figure>\n<h4 id=\"cluster模式\"><a href=\"#cluster模式\" class=\"headerlink\" title=\"cluster模式\"></a>cluster模式</h4><p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master yarn-cluster –executor-memory 1G –num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100</p>\n<h2 id=\"spark-sql\"><a href=\"#spark-sql\" class=\"headerlink\" title=\"spark sql\"></a>spark sql</h2>"},{"title":"spark学习","date":"2018-03-04T03:12:58.000Z","_content":"\n# spark 学习\n<!-- more -->\n\n```\nspark 作为主流的实时计算引擎,需要高度掌握\n```\n<!-- more -->\n\n## spark介绍\n\nApache Spark是一用于实时处理的开源集群计算框架.持多种语言编程,Spark Streaming有高吞吐量和容错能力强等特点.\n数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算,而结果也能保存在很多地方，如HDFS，数据库等。另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。\n\n优点\n+ 易用\n+ 容错\n+ spark体系整合\n\n![spark&storm对比](http://img.wqkenqing.ren/2019-03-04-15-45-38.png)\n\n\n\n## RDD详解\n### RDD是什么\nRDD：Spark的核心概念是RDD (resilientdistributed dataset)，指的是一个只读的，可分区的分布式数据集，这个数据集的全部或部分可以缓存在内存中，在多次计算间重用。\n\n另:RDD即弹性分布式数据集，有容错机制并可以被并行操作的元素集合，具有只读、分区、容错、高效、无需物化、可以缓存、RDD依赖等特征。RDD只是数据集的抽象，分区内部并不会存储具体的数据。\n\nRDD的五个特性\n1. 有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。 \n2. 有一个函数计算每一个分片，这里指的是下面会提到的compute函数.\n3. 对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖.\n4. 可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。\n5. 可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。(存储的是一个表，可以将处理的分区“本地化”).\n\n\n```scala\n//只计算一次  \n  protected def getPartitions: Array[Partition]  \n  //对一个分片进行计算，得出一个可遍历的结果\n  def compute(split: Partition, context: TaskContext): Iterator[T]\n  //只计算一次，计算RDD对父RDD的依赖\n  protected def getDependencies: Seq[Dependency[_]] = deps\n  //可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce\n  @transient val partitioner: Option[Partitioner] = None\n  //可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置\n  protected def getPreferredLocations(split: Partition): Seq[String] = Nil\n\n```\n\n### 为什么会产生RDD\n\n### RDD数据集\n1. 并行集合\n\n接收一个已经存在的集合,然后进行各种并行计算.并行化集合是通过调用SparkContext的parallelize方法，在一个已经存在的Scala集合上创建（一个Seq对象）。集合的对象将会被拷贝，创建出一个可以被并行操作的分布式数据集。\n\n2. Hadoop数据集\n\nSpark可以将任何Hadoop所支持的存储资源转化成RDD，只要文件系统是HDFS，或者Hadoop支持的任意存储系统即可，如本地文件（需要网络文件系统，所有的节点都必须能访问到）、HDFS、Cassandra、HBase、Amazon S3等，Spark支持文本文件、SequenceFiles和任何Hadoop InputFormat格式。\n\n此两种类型的RDD都可以通过相同的方式进行操作，从而获得子RDD等一系列拓展，形成lineage血统关系图。\n\n\n### Spark RDD算子\n1. Transformation\n不触发提交作业，完成作业中间处理过程。\n\n\n\n## DStream\n### 什么是DStream\n\nDiscretized Stream :代表持续性的数据流和经过各种Spark原语操作后的结果数据流,在内部实现上是一系列连续的RDD来表示.每个RDD含有一段时间间隔内的数据,如下图\n![DStream](http://img.wqkenqing.ren/2019-03-04-15-50-41.png)\n\n计算则由spark engine来完成\n![spark engine流程](http://img.wqkenqing.ren/2019-03-04-15-51-58.png)\n\n\n\n\n## spark java \n\n因为我是主要掌握的语言是java,从效率上来考虑,这里\n\n\n\n\n\n\n## 参考博客\n\nhttps://blog.csdn.net/wangxiaotongfan/article/details/51395769 RDD详解\nhttps://blog.csdn.net/zuochang_liu/article/details/81459185  spark streaming学习\nhttps://blog.csdn.net/hellozhxy/article/details/81672845 spark java 使用指南\nhttps://blog.csdn.net/t1dmzks/article/details/70198430 sparkRDD算子介绍\nhttps://blog.csdn.net/wxycx11111/article/details/79123482 **sparkRDD入门介绍**\nhttps://github.com/zhaikaishun/spark_tutorial **RDD算子介绍**\n","source":"_posts/技术/hexo/old/spark学习.md","raw":"---\ntitle: spark学习\ndate: 2018-03-04 11:12:58\ntags: 学习spark\n---\n\n# spark 学习\n<!-- more -->\n\n```\nspark 作为主流的实时计算引擎,需要高度掌握\n```\n<!-- more -->\n\n## spark介绍\n\nApache Spark是一用于实时处理的开源集群计算框架.持多种语言编程,Spark Streaming有高吞吐量和容错能力强等特点.\n数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算,而结果也能保存在很多地方，如HDFS，数据库等。另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。\n\n优点\n+ 易用\n+ 容错\n+ spark体系整合\n\n![spark&storm对比](http://img.wqkenqing.ren/2019-03-04-15-45-38.png)\n\n\n\n## RDD详解\n### RDD是什么\nRDD：Spark的核心概念是RDD (resilientdistributed dataset)，指的是一个只读的，可分区的分布式数据集，这个数据集的全部或部分可以缓存在内存中，在多次计算间重用。\n\n另:RDD即弹性分布式数据集，有容错机制并可以被并行操作的元素集合，具有只读、分区、容错、高效、无需物化、可以缓存、RDD依赖等特征。RDD只是数据集的抽象，分区内部并不会存储具体的数据。\n\nRDD的五个特性\n1. 有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。 \n2. 有一个函数计算每一个分片，这里指的是下面会提到的compute函数.\n3. 对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖.\n4. 可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。\n5. 可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。(存储的是一个表，可以将处理的分区“本地化”).\n\n\n```scala\n//只计算一次  \n  protected def getPartitions: Array[Partition]  \n  //对一个分片进行计算，得出一个可遍历的结果\n  def compute(split: Partition, context: TaskContext): Iterator[T]\n  //只计算一次，计算RDD对父RDD的依赖\n  protected def getDependencies: Seq[Dependency[_]] = deps\n  //可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce\n  @transient val partitioner: Option[Partitioner] = None\n  //可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置\n  protected def getPreferredLocations(split: Partition): Seq[String] = Nil\n\n```\n\n### 为什么会产生RDD\n\n### RDD数据集\n1. 并行集合\n\n接收一个已经存在的集合,然后进行各种并行计算.并行化集合是通过调用SparkContext的parallelize方法，在一个已经存在的Scala集合上创建（一个Seq对象）。集合的对象将会被拷贝，创建出一个可以被并行操作的分布式数据集。\n\n2. Hadoop数据集\n\nSpark可以将任何Hadoop所支持的存储资源转化成RDD，只要文件系统是HDFS，或者Hadoop支持的任意存储系统即可，如本地文件（需要网络文件系统，所有的节点都必须能访问到）、HDFS、Cassandra、HBase、Amazon S3等，Spark支持文本文件、SequenceFiles和任何Hadoop InputFormat格式。\n\n此两种类型的RDD都可以通过相同的方式进行操作，从而获得子RDD等一系列拓展，形成lineage血统关系图。\n\n\n### Spark RDD算子\n1. Transformation\n不触发提交作业，完成作业中间处理过程。\n\n\n\n## DStream\n### 什么是DStream\n\nDiscretized Stream :代表持续性的数据流和经过各种Spark原语操作后的结果数据流,在内部实现上是一系列连续的RDD来表示.每个RDD含有一段时间间隔内的数据,如下图\n![DStream](http://img.wqkenqing.ren/2019-03-04-15-50-41.png)\n\n计算则由spark engine来完成\n![spark engine流程](http://img.wqkenqing.ren/2019-03-04-15-51-58.png)\n\n\n\n\n## spark java \n\n因为我是主要掌握的语言是java,从效率上来考虑,这里\n\n\n\n\n\n\n## 参考博客\n\nhttps://blog.csdn.net/wangxiaotongfan/article/details/51395769 RDD详解\nhttps://blog.csdn.net/zuochang_liu/article/details/81459185  spark streaming学习\nhttps://blog.csdn.net/hellozhxy/article/details/81672845 spark java 使用指南\nhttps://blog.csdn.net/t1dmzks/article/details/70198430 sparkRDD算子介绍\nhttps://blog.csdn.net/wxycx11111/article/details/79123482 **sparkRDD入门介绍**\nhttps://github.com/zhaikaishun/spark_tutorial **RDD算子介绍**\n","slug":"技术/hexo/old/spark学习","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd330002238pwfljd6vsz","content":"<h1 id=\"spark-学习\"><a href=\"#spark-学习\" class=\"headerlink\" title=\"spark 学习\"></a>spark 学习</h1><a id=\"more\"></a>\n\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">spark 作为主流的实时计算引擎,需要高度掌握</span><br></pre></td></tr></table></figure>\n<!-- more -->\n\n<h2 id=\"spark介绍\"><a href=\"#spark介绍\" class=\"headerlink\" title=\"spark介绍\"></a>spark介绍</h2><p>Apache Spark是一用于实时处理的开源集群计算框架.持多种语言编程,Spark Streaming有高吞吐量和容错能力强等特点.<br>数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算,而结果也能保存在很多地方，如HDFS，数据库等。另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。</p>\n<p>优点</p>\n<ul>\n<li>易用</li>\n<li>容错</li>\n<li>spark体系整合</li>\n</ul>\n<p><img src=\"http://img.wqkenqing.ren/2019-03-04-15-45-38.png\" alt=\"spark&amp;storm对比\"></p>\n<h2 id=\"RDD详解\"><a href=\"#RDD详解\" class=\"headerlink\" title=\"RDD详解\"></a>RDD详解</h2><h3 id=\"RDD是什么\"><a href=\"#RDD是什么\" class=\"headerlink\" title=\"RDD是什么\"></a>RDD是什么</h3><p>RDD：Spark的核心概念是RDD (resilientdistributed dataset)，指的是一个只读的，可分区的分布式数据集，这个数据集的全部或部分可以缓存在内存中，在多次计算间重用。</p>\n<p>另:RDD即弹性分布式数据集，有容错机制并可以被并行操作的元素集合，具有只读、分区、容错、高效、无需物化、可以缓存、RDD依赖等特征。RDD只是数据集的抽象，分区内部并不会存储具体的数据。</p>\n<p>RDD的五个特性</p>\n<ol>\n<li>有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。 </li>\n<li>有一个函数计算每一个分片，这里指的是下面会提到的compute函数.</li>\n<li>对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖.</li>\n<li>可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。</li>\n<li>可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。(存储的是一个表，可以将处理的分区“本地化”).</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//只计算一次  </span></span><br><span class=\"line\">  <span class=\"keyword\">protected</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>]  </span><br><span class=\"line\">  <span class=\"comment\">//对一个分片进行计算，得出一个可遍历的结果</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]</span><br><span class=\"line\">  <span class=\"comment\">//只计算一次，计算RDD对父RDD的依赖</span></span><br><span class=\"line\">  <span class=\"keyword\">protected</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getDependencies</span></span>: <span class=\"type\">Seq</span>[<span class=\"type\">Dependency</span>[_]] = deps</span><br><span class=\"line\">  <span class=\"comment\">//可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce</span></span><br><span class=\"line\">  <span class=\"meta\">@transient</span> <span class=\"keyword\">val</span> partitioner: <span class=\"type\">Option</span>[<span class=\"type\">Partitioner</span>] = <span class=\"type\">None</span></span><br><span class=\"line\">  <span class=\"comment\">//可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置</span></span><br><span class=\"line\">  <span class=\"keyword\">protected</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPreferredLocations</span></span>(split: <span class=\"type\">Partition</span>): <span class=\"type\">Seq</span>[<span class=\"type\">String</span>] = <span class=\"type\">Nil</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"为什么会产生RDD\"><a href=\"#为什么会产生RDD\" class=\"headerlink\" title=\"为什么会产生RDD\"></a>为什么会产生RDD</h3><h3 id=\"RDD数据集\"><a href=\"#RDD数据集\" class=\"headerlink\" title=\"RDD数据集\"></a>RDD数据集</h3><ol>\n<li>并行集合</li>\n</ol>\n<p>接收一个已经存在的集合,然后进行各种并行计算.并行化集合是通过调用SparkContext的parallelize方法，在一个已经存在的Scala集合上创建（一个Seq对象）。集合的对象将会被拷贝，创建出一个可以被并行操作的分布式数据集。</p>\n<ol start=\"2\">\n<li>Hadoop数据集</li>\n</ol>\n<p>Spark可以将任何Hadoop所支持的存储资源转化成RDD，只要文件系统是HDFS，或者Hadoop支持的任意存储系统即可，如本地文件（需要网络文件系统，所有的节点都必须能访问到）、HDFS、Cassandra、HBase、Amazon S3等，Spark支持文本文件、SequenceFiles和任何Hadoop InputFormat格式。</p>\n<p>此两种类型的RDD都可以通过相同的方式进行操作，从而获得子RDD等一系列拓展，形成lineage血统关系图。</p>\n<h3 id=\"Spark-RDD算子\"><a href=\"#Spark-RDD算子\" class=\"headerlink\" title=\"Spark RDD算子\"></a>Spark RDD算子</h3><ol>\n<li>Transformation<br>不触发提交作业，完成作业中间处理过程。</li>\n</ol>\n<h2 id=\"DStream\"><a href=\"#DStream\" class=\"headerlink\" title=\"DStream\"></a>DStream</h2><h3 id=\"什么是DStream\"><a href=\"#什么是DStream\" class=\"headerlink\" title=\"什么是DStream\"></a>什么是DStream</h3><p>Discretized Stream :代表持续性的数据流和经过各种Spark原语操作后的结果数据流,在内部实现上是一系列连续的RDD来表示.每个RDD含有一段时间间隔内的数据,如下图<br><img src=\"http://img.wqkenqing.ren/2019-03-04-15-50-41.png\" alt=\"DStream\"></p>\n<p>计算则由spark engine来完成<br><img src=\"http://img.wqkenqing.ren/2019-03-04-15-51-58.png\" alt=\"spark engine流程\"></p>\n<h2 id=\"spark-java\"><a href=\"#spark-java\" class=\"headerlink\" title=\"spark java\"></a>spark java</h2><p>因为我是主要掌握的语言是java,从效率上来考虑,这里</p>\n<h2 id=\"参考博客\"><a href=\"#参考博客\" class=\"headerlink\" title=\"参考博客\"></a>参考博客</h2><p><a href=\"https://blog.csdn.net/wangxiaotongfan/article/details/51395769\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/wangxiaotongfan/article/details/51395769</a> RDD详解<br><a href=\"https://blog.csdn.net/zuochang_liu/article/details/81459185\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/zuochang_liu/article/details/81459185</a>  spark streaming学习<br><a href=\"https://blog.csdn.net/hellozhxy/article/details/81672845\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/hellozhxy/article/details/81672845</a> spark java 使用指南<br><a href=\"https://blog.csdn.net/t1dmzks/article/details/70198430\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/t1dmzks/article/details/70198430</a> sparkRDD算子介绍<br><a href=\"https://blog.csdn.net/wxycx11111/article/details/79123482\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/wxycx11111/article/details/79123482</a> <strong>sparkRDD入门介绍</strong><br><a href=\"https://github.com/zhaikaishun/spark_tutorial\" target=\"_blank\" rel=\"noopener\">https://github.com/zhaikaishun/spark_tutorial</a> <strong>RDD算子介绍</strong></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"spark-学习\"><a href=\"#spark-学习\" class=\"headerlink\" title=\"spark 学习\"></a>spark 学习</h1>","more":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">spark 作为主流的实时计算引擎,需要高度掌握</span><br></pre></td></tr></table></figure>\n<!-- more -->\n\n<h2 id=\"spark介绍\"><a href=\"#spark介绍\" class=\"headerlink\" title=\"spark介绍\"></a>spark介绍</h2><p>Apache Spark是一用于实时处理的开源集群计算框架.持多种语言编程,Spark Streaming有高吞吐量和容错能力强等特点.<br>数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算,而结果也能保存在很多地方，如HDFS，数据库等。另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。</p>\n<p>优点</p>\n<ul>\n<li>易用</li>\n<li>容错</li>\n<li>spark体系整合</li>\n</ul>\n<p><img src=\"http://img.wqkenqing.ren/2019-03-04-15-45-38.png\" alt=\"spark&amp;storm对比\"></p>\n<h2 id=\"RDD详解\"><a href=\"#RDD详解\" class=\"headerlink\" title=\"RDD详解\"></a>RDD详解</h2><h3 id=\"RDD是什么\"><a href=\"#RDD是什么\" class=\"headerlink\" title=\"RDD是什么\"></a>RDD是什么</h3><p>RDD：Spark的核心概念是RDD (resilientdistributed dataset)，指的是一个只读的，可分区的分布式数据集，这个数据集的全部或部分可以缓存在内存中，在多次计算间重用。</p>\n<p>另:RDD即弹性分布式数据集，有容错机制并可以被并行操作的元素集合，具有只读、分区、容错、高效、无需物化、可以缓存、RDD依赖等特征。RDD只是数据集的抽象，分区内部并不会存储具体的数据。</p>\n<p>RDD的五个特性</p>\n<ol>\n<li>有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。 </li>\n<li>有一个函数计算每一个分片，这里指的是下面会提到的compute函数.</li>\n<li>对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖.</li>\n<li>可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。</li>\n<li>可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。(存储的是一个表，可以将处理的分区“本地化”).</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//只计算一次  </span></span><br><span class=\"line\">  <span class=\"keyword\">protected</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>]  </span><br><span class=\"line\">  <span class=\"comment\">//对一个分片进行计算，得出一个可遍历的结果</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]</span><br><span class=\"line\">  <span class=\"comment\">//只计算一次，计算RDD对父RDD的依赖</span></span><br><span class=\"line\">  <span class=\"keyword\">protected</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getDependencies</span></span>: <span class=\"type\">Seq</span>[<span class=\"type\">Dependency</span>[_]] = deps</span><br><span class=\"line\">  <span class=\"comment\">//可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce</span></span><br><span class=\"line\">  <span class=\"meta\">@transient</span> <span class=\"keyword\">val</span> partitioner: <span class=\"type\">Option</span>[<span class=\"type\">Partitioner</span>] = <span class=\"type\">None</span></span><br><span class=\"line\">  <span class=\"comment\">//可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置</span></span><br><span class=\"line\">  <span class=\"keyword\">protected</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPreferredLocations</span></span>(split: <span class=\"type\">Partition</span>): <span class=\"type\">Seq</span>[<span class=\"type\">String</span>] = <span class=\"type\">Nil</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"为什么会产生RDD\"><a href=\"#为什么会产生RDD\" class=\"headerlink\" title=\"为什么会产生RDD\"></a>为什么会产生RDD</h3><h3 id=\"RDD数据集\"><a href=\"#RDD数据集\" class=\"headerlink\" title=\"RDD数据集\"></a>RDD数据集</h3><ol>\n<li>并行集合</li>\n</ol>\n<p>接收一个已经存在的集合,然后进行各种并行计算.并行化集合是通过调用SparkContext的parallelize方法，在一个已经存在的Scala集合上创建（一个Seq对象）。集合的对象将会被拷贝，创建出一个可以被并行操作的分布式数据集。</p>\n<ol start=\"2\">\n<li>Hadoop数据集</li>\n</ol>\n<p>Spark可以将任何Hadoop所支持的存储资源转化成RDD，只要文件系统是HDFS，或者Hadoop支持的任意存储系统即可，如本地文件（需要网络文件系统，所有的节点都必须能访问到）、HDFS、Cassandra、HBase、Amazon S3等，Spark支持文本文件、SequenceFiles和任何Hadoop InputFormat格式。</p>\n<p>此两种类型的RDD都可以通过相同的方式进行操作，从而获得子RDD等一系列拓展，形成lineage血统关系图。</p>\n<h3 id=\"Spark-RDD算子\"><a href=\"#Spark-RDD算子\" class=\"headerlink\" title=\"Spark RDD算子\"></a>Spark RDD算子</h3><ol>\n<li>Transformation<br>不触发提交作业，完成作业中间处理过程。</li>\n</ol>\n<h2 id=\"DStream\"><a href=\"#DStream\" class=\"headerlink\" title=\"DStream\"></a>DStream</h2><h3 id=\"什么是DStream\"><a href=\"#什么是DStream\" class=\"headerlink\" title=\"什么是DStream\"></a>什么是DStream</h3><p>Discretized Stream :代表持续性的数据流和经过各种Spark原语操作后的结果数据流,在内部实现上是一系列连续的RDD来表示.每个RDD含有一段时间间隔内的数据,如下图<br><img src=\"http://img.wqkenqing.ren/2019-03-04-15-50-41.png\" alt=\"DStream\"></p>\n<p>计算则由spark engine来完成<br><img src=\"http://img.wqkenqing.ren/2019-03-04-15-51-58.png\" alt=\"spark engine流程\"></p>\n<h2 id=\"spark-java\"><a href=\"#spark-java\" class=\"headerlink\" title=\"spark java\"></a>spark java</h2><p>因为我是主要掌握的语言是java,从效率上来考虑,这里</p>\n<h2 id=\"参考博客\"><a href=\"#参考博客\" class=\"headerlink\" title=\"参考博客\"></a>参考博客</h2><p><a href=\"https://blog.csdn.net/wangxiaotongfan/article/details/51395769\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/wangxiaotongfan/article/details/51395769</a> RDD详解<br><a href=\"https://blog.csdn.net/zuochang_liu/article/details/81459185\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/zuochang_liu/article/details/81459185</a>  spark streaming学习<br><a href=\"https://blog.csdn.net/hellozhxy/article/details/81672845\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/hellozhxy/article/details/81672845</a> spark java 使用指南<br><a href=\"https://blog.csdn.net/t1dmzks/article/details/70198430\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/t1dmzks/article/details/70198430</a> sparkRDD算子介绍<br><a href=\"https://blog.csdn.net/wxycx11111/article/details/79123482\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/wxycx11111/article/details/79123482</a> <strong>sparkRDD入门介绍</strong><br><a href=\"https://github.com/zhaikaishun/spark_tutorial\" target=\"_blank\" rel=\"noopener\">https://github.com/zhaikaishun/spark_tutorial</a> <strong>RDD算子介绍</strong></p>"},{"title":"mapreduce组件总结","_content":"spark-core,spark-streaming再深造\n<!--more -->\n# spark go on \n\n初始规划\n\nspark-core\nspark-streaming\n\n\n\n\n","source":"_posts/技术/hexo/old/spark学习3.md","raw":"---\ntitle:  mapreduce组件总结\ntags: bigdata\n---\nspark-core,spark-streaming再深造\n<!--more -->\n# spark go on \n\n初始规划\n\nspark-core\nspark-streaming\n\n\n\n\n","slug":"技术/hexo/old/spark学习3","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd332002438pw137a9psp","content":"<p>spark-core,spark-streaming再深造</p>\n<a id=\"more\"></a>\n<h1 id=\"spark-go-on\"><a href=\"#spark-go-on\" class=\"headerlink\" title=\"spark go on\"></a>spark go on</h1><p>初始规划</p>\n<p>spark-core<br>spark-streaming</p>\n","site":{"data":{}},"excerpt":"<p>spark-core,spark-streaming再深造</p>","more":"<h1 id=\"spark-go-on\"><a href=\"#spark-go-on\" class=\"headerlink\" title=\"spark go on\"></a>spark go on</h1><p>初始规划</p>\n<p>spark-core<br>spark-streaming</p>"},{"title":"spark算子","date":"2018-03-04T03:12:05.000Z","_content":"\n\n# spark 算子\n<!-- more -->\n```\nsparkRDD封装的函数方法又称算子,通过这些算子可以对RDD进行相关处理,从而获我们想要的结果,因为可能涉及的算子较多.因此单独开篇进行粒度更细,更集中的总结.\n\n总得来讲spark的算子,本就是scala集合的一些高阶用法.\n\n```\n## Transformation(转换)\n不触发提交作业，完成作业中间处理过程。\n\n### parallelize (并行化)\n将一个存在的集合，变成一个RDD ,返回的是一个JavaRDD[T] \n** in scala **\n``` scala \n sc.parallelize(List(\"shenzhen\", \"is a beautiful city\"))\n ```\n ** in java **\n ```java\n JavaRDD<String> javaStringRDD = sc.parallelize(Arrays.asList(\"shenzhen\", \"is a beautiful city\"));\n```\n### makeRDD\n只有scala版本的才有makeRDD ,与parallelize类似.\n\n### textFile\n调用SparkContext.textFile()方法，从外部存储中读取数据来创建 RDD \n** in scala **\n ``` scala\n var lines = sc.textFile(inpath) \n```\n```java\n// java\n JavaRDD<String> lines = sc.textFile(inpath);\n```\n\n### filter\n对RDD数据进行过滤\n### map\n接收一个函数,并将这个函数作用于RDD中的每个元素.RDD 中对应**元素的值 map是一对一的关系 **\n\n### flatMap\n有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap() ,faltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器\n\n### distinct\n去重,我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销很大 \n### union\n两个RDD进行合并 \n### intersection\nRDD1.intersection(RDD2) 返回两个RDD的交集，** 并且去重 **\nintersection 需要混洗数据，比较浪费性能\n### subtract\nRDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重 \n### cartesian\ncartesian(RDD2) 返回RDD1和RDD2的笛卡儿积，这个开销非常大\n### mapToPair \n将元素该成key-value形式\n### flatMapToPair\n差异同mapToPair\n### combineByKey\n该方法主要针对不同分区的同一key进行元素合并函数操作.\n需要对pairRDD进行\n1. createCombiner  会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过,要么就 \n和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 \n那个键对应的累加器的初始值\n2. mergeValue 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并\n3. mergeCombiners 于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更 \n多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各 \n个分区的结果进行合并。\n### reduceByKey\n接收一个函数，按照相同的key进行reduce操\n### foldByKey\n该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V ,与reduce不同的是 foldByKey开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素 \n### sortByKey\nSortByKey用于对pairRDD按照key进行排序，第一个参数可以设置true或者false，默认是true \n### groupByKey\ngroupByKey会将RDD[key,value] 按照相同的key进行分组，形成RDD[key,Iterable[value]]的形式， 有点类似于sql中的groupby，例如类似于mysql中的group_concat \n### cogroup\ngroupByKey是对单个 RDD 的数据进行分组，还可以使用一个叫作 cogroup() 的函数对多个共享同一个键的 RDD 进行分组\nRDD1.cogroup(RDD2) 会将RDD1和RDD2按照相同的key进行分组，得到(key,RDD[key,Iterable[value1],Iterable[value2]])的形式 \n### subtractByKey\n类似于subtrac，删掉 RDD 中键与 other RDD 中的键相同的元素\n### join\n可以把RDD1,RDD2中的相同的key给连接起来，类似于sql中的join操作\nRDD1.join(RDD2) \n### fullOuterJoin\n全连接\n### leftOuterJoin\n### rightOuterJoin\n\n\n## Action\n### first\n返回第一个元素 \n### take\nrdd.take(n)返回第n个元素 \n### collect\nrdd.collect() 返回 RDD 中的所有元素 \n### count\nrdd.count() 返回 RDD 中的元素个数 \n### countByValue\n各元素在 RDD 中出现的次数 返回{(key1,次数),(key2,次数),…(keyn,次数)} \n### reduce\n并行整合RDD中所有数据\n### fold\n和 reduce() 一 样， 但是提供了初始值num,每个元素计算时，先要合这个初始值进行折叠, 注意，这里会按照每个分区进行fold，然后分区之间还会再次进行fold \n### top\nrdd.top(n) \n按照降序的或者指定的排序规则，返回前n个元素 \n### takeOrdered\nrdd.take(n) \n对RDD元素进行升序排序,取出前n个元素并返回，也可以自定义比较器（这里不介绍），类似于top的相反的方法 \n### foreach\n对 RDD 中的每个元素使用给 \n定的函数\n### countByKey\n以RDD{(1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)}为例 rdd.countByKey会返回{(1,1),(2,2),(3,3)} \n### collectAsMap\n将pair类型(键值对类型)的RDD转换成map, 还是上面的例子\n### saveAsTextFile\nsaveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。\n### saveAsSequenceFile\nsaveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。\n### saveAsObjectFile\nsaveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。\n### saveAsHadoopFile\n\n### saveAsNewAPIHadoopFile\n\n### mapPartitions \n### mapPartitionsWithIndex\n### HashPartitioner\n### RangePartitioner\n### 自定义分区","source":"_posts/技术/hexo/old/spark算子.md","raw":"---\ntitle: spark算子\ntags: spark学习\ndate: 2018-03-04 11:12:5\n---\n\n\n# spark 算子\n<!-- more -->\n```\nsparkRDD封装的函数方法又称算子,通过这些算子可以对RDD进行相关处理,从而获我们想要的结果,因为可能涉及的算子较多.因此单独开篇进行粒度更细,更集中的总结.\n\n总得来讲spark的算子,本就是scala集合的一些高阶用法.\n\n```\n## Transformation(转换)\n不触发提交作业，完成作业中间处理过程。\n\n### parallelize (并行化)\n将一个存在的集合，变成一个RDD ,返回的是一个JavaRDD[T] \n** in scala **\n``` scala \n sc.parallelize(List(\"shenzhen\", \"is a beautiful city\"))\n ```\n ** in java **\n ```java\n JavaRDD<String> javaStringRDD = sc.parallelize(Arrays.asList(\"shenzhen\", \"is a beautiful city\"));\n```\n### makeRDD\n只有scala版本的才有makeRDD ,与parallelize类似.\n\n### textFile\n调用SparkContext.textFile()方法，从外部存储中读取数据来创建 RDD \n** in scala **\n ``` scala\n var lines = sc.textFile(inpath) \n```\n```java\n// java\n JavaRDD<String> lines = sc.textFile(inpath);\n```\n\n### filter\n对RDD数据进行过滤\n### map\n接收一个函数,并将这个函数作用于RDD中的每个元素.RDD 中对应**元素的值 map是一对一的关系 **\n\n### flatMap\n有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap() ,faltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器\n\n### distinct\n去重,我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销很大 \n### union\n两个RDD进行合并 \n### intersection\nRDD1.intersection(RDD2) 返回两个RDD的交集，** 并且去重 **\nintersection 需要混洗数据，比较浪费性能\n### subtract\nRDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重 \n### cartesian\ncartesian(RDD2) 返回RDD1和RDD2的笛卡儿积，这个开销非常大\n### mapToPair \n将元素该成key-value形式\n### flatMapToPair\n差异同mapToPair\n### combineByKey\n该方法主要针对不同分区的同一key进行元素合并函数操作.\n需要对pairRDD进行\n1. createCombiner  会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过,要么就 \n和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 \n那个键对应的累加器的初始值\n2. mergeValue 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并\n3. mergeCombiners 于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更 \n多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各 \n个分区的结果进行合并。\n### reduceByKey\n接收一个函数，按照相同的key进行reduce操\n### foldByKey\n该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V ,与reduce不同的是 foldByKey开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素 \n### sortByKey\nSortByKey用于对pairRDD按照key进行排序，第一个参数可以设置true或者false，默认是true \n### groupByKey\ngroupByKey会将RDD[key,value] 按照相同的key进行分组，形成RDD[key,Iterable[value]]的形式， 有点类似于sql中的groupby，例如类似于mysql中的group_concat \n### cogroup\ngroupByKey是对单个 RDD 的数据进行分组，还可以使用一个叫作 cogroup() 的函数对多个共享同一个键的 RDD 进行分组\nRDD1.cogroup(RDD2) 会将RDD1和RDD2按照相同的key进行分组，得到(key,RDD[key,Iterable[value1],Iterable[value2]])的形式 \n### subtractByKey\n类似于subtrac，删掉 RDD 中键与 other RDD 中的键相同的元素\n### join\n可以把RDD1,RDD2中的相同的key给连接起来，类似于sql中的join操作\nRDD1.join(RDD2) \n### fullOuterJoin\n全连接\n### leftOuterJoin\n### rightOuterJoin\n\n\n## Action\n### first\n返回第一个元素 \n### take\nrdd.take(n)返回第n个元素 \n### collect\nrdd.collect() 返回 RDD 中的所有元素 \n### count\nrdd.count() 返回 RDD 中的元素个数 \n### countByValue\n各元素在 RDD 中出现的次数 返回{(key1,次数),(key2,次数),…(keyn,次数)} \n### reduce\n并行整合RDD中所有数据\n### fold\n和 reduce() 一 样， 但是提供了初始值num,每个元素计算时，先要合这个初始值进行折叠, 注意，这里会按照每个分区进行fold，然后分区之间还会再次进行fold \n### top\nrdd.top(n) \n按照降序的或者指定的排序规则，返回前n个元素 \n### takeOrdered\nrdd.take(n) \n对RDD元素进行升序排序,取出前n个元素并返回，也可以自定义比较器（这里不介绍），类似于top的相反的方法 \n### foreach\n对 RDD 中的每个元素使用给 \n定的函数\n### countByKey\n以RDD{(1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)}为例 rdd.countByKey会返回{(1,1),(2,2),(3,3)} \n### collectAsMap\n将pair类型(键值对类型)的RDD转换成map, 还是上面的例子\n### saveAsTextFile\nsaveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。\n### saveAsSequenceFile\nsaveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。\n### saveAsObjectFile\nsaveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。\n### saveAsHadoopFile\n\n### saveAsNewAPIHadoopFile\n\n### mapPartitions \n### mapPartitionsWithIndex\n### HashPartitioner\n### RangePartitioner\n### 自定义分区","slug":"技术/hexo/old/spark算子","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd333002638pw417xfj8o","content":"<h1 id=\"spark-算子\"><a href=\"#spark-算子\" class=\"headerlink\" title=\"spark 算子\"></a>spark 算子</h1><a id=\"more\"></a>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sparkRDD封装的函数方法又称算子,通过这些算子可以对RDD进行相关处理,从而获我们想要的结果,因为可能涉及的算子较多.因此单独开篇进行粒度更细,更集中的总结.</span><br><span class=\"line\"></span><br><span class=\"line\">总得来讲spark的算子,本就是scala集合的一些高阶用法.</span><br></pre></td></tr></table></figure>\n<h2 id=\"Transformation-转换\"><a href=\"#Transformation-转换\" class=\"headerlink\" title=\"Transformation(转换)\"></a>Transformation(转换)</h2><p>不触发提交作业，完成作业中间处理过程。</p>\n<h3 id=\"parallelize-并行化\"><a href=\"#parallelize-并行化\" class=\"headerlink\" title=\"parallelize (并行化)\"></a>parallelize (并行化)</h3><p>将一个存在的集合，变成一个RDD ,返回的是一个JavaRDD[T]<br>** in scala **</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"shenzhen\"</span>, <span class=\"string\">\"is a beautiful city\"</span>))</span><br></pre></td></tr></table></figure>\n<p> ** in java **<br> <figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">JavaRDD&lt;String&gt; javaStringRDD = sc.parallelize(Arrays.asList(<span class=\"string\">\"shenzhen\"</span>, <span class=\"string\">\"is a beautiful city\"</span>));</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"makeRDD\"><a href=\"#makeRDD\" class=\"headerlink\" title=\"makeRDD\"></a>makeRDD</h3><p>只有scala版本的才有makeRDD ,与parallelize类似.</p>\n<h3 id=\"textFile\"><a href=\"#textFile\" class=\"headerlink\" title=\"textFile\"></a>textFile</h3><p>调用SparkContext.textFile()方法，从外部存储中读取数据来创建 RDD<br>** in scala **<br> <figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> lines = sc.textFile(inpath)</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// java</span></span><br><span class=\"line\"> JavaRDD&lt;String&gt; lines = sc.textFile(inpath);</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"filter\"><a href=\"#filter\" class=\"headerlink\" title=\"filter\"></a>filter</h3><p>对RDD数据进行过滤</p>\n<h3 id=\"map\"><a href=\"#map\" class=\"headerlink\" title=\"map\"></a>map</h3><p>接收一个函数,并将这个函数作用于RDD中的每个元素.RDD 中对应*<em>元素的值 map是一对一的关系 *</em></p>\n<h3 id=\"flatMap\"><a href=\"#flatMap\" class=\"headerlink\" title=\"flatMap\"></a>flatMap</h3><p>有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap() ,faltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器</p>\n<h3 id=\"distinct\"><a href=\"#distinct\" class=\"headerlink\" title=\"distinct\"></a>distinct</h3><p>去重,我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销很大 </p>\n<h3 id=\"union\"><a href=\"#union\" class=\"headerlink\" title=\"union\"></a>union</h3><p>两个RDD进行合并 </p>\n<h3 id=\"intersection\"><a href=\"#intersection\" class=\"headerlink\" title=\"intersection\"></a>intersection</h3><p>RDD1.intersection(RDD2) 返回两个RDD的交集，** 并且去重 **<br>intersection 需要混洗数据，比较浪费性能</p>\n<h3 id=\"subtract\"><a href=\"#subtract\" class=\"headerlink\" title=\"subtract\"></a>subtract</h3><p>RDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重 </p>\n<h3 id=\"cartesian\"><a href=\"#cartesian\" class=\"headerlink\" title=\"cartesian\"></a>cartesian</h3><p>cartesian(RDD2) 返回RDD1和RDD2的笛卡儿积，这个开销非常大</p>\n<h3 id=\"mapToPair\"><a href=\"#mapToPair\" class=\"headerlink\" title=\"mapToPair\"></a>mapToPair</h3><p>将元素该成key-value形式</p>\n<h3 id=\"flatMapToPair\"><a href=\"#flatMapToPair\" class=\"headerlink\" title=\"flatMapToPair\"></a>flatMapToPair</h3><p>差异同mapToPair</p>\n<h3 id=\"combineByKey\"><a href=\"#combineByKey\" class=\"headerlink\" title=\"combineByKey\"></a>combineByKey</h3><p>该方法主要针对不同分区的同一key进行元素合并函数操作.<br>需要对pairRDD进行</p>\n<ol>\n<li>createCombiner  会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过,要么就<br>和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建<br>那个键对应的累加器的初始值</li>\n<li>mergeValue 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并</li>\n<li>mergeCombiners 于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更<br>多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各<br>个分区的结果进行合并。<h3 id=\"reduceByKey\"><a href=\"#reduceByKey\" class=\"headerlink\" title=\"reduceByKey\"></a>reduceByKey</h3>接收一个函数，按照相同的key进行reduce操<h3 id=\"foldByKey\"><a href=\"#foldByKey\" class=\"headerlink\" title=\"foldByKey\"></a>foldByKey</h3>该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V ,与reduce不同的是 foldByKey开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素 <h3 id=\"sortByKey\"><a href=\"#sortByKey\" class=\"headerlink\" title=\"sortByKey\"></a>sortByKey</h3>SortByKey用于对pairRDD按照key进行排序，第一个参数可以设置true或者false，默认是true <h3 id=\"groupByKey\"><a href=\"#groupByKey\" class=\"headerlink\" title=\"groupByKey\"></a>groupByKey</h3>groupByKey会将RDD[key,value] 按照相同的key进行分组，形成RDD[key,Iterable[value]]的形式， 有点类似于sql中的groupby，例如类似于mysql中的group_concat <h3 id=\"cogroup\"><a href=\"#cogroup\" class=\"headerlink\" title=\"cogroup\"></a>cogroup</h3>groupByKey是对单个 RDD 的数据进行分组，还可以使用一个叫作 cogroup() 的函数对多个共享同一个键的 RDD 进行分组<br>RDD1.cogroup(RDD2) 会将RDD1和RDD2按照相同的key进行分组，得到(key,RDD[key,Iterable[value1],Iterable[value2]])的形式 <h3 id=\"subtractByKey\"><a href=\"#subtractByKey\" class=\"headerlink\" title=\"subtractByKey\"></a>subtractByKey</h3>类似于subtrac，删掉 RDD 中键与 other RDD 中的键相同的元素<h3 id=\"join\"><a href=\"#join\" class=\"headerlink\" title=\"join\"></a>join</h3>可以把RDD1,RDD2中的相同的key给连接起来，类似于sql中的join操作<br>RDD1.join(RDD2) <h3 id=\"fullOuterJoin\"><a href=\"#fullOuterJoin\" class=\"headerlink\" title=\"fullOuterJoin\"></a>fullOuterJoin</h3>全连接<h3 id=\"leftOuterJoin\"><a href=\"#leftOuterJoin\" class=\"headerlink\" title=\"leftOuterJoin\"></a>leftOuterJoin</h3><h3 id=\"rightOuterJoin\"><a href=\"#rightOuterJoin\" class=\"headerlink\" title=\"rightOuterJoin\"></a>rightOuterJoin</h3></li>\n</ol>\n<h2 id=\"Action\"><a href=\"#Action\" class=\"headerlink\" title=\"Action\"></a>Action</h2><h3 id=\"first\"><a href=\"#first\" class=\"headerlink\" title=\"first\"></a>first</h3><p>返回第一个元素 </p>\n<h3 id=\"take\"><a href=\"#take\" class=\"headerlink\" title=\"take\"></a>take</h3><p>rdd.take(n)返回第n个元素 </p>\n<h3 id=\"collect\"><a href=\"#collect\" class=\"headerlink\" title=\"collect\"></a>collect</h3><p>rdd.collect() 返回 RDD 中的所有元素 </p>\n<h3 id=\"count\"><a href=\"#count\" class=\"headerlink\" title=\"count\"></a>count</h3><p>rdd.count() 返回 RDD 中的元素个数 </p>\n<h3 id=\"countByValue\"><a href=\"#countByValue\" class=\"headerlink\" title=\"countByValue\"></a>countByValue</h3><p>各元素在 RDD 中出现的次数 返回{(key1,次数),(key2,次数),…(keyn,次数)} </p>\n<h3 id=\"reduce\"><a href=\"#reduce\" class=\"headerlink\" title=\"reduce\"></a>reduce</h3><p>并行整合RDD中所有数据</p>\n<h3 id=\"fold\"><a href=\"#fold\" class=\"headerlink\" title=\"fold\"></a>fold</h3><p>和 reduce() 一 样， 但是提供了初始值num,每个元素计算时，先要合这个初始值进行折叠, 注意，这里会按照每个分区进行fold，然后分区之间还会再次进行fold </p>\n<h3 id=\"top\"><a href=\"#top\" class=\"headerlink\" title=\"top\"></a>top</h3><p>rdd.top(n)<br>按照降序的或者指定的排序规则，返回前n个元素 </p>\n<h3 id=\"takeOrdered\"><a href=\"#takeOrdered\" class=\"headerlink\" title=\"takeOrdered\"></a>takeOrdered</h3><p>rdd.take(n)<br>对RDD元素进行升序排序,取出前n个元素并返回，也可以自定义比较器（这里不介绍），类似于top的相反的方法 </p>\n<h3 id=\"foreach\"><a href=\"#foreach\" class=\"headerlink\" title=\"foreach\"></a>foreach</h3><p>对 RDD 中的每个元素使用给<br>定的函数</p>\n<h3 id=\"countByKey\"><a href=\"#countByKey\" class=\"headerlink\" title=\"countByKey\"></a>countByKey</h3><p>以RDD{(1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)}为例 rdd.countByKey会返回{(1,1),(2,2),(3,3)} </p>\n<h3 id=\"collectAsMap\"><a href=\"#collectAsMap\" class=\"headerlink\" title=\"collectAsMap\"></a>collectAsMap</h3><p>将pair类型(键值对类型)的RDD转换成map, 还是上面的例子</p>\n<h3 id=\"saveAsTextFile\"><a href=\"#saveAsTextFile\" class=\"headerlink\" title=\"saveAsTextFile\"></a>saveAsTextFile</h3><p>saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。</p>\n<h3 id=\"saveAsSequenceFile\"><a href=\"#saveAsSequenceFile\" class=\"headerlink\" title=\"saveAsSequenceFile\"></a>saveAsSequenceFile</h3><p>saveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。</p>\n<h3 id=\"saveAsObjectFile\"><a href=\"#saveAsObjectFile\" class=\"headerlink\" title=\"saveAsObjectFile\"></a>saveAsObjectFile</h3><p>saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。</p>\n<h3 id=\"saveAsHadoopFile\"><a href=\"#saveAsHadoopFile\" class=\"headerlink\" title=\"saveAsHadoopFile\"></a>saveAsHadoopFile</h3><h3 id=\"saveAsNewAPIHadoopFile\"><a href=\"#saveAsNewAPIHadoopFile\" class=\"headerlink\" title=\"saveAsNewAPIHadoopFile\"></a>saveAsNewAPIHadoopFile</h3><h3 id=\"mapPartitions\"><a href=\"#mapPartitions\" class=\"headerlink\" title=\"mapPartitions\"></a>mapPartitions</h3><h3 id=\"mapPartitionsWithIndex\"><a href=\"#mapPartitionsWithIndex\" class=\"headerlink\" title=\"mapPartitionsWithIndex\"></a>mapPartitionsWithIndex</h3><h3 id=\"HashPartitioner\"><a href=\"#HashPartitioner\" class=\"headerlink\" title=\"HashPartitioner\"></a>HashPartitioner</h3><h3 id=\"RangePartitioner\"><a href=\"#RangePartitioner\" class=\"headerlink\" title=\"RangePartitioner\"></a>RangePartitioner</h3><h3 id=\"自定义分区\"><a href=\"#自定义分区\" class=\"headerlink\" title=\"自定义分区\"></a>自定义分区</h3>","site":{"data":{}},"excerpt":"<h1 id=\"spark-算子\"><a href=\"#spark-算子\" class=\"headerlink\" title=\"spark 算子\"></a>spark 算子</h1>","more":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sparkRDD封装的函数方法又称算子,通过这些算子可以对RDD进行相关处理,从而获我们想要的结果,因为可能涉及的算子较多.因此单独开篇进行粒度更细,更集中的总结.</span><br><span class=\"line\"></span><br><span class=\"line\">总得来讲spark的算子,本就是scala集合的一些高阶用法.</span><br></pre></td></tr></table></figure>\n<h2 id=\"Transformation-转换\"><a href=\"#Transformation-转换\" class=\"headerlink\" title=\"Transformation(转换)\"></a>Transformation(转换)</h2><p>不触发提交作业，完成作业中间处理过程。</p>\n<h3 id=\"parallelize-并行化\"><a href=\"#parallelize-并行化\" class=\"headerlink\" title=\"parallelize (并行化)\"></a>parallelize (并行化)</h3><p>将一个存在的集合，变成一个RDD ,返回的是一个JavaRDD[T]<br>** in scala **</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"shenzhen\"</span>, <span class=\"string\">\"is a beautiful city\"</span>))</span><br></pre></td></tr></table></figure>\n<p> ** in java **<br> <figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">JavaRDD&lt;String&gt; javaStringRDD = sc.parallelize(Arrays.asList(<span class=\"string\">\"shenzhen\"</span>, <span class=\"string\">\"is a beautiful city\"</span>));</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"makeRDD\"><a href=\"#makeRDD\" class=\"headerlink\" title=\"makeRDD\"></a>makeRDD</h3><p>只有scala版本的才有makeRDD ,与parallelize类似.</p>\n<h3 id=\"textFile\"><a href=\"#textFile\" class=\"headerlink\" title=\"textFile\"></a>textFile</h3><p>调用SparkContext.textFile()方法，从外部存储中读取数据来创建 RDD<br>** in scala **<br> <figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> lines = sc.textFile(inpath)</span><br></pre></td></tr></table></figure></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// java</span></span><br><span class=\"line\"> JavaRDD&lt;String&gt; lines = sc.textFile(inpath);</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"filter\"><a href=\"#filter\" class=\"headerlink\" title=\"filter\"></a>filter</h3><p>对RDD数据进行过滤</p>\n<h3 id=\"map\"><a href=\"#map\" class=\"headerlink\" title=\"map\"></a>map</h3><p>接收一个函数,并将这个函数作用于RDD中的每个元素.RDD 中对应*<em>元素的值 map是一对一的关系 *</em></p>\n<h3 id=\"flatMap\"><a href=\"#flatMap\" class=\"headerlink\" title=\"flatMap\"></a>flatMap</h3><p>有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap() ,faltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器</p>\n<h3 id=\"distinct\"><a href=\"#distinct\" class=\"headerlink\" title=\"distinct\"></a>distinct</h3><p>去重,我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销很大 </p>\n<h3 id=\"union\"><a href=\"#union\" class=\"headerlink\" title=\"union\"></a>union</h3><p>两个RDD进行合并 </p>\n<h3 id=\"intersection\"><a href=\"#intersection\" class=\"headerlink\" title=\"intersection\"></a>intersection</h3><p>RDD1.intersection(RDD2) 返回两个RDD的交集，** 并且去重 **<br>intersection 需要混洗数据，比较浪费性能</p>\n<h3 id=\"subtract\"><a href=\"#subtract\" class=\"headerlink\" title=\"subtract\"></a>subtract</h3><p>RDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重 </p>\n<h3 id=\"cartesian\"><a href=\"#cartesian\" class=\"headerlink\" title=\"cartesian\"></a>cartesian</h3><p>cartesian(RDD2) 返回RDD1和RDD2的笛卡儿积，这个开销非常大</p>\n<h3 id=\"mapToPair\"><a href=\"#mapToPair\" class=\"headerlink\" title=\"mapToPair\"></a>mapToPair</h3><p>将元素该成key-value形式</p>\n<h3 id=\"flatMapToPair\"><a href=\"#flatMapToPair\" class=\"headerlink\" title=\"flatMapToPair\"></a>flatMapToPair</h3><p>差异同mapToPair</p>\n<h3 id=\"combineByKey\"><a href=\"#combineByKey\" class=\"headerlink\" title=\"combineByKey\"></a>combineByKey</h3><p>该方法主要针对不同分区的同一key进行元素合并函数操作.<br>需要对pairRDD进行</p>\n<ol>\n<li>createCombiner  会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过,要么就<br>和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建<br>那个键对应的累加器的初始值</li>\n<li>mergeValue 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并</li>\n<li>mergeCombiners 于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更<br>多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各<br>个分区的结果进行合并。<h3 id=\"reduceByKey\"><a href=\"#reduceByKey\" class=\"headerlink\" title=\"reduceByKey\"></a>reduceByKey</h3>接收一个函数，按照相同的key进行reduce操<h3 id=\"foldByKey\"><a href=\"#foldByKey\" class=\"headerlink\" title=\"foldByKey\"></a>foldByKey</h3>该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V ,与reduce不同的是 foldByKey开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素 <h3 id=\"sortByKey\"><a href=\"#sortByKey\" class=\"headerlink\" title=\"sortByKey\"></a>sortByKey</h3>SortByKey用于对pairRDD按照key进行排序，第一个参数可以设置true或者false，默认是true <h3 id=\"groupByKey\"><a href=\"#groupByKey\" class=\"headerlink\" title=\"groupByKey\"></a>groupByKey</h3>groupByKey会将RDD[key,value] 按照相同的key进行分组，形成RDD[key,Iterable[value]]的形式， 有点类似于sql中的groupby，例如类似于mysql中的group_concat <h3 id=\"cogroup\"><a href=\"#cogroup\" class=\"headerlink\" title=\"cogroup\"></a>cogroup</h3>groupByKey是对单个 RDD 的数据进行分组，还可以使用一个叫作 cogroup() 的函数对多个共享同一个键的 RDD 进行分组<br>RDD1.cogroup(RDD2) 会将RDD1和RDD2按照相同的key进行分组，得到(key,RDD[key,Iterable[value1],Iterable[value2]])的形式 <h3 id=\"subtractByKey\"><a href=\"#subtractByKey\" class=\"headerlink\" title=\"subtractByKey\"></a>subtractByKey</h3>类似于subtrac，删掉 RDD 中键与 other RDD 中的键相同的元素<h3 id=\"join\"><a href=\"#join\" class=\"headerlink\" title=\"join\"></a>join</h3>可以把RDD1,RDD2中的相同的key给连接起来，类似于sql中的join操作<br>RDD1.join(RDD2) <h3 id=\"fullOuterJoin\"><a href=\"#fullOuterJoin\" class=\"headerlink\" title=\"fullOuterJoin\"></a>fullOuterJoin</h3>全连接<h3 id=\"leftOuterJoin\"><a href=\"#leftOuterJoin\" class=\"headerlink\" title=\"leftOuterJoin\"></a>leftOuterJoin</h3><h3 id=\"rightOuterJoin\"><a href=\"#rightOuterJoin\" class=\"headerlink\" title=\"rightOuterJoin\"></a>rightOuterJoin</h3></li>\n</ol>\n<h2 id=\"Action\"><a href=\"#Action\" class=\"headerlink\" title=\"Action\"></a>Action</h2><h3 id=\"first\"><a href=\"#first\" class=\"headerlink\" title=\"first\"></a>first</h3><p>返回第一个元素 </p>\n<h3 id=\"take\"><a href=\"#take\" class=\"headerlink\" title=\"take\"></a>take</h3><p>rdd.take(n)返回第n个元素 </p>\n<h3 id=\"collect\"><a href=\"#collect\" class=\"headerlink\" title=\"collect\"></a>collect</h3><p>rdd.collect() 返回 RDD 中的所有元素 </p>\n<h3 id=\"count\"><a href=\"#count\" class=\"headerlink\" title=\"count\"></a>count</h3><p>rdd.count() 返回 RDD 中的元素个数 </p>\n<h3 id=\"countByValue\"><a href=\"#countByValue\" class=\"headerlink\" title=\"countByValue\"></a>countByValue</h3><p>各元素在 RDD 中出现的次数 返回{(key1,次数),(key2,次数),…(keyn,次数)} </p>\n<h3 id=\"reduce\"><a href=\"#reduce\" class=\"headerlink\" title=\"reduce\"></a>reduce</h3><p>并行整合RDD中所有数据</p>\n<h3 id=\"fold\"><a href=\"#fold\" class=\"headerlink\" title=\"fold\"></a>fold</h3><p>和 reduce() 一 样， 但是提供了初始值num,每个元素计算时，先要合这个初始值进行折叠, 注意，这里会按照每个分区进行fold，然后分区之间还会再次进行fold </p>\n<h3 id=\"top\"><a href=\"#top\" class=\"headerlink\" title=\"top\"></a>top</h3><p>rdd.top(n)<br>按照降序的或者指定的排序规则，返回前n个元素 </p>\n<h3 id=\"takeOrdered\"><a href=\"#takeOrdered\" class=\"headerlink\" title=\"takeOrdered\"></a>takeOrdered</h3><p>rdd.take(n)<br>对RDD元素进行升序排序,取出前n个元素并返回，也可以自定义比较器（这里不介绍），类似于top的相反的方法 </p>\n<h3 id=\"foreach\"><a href=\"#foreach\" class=\"headerlink\" title=\"foreach\"></a>foreach</h3><p>对 RDD 中的每个元素使用给<br>定的函数</p>\n<h3 id=\"countByKey\"><a href=\"#countByKey\" class=\"headerlink\" title=\"countByKey\"></a>countByKey</h3><p>以RDD{(1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)}为例 rdd.countByKey会返回{(1,1),(2,2),(3,3)} </p>\n<h3 id=\"collectAsMap\"><a href=\"#collectAsMap\" class=\"headerlink\" title=\"collectAsMap\"></a>collectAsMap</h3><p>将pair类型(键值对类型)的RDD转换成map, 还是上面的例子</p>\n<h3 id=\"saveAsTextFile\"><a href=\"#saveAsTextFile\" class=\"headerlink\" title=\"saveAsTextFile\"></a>saveAsTextFile</h3><p>saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。</p>\n<h3 id=\"saveAsSequenceFile\"><a href=\"#saveAsSequenceFile\" class=\"headerlink\" title=\"saveAsSequenceFile\"></a>saveAsSequenceFile</h3><p>saveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。</p>\n<h3 id=\"saveAsObjectFile\"><a href=\"#saveAsObjectFile\" class=\"headerlink\" title=\"saveAsObjectFile\"></a>saveAsObjectFile</h3><p>saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。</p>\n<h3 id=\"saveAsHadoopFile\"><a href=\"#saveAsHadoopFile\" class=\"headerlink\" title=\"saveAsHadoopFile\"></a>saveAsHadoopFile</h3><h3 id=\"saveAsNewAPIHadoopFile\"><a href=\"#saveAsNewAPIHadoopFile\" class=\"headerlink\" title=\"saveAsNewAPIHadoopFile\"></a>saveAsNewAPIHadoopFile</h3><h3 id=\"mapPartitions\"><a href=\"#mapPartitions\" class=\"headerlink\" title=\"mapPartitions\"></a>mapPartitions</h3><h3 id=\"mapPartitionsWithIndex\"><a href=\"#mapPartitionsWithIndex\" class=\"headerlink\" title=\"mapPartitionsWithIndex\"></a>mapPartitionsWithIndex</h3><h3 id=\"HashPartitioner\"><a href=\"#HashPartitioner\" class=\"headerlink\" title=\"HashPartitioner\"></a>HashPartitioner</h3><h3 id=\"RangePartitioner\"><a href=\"#RangePartitioner\" class=\"headerlink\" title=\"RangePartitioner\"></a>RangePartitioner</h3><h3 id=\"自定义分区\"><a href=\"#自定义分区\" class=\"headerlink\" title=\"自定义分区\"></a>自定义分区</h3>"},{"title":"hive总结","date":"2018-12-24T15:07:45.000Z","_content":"# Hive相关点小结\n<!-- more -->\n\n## 启动指令\n1. hive ==  hive --service cli\n不需要启动server，使用本地的metastore，可以直接做一些简单的数据操作和测试。\n2. 启动hiveserver2\nhive --service hiveserver2\n3. beeline工具测试使用jdbc方式连接\nbeeline -u jdbc:hive2://localhost:10000\n\n1.managed table\n管理表。\n删除表时，数据也删除了\n\n2.external table\n外部表。\n删除表时，数据不删\n\n## 建表:\nCREATE TABLE IF NOT EXISTS t2(id int,name string,age int)\nCOMMENT 'xx'                                     //注释\nROW FORMAT DELIMITED                             //行分隔符\nFIELDS TERMINATED BY ','                         //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改\nSTORED AS TEXTFILE ;\n### 外部表:\n CREATE  TABLE IF NOT EXISTS t2(id int,name string,age int)\n COMMENT 'xx' \n ROW FORMAT DELIMITED \n FIELDS TERMINATED BY ',' \n STORED AS TEXTFILE ; \n### 分区表，桶表\n#### 分区表\nHive中有分区表的概念。我们可以看到分区表具有重要的性能，而且分区表还可以将数据以一种符合逻辑的方式进行组织，比如分层存储。Hive的分区表，是把数据放在满足条件的分区目录下\nCREATE TABLE t3(id int,name string,age int) \n\nPARTITIONED BY (Year INT, Month INT)   //按照年月进行分区\n\n ROW FORMAT DELIMITED                      //行分隔符\n\nFIELDS TERMINATED BY ',' ;                    //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改\nload data local inpath '/home/zpx/customers.txt' into table t3 partition\n#### 分桶表\n这样做，在查找数据的时候就可以跨越多个桶，直接查找复合条件的数据了。速度快，时间成本低。Hive中的桶表默认使用的机制也是hash。\nCREATE TABLE t4(id int,name string,age int) \n\n                   CLUSTERED BY (id) INTO 3 BUCKETS      //创建3个通桶表，按照字段id进行分桶\n\n                   ROW FORMAT DELIMITED                     //行分隔符\n\n                   FIELDS TERMINATED BY ',' ; \n\nload data local inpath '/home/centos/customers.txt' into table t4 ;\n\n## 导入数据\nload data local inpath '/home/zpx/customers.txt' into table t2 ; //local上传文件\nload data inpath '/user/zpx/customers.txt' [overwrite] into table t2 //分布式文件系统上移动文件\n\n## 建视图\nHive也可以建立视图，是一张虚表，方便我们进行操作.\n\ncreate view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ;\n\n## Hive的严格模式\nHive提供了一个严格模式，可以防止用户执行那些产生意想不到的不好的影响的查询。\n使用了严格模式之后主要对以下3种不良操作进行控制：\n\n1.分区表必须指定分区进行查询。\n2.order by时必须使用limit子句。\n3.不允许笛卡尔积。\n![2019-03-18-17-13-36](http://img.wqkenqing.ren/2019-03-18-17-13-36.png)\n\n## Hive的动态分区\n像分区表里面存储了数据。我们在进行存储数据的时候，都是明确的指定了分区。在这个过程中Hive也提供了一种比较任性化的操作，就是动态分区，不需要我们指定分区目录，Hive能够把数据进行动态的分发,**我们需要将当前的严格模式设置成非严格模式，否则不允许使用动态分区**\nset hive.exec.dynamic.partition.mode=nonstrict//设置非严格模式\n## Hive的排序\n\nHive也提供了一些排序的语法，包括order by,sort by。\n\norder by=MapReduce的全排序\nsort by=MapReduce的部分排序\ndistribute by=MapReduce的分区\n\nselece .......from ...... order by 字段；//按照这个字段全排序\n\nselece .......from ...... sort by 字段； //按照这个字段局部有序\n\nselece 字段.....from ...... distribute by 字段；//按照这个字段分区\n特别注意的是：\n\n1. 在上面的最后一个distribute by使用过程中，按照排序的字段要出现在最左侧也就是select中有这个字段，因为我们要告诉MapReduce你要按照哪一个字段分区，当然获取的数据中要出现这个字段了。类似于我们使用group by的用法，字段也必须出现在最左侧，因为数据要包含这个字段，才能按照这个字段分组，至于Hive什么时候会自行的开启MapReduce，那就是在使用聚合的情况下开启，使用select ...from ....以及使用分区表的selece ....from......where .....不会开启\n2. distribute by与sort by可以组合使用，但是distribute by要放在前边，因为MapReduce要先分区，后排序，再归并\n\nselect 字段a,........from .......distribute by字段a，sort by字段\n如果distribute by与sort by使用的字段一样，则可以使用cluster by 字段替代：\nselect 字段a,........from .......cluster by 字段\n\n## 函数\n\n1. show functions; 展示相关函数\n2. desc function split;\n3. desc function  extended split;  //查看函数的扩展信息\n\n### 用户自定义函数（UDF）\n具体步骤如下：\n\n（1）.自定义类（继承UDF，或是GenericUDF。GenericUDF是更为复杂的抽象概念，但是其支持更好的null值处理同时还可以处理一些标准的UDF无法支持的编程操作）。\n（2）.导出jar包，通过命令添加到hive的类路径。\n$hive>add jar xxx.jar\n（3）.注册函数\n$hive>CREATE TEMPORARY FUNCTION 函数名 AS '具体类路径：包.类';\n（4）.使用\n $hive>select 函数名(参数);\n自定义实现类如下(继承UDF)：\n\n","source":"_posts/技术/hexo/old/hive总结.md","raw":"---\ntitle:  hive总结\ndate: 2018-12-24 23:07:45\ntags: bigdata\n---\n# Hive相关点小结\n<!-- more -->\n\n## 启动指令\n1. hive ==  hive --service cli\n不需要启动server，使用本地的metastore，可以直接做一些简单的数据操作和测试。\n2. 启动hiveserver2\nhive --service hiveserver2\n3. beeline工具测试使用jdbc方式连接\nbeeline -u jdbc:hive2://localhost:10000\n\n1.managed table\n管理表。\n删除表时，数据也删除了\n\n2.external table\n外部表。\n删除表时，数据不删\n\n## 建表:\nCREATE TABLE IF NOT EXISTS t2(id int,name string,age int)\nCOMMENT 'xx'                                     //注释\nROW FORMAT DELIMITED                             //行分隔符\nFIELDS TERMINATED BY ','                         //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改\nSTORED AS TEXTFILE ;\n### 外部表:\n CREATE  TABLE IF NOT EXISTS t2(id int,name string,age int)\n COMMENT 'xx' \n ROW FORMAT DELIMITED \n FIELDS TERMINATED BY ',' \n STORED AS TEXTFILE ; \n### 分区表，桶表\n#### 分区表\nHive中有分区表的概念。我们可以看到分区表具有重要的性能，而且分区表还可以将数据以一种符合逻辑的方式进行组织，比如分层存储。Hive的分区表，是把数据放在满足条件的分区目录下\nCREATE TABLE t3(id int,name string,age int) \n\nPARTITIONED BY (Year INT, Month INT)   //按照年月进行分区\n\n ROW FORMAT DELIMITED                      //行分隔符\n\nFIELDS TERMINATED BY ',' ;                    //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改\nload data local inpath '/home/zpx/customers.txt' into table t3 partition\n#### 分桶表\n这样做，在查找数据的时候就可以跨越多个桶，直接查找复合条件的数据了。速度快，时间成本低。Hive中的桶表默认使用的机制也是hash。\nCREATE TABLE t4(id int,name string,age int) \n\n                   CLUSTERED BY (id) INTO 3 BUCKETS      //创建3个通桶表，按照字段id进行分桶\n\n                   ROW FORMAT DELIMITED                     //行分隔符\n\n                   FIELDS TERMINATED BY ',' ; \n\nload data local inpath '/home/centos/customers.txt' into table t4 ;\n\n## 导入数据\nload data local inpath '/home/zpx/customers.txt' into table t2 ; //local上传文件\nload data inpath '/user/zpx/customers.txt' [overwrite] into table t2 //分布式文件系统上移动文件\n\n## 建视图\nHive也可以建立视图，是一张虚表，方便我们进行操作.\n\ncreate view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ;\n\n## Hive的严格模式\nHive提供了一个严格模式，可以防止用户执行那些产生意想不到的不好的影响的查询。\n使用了严格模式之后主要对以下3种不良操作进行控制：\n\n1.分区表必须指定分区进行查询。\n2.order by时必须使用limit子句。\n3.不允许笛卡尔积。\n![2019-03-18-17-13-36](http://img.wqkenqing.ren/2019-03-18-17-13-36.png)\n\n## Hive的动态分区\n像分区表里面存储了数据。我们在进行存储数据的时候，都是明确的指定了分区。在这个过程中Hive也提供了一种比较任性化的操作，就是动态分区，不需要我们指定分区目录，Hive能够把数据进行动态的分发,**我们需要将当前的严格模式设置成非严格模式，否则不允许使用动态分区**\nset hive.exec.dynamic.partition.mode=nonstrict//设置非严格模式\n## Hive的排序\n\nHive也提供了一些排序的语法，包括order by,sort by。\n\norder by=MapReduce的全排序\nsort by=MapReduce的部分排序\ndistribute by=MapReduce的分区\n\nselece .......from ...... order by 字段；//按照这个字段全排序\n\nselece .......from ...... sort by 字段； //按照这个字段局部有序\n\nselece 字段.....from ...... distribute by 字段；//按照这个字段分区\n特别注意的是：\n\n1. 在上面的最后一个distribute by使用过程中，按照排序的字段要出现在最左侧也就是select中有这个字段，因为我们要告诉MapReduce你要按照哪一个字段分区，当然获取的数据中要出现这个字段了。类似于我们使用group by的用法，字段也必须出现在最左侧，因为数据要包含这个字段，才能按照这个字段分组，至于Hive什么时候会自行的开启MapReduce，那就是在使用聚合的情况下开启，使用select ...from ....以及使用分区表的selece ....from......where .....不会开启\n2. distribute by与sort by可以组合使用，但是distribute by要放在前边，因为MapReduce要先分区，后排序，再归并\n\nselect 字段a,........from .......distribute by字段a，sort by字段\n如果distribute by与sort by使用的字段一样，则可以使用cluster by 字段替代：\nselect 字段a,........from .......cluster by 字段\n\n## 函数\n\n1. show functions; 展示相关函数\n2. desc function split;\n3. desc function  extended split;  //查看函数的扩展信息\n\n### 用户自定义函数（UDF）\n具体步骤如下：\n\n（1）.自定义类（继承UDF，或是GenericUDF。GenericUDF是更为复杂的抽象概念，但是其支持更好的null值处理同时还可以处理一些标准的UDF无法支持的编程操作）。\n（2）.导出jar包，通过命令添加到hive的类路径。\n$hive>add jar xxx.jar\n（3）.注册函数\n$hive>CREATE TEMPORARY FUNCTION 函数名 AS '具体类路径：包.类';\n（4）.使用\n $hive>select 函数名(参数);\n自定义实现类如下(继承UDF)：\n\n","slug":"技术/hexo/old/hive总结","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd334002938pw47ozeuzl","content":"<h1 id=\"Hive相关点小结\"><a href=\"#Hive相关点小结\" class=\"headerlink\" title=\"Hive相关点小结\"></a>Hive相关点小结</h1><a id=\"more\"></a>\n\n<h2 id=\"启动指令\"><a href=\"#启动指令\" class=\"headerlink\" title=\"启动指令\"></a>启动指令</h2><ol>\n<li>hive ==  hive –service cli<br>不需要启动server，使用本地的metastore，可以直接做一些简单的数据操作和测试。</li>\n<li>启动hiveserver2<br>hive –service hiveserver2</li>\n<li>beeline工具测试使用jdbc方式连接<br>beeline -u jdbc:hive2://localhost:10000</li>\n</ol>\n<p>1.managed table<br>管理表。<br>删除表时，数据也删除了</p>\n<p>2.external table<br>外部表。<br>删除表时，数据不删</p>\n<h2 id=\"建表\"><a href=\"#建表\" class=\"headerlink\" title=\"建表:\"></a>建表:</h2><p>CREATE TABLE IF NOT EXISTS t2(id int,name string,age int)<br>COMMENT ‘xx’                                     //注释<br>ROW FORMAT DELIMITED                             //行分隔符<br>FIELDS TERMINATED BY ‘,’                         //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改<br>STORED AS TEXTFILE ;</p>\n<h3 id=\"外部表\"><a href=\"#外部表\" class=\"headerlink\" title=\"外部表:\"></a>外部表:</h3><p> CREATE  TABLE IF NOT EXISTS t2(id int,name string,age int)<br> COMMENT ‘xx’<br> ROW FORMAT DELIMITED<br> FIELDS TERMINATED BY ‘,’<br> STORED AS TEXTFILE ; </p>\n<h3 id=\"分区表，桶表\"><a href=\"#分区表，桶表\" class=\"headerlink\" title=\"分区表，桶表\"></a>分区表，桶表</h3><h4 id=\"分区表\"><a href=\"#分区表\" class=\"headerlink\" title=\"分区表\"></a>分区表</h4><p>Hive中有分区表的概念。我们可以看到分区表具有重要的性能，而且分区表还可以将数据以一种符合逻辑的方式进行组织，比如分层存储。Hive的分区表，是把数据放在满足条件的分区目录下<br>CREATE TABLE t3(id int,name string,age int) </p>\n<p>PARTITIONED BY (Year INT, Month INT)   //按照年月进行分区</p>\n<p> ROW FORMAT DELIMITED                      //行分隔符</p>\n<p>FIELDS TERMINATED BY ‘,’ ;                    //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改<br>load data local inpath ‘/home/zpx/customers.txt’ into table t3 partition</p>\n<h4 id=\"分桶表\"><a href=\"#分桶表\" class=\"headerlink\" title=\"分桶表\"></a>分桶表</h4><p>这样做，在查找数据的时候就可以跨越多个桶，直接查找复合条件的数据了。速度快，时间成本低。Hive中的桶表默认使用的机制也是hash。<br>CREATE TABLE t4(id int,name string,age int) </p>\n<pre><code>CLUSTERED BY (id) INTO 3 BUCKETS      //创建3个通桶表，按照字段id进行分桶\n\nROW FORMAT DELIMITED                     //行分隔符\n\nFIELDS TERMINATED BY &apos;,&apos; ; </code></pre><p>load data local inpath ‘/home/centos/customers.txt’ into table t4 ;</p>\n<h2 id=\"导入数据\"><a href=\"#导入数据\" class=\"headerlink\" title=\"导入数据\"></a>导入数据</h2><p>load data local inpath ‘/home/zpx/customers.txt’ into table t2 ; //local上传文件<br>load data inpath ‘/user/zpx/customers.txt’ [overwrite] into table t2 //分布式文件系统上移动文件</p>\n<h2 id=\"建视图\"><a href=\"#建视图\" class=\"headerlink\" title=\"建视图\"></a>建视图</h2><p>Hive也可以建立视图，是一张虚表，方便我们进行操作.</p>\n<p>create view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ;</p>\n<h2 id=\"Hive的严格模式\"><a href=\"#Hive的严格模式\" class=\"headerlink\" title=\"Hive的严格模式\"></a>Hive的严格模式</h2><p>Hive提供了一个严格模式，可以防止用户执行那些产生意想不到的不好的影响的查询。<br>使用了严格模式之后主要对以下3种不良操作进行控制：</p>\n<p>1.分区表必须指定分区进行查询。<br>2.order by时必须使用limit子句。<br>3.不允许笛卡尔积。<br><img src=\"http://img.wqkenqing.ren/2019-03-18-17-13-36.png\" alt=\"2019-03-18-17-13-36\"></p>\n<h2 id=\"Hive的动态分区\"><a href=\"#Hive的动态分区\" class=\"headerlink\" title=\"Hive的动态分区\"></a>Hive的动态分区</h2><p>像分区表里面存储了数据。我们在进行存储数据的时候，都是明确的指定了分区。在这个过程中Hive也提供了一种比较任性化的操作，就是动态分区，不需要我们指定分区目录，Hive能够把数据进行动态的分发,<strong>我们需要将当前的严格模式设置成非严格模式，否则不允许使用动态分区</strong><br>set hive.exec.dynamic.partition.mode=nonstrict//设置非严格模式</p>\n<h2 id=\"Hive的排序\"><a href=\"#Hive的排序\" class=\"headerlink\" title=\"Hive的排序\"></a>Hive的排序</h2><p>Hive也提供了一些排序的语法，包括order by,sort by。</p>\n<p>order by=MapReduce的全排序<br>sort by=MapReduce的部分排序<br>distribute by=MapReduce的分区</p>\n<p>selece …….from …… order by 字段；//按照这个字段全排序</p>\n<p>selece …….from …… sort by 字段； //按照这个字段局部有序</p>\n<p>selece 字段…..from …… distribute by 字段；//按照这个字段分区<br>特别注意的是：</p>\n<ol>\n<li>在上面的最后一个distribute by使用过程中，按照排序的字段要出现在最左侧也就是select中有这个字段，因为我们要告诉MapReduce你要按照哪一个字段分区，当然获取的数据中要出现这个字段了。类似于我们使用group by的用法，字段也必须出现在最左侧，因为数据要包含这个字段，才能按照这个字段分组，至于Hive什么时候会自行的开启MapReduce，那就是在使用聚合的情况下开启，使用select …from ….以及使用分区表的selece ….from……where …..不会开启</li>\n<li>distribute by与sort by可以组合使用，但是distribute by要放在前边，因为MapReduce要先分区，后排序，再归并</li>\n</ol>\n<p>select 字段a,……..from …….distribute by字段a，sort by字段<br>如果distribute by与sort by使用的字段一样，则可以使用cluster by 字段替代：<br>select 字段a,……..from …….cluster by 字段</p>\n<h2 id=\"函数\"><a href=\"#函数\" class=\"headerlink\" title=\"函数\"></a>函数</h2><ol>\n<li>show functions; 展示相关函数</li>\n<li>desc function split;</li>\n<li>desc function  extended split;  //查看函数的扩展信息</li>\n</ol>\n<h3 id=\"用户自定义函数（UDF）\"><a href=\"#用户自定义函数（UDF）\" class=\"headerlink\" title=\"用户自定义函数（UDF）\"></a>用户自定义函数（UDF）</h3><p>具体步骤如下：</p>\n<p>（1）.自定义类（继承UDF，或是GenericUDF。GenericUDF是更为复杂的抽象概念，但是其支持更好的null值处理同时还可以处理一些标准的UDF无法支持的编程操作）。<br>（2）.导出jar包，通过命令添加到hive的类路径。<br>$hive&gt;add jar xxx.jar<br>（3）.注册函数<br>$hive&gt;CREATE TEMPORARY FUNCTION 函数名 AS ‘具体类路径：包.类’;<br>（4）.使用<br> $hive&gt;select 函数名(参数);<br>自定义实现类如下(继承UDF)：</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"Hive相关点小结\"><a href=\"#Hive相关点小结\" class=\"headerlink\" title=\"Hive相关点小结\"></a>Hive相关点小结</h1>","more":"<h2 id=\"启动指令\"><a href=\"#启动指令\" class=\"headerlink\" title=\"启动指令\"></a>启动指令</h2><ol>\n<li>hive ==  hive –service cli<br>不需要启动server，使用本地的metastore，可以直接做一些简单的数据操作和测试。</li>\n<li>启动hiveserver2<br>hive –service hiveserver2</li>\n<li>beeline工具测试使用jdbc方式连接<br>beeline -u jdbc:hive2://localhost:10000</li>\n</ol>\n<p>1.managed table<br>管理表。<br>删除表时，数据也删除了</p>\n<p>2.external table<br>外部表。<br>删除表时，数据不删</p>\n<h2 id=\"建表\"><a href=\"#建表\" class=\"headerlink\" title=\"建表:\"></a>建表:</h2><p>CREATE TABLE IF NOT EXISTS t2(id int,name string,age int)<br>COMMENT ‘xx’                                     //注释<br>ROW FORMAT DELIMITED                             //行分隔符<br>FIELDS TERMINATED BY ‘,’                         //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改<br>STORED AS TEXTFILE ;</p>\n<h3 id=\"外部表\"><a href=\"#外部表\" class=\"headerlink\" title=\"外部表:\"></a>外部表:</h3><p> CREATE  TABLE IF NOT EXISTS t2(id int,name string,age int)<br> COMMENT ‘xx’<br> ROW FORMAT DELIMITED<br> FIELDS TERMINATED BY ‘,’<br> STORED AS TEXTFILE ; </p>\n<h3 id=\"分区表，桶表\"><a href=\"#分区表，桶表\" class=\"headerlink\" title=\"分区表，桶表\"></a>分区表，桶表</h3><h4 id=\"分区表\"><a href=\"#分区表\" class=\"headerlink\" title=\"分区表\"></a>分区表</h4><p>Hive中有分区表的概念。我们可以看到分区表具有重要的性能，而且分区表还可以将数据以一种符合逻辑的方式进行组织，比如分层存储。Hive的分区表，是把数据放在满足条件的分区目录下<br>CREATE TABLE t3(id int,name string,age int) </p>\n<p>PARTITIONED BY (Year INT, Month INT)   //按照年月进行分区</p>\n<p> ROW FORMAT DELIMITED                      //行分隔符</p>\n<p>FIELDS TERMINATED BY ‘,’ ;                    //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改<br>load data local inpath ‘/home/zpx/customers.txt’ into table t3 partition</p>\n<h4 id=\"分桶表\"><a href=\"#分桶表\" class=\"headerlink\" title=\"分桶表\"></a>分桶表</h4><p>这样做，在查找数据的时候就可以跨越多个桶，直接查找复合条件的数据了。速度快，时间成本低。Hive中的桶表默认使用的机制也是hash。<br>CREATE TABLE t4(id int,name string,age int) </p>\n<pre><code>CLUSTERED BY (id) INTO 3 BUCKETS      //创建3个通桶表，按照字段id进行分桶\n\nROW FORMAT DELIMITED                     //行分隔符\n\nFIELDS TERMINATED BY &apos;,&apos; ; </code></pre><p>load data local inpath ‘/home/centos/customers.txt’ into table t4 ;</p>\n<h2 id=\"导入数据\"><a href=\"#导入数据\" class=\"headerlink\" title=\"导入数据\"></a>导入数据</h2><p>load data local inpath ‘/home/zpx/customers.txt’ into table t2 ; //local上传文件<br>load data inpath ‘/user/zpx/customers.txt’ [overwrite] into table t2 //分布式文件系统上移动文件</p>\n<h2 id=\"建视图\"><a href=\"#建视图\" class=\"headerlink\" title=\"建视图\"></a>建视图</h2><p>Hive也可以建立视图，是一张虚表，方便我们进行操作.</p>\n<p>create view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ;</p>\n<h2 id=\"Hive的严格模式\"><a href=\"#Hive的严格模式\" class=\"headerlink\" title=\"Hive的严格模式\"></a>Hive的严格模式</h2><p>Hive提供了一个严格模式，可以防止用户执行那些产生意想不到的不好的影响的查询。<br>使用了严格模式之后主要对以下3种不良操作进行控制：</p>\n<p>1.分区表必须指定分区进行查询。<br>2.order by时必须使用limit子句。<br>3.不允许笛卡尔积。<br><img src=\"http://img.wqkenqing.ren/2019-03-18-17-13-36.png\" alt=\"2019-03-18-17-13-36\"></p>\n<h2 id=\"Hive的动态分区\"><a href=\"#Hive的动态分区\" class=\"headerlink\" title=\"Hive的动态分区\"></a>Hive的动态分区</h2><p>像分区表里面存储了数据。我们在进行存储数据的时候，都是明确的指定了分区。在这个过程中Hive也提供了一种比较任性化的操作，就是动态分区，不需要我们指定分区目录，Hive能够把数据进行动态的分发,<strong>我们需要将当前的严格模式设置成非严格模式，否则不允许使用动态分区</strong><br>set hive.exec.dynamic.partition.mode=nonstrict//设置非严格模式</p>\n<h2 id=\"Hive的排序\"><a href=\"#Hive的排序\" class=\"headerlink\" title=\"Hive的排序\"></a>Hive的排序</h2><p>Hive也提供了一些排序的语法，包括order by,sort by。</p>\n<p>order by=MapReduce的全排序<br>sort by=MapReduce的部分排序<br>distribute by=MapReduce的分区</p>\n<p>selece …….from …… order by 字段；//按照这个字段全排序</p>\n<p>selece …….from …… sort by 字段； //按照这个字段局部有序</p>\n<p>selece 字段…..from …… distribute by 字段；//按照这个字段分区<br>特别注意的是：</p>\n<ol>\n<li>在上面的最后一个distribute by使用过程中，按照排序的字段要出现在最左侧也就是select中有这个字段，因为我们要告诉MapReduce你要按照哪一个字段分区，当然获取的数据中要出现这个字段了。类似于我们使用group by的用法，字段也必须出现在最左侧，因为数据要包含这个字段，才能按照这个字段分组，至于Hive什么时候会自行的开启MapReduce，那就是在使用聚合的情况下开启，使用select …from ….以及使用分区表的selece ….from……where …..不会开启</li>\n<li>distribute by与sort by可以组合使用，但是distribute by要放在前边，因为MapReduce要先分区，后排序，再归并</li>\n</ol>\n<p>select 字段a,……..from …….distribute by字段a，sort by字段<br>如果distribute by与sort by使用的字段一样，则可以使用cluster by 字段替代：<br>select 字段a,……..from …….cluster by 字段</p>\n<h2 id=\"函数\"><a href=\"#函数\" class=\"headerlink\" title=\"函数\"></a>函数</h2><ol>\n<li>show functions; 展示相关函数</li>\n<li>desc function split;</li>\n<li>desc function  extended split;  //查看函数的扩展信息</li>\n</ol>\n<h3 id=\"用户自定义函数（UDF）\"><a href=\"#用户自定义函数（UDF）\" class=\"headerlink\" title=\"用户自定义函数（UDF）\"></a>用户自定义函数（UDF）</h3><p>具体步骤如下：</p>\n<p>（1）.自定义类（继承UDF，或是GenericUDF。GenericUDF是更为复杂的抽象概念，但是其支持更好的null值处理同时还可以处理一些标准的UDF无法支持的编程操作）。<br>（2）.导出jar包，通过命令添加到hive的类路径。<br>$hive&gt;add jar xxx.jar<br>（3）.注册函数<br>$hive&gt;CREATE TEMPORARY FUNCTION 函数名 AS ‘具体类路径：包.类’;<br>（4）.使用<br> $hive&gt;select 函数名(参数);<br>自定义实现类如下(继承UDF)：</p>"},{"title":"sqoop记录","date":"2018-03-04T02:54:48.000Z","abstract":"sqoop记录","_content":"\n##  将Mysql数据导入Hive中\n<!-- more -->\n命令:\n```\nsqoop import  \n-Dorg.apache.sqoop.splitter.allow_text_splitter=true       \n--connect jdbc:mysql://211.159.172.76:3306/solo\n--username root \n--password 125323Wkq \n--table  tablename \n--hive-import \n--hive-table tablename \n```\n\n### 整库导入\n\n```\nsqoop import-all-tables --connect jdbc:mysql://211.159.172.76:3306/ --username root --password 125323Wkq --hive-database solo  -m 10  \n--create-hive-table  \n--fields-terminated-by \"\\t\"\n--hive-import --hive-database qianyang --hive-overwrite\n```\nsqoop  import-all-tables -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:mysql://211.159.172.76:3306/solo --username root --password 125323Wkq --hive-database blog  --create-hive-table  --hive-import --hive-overwrite -m 10 \n  \n\n\n### 单表导入\n\nsqoop import  --connect   jdbc:mysql://211.159.172.76:3306/solo --username root     --password 125323Wkq    --table b3_solo_article --target-dir /blog/article   --hive-import  --hive-database blog  \n--fields-terminated-by \"\\t\" --hive-table article  --hive-overwrite\n--m 10  \n\nsqoop  import  --connect jdbc:mysql://211.159.172.76:3306/solo --username root --password 125323Wkq --table b3_solo_article --target-dir /blog/article --hive-import --hive-database blog  --create-hive-table  --hive-table article --hive-overwrite -m 1 \n","source":"_posts/技术/hexo/old/sqoop记录.md","raw":"---\ntitle: sqoop记录\ndate: 2018-03-04 10:54:48\ntags: 日常总结\nabstract: sqoop记录\n---\n\n##  将Mysql数据导入Hive中\n<!-- more -->\n命令:\n```\nsqoop import  \n-Dorg.apache.sqoop.splitter.allow_text_splitter=true       \n--connect jdbc:mysql://211.159.172.76:3306/solo\n--username root \n--password 125323Wkq \n--table  tablename \n--hive-import \n--hive-table tablename \n```\n\n### 整库导入\n\n```\nsqoop import-all-tables --connect jdbc:mysql://211.159.172.76:3306/ --username root --password 125323Wkq --hive-database solo  -m 10  \n--create-hive-table  \n--fields-terminated-by \"\\t\"\n--hive-import --hive-database qianyang --hive-overwrite\n```\nsqoop  import-all-tables -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:mysql://211.159.172.76:3306/solo --username root --password 125323Wkq --hive-database blog  --create-hive-table  --hive-import --hive-overwrite -m 10 \n  \n\n\n### 单表导入\n\nsqoop import  --connect   jdbc:mysql://211.159.172.76:3306/solo --username root     --password 125323Wkq    --table b3_solo_article --target-dir /blog/article   --hive-import  --hive-database blog  \n--fields-terminated-by \"\\t\" --hive-table article  --hive-overwrite\n--m 10  \n\nsqoop  import  --connect jdbc:mysql://211.159.172.76:3306/solo --username root --password 125323Wkq --table b3_solo_article --target-dir /blog/article --hive-import --hive-database blog  --create-hive-table  --hive-table article --hive-overwrite -m 1 \n","slug":"技术/hexo/old/sqoop记录","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd334002b38pw39rgbw0b","content":"<h2 id=\"将Mysql数据导入Hive中\"><a href=\"#将Mysql数据导入Hive中\" class=\"headerlink\" title=\"将Mysql数据导入Hive中\"></a>将Mysql数据导入Hive中</h2><a id=\"more\"></a>\n<p>命令:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import  </span><br><span class=\"line\">-Dorg.apache.sqoop.splitter.allow_text_splitter&#x3D;true       </span><br><span class=\"line\">--connect jdbc:mysql:&#x2F;&#x2F;211.159.172.76:3306&#x2F;solo</span><br><span class=\"line\">--username root </span><br><span class=\"line\">--password 125323Wkq </span><br><span class=\"line\">--table  tablename </span><br><span class=\"line\">--hive-import </span><br><span class=\"line\">--hive-table tablename</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"整库导入\"><a href=\"#整库导入\" class=\"headerlink\" title=\"整库导入\"></a>整库导入</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import-all-tables --connect jdbc:mysql:&#x2F;&#x2F;211.159.172.76:3306&#x2F; --username root --password 125323Wkq --hive-database solo  -m 10  </span><br><span class=\"line\">--create-hive-table  </span><br><span class=\"line\">--fields-terminated-by &quot;\\t&quot;</span><br><span class=\"line\">--hive-import --hive-database qianyang --hive-overwrite</span><br></pre></td></tr></table></figure>\n<p>sqoop  import-all-tables -Dorg.apache.sqoop.splitter.allow_text_splitter=true –connect jdbc:mysql://211.159.172.76:3306/solo –username root –password 125323Wkq –hive-database blog  –create-hive-table  –hive-import –hive-overwrite -m 10 </p>\n<h3 id=\"单表导入\"><a href=\"#单表导入\" class=\"headerlink\" title=\"单表导入\"></a>单表导入</h3><p>sqoop import  –connect   jdbc:mysql://211.159.172.76:3306/solo –username root     –password 125323Wkq    –table b3_solo_article –target-dir /blog/article   –hive-import  –hive-database blog<br>–fields-terminated-by “\\t” –hive-table article  –hive-overwrite<br>–m 10  </p>\n<p>sqoop  import  –connect jdbc:mysql://211.159.172.76:3306/solo –username root –password 125323Wkq –table b3_solo_article –target-dir /blog/article –hive-import –hive-database blog  –create-hive-table  –hive-table article –hive-overwrite -m 1 </p>\n","site":{"data":{}},"excerpt":"<h2 id=\"将Mysql数据导入Hive中\"><a href=\"#将Mysql数据导入Hive中\" class=\"headerlink\" title=\"将Mysql数据导入Hive中\"></a>将Mysql数据导入Hive中</h2>","more":"<p>命令:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import  </span><br><span class=\"line\">-Dorg.apache.sqoop.splitter.allow_text_splitter&#x3D;true       </span><br><span class=\"line\">--connect jdbc:mysql:&#x2F;&#x2F;211.159.172.76:3306&#x2F;solo</span><br><span class=\"line\">--username root </span><br><span class=\"line\">--password 125323Wkq </span><br><span class=\"line\">--table  tablename </span><br><span class=\"line\">--hive-import </span><br><span class=\"line\">--hive-table tablename</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"整库导入\"><a href=\"#整库导入\" class=\"headerlink\" title=\"整库导入\"></a>整库导入</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sqoop import-all-tables --connect jdbc:mysql:&#x2F;&#x2F;211.159.172.76:3306&#x2F; --username root --password 125323Wkq --hive-database solo  -m 10  </span><br><span class=\"line\">--create-hive-table  </span><br><span class=\"line\">--fields-terminated-by &quot;\\t&quot;</span><br><span class=\"line\">--hive-import --hive-database qianyang --hive-overwrite</span><br></pre></td></tr></table></figure>\n<p>sqoop  import-all-tables -Dorg.apache.sqoop.splitter.allow_text_splitter=true –connect jdbc:mysql://211.159.172.76:3306/solo –username root –password 125323Wkq –hive-database blog  –create-hive-table  –hive-import –hive-overwrite -m 10 </p>\n<h3 id=\"单表导入\"><a href=\"#单表导入\" class=\"headerlink\" title=\"单表导入\"></a>单表导入</h3><p>sqoop import  –connect   jdbc:mysql://211.159.172.76:3306/solo –username root     –password 125323Wkq    –table b3_solo_article –target-dir /blog/article   –hive-import  –hive-database blog<br>–fields-terminated-by “\\t” –hive-table article  –hive-overwrite<br>–m 10  </p>\n<p>sqoop  import  –connect jdbc:mysql://211.159.172.76:3306/solo –username root –password 125323Wkq –table b3_solo_article –target-dir /blog/article –hive-import –hive-database blog  –create-hive-table  –hive-table article –hive-overwrite -m 1 </p>"},{"title":"准备小结","_content":"# 准备小结\n\n<!-- more -->\n## hdfs存储机制是怎样的?\nclient端发送写文件请求，namenode检查文件是否存在，如果已存在，直接返回错误信息，否则，发送给client一些可用namenode节点\nclient将文件分块，并行存储到不同节点上datanode上，发送完成后，client同时发送信息给namenode和datanode\nnamenode收到的client信息后，发送确信信息给datanode\ndatanode同时收到namenode和datanode的确认信息后，提交写操作。\n## hadoop中combiner的作用是什么?\n当map生成的数据过大时，带宽就成了瓶颈，怎样精简压缩传给Reduce的数据，又不影响最终的结果呢。有一种方法就是使用Combiner，Combiner号称本地的Reduce，Reduce最终的输入，是Combiner的输出。\n##  你们数据库怎么导入hive 的,有没有出现问题\n在导入hive的时候，如果数据库中有blob或者text字段，会报错，解决方案在sqoop笔记中。在将数据由Oracle数据库导入到Hive时，发现带有clob字段的表的数据会错乱，出现一些字段全为NULL的空行。由于在项目中CLOB字段没有实际的分析用途，因此考虑将CLOB字段去掉。\n## hdfs-site.xml的3个主要属性?\ndfs.name.dir决定的是元数据存储的路径以及DFS的存储方式(磁盘或是远端)\ndfs.data.dir决定的是数据存储的路径\nfs.checkpoint.dir用于第二Namenode\n## 下列哪项通常是集群的最主要瓶颈\n磁盘 IO \n答案：C 磁盘 \n首先集群的目的是为了节省成本，用廉价的 pc 机，取代小型机及大型机。小型机和大型机有什么特点？ \n1.cpu 处理能力强 \n2.内存够大，所以集群的瓶颈不可能是 a 和 d \n3.如果是互联网有瓶颈，可以让集群搭建内网。每次写入数据都要通过网络（集群是内网），然后还要写入 3 份数据，所以 IO 就会打折扣。\n\n## 关于 SecondaryNameNode 哪项是正确的？\n它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间 \n## mapreduce的原理?\nMapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果， \n得到最终结果。简单地说，MapReduce就是”任务的分解与结果的汇总”。 \n在Hadoop中，用于执行MapReduce任务的机器角色有两个：一个是JobTracker；另一个是TaskTracker，JobTracker是用于调度工作的，TaskTracker \n是用于执行工作的。一个Hadoop集群中只有一台JobTracker。 \n在分布式计算中，MapReduce框架负责处理了并行编程中分布式存储、工作调度、负载均衡、容错均衡、容错处理以及网络通信等复杂问题，把处理 \n过程高度抽象为两个函数：map和reduce，map负责把任务分解成多个任务，reduce负责把分解后多任务处理的结果汇总起来。 \n需要注意的是，用MapReduce来处理的数据集（或任务）必须具备这样的特点：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都 \n可以完全并行地进行处理。\n## HDFS存储的机制?\n### 写流程： \nclient链接namenode存数据 \nnamenode记录一条数据位置信息（元数据），告诉client存哪。 \nclient用hdfs的api将数据块（默认是64M）存储到datanode上。 \ndatanode将数据水平备份。并且备份完将反馈client。 \nclient通知namenode存储块完毕。 \nnamenode将元数据同步到内存中。 \n另一块循环上面的过程。\n###  读流程\n## 举一个简单的例子说明mapreduce是怎么来运行的 ?\nMapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。 \n　　Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。 \nMapper任务的执行过程详解 \n　　每个Mapper任务是一个Java进程，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后， \n转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下六个阶段： \n　　第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit) \n的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值128MB，输入文件有两个，一个是32MB，一个是　172MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由　一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。\n\n　　第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一　行的起始位置(单位是字节)，“值”是本行的文本内容。 \n　　 \n　　第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会　调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。\n\n　　第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、　山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer　任务运行的数量。默认只有一个Reducer任务。 \n第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值　对<2,2>、<1,3>、<2,1>，键和值分别是整数。那么排序后的结果是<1,3>、<2,1>、<2,2>。如果有第六阶段，那么进入\n\n第六阶段　如果没有，直接输出到本地的Linux文件中。　第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。　归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。　Reducer任务的执行过程详解 \n每个Reducer任务是一个java进程。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为三个阶段： \n第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。 \n第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。 \n第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。 \n最后把这些输出的键值对写入到HDFS文件中。 \n在整个MapReduce程序的开发过程中，我们最大的工作量是覆盖map函数和覆盖reduce函数。\n\n## 了解hashMap 和hashTable吗介绍下，他们有什么区别。\n\n\n\n## 为什么重写equals还要重写hashcode\n因为equals比较的是内容是一致.但hashcode\n\n## 说一下map的分类和常见的情况\n hashmap,hashtable,treemap,LinkedHashMap\n* 根据键得到值，因此不允许键重复(重复了覆盖了),但允许值重复\n### Hashmap \n是一个最常用的Map\n* 它根据键的HashCode值存储数据,根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的\n* 最多只允许一条记录的键为Null;允许多条记录的值为 Null;\n* HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。\n* 如果需要同步，可以用 Collections的synchronizedMap方法使HashMap具有同步的能力，或者使用ConcurrentHashMap\n### Hashtable\nHashtable与 HashMap类似,它继承自Dictionary类,不同的是:它不允许记录的键或者值为空;\n* 它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了 Hashtable在写入时会比较慢\n### LinkedHashMap\n是 HashMap 的一个子类，保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的.\n也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比 HashMap 慢，不过有种情况例外，当 HashMap 容量很大，实际数据较少时，遍历起来可能会比 LinkedHashMap 慢，因为 LinkedHashMap 的遍历速度只和实际数据有关，和容量无关，而 HashMap 的遍历速度和他的容量有关\n### TreeMap\n实现 SortMap 接口,能够把它保存的记录根据键排序, 默认是按键值的升序排序，也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的\n\nHashMap，链表法存储，entry[]数组，线程不安全，可能死锁 concurrentHashMap，segment数组，每个segent下维护一组entry[]数组，每个segment是一把锁，线程安全 LinkedHashMap\n\n---\n\n## Object若不重写hashCode()的话，hashCode()如何计算出来的？\nhashcode采用的是\n\n\n\n## spark\n\n### 1. spark的有几种部署模式，每种模式特点？\n\n#### 本地模式\n本地模式分三类\n* local：只启动一个executor\n* local[k]: 启动k个executor\n* local[*]：启动跟cpu数目相同的 executor\n\n### cluster模式\ncluster模式肯定就是运行很多机器上了，但是它又分为以下三种模式，区别在于谁去管理资源调度。（说白了，就好像后勤管家，哪里需要资源，后勤管家要负责调度这些资源）\n#### standalone模式\n分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础\n\n#### Spark on yarn模式\n分布式部署集群，资源和任务监控交给yarn管理\n粗粒度资源分配方式，包含cluster和client运行模式\ncluster 适合生产，driver运行在集群子节点，具有容错功能\nclient 适合调试，dirver运行在客户端\n\n###  2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？\n\n#### Spark core\n是其它组件的基础，spark的内核\n主要包含：有向循环图、RDD、Lingage、Cache、broadcast等\n\n#### SparkStreaming\n是一个对实时数据流进行高通量、容错处理的流式处理系统\n将流式计算分解成一系列短小的批处理作业\n\n#### Spark sql：\n能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询\n#### MLBase\n是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低\nMLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。\n#### GraphX\n是Spark中用于图和图并行计算\n#### spark有哪些组件\nmaster：管理集群和节点，不参与计算。\nworker：计算节点，进程本身不参与计算，和master汇报。\nDriver：运行程序的main方法，创建spark context对象。\nspark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。\nclient：用户提交程序的入口。\n\n\n*   https://blog.csdn.net/yirenboy/article/details/47441465","source":"_posts/技术/hexo/old/准备小结.md","raw":"---\ntitle: 准备小结\ntags: 小结\n---\n# 准备小结\n\n<!-- more -->\n## hdfs存储机制是怎样的?\nclient端发送写文件请求，namenode检查文件是否存在，如果已存在，直接返回错误信息，否则，发送给client一些可用namenode节点\nclient将文件分块，并行存储到不同节点上datanode上，发送完成后，client同时发送信息给namenode和datanode\nnamenode收到的client信息后，发送确信信息给datanode\ndatanode同时收到namenode和datanode的确认信息后，提交写操作。\n## hadoop中combiner的作用是什么?\n当map生成的数据过大时，带宽就成了瓶颈，怎样精简压缩传给Reduce的数据，又不影响最终的结果呢。有一种方法就是使用Combiner，Combiner号称本地的Reduce，Reduce最终的输入，是Combiner的输出。\n##  你们数据库怎么导入hive 的,有没有出现问题\n在导入hive的时候，如果数据库中有blob或者text字段，会报错，解决方案在sqoop笔记中。在将数据由Oracle数据库导入到Hive时，发现带有clob字段的表的数据会错乱，出现一些字段全为NULL的空行。由于在项目中CLOB字段没有实际的分析用途，因此考虑将CLOB字段去掉。\n## hdfs-site.xml的3个主要属性?\ndfs.name.dir决定的是元数据存储的路径以及DFS的存储方式(磁盘或是远端)\ndfs.data.dir决定的是数据存储的路径\nfs.checkpoint.dir用于第二Namenode\n## 下列哪项通常是集群的最主要瓶颈\n磁盘 IO \n答案：C 磁盘 \n首先集群的目的是为了节省成本，用廉价的 pc 机，取代小型机及大型机。小型机和大型机有什么特点？ \n1.cpu 处理能力强 \n2.内存够大，所以集群的瓶颈不可能是 a 和 d \n3.如果是互联网有瓶颈，可以让集群搭建内网。每次写入数据都要通过网络（集群是内网），然后还要写入 3 份数据，所以 IO 就会打折扣。\n\n## 关于 SecondaryNameNode 哪项是正确的？\n它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间 \n## mapreduce的原理?\nMapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果， \n得到最终结果。简单地说，MapReduce就是”任务的分解与结果的汇总”。 \n在Hadoop中，用于执行MapReduce任务的机器角色有两个：一个是JobTracker；另一个是TaskTracker，JobTracker是用于调度工作的，TaskTracker \n是用于执行工作的。一个Hadoop集群中只有一台JobTracker。 \n在分布式计算中，MapReduce框架负责处理了并行编程中分布式存储、工作调度、负载均衡、容错均衡、容错处理以及网络通信等复杂问题，把处理 \n过程高度抽象为两个函数：map和reduce，map负责把任务分解成多个任务，reduce负责把分解后多任务处理的结果汇总起来。 \n需要注意的是，用MapReduce来处理的数据集（或任务）必须具备这样的特点：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都 \n可以完全并行地进行处理。\n## HDFS存储的机制?\n### 写流程： \nclient链接namenode存数据 \nnamenode记录一条数据位置信息（元数据），告诉client存哪。 \nclient用hdfs的api将数据块（默认是64M）存储到datanode上。 \ndatanode将数据水平备份。并且备份完将反馈client。 \nclient通知namenode存储块完毕。 \nnamenode将元数据同步到内存中。 \n另一块循环上面的过程。\n###  读流程\n## 举一个简单的例子说明mapreduce是怎么来运行的 ?\nMapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。 \n　　Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。 \nMapper任务的执行过程详解 \n　　每个Mapper任务是一个Java进程，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后， \n转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下六个阶段： \n　　第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit) \n的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值128MB，输入文件有两个，一个是32MB，一个是　172MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由　一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。\n\n　　第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一　行的起始位置(单位是字节)，“值”是本行的文本内容。 \n　　 \n　　第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会　调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。\n\n　　第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、　山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer　任务运行的数量。默认只有一个Reducer任务。 \n第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值　对<2,2>、<1,3>、<2,1>，键和值分别是整数。那么排序后的结果是<1,3>、<2,1>、<2,2>。如果有第六阶段，那么进入\n\n第六阶段　如果没有，直接输出到本地的Linux文件中。　第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。　归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。　Reducer任务的执行过程详解 \n每个Reducer任务是一个java进程。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为三个阶段： \n第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。 \n第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。 \n第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。 \n最后把这些输出的键值对写入到HDFS文件中。 \n在整个MapReduce程序的开发过程中，我们最大的工作量是覆盖map函数和覆盖reduce函数。\n\n## 了解hashMap 和hashTable吗介绍下，他们有什么区别。\n\n\n\n## 为什么重写equals还要重写hashcode\n因为equals比较的是内容是一致.但hashcode\n\n## 说一下map的分类和常见的情况\n hashmap,hashtable,treemap,LinkedHashMap\n* 根据键得到值，因此不允许键重复(重复了覆盖了),但允许值重复\n### Hashmap \n是一个最常用的Map\n* 它根据键的HashCode值存储数据,根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的\n* 最多只允许一条记录的键为Null;允许多条记录的值为 Null;\n* HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。\n* 如果需要同步，可以用 Collections的synchronizedMap方法使HashMap具有同步的能力，或者使用ConcurrentHashMap\n### Hashtable\nHashtable与 HashMap类似,它继承自Dictionary类,不同的是:它不允许记录的键或者值为空;\n* 它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了 Hashtable在写入时会比较慢\n### LinkedHashMap\n是 HashMap 的一个子类，保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的.\n也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比 HashMap 慢，不过有种情况例外，当 HashMap 容量很大，实际数据较少时，遍历起来可能会比 LinkedHashMap 慢，因为 LinkedHashMap 的遍历速度只和实际数据有关，和容量无关，而 HashMap 的遍历速度和他的容量有关\n### TreeMap\n实现 SortMap 接口,能够把它保存的记录根据键排序, 默认是按键值的升序排序，也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的\n\nHashMap，链表法存储，entry[]数组，线程不安全，可能死锁 concurrentHashMap，segment数组，每个segent下维护一组entry[]数组，每个segment是一把锁，线程安全 LinkedHashMap\n\n---\n\n## Object若不重写hashCode()的话，hashCode()如何计算出来的？\nhashcode采用的是\n\n\n\n## spark\n\n### 1. spark的有几种部署模式，每种模式特点？\n\n#### 本地模式\n本地模式分三类\n* local：只启动一个executor\n* local[k]: 启动k个executor\n* local[*]：启动跟cpu数目相同的 executor\n\n### cluster模式\ncluster模式肯定就是运行很多机器上了，但是它又分为以下三种模式，区别在于谁去管理资源调度。（说白了，就好像后勤管家，哪里需要资源，后勤管家要负责调度这些资源）\n#### standalone模式\n分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础\n\n#### Spark on yarn模式\n分布式部署集群，资源和任务监控交给yarn管理\n粗粒度资源分配方式，包含cluster和client运行模式\ncluster 适合生产，driver运行在集群子节点，具有容错功能\nclient 适合调试，dirver运行在客户端\n\n###  2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？\n\n#### Spark core\n是其它组件的基础，spark的内核\n主要包含：有向循环图、RDD、Lingage、Cache、broadcast等\n\n#### SparkStreaming\n是一个对实时数据流进行高通量、容错处理的流式处理系统\n将流式计算分解成一系列短小的批处理作业\n\n#### Spark sql：\n能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询\n#### MLBase\n是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低\nMLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。\n#### GraphX\n是Spark中用于图和图并行计算\n#### spark有哪些组件\nmaster：管理集群和节点，不参与计算。\nworker：计算节点，进程本身不参与计算，和master汇报。\nDriver：运行程序的main方法，创建spark context对象。\nspark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。\nclient：用户提交程序的入口。\n\n\n*   https://blog.csdn.net/yirenboy/article/details/47441465","slug":"技术/hexo/old/准备小结","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd336002e38pwctxo7cch","content":"<h1 id=\"准备小结\"><a href=\"#准备小结\" class=\"headerlink\" title=\"准备小结\"></a>准备小结</h1><a id=\"more\"></a>\n<h2 id=\"hdfs存储机制是怎样的\"><a href=\"#hdfs存储机制是怎样的\" class=\"headerlink\" title=\"hdfs存储机制是怎样的?\"></a>hdfs存储机制是怎样的?</h2><p>client端发送写文件请求，namenode检查文件是否存在，如果已存在，直接返回错误信息，否则，发送给client一些可用namenode节点<br>client将文件分块，并行存储到不同节点上datanode上，发送完成后，client同时发送信息给namenode和datanode<br>namenode收到的client信息后，发送确信信息给datanode<br>datanode同时收到namenode和datanode的确认信息后，提交写操作。</p>\n<h2 id=\"hadoop中combiner的作用是什么\"><a href=\"#hadoop中combiner的作用是什么\" class=\"headerlink\" title=\"hadoop中combiner的作用是什么?\"></a>hadoop中combiner的作用是什么?</h2><p>当map生成的数据过大时，带宽就成了瓶颈，怎样精简压缩传给Reduce的数据，又不影响最终的结果呢。有一种方法就是使用Combiner，Combiner号称本地的Reduce，Reduce最终的输入，是Combiner的输出。</p>\n<h2 id=\"你们数据库怎么导入hive-的-有没有出现问题\"><a href=\"#你们数据库怎么导入hive-的-有没有出现问题\" class=\"headerlink\" title=\"你们数据库怎么导入hive 的,有没有出现问题\"></a>你们数据库怎么导入hive 的,有没有出现问题</h2><p>在导入hive的时候，如果数据库中有blob或者text字段，会报错，解决方案在sqoop笔记中。在将数据由Oracle数据库导入到Hive时，发现带有clob字段的表的数据会错乱，出现一些字段全为NULL的空行。由于在项目中CLOB字段没有实际的分析用途，因此考虑将CLOB字段去掉。</p>\n<h2 id=\"hdfs-site-xml的3个主要属性\"><a href=\"#hdfs-site-xml的3个主要属性\" class=\"headerlink\" title=\"hdfs-site.xml的3个主要属性?\"></a>hdfs-site.xml的3个主要属性?</h2><p>dfs.name.dir决定的是元数据存储的路径以及DFS的存储方式(磁盘或是远端)<br>dfs.data.dir决定的是数据存储的路径<br>fs.checkpoint.dir用于第二Namenode</p>\n<h2 id=\"下列哪项通常是集群的最主要瓶颈\"><a href=\"#下列哪项通常是集群的最主要瓶颈\" class=\"headerlink\" title=\"下列哪项通常是集群的最主要瓶颈\"></a>下列哪项通常是集群的最主要瓶颈</h2><p>磁盘 IO<br>答案：C 磁盘<br>首先集群的目的是为了节省成本，用廉价的 pc 机，取代小型机及大型机。小型机和大型机有什么特点？<br>1.cpu 处理能力强<br>2.内存够大，所以集群的瓶颈不可能是 a 和 d<br>3.如果是互联网有瓶颈，可以让集群搭建内网。每次写入数据都要通过网络（集群是内网），然后还要写入 3 份数据，所以 IO 就会打折扣。</p>\n<h2 id=\"关于-SecondaryNameNode-哪项是正确的？\"><a href=\"#关于-SecondaryNameNode-哪项是正确的？\" class=\"headerlink\" title=\"关于 SecondaryNameNode 哪项是正确的？\"></a>关于 SecondaryNameNode 哪项是正确的？</h2><p>它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间 </p>\n<h2 id=\"mapreduce的原理\"><a href=\"#mapreduce的原理\" class=\"headerlink\" title=\"mapreduce的原理?\"></a>mapreduce的原理?</h2><p>MapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果，<br>得到最终结果。简单地说，MapReduce就是”任务的分解与结果的汇总”。<br>在Hadoop中，用于执行MapReduce任务的机器角色有两个：一个是JobTracker；另一个是TaskTracker，JobTracker是用于调度工作的，TaskTracker<br>是用于执行工作的。一个Hadoop集群中只有一台JobTracker。<br>在分布式计算中，MapReduce框架负责处理了并行编程中分布式存储、工作调度、负载均衡、容错均衡、容错处理以及网络通信等复杂问题，把处理<br>过程高度抽象为两个函数：map和reduce，map负责把任务分解成多个任务，reduce负责把分解后多任务处理的结果汇总起来。<br>需要注意的是，用MapReduce来处理的数据集（或任务）必须具备这样的特点：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都<br>可以完全并行地进行处理。</p>\n<h2 id=\"HDFS存储的机制\"><a href=\"#HDFS存储的机制\" class=\"headerlink\" title=\"HDFS存储的机制?\"></a>HDFS存储的机制?</h2><h3 id=\"写流程：\"><a href=\"#写流程：\" class=\"headerlink\" title=\"写流程：\"></a>写流程：</h3><p>client链接namenode存数据<br>namenode记录一条数据位置信息（元数据），告诉client存哪。<br>client用hdfs的api将数据块（默认是64M）存储到datanode上。<br>datanode将数据水平备份。并且备份完将反馈client。<br>client通知namenode存储块完毕。<br>namenode将元数据同步到内存中。<br>另一块循环上面的过程。</p>\n<h3 id=\"读流程\"><a href=\"#读流程\" class=\"headerlink\" title=\"读流程\"></a>读流程</h3><h2 id=\"举一个简单的例子说明mapreduce是怎么来运行的\"><a href=\"#举一个简单的例子说明mapreduce是怎么来运行的\" class=\"headerlink\" title=\"举一个简单的例子说明mapreduce是怎么来运行的 ?\"></a>举一个简单的例子说明mapreduce是怎么来运行的 ?</h2><p>MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。<br>　　Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。<br>Mapper任务的执行过程详解<br>　　每个Mapper任务是一个Java进程，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后，<br>转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下六个阶段：<br>　　第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit)<br>的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值128MB，输入文件有两个，一个是32MB，一个是　172MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由　一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。</p>\n<p>　　第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一　行的起始位置(单位是字节)，“值”是本行的文本内容。<br>　　<br>　　第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会　调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。</p>\n<p>　　第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、　山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer　任务运行的数量。默认只有一个Reducer任务。<br>第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值　对&lt;2,2&gt;、&lt;1,3&gt;、&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是&lt;1,3&gt;、&lt;2,1&gt;、&lt;2,2&gt;。如果有第六阶段，那么进入</p>\n<p>第六阶段　如果没有，直接输出到本地的Linux文件中。　第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。　归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。　Reducer任务的执行过程详解<br>每个Reducer任务是一个java进程。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为三个阶段：<br>第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。<br>第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。<br>第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。<br>最后把这些输出的键值对写入到HDFS文件中。<br>在整个MapReduce程序的开发过程中，我们最大的工作量是覆盖map函数和覆盖reduce函数。</p>\n<h2 id=\"了解hashMap-和hashTable吗介绍下，他们有什么区别。\"><a href=\"#了解hashMap-和hashTable吗介绍下，他们有什么区别。\" class=\"headerlink\" title=\"了解hashMap 和hashTable吗介绍下，他们有什么区别。\"></a>了解hashMap 和hashTable吗介绍下，他们有什么区别。</h2><h2 id=\"为什么重写equals还要重写hashcode\"><a href=\"#为什么重写equals还要重写hashcode\" class=\"headerlink\" title=\"为什么重写equals还要重写hashcode\"></a>为什么重写equals还要重写hashcode</h2><p>因为equals比较的是内容是一致.但hashcode</p>\n<h2 id=\"说一下map的分类和常见的情况\"><a href=\"#说一下map的分类和常见的情况\" class=\"headerlink\" title=\"说一下map的分类和常见的情况\"></a>说一下map的分类和常见的情况</h2><p> hashmap,hashtable,treemap,LinkedHashMap</p>\n<ul>\n<li>根据键得到值，因此不允许键重复(重复了覆盖了),但允许值重复<h3 id=\"Hashmap\"><a href=\"#Hashmap\" class=\"headerlink\" title=\"Hashmap\"></a>Hashmap</h3>是一个最常用的Map</li>\n<li>它根据键的HashCode值存储数据,根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的</li>\n<li>最多只允许一条记录的键为Null;允许多条记录的值为 Null;</li>\n<li>HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。</li>\n<li>如果需要同步，可以用 Collections的synchronizedMap方法使HashMap具有同步的能力，或者使用ConcurrentHashMap<h3 id=\"Hashtable\"><a href=\"#Hashtable\" class=\"headerlink\" title=\"Hashtable\"></a>Hashtable</h3>Hashtable与 HashMap类似,它继承自Dictionary类,不同的是:它不允许记录的键或者值为空;</li>\n<li>它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了 Hashtable在写入时会比较慢<h3 id=\"LinkedHashMap\"><a href=\"#LinkedHashMap\" class=\"headerlink\" title=\"LinkedHashMap\"></a>LinkedHashMap</h3>是 HashMap 的一个子类，保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的.<br>也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比 HashMap 慢，不过有种情况例外，当 HashMap 容量很大，实际数据较少时，遍历起来可能会比 LinkedHashMap 慢，因为 LinkedHashMap 的遍历速度只和实际数据有关，和容量无关，而 HashMap 的遍历速度和他的容量有关<h3 id=\"TreeMap\"><a href=\"#TreeMap\" class=\"headerlink\" title=\"TreeMap\"></a>TreeMap</h3>实现 SortMap 接口,能够把它保存的记录根据键排序, 默认是按键值的升序排序，也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的</li>\n</ul>\n<p>HashMap，链表法存储，entry[]数组，线程不安全，可能死锁 concurrentHashMap，segment数组，每个segent下维护一组entry[]数组，每个segment是一把锁，线程安全 LinkedHashMap</p>\n<hr>\n<h2 id=\"Object若不重写hashCode-的话，hashCode-如何计算出来的？\"><a href=\"#Object若不重写hashCode-的话，hashCode-如何计算出来的？\" class=\"headerlink\" title=\"Object若不重写hashCode()的话，hashCode()如何计算出来的？\"></a>Object若不重写hashCode()的话，hashCode()如何计算出来的？</h2><p>hashcode采用的是</p>\n<h2 id=\"spark\"><a href=\"#spark\" class=\"headerlink\" title=\"spark\"></a>spark</h2><h3 id=\"1-spark的有几种部署模式，每种模式特点？\"><a href=\"#1-spark的有几种部署模式，每种模式特点？\" class=\"headerlink\" title=\"1. spark的有几种部署模式，每种模式特点？\"></a>1. spark的有几种部署模式，每种模式特点？</h3><h4 id=\"本地模式\"><a href=\"#本地模式\" class=\"headerlink\" title=\"本地模式\"></a>本地模式</h4><p>本地模式分三类</p>\n<ul>\n<li>local：只启动一个executor</li>\n<li>local[k]: 启动k个executor</li>\n<li>local[*]：启动跟cpu数目相同的 executor</li>\n</ul>\n<h3 id=\"cluster模式\"><a href=\"#cluster模式\" class=\"headerlink\" title=\"cluster模式\"></a>cluster模式</h3><p>cluster模式肯定就是运行很多机器上了，但是它又分为以下三种模式，区别在于谁去管理资源调度。（说白了，就好像后勤管家，哪里需要资源，后勤管家要负责调度这些资源）</p>\n<h4 id=\"standalone模式\"><a href=\"#standalone模式\" class=\"headerlink\" title=\"standalone模式\"></a>standalone模式</h4><p>分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础</p>\n<h4 id=\"Spark-on-yarn模式\"><a href=\"#Spark-on-yarn模式\" class=\"headerlink\" title=\"Spark on yarn模式\"></a>Spark on yarn模式</h4><p>分布式部署集群，资源和任务监控交给yarn管理<br>粗粒度资源分配方式，包含cluster和client运行模式<br>cluster 适合生产，driver运行在集群子节点，具有容错功能<br>client 适合调试，dirver运行在客户端</p>\n<h3 id=\"2-Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？\"><a href=\"#2-Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？\" class=\"headerlink\" title=\"2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？\"></a>2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</h3><h4 id=\"Spark-core\"><a href=\"#Spark-core\" class=\"headerlink\" title=\"Spark core\"></a>Spark core</h4><p>是其它组件的基础，spark的内核<br>主要包含：有向循环图、RDD、Lingage、Cache、broadcast等</p>\n<h4 id=\"SparkStreaming\"><a href=\"#SparkStreaming\" class=\"headerlink\" title=\"SparkStreaming\"></a>SparkStreaming</h4><p>是一个对实时数据流进行高通量、容错处理的流式处理系统<br>将流式计算分解成一系列短小的批处理作业</p>\n<h4 id=\"Spark-sql：\"><a href=\"#Spark-sql：\" class=\"headerlink\" title=\"Spark sql：\"></a>Spark sql：</h4><p>能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询</p>\n<h4 id=\"MLBase\"><a href=\"#MLBase\" class=\"headerlink\" title=\"MLBase\"></a>MLBase</h4><p>是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低<br>MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。</p>\n<h4 id=\"GraphX\"><a href=\"#GraphX\" class=\"headerlink\" title=\"GraphX\"></a>GraphX</h4><p>是Spark中用于图和图并行计算</p>\n<h4 id=\"spark有哪些组件\"><a href=\"#spark有哪些组件\" class=\"headerlink\" title=\"spark有哪些组件\"></a>spark有哪些组件</h4><p>master：管理集群和节点，不参与计算。<br>worker：计算节点，进程本身不参与计算，和master汇报。<br>Driver：运行程序的main方法，创建spark context对象。<br>spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。<br>client：用户提交程序的入口。</p>\n<ul>\n<li><a href=\"https://blog.csdn.net/yirenboy/article/details/47441465\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/yirenboy/article/details/47441465</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"准备小结\"><a href=\"#准备小结\" class=\"headerlink\" title=\"准备小结\"></a>准备小结</h1>","more":"<h2 id=\"hdfs存储机制是怎样的\"><a href=\"#hdfs存储机制是怎样的\" class=\"headerlink\" title=\"hdfs存储机制是怎样的?\"></a>hdfs存储机制是怎样的?</h2><p>client端发送写文件请求，namenode检查文件是否存在，如果已存在，直接返回错误信息，否则，发送给client一些可用namenode节点<br>client将文件分块，并行存储到不同节点上datanode上，发送完成后，client同时发送信息给namenode和datanode<br>namenode收到的client信息后，发送确信信息给datanode<br>datanode同时收到namenode和datanode的确认信息后，提交写操作。</p>\n<h2 id=\"hadoop中combiner的作用是什么\"><a href=\"#hadoop中combiner的作用是什么\" class=\"headerlink\" title=\"hadoop中combiner的作用是什么?\"></a>hadoop中combiner的作用是什么?</h2><p>当map生成的数据过大时，带宽就成了瓶颈，怎样精简压缩传给Reduce的数据，又不影响最终的结果呢。有一种方法就是使用Combiner，Combiner号称本地的Reduce，Reduce最终的输入，是Combiner的输出。</p>\n<h2 id=\"你们数据库怎么导入hive-的-有没有出现问题\"><a href=\"#你们数据库怎么导入hive-的-有没有出现问题\" class=\"headerlink\" title=\"你们数据库怎么导入hive 的,有没有出现问题\"></a>你们数据库怎么导入hive 的,有没有出现问题</h2><p>在导入hive的时候，如果数据库中有blob或者text字段，会报错，解决方案在sqoop笔记中。在将数据由Oracle数据库导入到Hive时，发现带有clob字段的表的数据会错乱，出现一些字段全为NULL的空行。由于在项目中CLOB字段没有实际的分析用途，因此考虑将CLOB字段去掉。</p>\n<h2 id=\"hdfs-site-xml的3个主要属性\"><a href=\"#hdfs-site-xml的3个主要属性\" class=\"headerlink\" title=\"hdfs-site.xml的3个主要属性?\"></a>hdfs-site.xml的3个主要属性?</h2><p>dfs.name.dir决定的是元数据存储的路径以及DFS的存储方式(磁盘或是远端)<br>dfs.data.dir决定的是数据存储的路径<br>fs.checkpoint.dir用于第二Namenode</p>\n<h2 id=\"下列哪项通常是集群的最主要瓶颈\"><a href=\"#下列哪项通常是集群的最主要瓶颈\" class=\"headerlink\" title=\"下列哪项通常是集群的最主要瓶颈\"></a>下列哪项通常是集群的最主要瓶颈</h2><p>磁盘 IO<br>答案：C 磁盘<br>首先集群的目的是为了节省成本，用廉价的 pc 机，取代小型机及大型机。小型机和大型机有什么特点？<br>1.cpu 处理能力强<br>2.内存够大，所以集群的瓶颈不可能是 a 和 d<br>3.如果是互联网有瓶颈，可以让集群搭建内网。每次写入数据都要通过网络（集群是内网），然后还要写入 3 份数据，所以 IO 就会打折扣。</p>\n<h2 id=\"关于-SecondaryNameNode-哪项是正确的？\"><a href=\"#关于-SecondaryNameNode-哪项是正确的？\" class=\"headerlink\" title=\"关于 SecondaryNameNode 哪项是正确的？\"></a>关于 SecondaryNameNode 哪项是正确的？</h2><p>它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间 </p>\n<h2 id=\"mapreduce的原理\"><a href=\"#mapreduce的原理\" class=\"headerlink\" title=\"mapreduce的原理?\"></a>mapreduce的原理?</h2><p>MapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果，<br>得到最终结果。简单地说，MapReduce就是”任务的分解与结果的汇总”。<br>在Hadoop中，用于执行MapReduce任务的机器角色有两个：一个是JobTracker；另一个是TaskTracker，JobTracker是用于调度工作的，TaskTracker<br>是用于执行工作的。一个Hadoop集群中只有一台JobTracker。<br>在分布式计算中，MapReduce框架负责处理了并行编程中分布式存储、工作调度、负载均衡、容错均衡、容错处理以及网络通信等复杂问题，把处理<br>过程高度抽象为两个函数：map和reduce，map负责把任务分解成多个任务，reduce负责把分解后多任务处理的结果汇总起来。<br>需要注意的是，用MapReduce来处理的数据集（或任务）必须具备这样的特点：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都<br>可以完全并行地进行处理。</p>\n<h2 id=\"HDFS存储的机制\"><a href=\"#HDFS存储的机制\" class=\"headerlink\" title=\"HDFS存储的机制?\"></a>HDFS存储的机制?</h2><h3 id=\"写流程：\"><a href=\"#写流程：\" class=\"headerlink\" title=\"写流程：\"></a>写流程：</h3><p>client链接namenode存数据<br>namenode记录一条数据位置信息（元数据），告诉client存哪。<br>client用hdfs的api将数据块（默认是64M）存储到datanode上。<br>datanode将数据水平备份。并且备份完将反馈client。<br>client通知namenode存储块完毕。<br>namenode将元数据同步到内存中。<br>另一块循环上面的过程。</p>\n<h3 id=\"读流程\"><a href=\"#读流程\" class=\"headerlink\" title=\"读流程\"></a>读流程</h3><h2 id=\"举一个简单的例子说明mapreduce是怎么来运行的\"><a href=\"#举一个简单的例子说明mapreduce是怎么来运行的\" class=\"headerlink\" title=\"举一个简单的例子说明mapreduce是怎么来运行的 ?\"></a>举一个简单的例子说明mapreduce是怎么来运行的 ?</h2><p>MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。<br>　　Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。<br>Mapper任务的执行过程详解<br>　　每个Mapper任务是一个Java进程，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后，<br>转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下六个阶段：<br>　　第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit)<br>的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值128MB，输入文件有两个，一个是32MB，一个是　172MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由　一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。</p>\n<p>　　第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一　行的起始位置(单位是字节)，“值”是本行的文本内容。<br>　　<br>　　第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会　调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。</p>\n<p>　　第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、　山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer　任务运行的数量。默认只有一个Reducer任务。<br>第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值　对&lt;2,2&gt;、&lt;1,3&gt;、&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是&lt;1,3&gt;、&lt;2,1&gt;、&lt;2,2&gt;。如果有第六阶段，那么进入</p>\n<p>第六阶段　如果没有，直接输出到本地的Linux文件中。　第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。　归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。　Reducer任务的执行过程详解<br>每个Reducer任务是一个java进程。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为三个阶段：<br>第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。<br>第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。<br>第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。<br>最后把这些输出的键值对写入到HDFS文件中。<br>在整个MapReduce程序的开发过程中，我们最大的工作量是覆盖map函数和覆盖reduce函数。</p>\n<h2 id=\"了解hashMap-和hashTable吗介绍下，他们有什么区别。\"><a href=\"#了解hashMap-和hashTable吗介绍下，他们有什么区别。\" class=\"headerlink\" title=\"了解hashMap 和hashTable吗介绍下，他们有什么区别。\"></a>了解hashMap 和hashTable吗介绍下，他们有什么区别。</h2><h2 id=\"为什么重写equals还要重写hashcode\"><a href=\"#为什么重写equals还要重写hashcode\" class=\"headerlink\" title=\"为什么重写equals还要重写hashcode\"></a>为什么重写equals还要重写hashcode</h2><p>因为equals比较的是内容是一致.但hashcode</p>\n<h2 id=\"说一下map的分类和常见的情况\"><a href=\"#说一下map的分类和常见的情况\" class=\"headerlink\" title=\"说一下map的分类和常见的情况\"></a>说一下map的分类和常见的情况</h2><p> hashmap,hashtable,treemap,LinkedHashMap</p>\n<ul>\n<li>根据键得到值，因此不允许键重复(重复了覆盖了),但允许值重复<h3 id=\"Hashmap\"><a href=\"#Hashmap\" class=\"headerlink\" title=\"Hashmap\"></a>Hashmap</h3>是一个最常用的Map</li>\n<li>它根据键的HashCode值存储数据,根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的</li>\n<li>最多只允许一条记录的键为Null;允许多条记录的值为 Null;</li>\n<li>HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。</li>\n<li>如果需要同步，可以用 Collections的synchronizedMap方法使HashMap具有同步的能力，或者使用ConcurrentHashMap<h3 id=\"Hashtable\"><a href=\"#Hashtable\" class=\"headerlink\" title=\"Hashtable\"></a>Hashtable</h3>Hashtable与 HashMap类似,它继承自Dictionary类,不同的是:它不允许记录的键或者值为空;</li>\n<li>它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了 Hashtable在写入时会比较慢<h3 id=\"LinkedHashMap\"><a href=\"#LinkedHashMap\" class=\"headerlink\" title=\"LinkedHashMap\"></a>LinkedHashMap</h3>是 HashMap 的一个子类，保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的.<br>也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比 HashMap 慢，不过有种情况例外，当 HashMap 容量很大，实际数据较少时，遍历起来可能会比 LinkedHashMap 慢，因为 LinkedHashMap 的遍历速度只和实际数据有关，和容量无关，而 HashMap 的遍历速度和他的容量有关<h3 id=\"TreeMap\"><a href=\"#TreeMap\" class=\"headerlink\" title=\"TreeMap\"></a>TreeMap</h3>实现 SortMap 接口,能够把它保存的记录根据键排序, 默认是按键值的升序排序，也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的</li>\n</ul>\n<p>HashMap，链表法存储，entry[]数组，线程不安全，可能死锁 concurrentHashMap，segment数组，每个segent下维护一组entry[]数组，每个segment是一把锁，线程安全 LinkedHashMap</p>\n<hr>\n<h2 id=\"Object若不重写hashCode-的话，hashCode-如何计算出来的？\"><a href=\"#Object若不重写hashCode-的话，hashCode-如何计算出来的？\" class=\"headerlink\" title=\"Object若不重写hashCode()的话，hashCode()如何计算出来的？\"></a>Object若不重写hashCode()的话，hashCode()如何计算出来的？</h2><p>hashcode采用的是</p>\n<h2 id=\"spark\"><a href=\"#spark\" class=\"headerlink\" title=\"spark\"></a>spark</h2><h3 id=\"1-spark的有几种部署模式，每种模式特点？\"><a href=\"#1-spark的有几种部署模式，每种模式特点？\" class=\"headerlink\" title=\"1. spark的有几种部署模式，每种模式特点？\"></a>1. spark的有几种部署模式，每种模式特点？</h3><h4 id=\"本地模式\"><a href=\"#本地模式\" class=\"headerlink\" title=\"本地模式\"></a>本地模式</h4><p>本地模式分三类</p>\n<ul>\n<li>local：只启动一个executor</li>\n<li>local[k]: 启动k个executor</li>\n<li>local[*]：启动跟cpu数目相同的 executor</li>\n</ul>\n<h3 id=\"cluster模式\"><a href=\"#cluster模式\" class=\"headerlink\" title=\"cluster模式\"></a>cluster模式</h3><p>cluster模式肯定就是运行很多机器上了，但是它又分为以下三种模式，区别在于谁去管理资源调度。（说白了，就好像后勤管家，哪里需要资源，后勤管家要负责调度这些资源）</p>\n<h4 id=\"standalone模式\"><a href=\"#standalone模式\" class=\"headerlink\" title=\"standalone模式\"></a>standalone模式</h4><p>分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础</p>\n<h4 id=\"Spark-on-yarn模式\"><a href=\"#Spark-on-yarn模式\" class=\"headerlink\" title=\"Spark on yarn模式\"></a>Spark on yarn模式</h4><p>分布式部署集群，资源和任务监控交给yarn管理<br>粗粒度资源分配方式，包含cluster和client运行模式<br>cluster 适合生产，driver运行在集群子节点，具有容错功能<br>client 适合调试，dirver运行在客户端</p>\n<h3 id=\"2-Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？\"><a href=\"#2-Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？\" class=\"headerlink\" title=\"2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？\"></a>2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</h3><h4 id=\"Spark-core\"><a href=\"#Spark-core\" class=\"headerlink\" title=\"Spark core\"></a>Spark core</h4><p>是其它组件的基础，spark的内核<br>主要包含：有向循环图、RDD、Lingage、Cache、broadcast等</p>\n<h4 id=\"SparkStreaming\"><a href=\"#SparkStreaming\" class=\"headerlink\" title=\"SparkStreaming\"></a>SparkStreaming</h4><p>是一个对实时数据流进行高通量、容错处理的流式处理系统<br>将流式计算分解成一系列短小的批处理作业</p>\n<h4 id=\"Spark-sql：\"><a href=\"#Spark-sql：\" class=\"headerlink\" title=\"Spark sql：\"></a>Spark sql：</h4><p>能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询</p>\n<h4 id=\"MLBase\"><a href=\"#MLBase\" class=\"headerlink\" title=\"MLBase\"></a>MLBase</h4><p>是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低<br>MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。</p>\n<h4 id=\"GraphX\"><a href=\"#GraphX\" class=\"headerlink\" title=\"GraphX\"></a>GraphX</h4><p>是Spark中用于图和图并行计算</p>\n<h4 id=\"spark有哪些组件\"><a href=\"#spark有哪些组件\" class=\"headerlink\" title=\"spark有哪些组件\"></a>spark有哪些组件</h4><p>master：管理集群和节点，不参与计算。<br>worker：计算节点，进程本身不参与计算，和master汇报。<br>Driver：运行程序的main方法，创建spark context对象。<br>spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。<br>client：用户提交程序的入口。</p>\n<ul>\n<li><a href=\"https://blog.csdn.net/yirenboy/article/details/47441465\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/yirenboy/article/details/47441465</a></li>\n</ul>"},{"title":"异常处理机制小结","date":"2019-07-15T16:00:00.000Z","_content":"\n此处简介\n\n<!--more-->\n\n### 异常处理机制小结\n![b89a2f8ca9684364889768d339985ef6-image.png](//img.wqkenqing.ren/file/2017/7/b89a2f8ca9684364889768d339985ef6-image.png)\n\n\n 在 Java 中，所有的异常都有一个共同的祖先 Throwable（可抛出）。Throwable 指定代码中可用异常传播机制通过 Java 应用程序传输的任何问题的共性。\n       Throwable： 有两个重要的子类：Exception（异常）和 Error（错误），二者都是 Java 异常处理的重要子类，各自都包含大量子类。\n        Error（错误）:是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。\n           Exception（异常）:是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 类及其子类表示“JVM 常用操作”引发的错误。例如，若试图使用空值对象引用、除数为零或数组越界，则分别引发运行时异常（NullPointerException、 ArithmeticException）和 ArrayIndexOutOfBoundException。\n\n`注意：异常和错误的区别：异常能被程序本身可以处理，错误是无法处理。`\n\n通常，Java的异常(包括Exception和Error)分为可查的异常（checked exceptions）和不可查的异常（unchecked exceptions）。\n      可查异常（编译器要求必须处置的异常）：正确的程序在运行中，很容易出现的、情理可容的异常状况。可查异常虽然是异常状况，但在一定程度上它的发生是可以预计的，而且一旦发生这种异常状况，就必须采取某种方式进行处理。","source":"_posts/技术/hexo/oldblog/blog10.md","raw":"---\n\ntitle: 异常处理机制小结\ndate: 2019-07-16\ntags:\n\n---\n\n此处简介\n\n<!--more-->\n\n### 异常处理机制小结\n![b89a2f8ca9684364889768d339985ef6-image.png](//img.wqkenqing.ren/file/2017/7/b89a2f8ca9684364889768d339985ef6-image.png)\n\n\n 在 Java 中，所有的异常都有一个共同的祖先 Throwable（可抛出）。Throwable 指定代码中可用异常传播机制通过 Java 应用程序传输的任何问题的共性。\n       Throwable： 有两个重要的子类：Exception（异常）和 Error（错误），二者都是 Java 异常处理的重要子类，各自都包含大量子类。\n        Error（错误）:是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。\n           Exception（异常）:是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 类及其子类表示“JVM 常用操作”引发的错误。例如，若试图使用空值对象引用、除数为零或数组越界，则分别引发运行时异常（NullPointerException、 ArithmeticException）和 ArrayIndexOutOfBoundException。\n\n`注意：异常和错误的区别：异常能被程序本身可以处理，错误是无法处理。`\n\n通常，Java的异常(包括Exception和Error)分为可查的异常（checked exceptions）和不可查的异常（unchecked exceptions）。\n      可查异常（编译器要求必须处置的异常）：正确的程序在运行中，很容易出现的、情理可容的异常状况。可查异常虽然是异常状况，但在一定程度上它的发生是可以预计的，而且一旦发生这种异常状况，就必须采取某种方式进行处理。","slug":"技术/hexo/oldblog/blog10","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd336002g38pwgpnw6mws","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h3 id=\"异常处理机制小结\"><a href=\"#异常处理机制小结\" class=\"headerlink\" title=\"异常处理机制小结\"></a>异常处理机制小结</h3><p><img src=\"//img.wqkenqing.ren/file/2017/7/b89a2f8ca9684364889768d339985ef6-image.png\" alt=\"b89a2f8ca9684364889768d339985ef6-image.png\"></p>\n<p> 在 Java 中，所有的异常都有一个共同的祖先 Throwable（可抛出）。Throwable 指定代码中可用异常传播机制通过 Java 应用程序传输的任何问题的共性。<br>       Throwable： 有两个重要的子类：Exception（异常）和 Error（错误），二者都是 Java 异常处理的重要子类，各自都包含大量子类。<br>        Error（错误）:是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。<br>           Exception（异常）:是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 类及其子类表示“JVM 常用操作”引发的错误。例如，若试图使用空值对象引用、除数为零或数组越界，则分别引发运行时异常（NullPointerException、 ArithmeticException）和 ArrayIndexOutOfBoundException。</p>\n<p><code>注意：异常和错误的区别：异常能被程序本身可以处理，错误是无法处理。</code></p>\n<p>通常，Java的异常(包括Exception和Error)分为可查的异常（checked exceptions）和不可查的异常（unchecked exceptions）。<br>      可查异常（编译器要求必须处置的异常）：正确的程序在运行中，很容易出现的、情理可容的异常状况。可查异常虽然是异常状况，但在一定程度上它的发生是可以预计的，而且一旦发生这种异常状况，就必须采取某种方式进行处理。</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h3 id=\"异常处理机制小结\"><a href=\"#异常处理机制小结\" class=\"headerlink\" title=\"异常处理机制小结\"></a>异常处理机制小结</h3><p><img src=\"//img.wqkenqing.ren/file/2017/7/b89a2f8ca9684364889768d339985ef6-image.png\" alt=\"b89a2f8ca9684364889768d339985ef6-image.png\"></p>\n<p> 在 Java 中，所有的异常都有一个共同的祖先 Throwable（可抛出）。Throwable 指定代码中可用异常传播机制通过 Java 应用程序传输的任何问题的共性。<br>       Throwable： 有两个重要的子类：Exception（异常）和 Error（错误），二者都是 Java 异常处理的重要子类，各自都包含大量子类。<br>        Error（错误）:是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。<br>           Exception（异常）:是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 类及其子类表示“JVM 常用操作”引发的错误。例如，若试图使用空值对象引用、除数为零或数组越界，则分别引发运行时异常（NullPointerException、 ArithmeticException）和 ArrayIndexOutOfBoundException。</p>\n<p><code>注意：异常和错误的区别：异常能被程序本身可以处理，错误是无法处理。</code></p>\n<p>通常，Java的异常(包括Exception和Error)分为可查的异常（checked exceptions）和不可查的异常（unchecked exceptions）。<br>      可查异常（编译器要求必须处置的异常）：正确的程序在运行中，很容易出现的、情理可容的异常状况。可查异常虽然是异常状况，但在一定程度上它的发生是可以预计的，而且一旦发生这种异常状况，就必须采取某种方式进行处理。</p>"},{"title":"java---GC机制","date":"2019-07-15T16:00:00.000Z","_content":"\n此处简介\n\n<!--more-->\n\n### java---GC机制\nJVM会有一个运行时数据区来管理内存\n+ 程序计数器(Program Counter Register)\n+ 虚拟机栈(VM Stack)\n+ 本地方法栈(Native Method Stack)\n+ 方法区(Method Area)\n+ 堆(Heap)\n\n#### What? -- 哪些内存需要回收？\n而其中程序计数器、虚拟机栈、本地方法栈是每个线程私有的内存空间，随线程而生，随线程而亡。\n例如栈中每一个栈帧中分配多少内存基本上在类结构去诶是哪个下来时就已知了，因此这3个区域的内存分配和回收都是确定的，无需考虑内存回收的问题。\n\n但方法区和堆就不同了，一个接口的多个实现类需要的内存可能不一样，我们只有在程序运行期间才会知道会创建哪些对象，这部分内存的分配和回收都是动态的，GC主要关注的是这部分内存。\n\n---\n总而言之，GC主要进行回收的内存是JVM中的方法区和堆；\n涉及到多线程(指堆)、多个对该对象不同类型的引用(指方法区)，才会涉及GC的回收。\n\n---\n#### When? -- 什么时候回收？\n堆\n```\n在面试中经常会碰到这样一个问题（事实上笔者也碰到过）：如何判断一个对象已经死去？\n\n很容易想到的一个答案是：对一个对象添加引用计数器。每当有地方引用它时，计数器值加1；当引用失效时，计数器值减1.而当计数器的值为0时这个对象就不会再被使用，判断为已死。是不是简单又直观。然而，很遗憾。这种做法是错误的！（面试时可千万别这样回答哦，我就是不假思索这样回答，然后就。。）为什么是错的呢？事实上，用引用计数法确实在大部分情况下是一个不错的解决方案，而在实际的应用中也有不少案例，但它却无法解决对象之间的循环引用问题。比如对象A中有一个字段指向了对象B，而对象B中也有一个字段指向了对象A，而事实上他们俩都不再使用，但计数器的值永远都不可能为0，也就不会被回收，然后就发生了内存泄露。。\n```\n\n在Java，C#等语言中，比较主流的判定一个对象已死的方法是：<Strong>可达性分析(Reachability Analysis).</Strong>\n所有生成的对象都是一个称为\"GC Roots\"的根的子树。从GC Roots开始向下搜索，搜索所经过的路径称为引用链(Reference Chain)，当一个对象到GC Roots没有任何引用链可以到达时，就称这个对象是不可达的（不可引用的），也就是可以被GC回收了\n![Alt text](./1483447208180.png)\n\n```\n无论是引用计数器还是可达性分析，判定对象是否存活都与引用有关！那么，如何定义对象的引用呢？\n\n```\n\n+ 强引用(Strong Reference):Object obj = new Object();只要强引用还存在，GC永远不会回收掉被引用的对象。\n+ 软引用(Soft Reference)：描述一些还有用但非必需的对象。在系统将会发生内存溢出之前，会把这些对象列入回收范围进行二次回收（即系统将会发生内存溢出了，才会对他们进行回收。）\n+ 弱引用(Weak Reference):程度比软引用还要弱一些。这些对象只能生存到下次GC之前。当GC工作时，无论内存是否足够都会将其回收（即只要进行GC，就会对他们进行回收。）\n+ 虚引用(Phantom Reference):一个对象是否存在虚引用，完全不会对其生存时间构成影响。\n\n<Strong>方法区</strong>\nWhat部分我们已经提到，GC主要回收的是堆和方法区中的内存，而上面的How主要是针对对象的回收，他们一般位于堆内。那么，方法区中的东西该怎么回收呢？\n\n关于方法区中需要回收的是一些废弃的常量和无用的类\n+ 废弃的常量的回收。这里看引用计数就可以了。没有对象引用该常量就可以放心的回收了。\n+ 无用的类的回收。什么是无用的类呢？\n\n* 该类所有的实例都已经被回收。也就是Java堆中不存在该类的任何实例；\n* 加载该类的ClassLoader已经被回收；\n* 该类对应的java.lang.Class对象没有任何地方被引用，无法在任何地方通过反射访问该类的方法。\n\n```\n总而言之，对于堆中的对象，主要用可达性分析判断一个对象是否还存在引用，如果该对象没有任何引用就应该被回收。而根据我们实际对引用的不同需求，又分成了4中引用，每种引用的回收机制也是不同的。\n对于方法区中的常量和类，当一个常量没有任何对象引用它，它就可以被回收了。而对于类，如果可以判定它为无用类，就可以被回收了。\n```\n#### How? -- 如何回收？\n##### 标记-清除(Mark-Sweep)算法\n分为两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。\n缺点：效率问题，标记和清除两个过程的效率都不高；空间问题，会产生很多碎片。\n##### 复制算法\n\n将可用内存按容量划分为大小相等的两块，每次只用其中一块。当这一块用完了，就将还存活的对象复制到另外一块上面，然后把原始空间全部回收。高效、简单。\n缺点：将内存缩小为原来的一半。\n\n##### 标记-整理(Mark-Compat)算法\n标记过程与标记-清除算法过程一样，但后面不是简单的清除，而是让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存。\n\n##### 分代收集(Generational Collection)算法\n* 新生代中，每次垃圾收集时都有大批对象死去，只有少量存活，就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集；\n* 老年代中，其存活率较高、没有额外空间对它进行分配担保，就应该使用“标记-整理”或“标记-清理”算法进行回收。\n####一些收集器\n##### Serial收集器\n单线程收集器，表示在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。\"Stop The World\".\n##### ParNew收集器\n实际就是Serial收集器的多线程版本。\n* 并发(Parallel):指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态；\n* 并行(Concurrent):指用户线程与垃圾收集线程同时执行，用户程序在继续运行，而垃圾收集程序运行于另一个CPU上。\n##### Parallel Scavenge收集器\n该收集器比较关注吞吐量(Throughout)(CPU用于用户代码的时间与CPU总消耗时间的比值)，保证吞吐量在一个可控的范围内。\n##### CMS(Concurrent Mark Sweep)收集器\nCMS收集器是一种以获得最短停顿时间为目标的收集器。\n##### G1(Garbage First)收集器\n从JDK1.7 Update 14之后的HotSpot虚拟机正式提供了商用的G1收集器，与其他收集器相比，它具有如下优点：并行与并发；分代收集；空间整合；可预测的停顿等。\n\n本部分主要分析了三种不同的垃圾回收算法：Mark-Sweep, Copy, Mark-Compact. 每种算法都有不同的优缺点，也有不同的适用范围。而JVM中对垃圾回收器并没有严格的要求，不同的收集器会结合多个算法进行垃圾回收。\n#####内存分配\nJava技术体系中所提倡的自动内存管理最终可以归结为自动化的解决2个问题：给对象分配内存以及回收分配给对象的内存。\n#####对象优先在Eden分配\n大多数情况下，对象在新生代Eden区分配。当Eden区没有足够的内存时，虚拟机将发起一次Minor GC。\n* Minor GC(新生代GC):指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC发生的非常频繁。\n* Full GC/Major GC(老年代GC):指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC。\n##### 大对象直接进老年代\n大对象是指需要大量连续内存空间的Java对象（例如很长的字符串以及数组）。\n##### 长期存活的对象将进入老年代\nJVM为每个对象定义一个对象年龄计数器。\n+ 如果对象在Eden出生并经历过第一次Minor GC后仍然存活，并且能够被Survivor容纳，则应该被移动到Survivor空间中，并且年龄对象设置为1；\n+ 对象在Survivor区中每熬过一次Minor GC，年龄就会增加1岁，当它的年龄增加到一定程度(默认为15岁，可通过参数-XX:MaxTenuringThreshold设置)，就会被晋升到老年代中。\n+ 要注意的是：JVM并不是永远的要求对象的年龄必须达到MaxTenuringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一般，年龄大于等于该年龄的对象就可以直接进入老年代，无需等到MaxTenuringThreshold中要求的年龄。\n##### 空间分配担保\n+ 在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，则进行Minor GC是安全的；\n+ 如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，则急促检查老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管它是有风险的；\n+ 如果小于或者HandePromotionFailure设置为不允许冒险，则这时要改为进行一次Full GC.","source":"_posts/技术/hexo/oldblog/blog11.md","raw":"---\n\ntitle: java---GC机制\ndate: 2019-07-16\ntags:\n\n---\n\n此处简介\n\n<!--more-->\n\n### java---GC机制\nJVM会有一个运行时数据区来管理内存\n+ 程序计数器(Program Counter Register)\n+ 虚拟机栈(VM Stack)\n+ 本地方法栈(Native Method Stack)\n+ 方法区(Method Area)\n+ 堆(Heap)\n\n#### What? -- 哪些内存需要回收？\n而其中程序计数器、虚拟机栈、本地方法栈是每个线程私有的内存空间，随线程而生，随线程而亡。\n例如栈中每一个栈帧中分配多少内存基本上在类结构去诶是哪个下来时就已知了，因此这3个区域的内存分配和回收都是确定的，无需考虑内存回收的问题。\n\n但方法区和堆就不同了，一个接口的多个实现类需要的内存可能不一样，我们只有在程序运行期间才会知道会创建哪些对象，这部分内存的分配和回收都是动态的，GC主要关注的是这部分内存。\n\n---\n总而言之，GC主要进行回收的内存是JVM中的方法区和堆；\n涉及到多线程(指堆)、多个对该对象不同类型的引用(指方法区)，才会涉及GC的回收。\n\n---\n#### When? -- 什么时候回收？\n堆\n```\n在面试中经常会碰到这样一个问题（事实上笔者也碰到过）：如何判断一个对象已经死去？\n\n很容易想到的一个答案是：对一个对象添加引用计数器。每当有地方引用它时，计数器值加1；当引用失效时，计数器值减1.而当计数器的值为0时这个对象就不会再被使用，判断为已死。是不是简单又直观。然而，很遗憾。这种做法是错误的！（面试时可千万别这样回答哦，我就是不假思索这样回答，然后就。。）为什么是错的呢？事实上，用引用计数法确实在大部分情况下是一个不错的解决方案，而在实际的应用中也有不少案例，但它却无法解决对象之间的循环引用问题。比如对象A中有一个字段指向了对象B，而对象B中也有一个字段指向了对象A，而事实上他们俩都不再使用，但计数器的值永远都不可能为0，也就不会被回收，然后就发生了内存泄露。。\n```\n\n在Java，C#等语言中，比较主流的判定一个对象已死的方法是：<Strong>可达性分析(Reachability Analysis).</Strong>\n所有生成的对象都是一个称为\"GC Roots\"的根的子树。从GC Roots开始向下搜索，搜索所经过的路径称为引用链(Reference Chain)，当一个对象到GC Roots没有任何引用链可以到达时，就称这个对象是不可达的（不可引用的），也就是可以被GC回收了\n![Alt text](./1483447208180.png)\n\n```\n无论是引用计数器还是可达性分析，判定对象是否存活都与引用有关！那么，如何定义对象的引用呢？\n\n```\n\n+ 强引用(Strong Reference):Object obj = new Object();只要强引用还存在，GC永远不会回收掉被引用的对象。\n+ 软引用(Soft Reference)：描述一些还有用但非必需的对象。在系统将会发生内存溢出之前，会把这些对象列入回收范围进行二次回收（即系统将会发生内存溢出了，才会对他们进行回收。）\n+ 弱引用(Weak Reference):程度比软引用还要弱一些。这些对象只能生存到下次GC之前。当GC工作时，无论内存是否足够都会将其回收（即只要进行GC，就会对他们进行回收。）\n+ 虚引用(Phantom Reference):一个对象是否存在虚引用，完全不会对其生存时间构成影响。\n\n<Strong>方法区</strong>\nWhat部分我们已经提到，GC主要回收的是堆和方法区中的内存，而上面的How主要是针对对象的回收，他们一般位于堆内。那么，方法区中的东西该怎么回收呢？\n\n关于方法区中需要回收的是一些废弃的常量和无用的类\n+ 废弃的常量的回收。这里看引用计数就可以了。没有对象引用该常量就可以放心的回收了。\n+ 无用的类的回收。什么是无用的类呢？\n\n* 该类所有的实例都已经被回收。也就是Java堆中不存在该类的任何实例；\n* 加载该类的ClassLoader已经被回收；\n* 该类对应的java.lang.Class对象没有任何地方被引用，无法在任何地方通过反射访问该类的方法。\n\n```\n总而言之，对于堆中的对象，主要用可达性分析判断一个对象是否还存在引用，如果该对象没有任何引用就应该被回收。而根据我们实际对引用的不同需求，又分成了4中引用，每种引用的回收机制也是不同的。\n对于方法区中的常量和类，当一个常量没有任何对象引用它，它就可以被回收了。而对于类，如果可以判定它为无用类，就可以被回收了。\n```\n#### How? -- 如何回收？\n##### 标记-清除(Mark-Sweep)算法\n分为两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。\n缺点：效率问题，标记和清除两个过程的效率都不高；空间问题，会产生很多碎片。\n##### 复制算法\n\n将可用内存按容量划分为大小相等的两块，每次只用其中一块。当这一块用完了，就将还存活的对象复制到另外一块上面，然后把原始空间全部回收。高效、简单。\n缺点：将内存缩小为原来的一半。\n\n##### 标记-整理(Mark-Compat)算法\n标记过程与标记-清除算法过程一样，但后面不是简单的清除，而是让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存。\n\n##### 分代收集(Generational Collection)算法\n* 新生代中，每次垃圾收集时都有大批对象死去，只有少量存活，就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集；\n* 老年代中，其存活率较高、没有额外空间对它进行分配担保，就应该使用“标记-整理”或“标记-清理”算法进行回收。\n####一些收集器\n##### Serial收集器\n单线程收集器，表示在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。\"Stop The World\".\n##### ParNew收集器\n实际就是Serial收集器的多线程版本。\n* 并发(Parallel):指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态；\n* 并行(Concurrent):指用户线程与垃圾收集线程同时执行，用户程序在继续运行，而垃圾收集程序运行于另一个CPU上。\n##### Parallel Scavenge收集器\n该收集器比较关注吞吐量(Throughout)(CPU用于用户代码的时间与CPU总消耗时间的比值)，保证吞吐量在一个可控的范围内。\n##### CMS(Concurrent Mark Sweep)收集器\nCMS收集器是一种以获得最短停顿时间为目标的收集器。\n##### G1(Garbage First)收集器\n从JDK1.7 Update 14之后的HotSpot虚拟机正式提供了商用的G1收集器，与其他收集器相比，它具有如下优点：并行与并发；分代收集；空间整合；可预测的停顿等。\n\n本部分主要分析了三种不同的垃圾回收算法：Mark-Sweep, Copy, Mark-Compact. 每种算法都有不同的优缺点，也有不同的适用范围。而JVM中对垃圾回收器并没有严格的要求，不同的收集器会结合多个算法进行垃圾回收。\n#####内存分配\nJava技术体系中所提倡的自动内存管理最终可以归结为自动化的解决2个问题：给对象分配内存以及回收分配给对象的内存。\n#####对象优先在Eden分配\n大多数情况下，对象在新生代Eden区分配。当Eden区没有足够的内存时，虚拟机将发起一次Minor GC。\n* Minor GC(新生代GC):指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC发生的非常频繁。\n* Full GC/Major GC(老年代GC):指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC。\n##### 大对象直接进老年代\n大对象是指需要大量连续内存空间的Java对象（例如很长的字符串以及数组）。\n##### 长期存活的对象将进入老年代\nJVM为每个对象定义一个对象年龄计数器。\n+ 如果对象在Eden出生并经历过第一次Minor GC后仍然存活，并且能够被Survivor容纳，则应该被移动到Survivor空间中，并且年龄对象设置为1；\n+ 对象在Survivor区中每熬过一次Minor GC，年龄就会增加1岁，当它的年龄增加到一定程度(默认为15岁，可通过参数-XX:MaxTenuringThreshold设置)，就会被晋升到老年代中。\n+ 要注意的是：JVM并不是永远的要求对象的年龄必须达到MaxTenuringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一般，年龄大于等于该年龄的对象就可以直接进入老年代，无需等到MaxTenuringThreshold中要求的年龄。\n##### 空间分配担保\n+ 在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，则进行Minor GC是安全的；\n+ 如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，则急促检查老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管它是有风险的；\n+ 如果小于或者HandePromotionFailure设置为不允许冒险，则这时要改为进行一次Full GC.","slug":"技术/hexo/oldblog/blog11","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd337002i38pwf543aput","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h3 id=\"java—GC机制\"><a href=\"#java—GC机制\" class=\"headerlink\" title=\"java—GC机制\"></a>java—GC机制</h3><p>JVM会有一个运行时数据区来管理内存</p>\n<ul>\n<li>程序计数器(Program Counter Register)</li>\n<li>虚拟机栈(VM Stack)</li>\n<li>本地方法栈(Native Method Stack)</li>\n<li>方法区(Method Area)</li>\n<li>堆(Heap)</li>\n</ul>\n<h4 id=\"What-–-哪些内存需要回收？\"><a href=\"#What-–-哪些内存需要回收？\" class=\"headerlink\" title=\"What? – 哪些内存需要回收？\"></a>What? – 哪些内存需要回收？</h4><p>而其中程序计数器、虚拟机栈、本地方法栈是每个线程私有的内存空间，随线程而生，随线程而亡。<br>例如栈中每一个栈帧中分配多少内存基本上在类结构去诶是哪个下来时就已知了，因此这3个区域的内存分配和回收都是确定的，无需考虑内存回收的问题。</p>\n<p>但方法区和堆就不同了，一个接口的多个实现类需要的内存可能不一样，我们只有在程序运行期间才会知道会创建哪些对象，这部分内存的分配和回收都是动态的，GC主要关注的是这部分内存。</p>\n<hr>\n<p>总而言之，GC主要进行回收的内存是JVM中的方法区和堆；<br>涉及到多线程(指堆)、多个对该对象不同类型的引用(指方法区)，才会涉及GC的回收。</p>\n<hr>\n<h4 id=\"When-–-什么时候回收？\"><a href=\"#When-–-什么时候回收？\" class=\"headerlink\" title=\"When? – 什么时候回收？\"></a>When? – 什么时候回收？</h4><p>堆</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在面试中经常会碰到这样一个问题（事实上笔者也碰到过）：如何判断一个对象已经死去？</span><br><span class=\"line\"></span><br><span class=\"line\">很容易想到的一个答案是：对一个对象添加引用计数器。每当有地方引用它时，计数器值加1；当引用失效时，计数器值减1.而当计数器的值为0时这个对象就不会再被使用，判断为已死。是不是简单又直观。然而，很遗憾。这种做法是错误的！（面试时可千万别这样回答哦，我就是不假思索这样回答，然后就。。）为什么是错的呢？事实上，用引用计数法确实在大部分情况下是一个不错的解决方案，而在实际的应用中也有不少案例，但它却无法解决对象之间的循环引用问题。比如对象A中有一个字段指向了对象B，而对象B中也有一个字段指向了对象A，而事实上他们俩都不再使用，但计数器的值永远都不可能为0，也就不会被回收，然后就发生了内存泄露。。</span><br></pre></td></tr></table></figure>\n\n<p>在Java，C#等语言中，比较主流的判定一个对象已死的方法是：<Strong>可达性分析(Reachability Analysis).</Strong><br>所有生成的对象都是一个称为”GC Roots”的根的子树。从GC Roots开始向下搜索，搜索所经过的路径称为引用链(Reference Chain)，当一个对象到GC Roots没有任何引用链可以到达时，就称这个对象是不可达的（不可引用的），也就是可以被GC回收了<br><img src=\"./1483447208180.png\" alt=\"Alt text\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">无论是引用计数器还是可达性分析，判定对象是否存活都与引用有关！那么，如何定义对象的引用呢？</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>强引用(Strong Reference):Object obj = new Object();只要强引用还存在，GC永远不会回收掉被引用的对象。</li>\n<li>软引用(Soft Reference)：描述一些还有用但非必需的对象。在系统将会发生内存溢出之前，会把这些对象列入回收范围进行二次回收（即系统将会发生内存溢出了，才会对他们进行回收。）</li>\n<li>弱引用(Weak Reference):程度比软引用还要弱一些。这些对象只能生存到下次GC之前。当GC工作时，无论内存是否足够都会将其回收（即只要进行GC，就会对他们进行回收。）</li>\n<li>虚引用(Phantom Reference):一个对象是否存在虚引用，完全不会对其生存时间构成影响。</li>\n</ul>\n<p><Strong>方法区</strong><br>What部分我们已经提到，GC主要回收的是堆和方法区中的内存，而上面的How主要是针对对象的回收，他们一般位于堆内。那么，方法区中的东西该怎么回收呢？</p>\n<p>关于方法区中需要回收的是一些废弃的常量和无用的类</p>\n<ul>\n<li>废弃的常量的回收。这里看引用计数就可以了。没有对象引用该常量就可以放心的回收了。</li>\n<li>无用的类的回收。什么是无用的类呢？</li>\n</ul>\n<ul>\n<li>该类所有的实例都已经被回收。也就是Java堆中不存在该类的任何实例；</li>\n<li>加载该类的ClassLoader已经被回收；</li>\n<li>该类对应的java.lang.Class对象没有任何地方被引用，无法在任何地方通过反射访问该类的方法。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">总而言之，对于堆中的对象，主要用可达性分析判断一个对象是否还存在引用，如果该对象没有任何引用就应该被回收。而根据我们实际对引用的不同需求，又分成了4中引用，每种引用的回收机制也是不同的。</span><br><span class=\"line\">对于方法区中的常量和类，当一个常量没有任何对象引用它，它就可以被回收了。而对于类，如果可以判定它为无用类，就可以被回收了。</span><br></pre></td></tr></table></figure>\n<h4 id=\"How-–-如何回收？\"><a href=\"#How-–-如何回收？\" class=\"headerlink\" title=\"How? – 如何回收？\"></a>How? – 如何回收？</h4><h5 id=\"标记-清除-Mark-Sweep-算法\"><a href=\"#标记-清除-Mark-Sweep-算法\" class=\"headerlink\" title=\"标记-清除(Mark-Sweep)算法\"></a>标记-清除(Mark-Sweep)算法</h5><p>分为两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。<br>缺点：效率问题，标记和清除两个过程的效率都不高；空间问题，会产生很多碎片。</p>\n<h5 id=\"复制算法\"><a href=\"#复制算法\" class=\"headerlink\" title=\"复制算法\"></a>复制算法</h5><p>将可用内存按容量划分为大小相等的两块，每次只用其中一块。当这一块用完了，就将还存活的对象复制到另外一块上面，然后把原始空间全部回收。高效、简单。<br>缺点：将内存缩小为原来的一半。</p>\n<h5 id=\"标记-整理-Mark-Compat-算法\"><a href=\"#标记-整理-Mark-Compat-算法\" class=\"headerlink\" title=\"标记-整理(Mark-Compat)算法\"></a>标记-整理(Mark-Compat)算法</h5><p>标记过程与标记-清除算法过程一样，但后面不是简单的清除，而是让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存。</p>\n<h5 id=\"分代收集-Generational-Collection-算法\"><a href=\"#分代收集-Generational-Collection-算法\" class=\"headerlink\" title=\"分代收集(Generational Collection)算法\"></a>分代收集(Generational Collection)算法</h5><ul>\n<li>新生代中，每次垃圾收集时都有大批对象死去，只有少量存活，就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集；</li>\n<li>老年代中，其存活率较高、没有额外空间对它进行分配担保，就应该使用“标记-整理”或“标记-清理”算法进行回收。<br>####一些收集器<h5 id=\"Serial收集器\"><a href=\"#Serial收集器\" class=\"headerlink\" title=\"Serial收集器\"></a>Serial收集器</h5>单线程收集器，表示在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。”Stop The World”.<h5 id=\"ParNew收集器\"><a href=\"#ParNew收集器\" class=\"headerlink\" title=\"ParNew收集器\"></a>ParNew收集器</h5>实际就是Serial收集器的多线程版本。</li>\n<li>并发(Parallel):指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态；</li>\n<li>并行(Concurrent):指用户线程与垃圾收集线程同时执行，用户程序在继续运行，而垃圾收集程序运行于另一个CPU上。<h5 id=\"Parallel-Scavenge收集器\"><a href=\"#Parallel-Scavenge收集器\" class=\"headerlink\" title=\"Parallel Scavenge收集器\"></a>Parallel Scavenge收集器</h5>该收集器比较关注吞吐量(Throughout)(CPU用于用户代码的时间与CPU总消耗时间的比值)，保证吞吐量在一个可控的范围内。<h5 id=\"CMS-Concurrent-Mark-Sweep-收集器\"><a href=\"#CMS-Concurrent-Mark-Sweep-收集器\" class=\"headerlink\" title=\"CMS(Concurrent Mark Sweep)收集器\"></a>CMS(Concurrent Mark Sweep)收集器</h5>CMS收集器是一种以获得最短停顿时间为目标的收集器。<h5 id=\"G1-Garbage-First-收集器\"><a href=\"#G1-Garbage-First-收集器\" class=\"headerlink\" title=\"G1(Garbage First)收集器\"></a>G1(Garbage First)收集器</h5>从JDK1.7 Update 14之后的HotSpot虚拟机正式提供了商用的G1收集器，与其他收集器相比，它具有如下优点：并行与并发；分代收集；空间整合；可预测的停顿等。</li>\n</ul>\n<p>本部分主要分析了三种不同的垃圾回收算法：Mark-Sweep, Copy, Mark-Compact. 每种算法都有不同的优缺点，也有不同的适用范围。而JVM中对垃圾回收器并没有严格的要求，不同的收集器会结合多个算法进行垃圾回收。<br>#####内存分配<br>Java技术体系中所提倡的自动内存管理最终可以归结为自动化的解决2个问题：给对象分配内存以及回收分配给对象的内存。<br>#####对象优先在Eden分配<br>大多数情况下，对象在新生代Eden区分配。当Eden区没有足够的内存时，虚拟机将发起一次Minor GC。</p>\n<ul>\n<li>Minor GC(新生代GC):指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC发生的非常频繁。</li>\n<li>Full GC/Major GC(老年代GC):指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC。<h5 id=\"大对象直接进老年代\"><a href=\"#大对象直接进老年代\" class=\"headerlink\" title=\"大对象直接进老年代\"></a>大对象直接进老年代</h5>大对象是指需要大量连续内存空间的Java对象（例如很长的字符串以及数组）。<h5 id=\"长期存活的对象将进入老年代\"><a href=\"#长期存活的对象将进入老年代\" class=\"headerlink\" title=\"长期存活的对象将进入老年代\"></a>长期存活的对象将进入老年代</h5>JVM为每个对象定义一个对象年龄计数器。</li>\n</ul>\n<ul>\n<li>如果对象在Eden出生并经历过第一次Minor GC后仍然存活，并且能够被Survivor容纳，则应该被移动到Survivor空间中，并且年龄对象设置为1；</li>\n<li>对象在Survivor区中每熬过一次Minor GC，年龄就会增加1岁，当它的年龄增加到一定程度(默认为15岁，可通过参数-XX:MaxTenuringThreshold设置)，就会被晋升到老年代中。</li>\n<li>要注意的是：JVM并不是永远的要求对象的年龄必须达到MaxTenuringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一般，年龄大于等于该年龄的对象就可以直接进入老年代，无需等到MaxTenuringThreshold中要求的年龄。<h5 id=\"空间分配担保\"><a href=\"#空间分配担保\" class=\"headerlink\" title=\"空间分配担保\"></a>空间分配担保</h5></li>\n<li>在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，则进行Minor GC是安全的；</li>\n<li>如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，则急促检查老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管它是有风险的；</li>\n<li>如果小于或者HandePromotionFailure设置为不允许冒险，则这时要改为进行一次Full GC.</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h3 id=\"java—GC机制\"><a href=\"#java—GC机制\" class=\"headerlink\" title=\"java—GC机制\"></a>java—GC机制</h3><p>JVM会有一个运行时数据区来管理内存</p>\n<ul>\n<li>程序计数器(Program Counter Register)</li>\n<li>虚拟机栈(VM Stack)</li>\n<li>本地方法栈(Native Method Stack)</li>\n<li>方法区(Method Area)</li>\n<li>堆(Heap)</li>\n</ul>\n<h4 id=\"What-–-哪些内存需要回收？\"><a href=\"#What-–-哪些内存需要回收？\" class=\"headerlink\" title=\"What? – 哪些内存需要回收？\"></a>What? – 哪些内存需要回收？</h4><p>而其中程序计数器、虚拟机栈、本地方法栈是每个线程私有的内存空间，随线程而生，随线程而亡。<br>例如栈中每一个栈帧中分配多少内存基本上在类结构去诶是哪个下来时就已知了，因此这3个区域的内存分配和回收都是确定的，无需考虑内存回收的问题。</p>\n<p>但方法区和堆就不同了，一个接口的多个实现类需要的内存可能不一样，我们只有在程序运行期间才会知道会创建哪些对象，这部分内存的分配和回收都是动态的，GC主要关注的是这部分内存。</p>\n<hr>\n<p>总而言之，GC主要进行回收的内存是JVM中的方法区和堆；<br>涉及到多线程(指堆)、多个对该对象不同类型的引用(指方法区)，才会涉及GC的回收。</p>\n<hr>\n<h4 id=\"When-–-什么时候回收？\"><a href=\"#When-–-什么时候回收？\" class=\"headerlink\" title=\"When? – 什么时候回收？\"></a>When? – 什么时候回收？</h4><p>堆</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在面试中经常会碰到这样一个问题（事实上笔者也碰到过）：如何判断一个对象已经死去？</span><br><span class=\"line\"></span><br><span class=\"line\">很容易想到的一个答案是：对一个对象添加引用计数器。每当有地方引用它时，计数器值加1；当引用失效时，计数器值减1.而当计数器的值为0时这个对象就不会再被使用，判断为已死。是不是简单又直观。然而，很遗憾。这种做法是错误的！（面试时可千万别这样回答哦，我就是不假思索这样回答，然后就。。）为什么是错的呢？事实上，用引用计数法确实在大部分情况下是一个不错的解决方案，而在实际的应用中也有不少案例，但它却无法解决对象之间的循环引用问题。比如对象A中有一个字段指向了对象B，而对象B中也有一个字段指向了对象A，而事实上他们俩都不再使用，但计数器的值永远都不可能为0，也就不会被回收，然后就发生了内存泄露。。</span><br></pre></td></tr></table></figure>\n\n<p>在Java，C#等语言中，比较主流的判定一个对象已死的方法是：<Strong>可达性分析(Reachability Analysis).</Strong><br>所有生成的对象都是一个称为”GC Roots”的根的子树。从GC Roots开始向下搜索，搜索所经过的路径称为引用链(Reference Chain)，当一个对象到GC Roots没有任何引用链可以到达时，就称这个对象是不可达的（不可引用的），也就是可以被GC回收了<br><img src=\"./1483447208180.png\" alt=\"Alt text\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">无论是引用计数器还是可达性分析，判定对象是否存活都与引用有关！那么，如何定义对象的引用呢？</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>强引用(Strong Reference):Object obj = new Object();只要强引用还存在，GC永远不会回收掉被引用的对象。</li>\n<li>软引用(Soft Reference)：描述一些还有用但非必需的对象。在系统将会发生内存溢出之前，会把这些对象列入回收范围进行二次回收（即系统将会发生内存溢出了，才会对他们进行回收。）</li>\n<li>弱引用(Weak Reference):程度比软引用还要弱一些。这些对象只能生存到下次GC之前。当GC工作时，无论内存是否足够都会将其回收（即只要进行GC，就会对他们进行回收。）</li>\n<li>虚引用(Phantom Reference):一个对象是否存在虚引用，完全不会对其生存时间构成影响。</li>\n</ul>\n<p><Strong>方法区</strong><br>What部分我们已经提到，GC主要回收的是堆和方法区中的内存，而上面的How主要是针对对象的回收，他们一般位于堆内。那么，方法区中的东西该怎么回收呢？</p>\n<p>关于方法区中需要回收的是一些废弃的常量和无用的类</p>\n<ul>\n<li>废弃的常量的回收。这里看引用计数就可以了。没有对象引用该常量就可以放心的回收了。</li>\n<li>无用的类的回收。什么是无用的类呢？</li>\n</ul>\n<ul>\n<li>该类所有的实例都已经被回收。也就是Java堆中不存在该类的任何实例；</li>\n<li>加载该类的ClassLoader已经被回收；</li>\n<li>该类对应的java.lang.Class对象没有任何地方被引用，无法在任何地方通过反射访问该类的方法。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">总而言之，对于堆中的对象，主要用可达性分析判断一个对象是否还存在引用，如果该对象没有任何引用就应该被回收。而根据我们实际对引用的不同需求，又分成了4中引用，每种引用的回收机制也是不同的。</span><br><span class=\"line\">对于方法区中的常量和类，当一个常量没有任何对象引用它，它就可以被回收了。而对于类，如果可以判定它为无用类，就可以被回收了。</span><br></pre></td></tr></table></figure>\n<h4 id=\"How-–-如何回收？\"><a href=\"#How-–-如何回收？\" class=\"headerlink\" title=\"How? – 如何回收？\"></a>How? – 如何回收？</h4><h5 id=\"标记-清除-Mark-Sweep-算法\"><a href=\"#标记-清除-Mark-Sweep-算法\" class=\"headerlink\" title=\"标记-清除(Mark-Sweep)算法\"></a>标记-清除(Mark-Sweep)算法</h5><p>分为两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。<br>缺点：效率问题，标记和清除两个过程的效率都不高；空间问题，会产生很多碎片。</p>\n<h5 id=\"复制算法\"><a href=\"#复制算法\" class=\"headerlink\" title=\"复制算法\"></a>复制算法</h5><p>将可用内存按容量划分为大小相等的两块，每次只用其中一块。当这一块用完了，就将还存活的对象复制到另外一块上面，然后把原始空间全部回收。高效、简单。<br>缺点：将内存缩小为原来的一半。</p>\n<h5 id=\"标记-整理-Mark-Compat-算法\"><a href=\"#标记-整理-Mark-Compat-算法\" class=\"headerlink\" title=\"标记-整理(Mark-Compat)算法\"></a>标记-整理(Mark-Compat)算法</h5><p>标记过程与标记-清除算法过程一样，但后面不是简单的清除，而是让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存。</p>\n<h5 id=\"分代收集-Generational-Collection-算法\"><a href=\"#分代收集-Generational-Collection-算法\" class=\"headerlink\" title=\"分代收集(Generational Collection)算法\"></a>分代收集(Generational Collection)算法</h5><ul>\n<li>新生代中，每次垃圾收集时都有大批对象死去，只有少量存活，就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集；</li>\n<li>老年代中，其存活率较高、没有额外空间对它进行分配担保，就应该使用“标记-整理”或“标记-清理”算法进行回收。<br>####一些收集器<h5 id=\"Serial收集器\"><a href=\"#Serial收集器\" class=\"headerlink\" title=\"Serial收集器\"></a>Serial收集器</h5>单线程收集器，表示在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。”Stop The World”.<h5 id=\"ParNew收集器\"><a href=\"#ParNew收集器\" class=\"headerlink\" title=\"ParNew收集器\"></a>ParNew收集器</h5>实际就是Serial收集器的多线程版本。</li>\n<li>并发(Parallel):指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态；</li>\n<li>并行(Concurrent):指用户线程与垃圾收集线程同时执行，用户程序在继续运行，而垃圾收集程序运行于另一个CPU上。<h5 id=\"Parallel-Scavenge收集器\"><a href=\"#Parallel-Scavenge收集器\" class=\"headerlink\" title=\"Parallel Scavenge收集器\"></a>Parallel Scavenge收集器</h5>该收集器比较关注吞吐量(Throughout)(CPU用于用户代码的时间与CPU总消耗时间的比值)，保证吞吐量在一个可控的范围内。<h5 id=\"CMS-Concurrent-Mark-Sweep-收集器\"><a href=\"#CMS-Concurrent-Mark-Sweep-收集器\" class=\"headerlink\" title=\"CMS(Concurrent Mark Sweep)收集器\"></a>CMS(Concurrent Mark Sweep)收集器</h5>CMS收集器是一种以获得最短停顿时间为目标的收集器。<h5 id=\"G1-Garbage-First-收集器\"><a href=\"#G1-Garbage-First-收集器\" class=\"headerlink\" title=\"G1(Garbage First)收集器\"></a>G1(Garbage First)收集器</h5>从JDK1.7 Update 14之后的HotSpot虚拟机正式提供了商用的G1收集器，与其他收集器相比，它具有如下优点：并行与并发；分代收集；空间整合；可预测的停顿等。</li>\n</ul>\n<p>本部分主要分析了三种不同的垃圾回收算法：Mark-Sweep, Copy, Mark-Compact. 每种算法都有不同的优缺点，也有不同的适用范围。而JVM中对垃圾回收器并没有严格的要求，不同的收集器会结合多个算法进行垃圾回收。<br>#####内存分配<br>Java技术体系中所提倡的自动内存管理最终可以归结为自动化的解决2个问题：给对象分配内存以及回收分配给对象的内存。<br>#####对象优先在Eden分配<br>大多数情况下，对象在新生代Eden区分配。当Eden区没有足够的内存时，虚拟机将发起一次Minor GC。</p>\n<ul>\n<li>Minor GC(新生代GC):指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC发生的非常频繁。</li>\n<li>Full GC/Major GC(老年代GC):指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC。<h5 id=\"大对象直接进老年代\"><a href=\"#大对象直接进老年代\" class=\"headerlink\" title=\"大对象直接进老年代\"></a>大对象直接进老年代</h5>大对象是指需要大量连续内存空间的Java对象（例如很长的字符串以及数组）。<h5 id=\"长期存活的对象将进入老年代\"><a href=\"#长期存活的对象将进入老年代\" class=\"headerlink\" title=\"长期存活的对象将进入老年代\"></a>长期存活的对象将进入老年代</h5>JVM为每个对象定义一个对象年龄计数器。</li>\n</ul>\n<ul>\n<li>如果对象在Eden出生并经历过第一次Minor GC后仍然存活，并且能够被Survivor容纳，则应该被移动到Survivor空间中，并且年龄对象设置为1；</li>\n<li>对象在Survivor区中每熬过一次Minor GC，年龄就会增加1岁，当它的年龄增加到一定程度(默认为15岁，可通过参数-XX:MaxTenuringThreshold设置)，就会被晋升到老年代中。</li>\n<li>要注意的是：JVM并不是永远的要求对象的年龄必须达到MaxTenuringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一般，年龄大于等于该年龄的对象就可以直接进入老年代，无需等到MaxTenuringThreshold中要求的年龄。<h5 id=\"空间分配担保\"><a href=\"#空间分配担保\" class=\"headerlink\" title=\"空间分配担保\"></a>空间分配担保</h5></li>\n<li>在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，则进行Minor GC是安全的；</li>\n<li>如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，则急促检查老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管它是有风险的；</li>\n<li>如果小于或者HandePromotionFailure设置为不允许冒险，则这时要改为进行一次Full GC.</li>\n</ul>"},{"title":"大数据分享","abstract":"大数据分享","_content":"\n\n## 开头语\n\n```工欲善其事，必先利其器```\n<!-- more -->\n本次分享,是我在公司的第一次分享,我考虑后,将本次分享的主要内容分为了三大部块.先是针对相关基础组件分类介绍.再介绍下通过对这些组件进行组织配搭的大数据基础环境架构.再结合我的一些经历,为大家介绍下相关的应用与产品落地.\n\n\n## 技术栈简介\n* 数据采集\n* 数据存储\n* 数据治理(清洗&处理)\n* 数据应用\n* 产品落地\n\n\n\n我又根据不同组件的特性将他们分\n* 采集类\n* 存储类\n* 计算处理类\n* 传输类\n* 管理类 \n* 其它类\n\n下面开始具体介绍\n\n## 采集类\n\n数据源:\n* 日志\n* 业务数据\n* 公网数据(爬虫)\n* 文本数据\n* 出行数据(gps,手机定位等)\n\n![2019-04-15-10-36-53](http://img.wqkenqing.ren/2019-04-15-10-36-53.png)\n\n* sqoop flume crawler datax kettle  elk\n1. Flume(水槽) 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单,可拓展\n2. Sqoop是一个在结构化数据和Hadoop之间进行批量数据迁移的工具，结构化数据可以是Mysql、Oracle等RDBMS。Sqoop底层用MapReduce程序实现抽取、转换、加载，MapReduce天生的特性保证了并行化和高容错率，而且相比Kettle等传统ETL工具，任务跑在Hadoop集群上，减少了ETL服务器资源的使用情况。在特定场景下，抽取过程会有很大的性能提升。\n3. crawler , jsoup ,httpclient, nutch 等.\n4. elk  集中式日志系统 ELK 协议栈详解\n![2019-04-15-10-11-16](http://img.wqkenqing.ren/2019-04-15-10-11-16.png)\n---\n\n## 存储类\n* hdfs\n* hbase\n* hive\n* mongdb\n* redis\n* RDBMS\n\n### hdfs\n    * 分布式文件存储系统\n    * 提供了高可靠性、高扩展性和高吞吐率的数据存储服务\n    * hdfs典型结构：物理结构+逻辑结构\n\n    * 文件线性切割成Block：偏移量（offset）\n    * Block分散存储在集群节点中\n    * 单一文件Block大小一致，文件与文件可以不一致\n    * Block可以设置副本数，副本分散在不同的节点中\n    * 副本数不要超过节点数量\n    * 文件上传可以设置Block大小和副本数\n    * 已上传的文件Block副本数可以调整，大小不变\n    * 只支持一次写入多次读取，同一时刻只有一个写入者\n    * 只能追加，不能修改\n\n![2019-04-15-11-12-40](http://img.wqkenqing.ren/2019-04-15-11-12-40.png)\n\n### hbase\nBase是一个构建在HDFS上的分布式列存储系统；\nBase是基于Google BigTable模型开发的，典型的key/value系统；\nBase是Apache Hadoop生态系统中的重要一员，主要用于海量结构化数据存储；\n\n大：一个表可以有数十亿行，上百万列；无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有截然不同的列；面向列：面向列（族）的存储和权限控制，列（族）独立检索；稀疏：空（null）列并不占用存储空间，表可以设计的非常稀疏；数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳；数据类型单一：Hbase中的数据都是字符串，没有类型\n#### openTSDB\n基于Hbase的分布式的，可伸缩的时间序列数据库。\n主要用途，就是做监控系统；譬如收集大规模集群（包括网络设备、操作系统、应用程序）的监控数据并进行存储，查询。\n![2019-04-15-11-27-46](http://img.wqkenqing.ren/2019-04-15-11-27-46.png)\n\n#### solr & Phoenix\n\n二级索引\n\n### hive \nive 是一个基于 Hadoop 文件系统之上的数据仓库架构。它可以将结构化的数据文件映射为一张数据库表，并提供简单的 sql 查询功能。还可以将 sql 语句转换为 MapReduce 任务运行。\n底部计算引擎还可以用用Tez, spark等.\n![2019-04-15-11-36-43](http://img.wqkenqing.ren/2019-04-15-11-36-43.png)\n\n##### Impala\nImpala是Cloudera公司推出，提供对HDFS、Hbase数据的高性能、低延迟的交互式SQL查询功能。\n* 基于Hive使用内存计算，兼顾数据仓库、具有实时、批处理、多并发等优点\n* 对内存依赖大,稳定性不如hive\n\n相比hive数据仓库,impala针对的量级相关少些,但会有效率的提升.但一般来讲,数据仓库一类需求对时间上的要要求一般不会太高,所以常规方式一般就符合大多数需求.\n\n## 计算处理类\n* mapreduce\n* mapreduce on oozie ,on tez \n* spark \n* flink\n\n### mapreduce \nMapreduce是一个计算框架，既然是做计算的框架，那么表现形式就是有个输入（input），mapreduce操作这个输（input），通过本身定义好的计算模型，得到一个输出（output），这个输出就是我们所需要的结果。我们要学习的就是这个计算模型的运行规则。在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。而程序员要做的就是定义好这两个阶段的函数：map函数和reduce函数。\n\n分布式计算；\n移动计算而不移动数据。\n![2019-04-15-11-59-56](http://img.wqkenqing.ren/2019-04-15-11-59-56.png)\n\n### spark\n相比一二代计算引擎,在兼并了一二代的特色之外,还引放了流计算这一能力,还丰富了计算函数.\n其中比较有代表性的主要就是spark&storm.\n也就是说这代计算引擎兼具无边界数据与有边界数据同样的处理能力.同时还具有DAG特性.\n这里主要介绍spark\n\nspark主要组成有以下\n* spark-core\n* spark-streaming\n* spark-sql\n* spark-mlib\n* spark-graphX。\n\nspark-core是一个提供内存计算的框架,其他的四大框架都是基于spark core上进行计算的,所以没有spark core,其他的框架是浮云.\nspark-core的主要内容就是对RDD的操作\nRDD的创建 ->RDD的转换 ->RDD的缓存 ->RDD的行动 ->RDD的输出\n\nspark-streaming中使用离散化流（discretized stream）作为抽象的表示，叫做DStream。它是随时间推移而收集数据的序列，每个时间段收集到的数据在DStream内部以一个RDD的形式存在。DStream支持从kafka，flume,hdfs,s3等获取输入。DStream也支持两种操作，即转化操作和输出操作\n\n\nspark-sql\nSpark SQL 提供了查询结构化数据及计算结果等信息的接口.\n查询结果以 Datasets and DataFrames 形式返回\n\n...\n\n### flink/blink\n\n略\n\n## 传输类\n### kafka\nKafka是分布式发布-订阅消息系统,一个分布式的，可划分的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。日常中常与spark-streaming结合实用,为其提供无边界数据\n\n![2019-04-16-15-06-25](http://img.wqkenqing.ren/2019-04-16-15-06-25.png)\n\n## 管理类 Hue cloudera-manager\nue与cm 都是由cloudera提供,后面cloudera将hue开源给了apache.如果基础集群环境是采用的是开源自主搭建,可考虑引入hue.另一些大数据服务公司,有集成打包自己的一些大数据产品,如cdh等.但这些服务收费,涉及到成本问题.所以如何选用,需要相关斟酌.\n\n## 其它类 zookeeper ,yarn等\nzookeeper在集中基础环境中主要作为配置分享中心,与kafka,hbase等组件集成.yarn则作为资源管理组件,可以与mapreduce ,spark等集成\n\n## 各类组件架构\n以上,已经大致介绍了各类工具,基本了解了相应的特性和使用场景,而根据它们的特性,进行合理的配备,架构,从而实现一个功能全面,稳定的大数据环境.\n\n于我个人经历与平时了解来讲,一般的架构主要如下\n![2019-04-17-09-23-07](http://img.wqkenqing.ren/2019-04-17-09-23-07.png)\n另:\n![2019-04-16-10-42-08](http://img.wqkenqing.ren/2019-04-16-10-42-08.png)\n\n\n总得来说,各类组件供选型一般来讲都不是单一的.所以,我们的大数据环境各部份组件都是插销式可插拔的.所以不同公司可能不一而同,具体看自身需求和实际情况.比如上图中的storm流式计算模块,就可以替换成spark-streaming等.\n\n通过对上图的架构的拆解,再组合,可能还会有以下组织架构.\n\n数据仓库\n可以理解为上图中间部份.作为一个数据集市的存在,算作数据中心的一部份.\n\n* ODS：是数据仓库第一层数据，直接从原始数据过来的，经过简单地处理，比喻：字段体重的数据为175cm等数据。\n* DW*：这个是数据仓库的第二层数据，DWD和DWS很多情况下是并列存在的，这一层储存经过处理后的标准数据，比喻订单、用户、页面点击流量等数据。\n* ADS：这个是数据仓库的最后一层数据，为应用层数据，直接可以给业务人员使用。\n![2019-04-16-11-01-33](http://img.wqkenqing.ren/2019-04-16-11-01-33.png)\n\n星型模型\n\n星型模型中有两个重要的概念：事实表和维度表。\n事实表：一些主键ID的集合，没有存放任何实际的内容\n维度表：存放详细的数据信息，有唯一的主键ID。如上面的关键词表、用户表等等。\n![2019-04-16-11-04-52](http://img.wqkenqing.ren/2019-04-16-11-04-52.png)\n\n 数据中心:\n概念相对更大一些,可能即作为具体平台产品集合,也可能是一个团队行政划分.总得来说,是如\n* 大数据基础平台\n* 数据仓库\n* DMP平台\n* 相关应用平台如推荐系统,报表系统,可视化平台等.\n\n\n数据中台:\n这个是由阿里于15年率先提出.主导思想是大中台,小前台.这块暂无特别明确的解释说法,但现在也有不少公司效仿.我个人从它的主导思想\"大中台,小前台\"的理解是,这个可能是体量更大,壁垒更少的一个数据集成体.比如阿里系的旗下公司,数据流都会归集到中台,同时阿里系下的公司也能获得不仅自己公司数据中心归集的数据反馈,还能获得阿里中台整合后流出的反馈数据.\n\n\n## 应用落地\n### 公共服务\n* 交通出行\n![2019-04-16-16-15-42](http://img.wqkenqing.ren/2019-04-16-16-15-42.png)\n* 智慧城市\n* ...\n\n### 产品应用\n* 用户画像\n* 征信模型\n* 推荐系统\n* 精确营销\n* 前沿科学(无人驾驶,人工智能,AR等)\n\n\n## 结语\n以上,就是我今天分享的主要内容.今天的主题是\"器\",但对这些工具的讲解浅尝辄止,在实际的开发实战中涉及的情况是更为复杂,需要掌握的内容更多,深度也更深.我这里主要是想抛砖引玉,为大家提供一点自己的理解,若能有所帮助,不胜荣幸.\n\n另外,工具始终是工具,菜刀再利也要厨子好,才能做好菜.所以如何利用这些工具,与我们的业务结合,实现我们想要的价值,这是我一直在探索的,也愿与各位同仁一同前行.\n\n附上图中涉及到的技术栈\n\n![2019-04-17-09-09-00](http://img.wqkenqing.ren/2019-04-17-09-09-00.png)\n\n\n\n\n\n\n","source":"_posts/技术/hexo/old/大数据相关分享.md","raw":"\n---\ntitle: 大数据分享\ntags: 日常总结\nabstract: 大数据分享\n\n---\n\n\n## 开头语\n\n```工欲善其事，必先利其器```\n<!-- more -->\n本次分享,是我在公司的第一次分享,我考虑后,将本次分享的主要内容分为了三大部块.先是针对相关基础组件分类介绍.再介绍下通过对这些组件进行组织配搭的大数据基础环境架构.再结合我的一些经历,为大家介绍下相关的应用与产品落地.\n\n\n## 技术栈简介\n* 数据采集\n* 数据存储\n* 数据治理(清洗&处理)\n* 数据应用\n* 产品落地\n\n\n\n我又根据不同组件的特性将他们分\n* 采集类\n* 存储类\n* 计算处理类\n* 传输类\n* 管理类 \n* 其它类\n\n下面开始具体介绍\n\n## 采集类\n\n数据源:\n* 日志\n* 业务数据\n* 公网数据(爬虫)\n* 文本数据\n* 出行数据(gps,手机定位等)\n\n![2019-04-15-10-36-53](http://img.wqkenqing.ren/2019-04-15-10-36-53.png)\n\n* sqoop flume crawler datax kettle  elk\n1. Flume(水槽) 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单,可拓展\n2. Sqoop是一个在结构化数据和Hadoop之间进行批量数据迁移的工具，结构化数据可以是Mysql、Oracle等RDBMS。Sqoop底层用MapReduce程序实现抽取、转换、加载，MapReduce天生的特性保证了并行化和高容错率，而且相比Kettle等传统ETL工具，任务跑在Hadoop集群上，减少了ETL服务器资源的使用情况。在特定场景下，抽取过程会有很大的性能提升。\n3. crawler , jsoup ,httpclient, nutch 等.\n4. elk  集中式日志系统 ELK 协议栈详解\n![2019-04-15-10-11-16](http://img.wqkenqing.ren/2019-04-15-10-11-16.png)\n---\n\n## 存储类\n* hdfs\n* hbase\n* hive\n* mongdb\n* redis\n* RDBMS\n\n### hdfs\n    * 分布式文件存储系统\n    * 提供了高可靠性、高扩展性和高吞吐率的数据存储服务\n    * hdfs典型结构：物理结构+逻辑结构\n\n    * 文件线性切割成Block：偏移量（offset）\n    * Block分散存储在集群节点中\n    * 单一文件Block大小一致，文件与文件可以不一致\n    * Block可以设置副本数，副本分散在不同的节点中\n    * 副本数不要超过节点数量\n    * 文件上传可以设置Block大小和副本数\n    * 已上传的文件Block副本数可以调整，大小不变\n    * 只支持一次写入多次读取，同一时刻只有一个写入者\n    * 只能追加，不能修改\n\n![2019-04-15-11-12-40](http://img.wqkenqing.ren/2019-04-15-11-12-40.png)\n\n### hbase\nBase是一个构建在HDFS上的分布式列存储系统；\nBase是基于Google BigTable模型开发的，典型的key/value系统；\nBase是Apache Hadoop生态系统中的重要一员，主要用于海量结构化数据存储；\n\n大：一个表可以有数十亿行，上百万列；无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有截然不同的列；面向列：面向列（族）的存储和权限控制，列（族）独立检索；稀疏：空（null）列并不占用存储空间，表可以设计的非常稀疏；数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳；数据类型单一：Hbase中的数据都是字符串，没有类型\n#### openTSDB\n基于Hbase的分布式的，可伸缩的时间序列数据库。\n主要用途，就是做监控系统；譬如收集大规模集群（包括网络设备、操作系统、应用程序）的监控数据并进行存储，查询。\n![2019-04-15-11-27-46](http://img.wqkenqing.ren/2019-04-15-11-27-46.png)\n\n#### solr & Phoenix\n\n二级索引\n\n### hive \nive 是一个基于 Hadoop 文件系统之上的数据仓库架构。它可以将结构化的数据文件映射为一张数据库表，并提供简单的 sql 查询功能。还可以将 sql 语句转换为 MapReduce 任务运行。\n底部计算引擎还可以用用Tez, spark等.\n![2019-04-15-11-36-43](http://img.wqkenqing.ren/2019-04-15-11-36-43.png)\n\n##### Impala\nImpala是Cloudera公司推出，提供对HDFS、Hbase数据的高性能、低延迟的交互式SQL查询功能。\n* 基于Hive使用内存计算，兼顾数据仓库、具有实时、批处理、多并发等优点\n* 对内存依赖大,稳定性不如hive\n\n相比hive数据仓库,impala针对的量级相关少些,但会有效率的提升.但一般来讲,数据仓库一类需求对时间上的要要求一般不会太高,所以常规方式一般就符合大多数需求.\n\n## 计算处理类\n* mapreduce\n* mapreduce on oozie ,on tez \n* spark \n* flink\n\n### mapreduce \nMapreduce是一个计算框架，既然是做计算的框架，那么表现形式就是有个输入（input），mapreduce操作这个输（input），通过本身定义好的计算模型，得到一个输出（output），这个输出就是我们所需要的结果。我们要学习的就是这个计算模型的运行规则。在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。而程序员要做的就是定义好这两个阶段的函数：map函数和reduce函数。\n\n分布式计算；\n移动计算而不移动数据。\n![2019-04-15-11-59-56](http://img.wqkenqing.ren/2019-04-15-11-59-56.png)\n\n### spark\n相比一二代计算引擎,在兼并了一二代的特色之外,还引放了流计算这一能力,还丰富了计算函数.\n其中比较有代表性的主要就是spark&storm.\n也就是说这代计算引擎兼具无边界数据与有边界数据同样的处理能力.同时还具有DAG特性.\n这里主要介绍spark\n\nspark主要组成有以下\n* spark-core\n* spark-streaming\n* spark-sql\n* spark-mlib\n* spark-graphX。\n\nspark-core是一个提供内存计算的框架,其他的四大框架都是基于spark core上进行计算的,所以没有spark core,其他的框架是浮云.\nspark-core的主要内容就是对RDD的操作\nRDD的创建 ->RDD的转换 ->RDD的缓存 ->RDD的行动 ->RDD的输出\n\nspark-streaming中使用离散化流（discretized stream）作为抽象的表示，叫做DStream。它是随时间推移而收集数据的序列，每个时间段收集到的数据在DStream内部以一个RDD的形式存在。DStream支持从kafka，flume,hdfs,s3等获取输入。DStream也支持两种操作，即转化操作和输出操作\n\n\nspark-sql\nSpark SQL 提供了查询结构化数据及计算结果等信息的接口.\n查询结果以 Datasets and DataFrames 形式返回\n\n...\n\n### flink/blink\n\n略\n\n## 传输类\n### kafka\nKafka是分布式发布-订阅消息系统,一个分布式的，可划分的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。日常中常与spark-streaming结合实用,为其提供无边界数据\n\n![2019-04-16-15-06-25](http://img.wqkenqing.ren/2019-04-16-15-06-25.png)\n\n## 管理类 Hue cloudera-manager\nue与cm 都是由cloudera提供,后面cloudera将hue开源给了apache.如果基础集群环境是采用的是开源自主搭建,可考虑引入hue.另一些大数据服务公司,有集成打包自己的一些大数据产品,如cdh等.但这些服务收费,涉及到成本问题.所以如何选用,需要相关斟酌.\n\n## 其它类 zookeeper ,yarn等\nzookeeper在集中基础环境中主要作为配置分享中心,与kafka,hbase等组件集成.yarn则作为资源管理组件,可以与mapreduce ,spark等集成\n\n## 各类组件架构\n以上,已经大致介绍了各类工具,基本了解了相应的特性和使用场景,而根据它们的特性,进行合理的配备,架构,从而实现一个功能全面,稳定的大数据环境.\n\n于我个人经历与平时了解来讲,一般的架构主要如下\n![2019-04-17-09-23-07](http://img.wqkenqing.ren/2019-04-17-09-23-07.png)\n另:\n![2019-04-16-10-42-08](http://img.wqkenqing.ren/2019-04-16-10-42-08.png)\n\n\n总得来说,各类组件供选型一般来讲都不是单一的.所以,我们的大数据环境各部份组件都是插销式可插拔的.所以不同公司可能不一而同,具体看自身需求和实际情况.比如上图中的storm流式计算模块,就可以替换成spark-streaming等.\n\n通过对上图的架构的拆解,再组合,可能还会有以下组织架构.\n\n数据仓库\n可以理解为上图中间部份.作为一个数据集市的存在,算作数据中心的一部份.\n\n* ODS：是数据仓库第一层数据，直接从原始数据过来的，经过简单地处理，比喻：字段体重的数据为175cm等数据。\n* DW*：这个是数据仓库的第二层数据，DWD和DWS很多情况下是并列存在的，这一层储存经过处理后的标准数据，比喻订单、用户、页面点击流量等数据。\n* ADS：这个是数据仓库的最后一层数据，为应用层数据，直接可以给业务人员使用。\n![2019-04-16-11-01-33](http://img.wqkenqing.ren/2019-04-16-11-01-33.png)\n\n星型模型\n\n星型模型中有两个重要的概念：事实表和维度表。\n事实表：一些主键ID的集合，没有存放任何实际的内容\n维度表：存放详细的数据信息，有唯一的主键ID。如上面的关键词表、用户表等等。\n![2019-04-16-11-04-52](http://img.wqkenqing.ren/2019-04-16-11-04-52.png)\n\n 数据中心:\n概念相对更大一些,可能即作为具体平台产品集合,也可能是一个团队行政划分.总得来说,是如\n* 大数据基础平台\n* 数据仓库\n* DMP平台\n* 相关应用平台如推荐系统,报表系统,可视化平台等.\n\n\n数据中台:\n这个是由阿里于15年率先提出.主导思想是大中台,小前台.这块暂无特别明确的解释说法,但现在也有不少公司效仿.我个人从它的主导思想\"大中台,小前台\"的理解是,这个可能是体量更大,壁垒更少的一个数据集成体.比如阿里系的旗下公司,数据流都会归集到中台,同时阿里系下的公司也能获得不仅自己公司数据中心归集的数据反馈,还能获得阿里中台整合后流出的反馈数据.\n\n\n## 应用落地\n### 公共服务\n* 交通出行\n![2019-04-16-16-15-42](http://img.wqkenqing.ren/2019-04-16-16-15-42.png)\n* 智慧城市\n* ...\n\n### 产品应用\n* 用户画像\n* 征信模型\n* 推荐系统\n* 精确营销\n* 前沿科学(无人驾驶,人工智能,AR等)\n\n\n## 结语\n以上,就是我今天分享的主要内容.今天的主题是\"器\",但对这些工具的讲解浅尝辄止,在实际的开发实战中涉及的情况是更为复杂,需要掌握的内容更多,深度也更深.我这里主要是想抛砖引玉,为大家提供一点自己的理解,若能有所帮助,不胜荣幸.\n\n另外,工具始终是工具,菜刀再利也要厨子好,才能做好菜.所以如何利用这些工具,与我们的业务结合,实现我们想要的价值,这是我一直在探索的,也愿与各位同仁一同前行.\n\n附上图中涉及到的技术栈\n\n![2019-04-17-09-09-00](http://img.wqkenqing.ren/2019-04-17-09-09-00.png)\n\n\n\n\n\n\n","slug":"技术/hexo/old/大数据相关分享","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd338002l38pw0e7m8hw5","content":"<h2 id=\"开头语\"><a href=\"#开头语\" class=\"headerlink\" title=\"开头语\"></a>开头语</h2><p><code>工欲善其事，必先利其器</code></p>\n<a id=\"more\"></a>\n<p>本次分享,是我在公司的第一次分享,我考虑后,将本次分享的主要内容分为了三大部块.先是针对相关基础组件分类介绍.再介绍下通过对这些组件进行组织配搭的大数据基础环境架构.再结合我的一些经历,为大家介绍下相关的应用与产品落地.</p>\n<h2 id=\"技术栈简介\"><a href=\"#技术栈简介\" class=\"headerlink\" title=\"技术栈简介\"></a>技术栈简介</h2><ul>\n<li>数据采集</li>\n<li>数据存储</li>\n<li>数据治理(清洗&amp;处理)</li>\n<li>数据应用</li>\n<li>产品落地</li>\n</ul>\n<p>我又根据不同组件的特性将他们分</p>\n<ul>\n<li>采集类</li>\n<li>存储类</li>\n<li>计算处理类</li>\n<li>传输类</li>\n<li>管理类 </li>\n<li>其它类</li>\n</ul>\n<p>下面开始具体介绍</p>\n<h2 id=\"采集类\"><a href=\"#采集类\" class=\"headerlink\" title=\"采集类\"></a>采集类</h2><p>数据源:</p>\n<ul>\n<li>日志</li>\n<li>业务数据</li>\n<li>公网数据(爬虫)</li>\n<li>文本数据</li>\n<li>出行数据(gps,手机定位等)</li>\n</ul>\n<p><img src=\"http://img.wqkenqing.ren/2019-04-15-10-36-53.png\" alt=\"2019-04-15-10-36-53\"></p>\n<ul>\n<li>sqoop flume crawler datax kettle  elk</li>\n</ul>\n<ol>\n<li>Flume(水槽) 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单,可拓展</li>\n<li>Sqoop是一个在结构化数据和Hadoop之间进行批量数据迁移的工具，结构化数据可以是Mysql、Oracle等RDBMS。Sqoop底层用MapReduce程序实现抽取、转换、加载，MapReduce天生的特性保证了并行化和高容错率，而且相比Kettle等传统ETL工具，任务跑在Hadoop集群上，减少了ETL服务器资源的使用情况。在特定场景下，抽取过程会有很大的性能提升。</li>\n<li>crawler , jsoup ,httpclient, nutch 等.</li>\n<li>elk  集中式日志系统 ELK 协议栈详解<br><img src=\"http://img.wqkenqing.ren/2019-04-15-10-11-16.png\" alt=\"2019-04-15-10-11-16\"></li>\n</ol>\n<hr>\n<h2 id=\"存储类\"><a href=\"#存储类\" class=\"headerlink\" title=\"存储类\"></a>存储类</h2><ul>\n<li>hdfs</li>\n<li>hbase</li>\n<li>hive</li>\n<li>mongdb</li>\n<li>redis</li>\n<li>RDBMS</li>\n</ul>\n<h3 id=\"hdfs\"><a href=\"#hdfs\" class=\"headerlink\" title=\"hdfs\"></a>hdfs</h3><pre><code>* 分布式文件存储系统\n* 提供了高可靠性、高扩展性和高吞吐率的数据存储服务\n* hdfs典型结构：物理结构+逻辑结构\n\n* 文件线性切割成Block：偏移量（offset）\n* Block分散存储在集群节点中\n* 单一文件Block大小一致，文件与文件可以不一致\n* Block可以设置副本数，副本分散在不同的节点中\n* 副本数不要超过节点数量\n* 文件上传可以设置Block大小和副本数\n* 已上传的文件Block副本数可以调整，大小不变\n* 只支持一次写入多次读取，同一时刻只有一个写入者\n* 只能追加，不能修改</code></pre><p><img src=\"http://img.wqkenqing.ren/2019-04-15-11-12-40.png\" alt=\"2019-04-15-11-12-40\"></p>\n<h3 id=\"hbase\"><a href=\"#hbase\" class=\"headerlink\" title=\"hbase\"></a>hbase</h3><p>Base是一个构建在HDFS上的分布式列存储系统；<br>Base是基于Google BigTable模型开发的，典型的key/value系统；<br>Base是Apache Hadoop生态系统中的重要一员，主要用于海量结构化数据存储；</p>\n<p>大：一个表可以有数十亿行，上百万列；无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有截然不同的列；面向列：面向列（族）的存储和权限控制，列（族）独立检索；稀疏：空（null）列并不占用存储空间，表可以设计的非常稀疏；数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳；数据类型单一：Hbase中的数据都是字符串，没有类型</p>\n<h4 id=\"openTSDB\"><a href=\"#openTSDB\" class=\"headerlink\" title=\"openTSDB\"></a>openTSDB</h4><p>基于Hbase的分布式的，可伸缩的时间序列数据库。<br>主要用途，就是做监控系统；譬如收集大规模集群（包括网络设备、操作系统、应用程序）的监控数据并进行存储，查询。<br><img src=\"http://img.wqkenqing.ren/2019-04-15-11-27-46.png\" alt=\"2019-04-15-11-27-46\"></p>\n<h4 id=\"solr-amp-Phoenix\"><a href=\"#solr-amp-Phoenix\" class=\"headerlink\" title=\"solr &amp; Phoenix\"></a>solr &amp; Phoenix</h4><p>二级索引</p>\n<h3 id=\"hive\"><a href=\"#hive\" class=\"headerlink\" title=\"hive\"></a>hive</h3><p>ive 是一个基于 Hadoop 文件系统之上的数据仓库架构。它可以将结构化的数据文件映射为一张数据库表，并提供简单的 sql 查询功能。还可以将 sql 语句转换为 MapReduce 任务运行。<br>底部计算引擎还可以用用Tez, spark等.<br><img src=\"http://img.wqkenqing.ren/2019-04-15-11-36-43.png\" alt=\"2019-04-15-11-36-43\"></p>\n<h5 id=\"Impala\"><a href=\"#Impala\" class=\"headerlink\" title=\"Impala\"></a>Impala</h5><p>Impala是Cloudera公司推出，提供对HDFS、Hbase数据的高性能、低延迟的交互式SQL查询功能。</p>\n<ul>\n<li>基于Hive使用内存计算，兼顾数据仓库、具有实时、批处理、多并发等优点</li>\n<li>对内存依赖大,稳定性不如hive</li>\n</ul>\n<p>相比hive数据仓库,impala针对的量级相关少些,但会有效率的提升.但一般来讲,数据仓库一类需求对时间上的要要求一般不会太高,所以常规方式一般就符合大多数需求.</p>\n<h2 id=\"计算处理类\"><a href=\"#计算处理类\" class=\"headerlink\" title=\"计算处理类\"></a>计算处理类</h2><ul>\n<li>mapreduce</li>\n<li>mapreduce on oozie ,on tez </li>\n<li>spark </li>\n<li>flink</li>\n</ul>\n<h3 id=\"mapreduce\"><a href=\"#mapreduce\" class=\"headerlink\" title=\"mapreduce\"></a>mapreduce</h3><p>Mapreduce是一个计算框架，既然是做计算的框架，那么表现形式就是有个输入（input），mapreduce操作这个输（input），通过本身定义好的计算模型，得到一个输出（output），这个输出就是我们所需要的结果。我们要学习的就是这个计算模型的运行规则。在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。而程序员要做的就是定义好这两个阶段的函数：map函数和reduce函数。</p>\n<p>分布式计算；<br>移动计算而不移动数据。<br><img src=\"http://img.wqkenqing.ren/2019-04-15-11-59-56.png\" alt=\"2019-04-15-11-59-56\"></p>\n<h3 id=\"spark\"><a href=\"#spark\" class=\"headerlink\" title=\"spark\"></a>spark</h3><p>相比一二代计算引擎,在兼并了一二代的特色之外,还引放了流计算这一能力,还丰富了计算函数.<br>其中比较有代表性的主要就是spark&amp;storm.<br>也就是说这代计算引擎兼具无边界数据与有边界数据同样的处理能力.同时还具有DAG特性.<br>这里主要介绍spark</p>\n<p>spark主要组成有以下</p>\n<ul>\n<li>spark-core</li>\n<li>spark-streaming</li>\n<li>spark-sql</li>\n<li>spark-mlib</li>\n<li>spark-graphX。</li>\n</ul>\n<p>spark-core是一个提供内存计算的框架,其他的四大框架都是基于spark core上进行计算的,所以没有spark core,其他的框架是浮云.<br>spark-core的主要内容就是对RDD的操作<br>RDD的创建 -&gt;RDD的转换 -&gt;RDD的缓存 -&gt;RDD的行动 -&gt;RDD的输出</p>\n<p>spark-streaming中使用离散化流（discretized stream）作为抽象的表示，叫做DStream。它是随时间推移而收集数据的序列，每个时间段收集到的数据在DStream内部以一个RDD的形式存在。DStream支持从kafka，flume,hdfs,s3等获取输入。DStream也支持两种操作，即转化操作和输出操作</p>\n<p>spark-sql<br>Spark SQL 提供了查询结构化数据及计算结果等信息的接口.<br>查询结果以 Datasets and DataFrames 形式返回</p>\n<p>…</p>\n<h3 id=\"flink-blink\"><a href=\"#flink-blink\" class=\"headerlink\" title=\"flink/blink\"></a>flink/blink</h3><p>略</p>\n<h2 id=\"传输类\"><a href=\"#传输类\" class=\"headerlink\" title=\"传输类\"></a>传输类</h2><h3 id=\"kafka\"><a href=\"#kafka\" class=\"headerlink\" title=\"kafka\"></a>kafka</h3><p>Kafka是分布式发布-订阅消息系统,一个分布式的，可划分的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。日常中常与spark-streaming结合实用,为其提供无边界数据</p>\n<p><img src=\"http://img.wqkenqing.ren/2019-04-16-15-06-25.png\" alt=\"2019-04-16-15-06-25\"></p>\n<h2 id=\"管理类-Hue-cloudera-manager\"><a href=\"#管理类-Hue-cloudera-manager\" class=\"headerlink\" title=\"管理类 Hue cloudera-manager\"></a>管理类 Hue cloudera-manager</h2><p>ue与cm 都是由cloudera提供,后面cloudera将hue开源给了apache.如果基础集群环境是采用的是开源自主搭建,可考虑引入hue.另一些大数据服务公司,有集成打包自己的一些大数据产品,如cdh等.但这些服务收费,涉及到成本问题.所以如何选用,需要相关斟酌.</p>\n<h2 id=\"其它类-zookeeper-yarn等\"><a href=\"#其它类-zookeeper-yarn等\" class=\"headerlink\" title=\"其它类 zookeeper ,yarn等\"></a>其它类 zookeeper ,yarn等</h2><p>zookeeper在集中基础环境中主要作为配置分享中心,与kafka,hbase等组件集成.yarn则作为资源管理组件,可以与mapreduce ,spark等集成</p>\n<h2 id=\"各类组件架构\"><a href=\"#各类组件架构\" class=\"headerlink\" title=\"各类组件架构\"></a>各类组件架构</h2><p>以上,已经大致介绍了各类工具,基本了解了相应的特性和使用场景,而根据它们的特性,进行合理的配备,架构,从而实现一个功能全面,稳定的大数据环境.</p>\n<p>于我个人经历与平时了解来讲,一般的架构主要如下<br><img src=\"http://img.wqkenqing.ren/2019-04-17-09-23-07.png\" alt=\"2019-04-17-09-23-07\"><br>另:<br><img src=\"http://img.wqkenqing.ren/2019-04-16-10-42-08.png\" alt=\"2019-04-16-10-42-08\"></p>\n<p>总得来说,各类组件供选型一般来讲都不是单一的.所以,我们的大数据环境各部份组件都是插销式可插拔的.所以不同公司可能不一而同,具体看自身需求和实际情况.比如上图中的storm流式计算模块,就可以替换成spark-streaming等.</p>\n<p>通过对上图的架构的拆解,再组合,可能还会有以下组织架构.</p>\n<p>数据仓库<br>可以理解为上图中间部份.作为一个数据集市的存在,算作数据中心的一部份.</p>\n<ul>\n<li>ODS：是数据仓库第一层数据，直接从原始数据过来的，经过简单地处理，比喻：字段体重的数据为175cm等数据。</li>\n<li>DW*：这个是数据仓库的第二层数据，DWD和DWS很多情况下是并列存在的，这一层储存经过处理后的标准数据，比喻订单、用户、页面点击流量等数据。</li>\n<li>ADS：这个是数据仓库的最后一层数据，为应用层数据，直接可以给业务人员使用。<br><img src=\"http://img.wqkenqing.ren/2019-04-16-11-01-33.png\" alt=\"2019-04-16-11-01-33\"></li>\n</ul>\n<p>星型模型</p>\n<p>星型模型中有两个重要的概念：事实表和维度表。<br>事实表：一些主键ID的集合，没有存放任何实际的内容<br>维度表：存放详细的数据信息，有唯一的主键ID。如上面的关键词表、用户表等等。<br><img src=\"http://img.wqkenqing.ren/2019-04-16-11-04-52.png\" alt=\"2019-04-16-11-04-52\"></p>\n<p> 数据中心:<br>概念相对更大一些,可能即作为具体平台产品集合,也可能是一个团队行政划分.总得来说,是如</p>\n<ul>\n<li>大数据基础平台</li>\n<li>数据仓库</li>\n<li>DMP平台</li>\n<li>相关应用平台如推荐系统,报表系统,可视化平台等.</li>\n</ul>\n<p>数据中台:<br>这个是由阿里于15年率先提出.主导思想是大中台,小前台.这块暂无特别明确的解释说法,但现在也有不少公司效仿.我个人从它的主导思想”大中台,小前台”的理解是,这个可能是体量更大,壁垒更少的一个数据集成体.比如阿里系的旗下公司,数据流都会归集到中台,同时阿里系下的公司也能获得不仅自己公司数据中心归集的数据反馈,还能获得阿里中台整合后流出的反馈数据.</p>\n<h2 id=\"应用落地\"><a href=\"#应用落地\" class=\"headerlink\" title=\"应用落地\"></a>应用落地</h2><h3 id=\"公共服务\"><a href=\"#公共服务\" class=\"headerlink\" title=\"公共服务\"></a>公共服务</h3><ul>\n<li>交通出行<br><img src=\"http://img.wqkenqing.ren/2019-04-16-16-15-42.png\" alt=\"2019-04-16-16-15-42\"></li>\n<li>智慧城市</li>\n<li>…</li>\n</ul>\n<h3 id=\"产品应用\"><a href=\"#产品应用\" class=\"headerlink\" title=\"产品应用\"></a>产品应用</h3><ul>\n<li>用户画像</li>\n<li>征信模型</li>\n<li>推荐系统</li>\n<li>精确营销</li>\n<li>前沿科学(无人驾驶,人工智能,AR等)</li>\n</ul>\n<h2 id=\"结语\"><a href=\"#结语\" class=\"headerlink\" title=\"结语\"></a>结语</h2><p>以上,就是我今天分享的主要内容.今天的主题是”器”,但对这些工具的讲解浅尝辄止,在实际的开发实战中涉及的情况是更为复杂,需要掌握的内容更多,深度也更深.我这里主要是想抛砖引玉,为大家提供一点自己的理解,若能有所帮助,不胜荣幸.</p>\n<p>另外,工具始终是工具,菜刀再利也要厨子好,才能做好菜.所以如何利用这些工具,与我们的业务结合,实现我们想要的价值,这是我一直在探索的,也愿与各位同仁一同前行.</p>\n<p>附上图中涉及到的技术栈</p>\n<p><img src=\"http://img.wqkenqing.ren/2019-04-17-09-09-00.png\" alt=\"2019-04-17-09-09-00\"></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"开头语\"><a href=\"#开头语\" class=\"headerlink\" title=\"开头语\"></a>开头语</h2><p><code>工欲善其事，必先利其器</code></p>","more":"<p>本次分享,是我在公司的第一次分享,我考虑后,将本次分享的主要内容分为了三大部块.先是针对相关基础组件分类介绍.再介绍下通过对这些组件进行组织配搭的大数据基础环境架构.再结合我的一些经历,为大家介绍下相关的应用与产品落地.</p>\n<h2 id=\"技术栈简介\"><a href=\"#技术栈简介\" class=\"headerlink\" title=\"技术栈简介\"></a>技术栈简介</h2><ul>\n<li>数据采集</li>\n<li>数据存储</li>\n<li>数据治理(清洗&amp;处理)</li>\n<li>数据应用</li>\n<li>产品落地</li>\n</ul>\n<p>我又根据不同组件的特性将他们分</p>\n<ul>\n<li>采集类</li>\n<li>存储类</li>\n<li>计算处理类</li>\n<li>传输类</li>\n<li>管理类 </li>\n<li>其它类</li>\n</ul>\n<p>下面开始具体介绍</p>\n<h2 id=\"采集类\"><a href=\"#采集类\" class=\"headerlink\" title=\"采集类\"></a>采集类</h2><p>数据源:</p>\n<ul>\n<li>日志</li>\n<li>业务数据</li>\n<li>公网数据(爬虫)</li>\n<li>文本数据</li>\n<li>出行数据(gps,手机定位等)</li>\n</ul>\n<p><img src=\"http://img.wqkenqing.ren/2019-04-15-10-36-53.png\" alt=\"2019-04-15-10-36-53\"></p>\n<ul>\n<li>sqoop flume crawler datax kettle  elk</li>\n</ul>\n<ol>\n<li>Flume(水槽) 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单,可拓展</li>\n<li>Sqoop是一个在结构化数据和Hadoop之间进行批量数据迁移的工具，结构化数据可以是Mysql、Oracle等RDBMS。Sqoop底层用MapReduce程序实现抽取、转换、加载，MapReduce天生的特性保证了并行化和高容错率，而且相比Kettle等传统ETL工具，任务跑在Hadoop集群上，减少了ETL服务器资源的使用情况。在特定场景下，抽取过程会有很大的性能提升。</li>\n<li>crawler , jsoup ,httpclient, nutch 等.</li>\n<li>elk  集中式日志系统 ELK 协议栈详解<br><img src=\"http://img.wqkenqing.ren/2019-04-15-10-11-16.png\" alt=\"2019-04-15-10-11-16\"></li>\n</ol>\n<hr>\n<h2 id=\"存储类\"><a href=\"#存储类\" class=\"headerlink\" title=\"存储类\"></a>存储类</h2><ul>\n<li>hdfs</li>\n<li>hbase</li>\n<li>hive</li>\n<li>mongdb</li>\n<li>redis</li>\n<li>RDBMS</li>\n</ul>\n<h3 id=\"hdfs\"><a href=\"#hdfs\" class=\"headerlink\" title=\"hdfs\"></a>hdfs</h3><pre><code>* 分布式文件存储系统\n* 提供了高可靠性、高扩展性和高吞吐率的数据存储服务\n* hdfs典型结构：物理结构+逻辑结构\n\n* 文件线性切割成Block：偏移量（offset）\n* Block分散存储在集群节点中\n* 单一文件Block大小一致，文件与文件可以不一致\n* Block可以设置副本数，副本分散在不同的节点中\n* 副本数不要超过节点数量\n* 文件上传可以设置Block大小和副本数\n* 已上传的文件Block副本数可以调整，大小不变\n* 只支持一次写入多次读取，同一时刻只有一个写入者\n* 只能追加，不能修改</code></pre><p><img src=\"http://img.wqkenqing.ren/2019-04-15-11-12-40.png\" alt=\"2019-04-15-11-12-40\"></p>\n<h3 id=\"hbase\"><a href=\"#hbase\" class=\"headerlink\" title=\"hbase\"></a>hbase</h3><p>Base是一个构建在HDFS上的分布式列存储系统；<br>Base是基于Google BigTable模型开发的，典型的key/value系统；<br>Base是Apache Hadoop生态系统中的重要一员，主要用于海量结构化数据存储；</p>\n<p>大：一个表可以有数十亿行，上百万列；无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有截然不同的列；面向列：面向列（族）的存储和权限控制，列（族）独立检索；稀疏：空（null）列并不占用存储空间，表可以设计的非常稀疏；数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳；数据类型单一：Hbase中的数据都是字符串，没有类型</p>\n<h4 id=\"openTSDB\"><a href=\"#openTSDB\" class=\"headerlink\" title=\"openTSDB\"></a>openTSDB</h4><p>基于Hbase的分布式的，可伸缩的时间序列数据库。<br>主要用途，就是做监控系统；譬如收集大规模集群（包括网络设备、操作系统、应用程序）的监控数据并进行存储，查询。<br><img src=\"http://img.wqkenqing.ren/2019-04-15-11-27-46.png\" alt=\"2019-04-15-11-27-46\"></p>\n<h4 id=\"solr-amp-Phoenix\"><a href=\"#solr-amp-Phoenix\" class=\"headerlink\" title=\"solr &amp; Phoenix\"></a>solr &amp; Phoenix</h4><p>二级索引</p>\n<h3 id=\"hive\"><a href=\"#hive\" class=\"headerlink\" title=\"hive\"></a>hive</h3><p>ive 是一个基于 Hadoop 文件系统之上的数据仓库架构。它可以将结构化的数据文件映射为一张数据库表，并提供简单的 sql 查询功能。还可以将 sql 语句转换为 MapReduce 任务运行。<br>底部计算引擎还可以用用Tez, spark等.<br><img src=\"http://img.wqkenqing.ren/2019-04-15-11-36-43.png\" alt=\"2019-04-15-11-36-43\"></p>\n<h5 id=\"Impala\"><a href=\"#Impala\" class=\"headerlink\" title=\"Impala\"></a>Impala</h5><p>Impala是Cloudera公司推出，提供对HDFS、Hbase数据的高性能、低延迟的交互式SQL查询功能。</p>\n<ul>\n<li>基于Hive使用内存计算，兼顾数据仓库、具有实时、批处理、多并发等优点</li>\n<li>对内存依赖大,稳定性不如hive</li>\n</ul>\n<p>相比hive数据仓库,impala针对的量级相关少些,但会有效率的提升.但一般来讲,数据仓库一类需求对时间上的要要求一般不会太高,所以常规方式一般就符合大多数需求.</p>\n<h2 id=\"计算处理类\"><a href=\"#计算处理类\" class=\"headerlink\" title=\"计算处理类\"></a>计算处理类</h2><ul>\n<li>mapreduce</li>\n<li>mapreduce on oozie ,on tez </li>\n<li>spark </li>\n<li>flink</li>\n</ul>\n<h3 id=\"mapreduce\"><a href=\"#mapreduce\" class=\"headerlink\" title=\"mapreduce\"></a>mapreduce</h3><p>Mapreduce是一个计算框架，既然是做计算的框架，那么表现形式就是有个输入（input），mapreduce操作这个输（input），通过本身定义好的计算模型，得到一个输出（output），这个输出就是我们所需要的结果。我们要学习的就是这个计算模型的运行规则。在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。而程序员要做的就是定义好这两个阶段的函数：map函数和reduce函数。</p>\n<p>分布式计算；<br>移动计算而不移动数据。<br><img src=\"http://img.wqkenqing.ren/2019-04-15-11-59-56.png\" alt=\"2019-04-15-11-59-56\"></p>\n<h3 id=\"spark\"><a href=\"#spark\" class=\"headerlink\" title=\"spark\"></a>spark</h3><p>相比一二代计算引擎,在兼并了一二代的特色之外,还引放了流计算这一能力,还丰富了计算函数.<br>其中比较有代表性的主要就是spark&amp;storm.<br>也就是说这代计算引擎兼具无边界数据与有边界数据同样的处理能力.同时还具有DAG特性.<br>这里主要介绍spark</p>\n<p>spark主要组成有以下</p>\n<ul>\n<li>spark-core</li>\n<li>spark-streaming</li>\n<li>spark-sql</li>\n<li>spark-mlib</li>\n<li>spark-graphX。</li>\n</ul>\n<p>spark-core是一个提供内存计算的框架,其他的四大框架都是基于spark core上进行计算的,所以没有spark core,其他的框架是浮云.<br>spark-core的主要内容就是对RDD的操作<br>RDD的创建 -&gt;RDD的转换 -&gt;RDD的缓存 -&gt;RDD的行动 -&gt;RDD的输出</p>\n<p>spark-streaming中使用离散化流（discretized stream）作为抽象的表示，叫做DStream。它是随时间推移而收集数据的序列，每个时间段收集到的数据在DStream内部以一个RDD的形式存在。DStream支持从kafka，flume,hdfs,s3等获取输入。DStream也支持两种操作，即转化操作和输出操作</p>\n<p>spark-sql<br>Spark SQL 提供了查询结构化数据及计算结果等信息的接口.<br>查询结果以 Datasets and DataFrames 形式返回</p>\n<p>…</p>\n<h3 id=\"flink-blink\"><a href=\"#flink-blink\" class=\"headerlink\" title=\"flink/blink\"></a>flink/blink</h3><p>略</p>\n<h2 id=\"传输类\"><a href=\"#传输类\" class=\"headerlink\" title=\"传输类\"></a>传输类</h2><h3 id=\"kafka\"><a href=\"#kafka\" class=\"headerlink\" title=\"kafka\"></a>kafka</h3><p>Kafka是分布式发布-订阅消息系统,一个分布式的，可划分的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。日常中常与spark-streaming结合实用,为其提供无边界数据</p>\n<p><img src=\"http://img.wqkenqing.ren/2019-04-16-15-06-25.png\" alt=\"2019-04-16-15-06-25\"></p>\n<h2 id=\"管理类-Hue-cloudera-manager\"><a href=\"#管理类-Hue-cloudera-manager\" class=\"headerlink\" title=\"管理类 Hue cloudera-manager\"></a>管理类 Hue cloudera-manager</h2><p>ue与cm 都是由cloudera提供,后面cloudera将hue开源给了apache.如果基础集群环境是采用的是开源自主搭建,可考虑引入hue.另一些大数据服务公司,有集成打包自己的一些大数据产品,如cdh等.但这些服务收费,涉及到成本问题.所以如何选用,需要相关斟酌.</p>\n<h2 id=\"其它类-zookeeper-yarn等\"><a href=\"#其它类-zookeeper-yarn等\" class=\"headerlink\" title=\"其它类 zookeeper ,yarn等\"></a>其它类 zookeeper ,yarn等</h2><p>zookeeper在集中基础环境中主要作为配置分享中心,与kafka,hbase等组件集成.yarn则作为资源管理组件,可以与mapreduce ,spark等集成</p>\n<h2 id=\"各类组件架构\"><a href=\"#各类组件架构\" class=\"headerlink\" title=\"各类组件架构\"></a>各类组件架构</h2><p>以上,已经大致介绍了各类工具,基本了解了相应的特性和使用场景,而根据它们的特性,进行合理的配备,架构,从而实现一个功能全面,稳定的大数据环境.</p>\n<p>于我个人经历与平时了解来讲,一般的架构主要如下<br><img src=\"http://img.wqkenqing.ren/2019-04-17-09-23-07.png\" alt=\"2019-04-17-09-23-07\"><br>另:<br><img src=\"http://img.wqkenqing.ren/2019-04-16-10-42-08.png\" alt=\"2019-04-16-10-42-08\"></p>\n<p>总得来说,各类组件供选型一般来讲都不是单一的.所以,我们的大数据环境各部份组件都是插销式可插拔的.所以不同公司可能不一而同,具体看自身需求和实际情况.比如上图中的storm流式计算模块,就可以替换成spark-streaming等.</p>\n<p>通过对上图的架构的拆解,再组合,可能还会有以下组织架构.</p>\n<p>数据仓库<br>可以理解为上图中间部份.作为一个数据集市的存在,算作数据中心的一部份.</p>\n<ul>\n<li>ODS：是数据仓库第一层数据，直接从原始数据过来的，经过简单地处理，比喻：字段体重的数据为175cm等数据。</li>\n<li>DW*：这个是数据仓库的第二层数据，DWD和DWS很多情况下是并列存在的，这一层储存经过处理后的标准数据，比喻订单、用户、页面点击流量等数据。</li>\n<li>ADS：这个是数据仓库的最后一层数据，为应用层数据，直接可以给业务人员使用。<br><img src=\"http://img.wqkenqing.ren/2019-04-16-11-01-33.png\" alt=\"2019-04-16-11-01-33\"></li>\n</ul>\n<p>星型模型</p>\n<p>星型模型中有两个重要的概念：事实表和维度表。<br>事实表：一些主键ID的集合，没有存放任何实际的内容<br>维度表：存放详细的数据信息，有唯一的主键ID。如上面的关键词表、用户表等等。<br><img src=\"http://img.wqkenqing.ren/2019-04-16-11-04-52.png\" alt=\"2019-04-16-11-04-52\"></p>\n<p> 数据中心:<br>概念相对更大一些,可能即作为具体平台产品集合,也可能是一个团队行政划分.总得来说,是如</p>\n<ul>\n<li>大数据基础平台</li>\n<li>数据仓库</li>\n<li>DMP平台</li>\n<li>相关应用平台如推荐系统,报表系统,可视化平台等.</li>\n</ul>\n<p>数据中台:<br>这个是由阿里于15年率先提出.主导思想是大中台,小前台.这块暂无特别明确的解释说法,但现在也有不少公司效仿.我个人从它的主导思想”大中台,小前台”的理解是,这个可能是体量更大,壁垒更少的一个数据集成体.比如阿里系的旗下公司,数据流都会归集到中台,同时阿里系下的公司也能获得不仅自己公司数据中心归集的数据反馈,还能获得阿里中台整合后流出的反馈数据.</p>\n<h2 id=\"应用落地\"><a href=\"#应用落地\" class=\"headerlink\" title=\"应用落地\"></a>应用落地</h2><h3 id=\"公共服务\"><a href=\"#公共服务\" class=\"headerlink\" title=\"公共服务\"></a>公共服务</h3><ul>\n<li>交通出行<br><img src=\"http://img.wqkenqing.ren/2019-04-16-16-15-42.png\" alt=\"2019-04-16-16-15-42\"></li>\n<li>智慧城市</li>\n<li>…</li>\n</ul>\n<h3 id=\"产品应用\"><a href=\"#产品应用\" class=\"headerlink\" title=\"产品应用\"></a>产品应用</h3><ul>\n<li>用户画像</li>\n<li>征信模型</li>\n<li>推荐系统</li>\n<li>精确营销</li>\n<li>前沿科学(无人驾驶,人工智能,AR等)</li>\n</ul>\n<h2 id=\"结语\"><a href=\"#结语\" class=\"headerlink\" title=\"结语\"></a>结语</h2><p>以上,就是我今天分享的主要内容.今天的主题是”器”,但对这些工具的讲解浅尝辄止,在实际的开发实战中涉及的情况是更为复杂,需要掌握的内容更多,深度也更深.我这里主要是想抛砖引玉,为大家提供一点自己的理解,若能有所帮助,不胜荣幸.</p>\n<p>另外,工具始终是工具,菜刀再利也要厨子好,才能做好菜.所以如何利用这些工具,与我们的业务结合,实现我们想要的价值,这是我一直在探索的,也愿与各位同仁一同前行.</p>\n<p>附上图中涉及到的技术栈</p>\n<p><img src=\"http://img.wqkenqing.ren/2019-04-17-09-09-00.png\" alt=\"2019-04-17-09-09-00\"></p>"},{"title":"maven小结","date":"2019-07-15T16:00:00.000Z","_content":"\n此处简介\n\n<!--more-->\n\n## maven小结\n### 什么是maven\n就是一款帮助程序员构建项目的工具,我们只需要告诉Maven需要哪些Jar 包，它会帮助我们下载所有的Jar，极大提升开发效率。\n\n### Maven规定的目录结构\n\n\n### Maven基本命令\n+ -v:查询Maven版本\n`本命令用于检查maven是否安装成功。Maven安装完成之后，在命令行输入mvn -v，若出现maven信息，则说明安装成功`\n\n+ compile：编译\n+ test:测试项目\n+ package:打包\n+ clean:删除target文件夹\n+ install:安装 将当前项目放到Maven的本地仓库中。供其他项目使用\n\n### 什么是Maven仓库？\nMaven仓库用来存放Maven管理的所有Jar包。分为：**本地仓库** 和 **本地仓库**。\n+ 本地仓库\nMaven本地的Jar包仓库。\n+ 中央仓库\nMaven官方提供的远程仓库。\n>当项目编译时，Maven首先从本地仓库中寻找项目所需的Jar包，若本地仓库没有，再到Maven的中央仓库下载所需Jar包。\n\n什么是“坐标”？\n在Maven中，坐标是Jar包的唯一标识，Maven通过坐标在仓库中找到项目所需的Jar包。\n\n如下代码中，groupId和artifactId构成了一个Jar包的坐标。\n\n```\n<dependency>\n    <groupId>ch.qos.logback</groupId>\n    <artifactId>logback-classic</artifactId>\n    <version>1.1.1</version>\n</dependency>\n```\n\n**groupId**:所需Jar包的项目名\n**artifactId**:所需Jar包的模块名\n**version**:所需Jar包的版本号\n\n传递依赖 与 排除依赖\n+ 传递依赖：如果我们的项目引用了一个Jar包，而该Jar包又引用了其他Jar包，那么在默认情况下项目编译时，Maven会把直接引用和简洁引用的Jar包都下载到本地。\n+ 排除依赖：如果我们只想下载直接引用的Jar包，那么需要在pom.xml中做如下配置：(将需要排除的Jar包的坐标写在中)\n\n```\n<exclusions>\n    <exclusion>\n        <groupId>ch.qos.logback</groupId>\n        <artifactId>logback-classic</artifactId>\n    </exclusion>\n</exclusions>\n```\n\n依赖冲突\n若项目中多个Jar同时引用了相同的Jar时，会产生依赖冲突，但Maven采用了两种避免冲突的策略，因此在Maven中是不存在依赖冲突的。\n短路优先\n本项目——>A.jar——>B.jar——>X.jar\n本项目——>C.jar——>X.jar\n声明优先\n若引用路径长度相同时，在pom.xml中谁先被声明，就使用谁。\n\n聚合\n什么是聚合？\n将多个项目同时运行就称为聚合。\n如何实现聚合？\n只需在pom中作如下配置即可实现聚合：\n\n```\n<modules>\n        <module>../模块1</module>\n        <module>../模块2</module>\n        <module>../模块3</module>\n    </modules>\n```\n继承\n什么是继承？\n在聚合多个项目时，如果这些被聚合的项目中需要引入相同的Jar，那么可以将这些Jar写入父pom中，各个子项目继承该pom即可。\n如何实现继承？\n父pom配置：将需要继承的Jar包的坐标放入标签即可。\n\n```\n<dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.apache.shiro</groupId>\n                <artifactId>shiro-spring</artifactId>\n                <version>1.2.2</version>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n```\n\n子pom配置：\n\n```\n<parent>\n    <groupId>父pom所在项目的groupId</groupId>\n    <artifactId>父pom所在项目的artifactId</artifactId>\n    <version>父pom所在项目的版本号</version>\n</parent>\n```\n\n---\nMaven本地资源库\n\n>通常情况下，可改变默认的 .m2 目录下的默认本地存储库文件夹到其他更有意义的名称，例如\n![Alt text](./1489394095038.png)\n\n当你建立一个 Maven 的项目，Maven 会检查你的 pom.xml 文件，以确定哪些依赖下载。首先，Maven 将从本地资源库获得 Maven 的本地资源库依赖资源，如果没有找到，然后把它会从默认的 Maven 中央存储库 – http://repo1.maven.org/maven2/ 查找下载","source":"_posts/技术/hexo/oldblog/blog13.md","raw":"---\n\ntitle: maven小结\ndate: 2019-07-16\ntags:\n\n---\n\n此处简介\n\n<!--more-->\n\n## maven小结\n### 什么是maven\n就是一款帮助程序员构建项目的工具,我们只需要告诉Maven需要哪些Jar 包，它会帮助我们下载所有的Jar，极大提升开发效率。\n\n### Maven规定的目录结构\n\n\n### Maven基本命令\n+ -v:查询Maven版本\n`本命令用于检查maven是否安装成功。Maven安装完成之后，在命令行输入mvn -v，若出现maven信息，则说明安装成功`\n\n+ compile：编译\n+ test:测试项目\n+ package:打包\n+ clean:删除target文件夹\n+ install:安装 将当前项目放到Maven的本地仓库中。供其他项目使用\n\n### 什么是Maven仓库？\nMaven仓库用来存放Maven管理的所有Jar包。分为：**本地仓库** 和 **本地仓库**。\n+ 本地仓库\nMaven本地的Jar包仓库。\n+ 中央仓库\nMaven官方提供的远程仓库。\n>当项目编译时，Maven首先从本地仓库中寻找项目所需的Jar包，若本地仓库没有，再到Maven的中央仓库下载所需Jar包。\n\n什么是“坐标”？\n在Maven中，坐标是Jar包的唯一标识，Maven通过坐标在仓库中找到项目所需的Jar包。\n\n如下代码中，groupId和artifactId构成了一个Jar包的坐标。\n\n```\n<dependency>\n    <groupId>ch.qos.logback</groupId>\n    <artifactId>logback-classic</artifactId>\n    <version>1.1.1</version>\n</dependency>\n```\n\n**groupId**:所需Jar包的项目名\n**artifactId**:所需Jar包的模块名\n**version**:所需Jar包的版本号\n\n传递依赖 与 排除依赖\n+ 传递依赖：如果我们的项目引用了一个Jar包，而该Jar包又引用了其他Jar包，那么在默认情况下项目编译时，Maven会把直接引用和简洁引用的Jar包都下载到本地。\n+ 排除依赖：如果我们只想下载直接引用的Jar包，那么需要在pom.xml中做如下配置：(将需要排除的Jar包的坐标写在中)\n\n```\n<exclusions>\n    <exclusion>\n        <groupId>ch.qos.logback</groupId>\n        <artifactId>logback-classic</artifactId>\n    </exclusion>\n</exclusions>\n```\n\n依赖冲突\n若项目中多个Jar同时引用了相同的Jar时，会产生依赖冲突，但Maven采用了两种避免冲突的策略，因此在Maven中是不存在依赖冲突的。\n短路优先\n本项目——>A.jar——>B.jar——>X.jar\n本项目——>C.jar——>X.jar\n声明优先\n若引用路径长度相同时，在pom.xml中谁先被声明，就使用谁。\n\n聚合\n什么是聚合？\n将多个项目同时运行就称为聚合。\n如何实现聚合？\n只需在pom中作如下配置即可实现聚合：\n\n```\n<modules>\n        <module>../模块1</module>\n        <module>../模块2</module>\n        <module>../模块3</module>\n    </modules>\n```\n继承\n什么是继承？\n在聚合多个项目时，如果这些被聚合的项目中需要引入相同的Jar，那么可以将这些Jar写入父pom中，各个子项目继承该pom即可。\n如何实现继承？\n父pom配置：将需要继承的Jar包的坐标放入标签即可。\n\n```\n<dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.apache.shiro</groupId>\n                <artifactId>shiro-spring</artifactId>\n                <version>1.2.2</version>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n```\n\n子pom配置：\n\n```\n<parent>\n    <groupId>父pom所在项目的groupId</groupId>\n    <artifactId>父pom所在项目的artifactId</artifactId>\n    <version>父pom所在项目的版本号</version>\n</parent>\n```\n\n---\nMaven本地资源库\n\n>通常情况下，可改变默认的 .m2 目录下的默认本地存储库文件夹到其他更有意义的名称，例如\n![Alt text](./1489394095038.png)\n\n当你建立一个 Maven 的项目，Maven 会检查你的 pom.xml 文件，以确定哪些依赖下载。首先，Maven 将从本地资源库获得 Maven 的本地资源库依赖资源，如果没有找到，然后把它会从默认的 Maven 中央存储库 – http://repo1.maven.org/maven2/ 查找下载","slug":"技术/hexo/oldblog/blog13","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd338002m38pw99nq4mar","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h2 id=\"maven小结\"><a href=\"#maven小结\" class=\"headerlink\" title=\"maven小结\"></a>maven小结</h2><h3 id=\"什么是maven\"><a href=\"#什么是maven\" class=\"headerlink\" title=\"什么是maven\"></a>什么是maven</h3><p>就是一款帮助程序员构建项目的工具,我们只需要告诉Maven需要哪些Jar 包，它会帮助我们下载所有的Jar，极大提升开发效率。</p>\n<h3 id=\"Maven规定的目录结构\"><a href=\"#Maven规定的目录结构\" class=\"headerlink\" title=\"Maven规定的目录结构\"></a>Maven规定的目录结构</h3><h3 id=\"Maven基本命令\"><a href=\"#Maven基本命令\" class=\"headerlink\" title=\"Maven基本命令\"></a>Maven基本命令</h3><ul>\n<li><p>-v:查询Maven版本<br><code>本命令用于检查maven是否安装成功。Maven安装完成之后，在命令行输入mvn -v，若出现maven信息，则说明安装成功</code></p>\n</li>\n<li><p>compile：编译</p>\n</li>\n<li><p>test:测试项目</p>\n</li>\n<li><p>package:打包</p>\n</li>\n<li><p>clean:删除target文件夹</p>\n</li>\n<li><p>install:安装 将当前项目放到Maven的本地仓库中。供其他项目使用</p>\n</li>\n</ul>\n<h3 id=\"什么是Maven仓库？\"><a href=\"#什么是Maven仓库？\" class=\"headerlink\" title=\"什么是Maven仓库？\"></a>什么是Maven仓库？</h3><p>Maven仓库用来存放Maven管理的所有Jar包。分为：<strong>本地仓库</strong> 和 <strong>本地仓库</strong>。</p>\n<ul>\n<li>本地仓库<br>Maven本地的Jar包仓库。</li>\n<li>中央仓库<br>Maven官方提供的远程仓库。<blockquote>\n<p>当项目编译时，Maven首先从本地仓库中寻找项目所需的Jar包，若本地仓库没有，再到Maven的中央仓库下载所需Jar包。</p>\n</blockquote>\n</li>\n</ul>\n<p>什么是“坐标”？<br>在Maven中，坐标是Jar包的唯一标识，Maven通过坐标在仓库中找到项目所需的Jar包。</p>\n<p>如下代码中，groupId和artifactId构成了一个Jar包的坐标。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;ch.qos.logback&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;logback-classic&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;1.1.1&lt;&#x2F;version&gt;</span><br><span class=\"line\">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>\n\n<p><strong>groupId</strong>:所需Jar包的项目名<br><strong>artifactId</strong>:所需Jar包的模块名<br><strong>version</strong>:所需Jar包的版本号</p>\n<p>传递依赖 与 排除依赖</p>\n<ul>\n<li>传递依赖：如果我们的项目引用了一个Jar包，而该Jar包又引用了其他Jar包，那么在默认情况下项目编译时，Maven会把直接引用和简洁引用的Jar包都下载到本地。</li>\n<li>排除依赖：如果我们只想下载直接引用的Jar包，那么需要在pom.xml中做如下配置：(将需要排除的Jar包的坐标写在中)</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;exclusions&gt;</span><br><span class=\"line\">    &lt;exclusion&gt;</span><br><span class=\"line\">        &lt;groupId&gt;ch.qos.logback&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">        &lt;artifactId&gt;logback-classic&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">    &lt;&#x2F;exclusion&gt;</span><br><span class=\"line\">&lt;&#x2F;exclusions&gt;</span><br></pre></td></tr></table></figure>\n\n<p>依赖冲突<br>若项目中多个Jar同时引用了相同的Jar时，会产生依赖冲突，但Maven采用了两种避免冲突的策略，因此在Maven中是不存在依赖冲突的。<br>短路优先<br>本项目——&gt;A.jar——&gt;B.jar——&gt;X.jar<br>本项目——&gt;C.jar——&gt;X.jar<br>声明优先<br>若引用路径长度相同时，在pom.xml中谁先被声明，就使用谁。</p>\n<p>聚合<br>什么是聚合？<br>将多个项目同时运行就称为聚合。<br>如何实现聚合？<br>只需在pom中作如下配置即可实现聚合：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;modules&gt;</span><br><span class=\"line\">        &lt;module&gt;..&#x2F;模块1&lt;&#x2F;module&gt;</span><br><span class=\"line\">        &lt;module&gt;..&#x2F;模块2&lt;&#x2F;module&gt;</span><br><span class=\"line\">        &lt;module&gt;..&#x2F;模块3&lt;&#x2F;module&gt;</span><br><span class=\"line\">    &lt;&#x2F;modules&gt;</span><br></pre></td></tr></table></figure>\n<p>继承<br>什么是继承？<br>在聚合多个项目时，如果这些被聚合的项目中需要引入相同的Jar，那么可以将这些Jar写入父pom中，各个子项目继承该pom即可。<br>如何实现继承？<br>父pom配置：将需要继承的Jar包的坐标放入标签即可。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependencyManagement&gt;</span><br><span class=\"line\">        &lt;dependencies&gt;</span><br><span class=\"line\">            &lt;dependency&gt;</span><br><span class=\"line\">                &lt;groupId&gt;org.apache.shiro&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">                &lt;artifactId&gt;shiro-spring&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">                &lt;version&gt;1.2.2&lt;&#x2F;version&gt;</span><br><span class=\"line\">            &lt;&#x2F;dependency&gt;</span><br><span class=\"line\">        &lt;&#x2F;dependencies&gt;</span><br><span class=\"line\">    &lt;&#x2F;dependencyManagement&gt;</span><br></pre></td></tr></table></figure>\n\n<p>子pom配置：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;parent&gt;</span><br><span class=\"line\">    &lt;groupId&gt;父pom所在项目的groupId&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;父pom所在项目的artifactId&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;父pom所在项目的版本号&lt;&#x2F;version&gt;</span><br><span class=\"line\">&lt;&#x2F;parent&gt;</span><br></pre></td></tr></table></figure>\n\n<hr>\n<p>Maven本地资源库</p>\n<blockquote>\n<p>通常情况下，可改变默认的 .m2 目录下的默认本地存储库文件夹到其他更有意义的名称，例如<br><img src=\"./1489394095038.png\" alt=\"Alt text\"></p>\n</blockquote>\n<p>当你建立一个 Maven 的项目，Maven 会检查你的 pom.xml 文件，以确定哪些依赖下载。首先，Maven 将从本地资源库获得 Maven 的本地资源库依赖资源，如果没有找到，然后把它会从默认的 Maven 中央存储库 – <a href=\"http://repo1.maven.org/maven2/\" target=\"_blank\" rel=\"noopener\">http://repo1.maven.org/maven2/</a> 查找下载</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h2 id=\"maven小结\"><a href=\"#maven小结\" class=\"headerlink\" title=\"maven小结\"></a>maven小结</h2><h3 id=\"什么是maven\"><a href=\"#什么是maven\" class=\"headerlink\" title=\"什么是maven\"></a>什么是maven</h3><p>就是一款帮助程序员构建项目的工具,我们只需要告诉Maven需要哪些Jar 包，它会帮助我们下载所有的Jar，极大提升开发效率。</p>\n<h3 id=\"Maven规定的目录结构\"><a href=\"#Maven规定的目录结构\" class=\"headerlink\" title=\"Maven规定的目录结构\"></a>Maven规定的目录结构</h3><h3 id=\"Maven基本命令\"><a href=\"#Maven基本命令\" class=\"headerlink\" title=\"Maven基本命令\"></a>Maven基本命令</h3><ul>\n<li><p>-v:查询Maven版本<br><code>本命令用于检查maven是否安装成功。Maven安装完成之后，在命令行输入mvn -v，若出现maven信息，则说明安装成功</code></p>\n</li>\n<li><p>compile：编译</p>\n</li>\n<li><p>test:测试项目</p>\n</li>\n<li><p>package:打包</p>\n</li>\n<li><p>clean:删除target文件夹</p>\n</li>\n<li><p>install:安装 将当前项目放到Maven的本地仓库中。供其他项目使用</p>\n</li>\n</ul>\n<h3 id=\"什么是Maven仓库？\"><a href=\"#什么是Maven仓库？\" class=\"headerlink\" title=\"什么是Maven仓库？\"></a>什么是Maven仓库？</h3><p>Maven仓库用来存放Maven管理的所有Jar包。分为：<strong>本地仓库</strong> 和 <strong>本地仓库</strong>。</p>\n<ul>\n<li>本地仓库<br>Maven本地的Jar包仓库。</li>\n<li>中央仓库<br>Maven官方提供的远程仓库。<blockquote>\n<p>当项目编译时，Maven首先从本地仓库中寻找项目所需的Jar包，若本地仓库没有，再到Maven的中央仓库下载所需Jar包。</p>\n</blockquote>\n</li>\n</ul>\n<p>什么是“坐标”？<br>在Maven中，坐标是Jar包的唯一标识，Maven通过坐标在仓库中找到项目所需的Jar包。</p>\n<p>如下代码中，groupId和artifactId构成了一个Jar包的坐标。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;ch.qos.logback&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;logback-classic&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;1.1.1&lt;&#x2F;version&gt;</span><br><span class=\"line\">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>\n\n<p><strong>groupId</strong>:所需Jar包的项目名<br><strong>artifactId</strong>:所需Jar包的模块名<br><strong>version</strong>:所需Jar包的版本号</p>\n<p>传递依赖 与 排除依赖</p>\n<ul>\n<li>传递依赖：如果我们的项目引用了一个Jar包，而该Jar包又引用了其他Jar包，那么在默认情况下项目编译时，Maven会把直接引用和简洁引用的Jar包都下载到本地。</li>\n<li>排除依赖：如果我们只想下载直接引用的Jar包，那么需要在pom.xml中做如下配置：(将需要排除的Jar包的坐标写在中)</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;exclusions&gt;</span><br><span class=\"line\">    &lt;exclusion&gt;</span><br><span class=\"line\">        &lt;groupId&gt;ch.qos.logback&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">        &lt;artifactId&gt;logback-classic&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">    &lt;&#x2F;exclusion&gt;</span><br><span class=\"line\">&lt;&#x2F;exclusions&gt;</span><br></pre></td></tr></table></figure>\n\n<p>依赖冲突<br>若项目中多个Jar同时引用了相同的Jar时，会产生依赖冲突，但Maven采用了两种避免冲突的策略，因此在Maven中是不存在依赖冲突的。<br>短路优先<br>本项目——&gt;A.jar——&gt;B.jar——&gt;X.jar<br>本项目——&gt;C.jar——&gt;X.jar<br>声明优先<br>若引用路径长度相同时，在pom.xml中谁先被声明，就使用谁。</p>\n<p>聚合<br>什么是聚合？<br>将多个项目同时运行就称为聚合。<br>如何实现聚合？<br>只需在pom中作如下配置即可实现聚合：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;modules&gt;</span><br><span class=\"line\">        &lt;module&gt;..&#x2F;模块1&lt;&#x2F;module&gt;</span><br><span class=\"line\">        &lt;module&gt;..&#x2F;模块2&lt;&#x2F;module&gt;</span><br><span class=\"line\">        &lt;module&gt;..&#x2F;模块3&lt;&#x2F;module&gt;</span><br><span class=\"line\">    &lt;&#x2F;modules&gt;</span><br></pre></td></tr></table></figure>\n<p>继承<br>什么是继承？<br>在聚合多个项目时，如果这些被聚合的项目中需要引入相同的Jar，那么可以将这些Jar写入父pom中，各个子项目继承该pom即可。<br>如何实现继承？<br>父pom配置：将需要继承的Jar包的坐标放入标签即可。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;dependencyManagement&gt;</span><br><span class=\"line\">        &lt;dependencies&gt;</span><br><span class=\"line\">            &lt;dependency&gt;</span><br><span class=\"line\">                &lt;groupId&gt;org.apache.shiro&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">                &lt;artifactId&gt;shiro-spring&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">                &lt;version&gt;1.2.2&lt;&#x2F;version&gt;</span><br><span class=\"line\">            &lt;&#x2F;dependency&gt;</span><br><span class=\"line\">        &lt;&#x2F;dependencies&gt;</span><br><span class=\"line\">    &lt;&#x2F;dependencyManagement&gt;</span><br></pre></td></tr></table></figure>\n\n<p>子pom配置：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;parent&gt;</span><br><span class=\"line\">    &lt;groupId&gt;父pom所在项目的groupId&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;父pom所在项目的artifactId&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;父pom所在项目的版本号&lt;&#x2F;version&gt;</span><br><span class=\"line\">&lt;&#x2F;parent&gt;</span><br></pre></td></tr></table></figure>\n\n<hr>\n<p>Maven本地资源库</p>\n<blockquote>\n<p>通常情况下，可改变默认的 .m2 目录下的默认本地存储库文件夹到其他更有意义的名称，例如<br><img src=\"./1489394095038.png\" alt=\"Alt text\"></p>\n</blockquote>\n<p>当你建立一个 Maven 的项目，Maven 会检查你的 pom.xml 文件，以确定哪些依赖下载。首先，Maven 将从本地资源库获得 Maven 的本地资源库依赖资源，如果没有找到，然后把它会从默认的 Maven 中央存储库 – <a href=\"http://repo1.maven.org/maven2/\" target=\"_blank\" rel=\"noopener\">http://repo1.maven.org/maven2/</a> 查找下载</p>"},{"title":"java八大数据类型总结","date":"2019-07-15T16:00:00.000Z","_content":"此处简介\n\n<!--more-->\n\n## java八大数据类型总结\n\n[TOC]\n\n\n`负数在电脑中的存储是用（该数值的绝对值的反码+1表示）`\n`最高位是符号位，1表示负数，0表示正数`\n`负数换算规则:负数的二进制=负数绝对值的二进制取反码+1。`\n### byte类型:\n        byte类型，使用一个字节存放一个数据，一个字节占八位，所以它取值范围是:1000 0000 ~ 0111 1111(-128-127)\n\n\n\n### short类型\n        short类型，使用两个字节存放一个数据，两个字节16位二进制表示，那么它的取值范围就是：\n    1000 0000 0000 0000 ~ 0111 1111 1111 1111(-32768-32767)换算同上述byte类型\n\n\n ---\n\n### char类型\n       char在java中占据两个字节，即用16位表示一个char类型的数据。由于char是无符号的所以其表示范围是0-65536.当计算超过其表示范围时，系统会自动将结果转换为int类型。\n\n### boolean类型:\n\t        boolean类型占用一个字节，八位二进制表示。boolean类型只有两个值true和flase。\n\n### float类型\n  其他特殊表示:\n\n     1.当指数部分和小数部分全为0时,表示0值,有+0和-0之分(符号位决定),0x00000000表示正0,0x80000000表示负0.\n    2.指数部分全1,小数部分全0时,表示无穷大,有正无穷和负无穷,0x7f800000表示正无穷,0xff800000表示负无穷.\n    3.指数部分全1,小数部分不全0时,表示NaN,分为QNaN和SNaN,Java中都是NaN.\n    可以看出浮点数的取值范围是:2^(-149)~~(2-2^(-23))*2^127,也就是Float.MIN_VALUE和Float.MAX_VALUE.\n\n### double类型\n        double类型占8个字节，一共是64位二进制表示。数符加尾数占48位，指数符加指数占16位.取值换算方式和float的换算方式一样。但是在使用float和double时最好先分析好目标数据的精度和性能要求，如果能够使用float满足的坚决不适用double，因为double类型使用内存占用是float的两倍，运算速度远不如float。\n\n### short类型\n        short类型，使用两个字节存放一个数据，两个字节16位二进制表示，那么它的取值范围就是：\n    1000 0000 0000 0000 ~ 0111 1111 1111 1111(-32768-32767)换算同上述byte类型。short类型使用时除了要注意取值范围\n\n### int类型\n        int类型，在Java中使用的是四个字节保存一个数据，一共是32为二进制表示，同上述的一样,取值范围:1000 0000 0000 0000 0000 0000 0000 0000 - 0111 1111 1111 1111 1111 1111 1111 1111\n        (-2^32~2^31)\n\n\n### long类型\n        long类型是Java的基础类型，使用8个字节存储一个数值，一共是64位二进制数。取值范围是（-2^64-2^63\n\n### 八大常用类型的最大值与最小值\n\n`\nfloat max=3.4028235E38\nfloat min=1.4E-45\ndouble max=1.7976931348623157E308\ndouble max=4.9E-324\nbyte max=127\nbyte min=-128\nchar max=?\nchar min=\nshort max=32767\nshort min=-32768\nint max=2147483647\nint min=-2147483648\nlong max=9223372036854775807\nlong min=-9223372036854775808\n`\n\n| 数据类型      | byte类型 |  short类型  |char类型|boolean类型|float类型|int类型|double类型|long类型|\n| :-------- | --------:| :------:\n|所占字节数  | 2| 2|2|8|4|4|8|8|","source":"_posts/技术/hexo/oldblog/blog1.md","raw":"\n---\n\ntitle: java八大数据类型总结\ndate: 2019-07-16\ntags:\n\n---\n此处简介\n\n<!--more-->\n\n## java八大数据类型总结\n\n[TOC]\n\n\n`负数在电脑中的存储是用（该数值的绝对值的反码+1表示）`\n`最高位是符号位，1表示负数，0表示正数`\n`负数换算规则:负数的二进制=负数绝对值的二进制取反码+1。`\n### byte类型:\n        byte类型，使用一个字节存放一个数据，一个字节占八位，所以它取值范围是:1000 0000 ~ 0111 1111(-128-127)\n\n\n\n### short类型\n        short类型，使用两个字节存放一个数据，两个字节16位二进制表示，那么它的取值范围就是：\n    1000 0000 0000 0000 ~ 0111 1111 1111 1111(-32768-32767)换算同上述byte类型\n\n\n ---\n\n### char类型\n       char在java中占据两个字节，即用16位表示一个char类型的数据。由于char是无符号的所以其表示范围是0-65536.当计算超过其表示范围时，系统会自动将结果转换为int类型。\n\n### boolean类型:\n\t        boolean类型占用一个字节，八位二进制表示。boolean类型只有两个值true和flase。\n\n### float类型\n  其他特殊表示:\n\n     1.当指数部分和小数部分全为0时,表示0值,有+0和-0之分(符号位决定),0x00000000表示正0,0x80000000表示负0.\n    2.指数部分全1,小数部分全0时,表示无穷大,有正无穷和负无穷,0x7f800000表示正无穷,0xff800000表示负无穷.\n    3.指数部分全1,小数部分不全0时,表示NaN,分为QNaN和SNaN,Java中都是NaN.\n    可以看出浮点数的取值范围是:2^(-149)~~(2-2^(-23))*2^127,也就是Float.MIN_VALUE和Float.MAX_VALUE.\n\n### double类型\n        double类型占8个字节，一共是64位二进制表示。数符加尾数占48位，指数符加指数占16位.取值换算方式和float的换算方式一样。但是在使用float和double时最好先分析好目标数据的精度和性能要求，如果能够使用float满足的坚决不适用double，因为double类型使用内存占用是float的两倍，运算速度远不如float。\n\n### short类型\n        short类型，使用两个字节存放一个数据，两个字节16位二进制表示，那么它的取值范围就是：\n    1000 0000 0000 0000 ~ 0111 1111 1111 1111(-32768-32767)换算同上述byte类型。short类型使用时除了要注意取值范围\n\n### int类型\n        int类型，在Java中使用的是四个字节保存一个数据，一共是32为二进制表示，同上述的一样,取值范围:1000 0000 0000 0000 0000 0000 0000 0000 - 0111 1111 1111 1111 1111 1111 1111 1111\n        (-2^32~2^31)\n\n\n### long类型\n        long类型是Java的基础类型，使用8个字节存储一个数值，一共是64位二进制数。取值范围是（-2^64-2^63\n\n### 八大常用类型的最大值与最小值\n\n`\nfloat max=3.4028235E38\nfloat min=1.4E-45\ndouble max=1.7976931348623157E308\ndouble max=4.9E-324\nbyte max=127\nbyte min=-128\nchar max=?\nchar min=\nshort max=32767\nshort min=-32768\nint max=2147483647\nint min=-2147483648\nlong max=9223372036854775807\nlong min=-9223372036854775808\n`\n\n| 数据类型      | byte类型 |  short类型  |char类型|boolean类型|float类型|int类型|double类型|long类型|\n| :-------- | --------:| :------:\n|所占字节数  | 2| 2|2|8|4|4|8|8|","slug":"技术/hexo/oldblog/blog1","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd339002p38pwfttjhtuh","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h2 id=\"java八大数据类型总结\"><a href=\"#java八大数据类型总结\" class=\"headerlink\" title=\"java八大数据类型总结\"></a>java八大数据类型总结</h2><p>[TOC]</p>\n<p><code>负数在电脑中的存储是用（该数值的绝对值的反码+1表示）</code><br><code>最高位是符号位，1表示负数，0表示正数</code><br><code>负数换算规则:负数的二进制=负数绝对值的二进制取反码+1。</code></p>\n<h3 id=\"byte类型\"><a href=\"#byte类型\" class=\"headerlink\" title=\"byte类型:\"></a>byte类型:</h3><pre><code>byte类型，使用一个字节存放一个数据，一个字节占八位，所以它取值范围是:1000 0000 ~ 0111 1111(-128-127)</code></pre><h3 id=\"short类型\"><a href=\"#short类型\" class=\"headerlink\" title=\"short类型\"></a>short类型</h3><pre><code>    short类型，使用两个字节存放一个数据，两个字节16位二进制表示，那么它的取值范围就是：\n1000 0000 0000 0000 ~ 0111 1111 1111 1111(-32768-32767)换算同上述byte类型</code></pre><hr>\n<h3 id=\"char类型\"><a href=\"#char类型\" class=\"headerlink\" title=\"char类型\"></a>char类型</h3><pre><code>char在java中占据两个字节，即用16位表示一个char类型的数据。由于char是无符号的所以其表示范围是0-65536.当计算超过其表示范围时，系统会自动将结果转换为int类型。</code></pre><h3 id=\"boolean类型\"><a href=\"#boolean类型\" class=\"headerlink\" title=\"boolean类型:\"></a>boolean类型:</h3><pre><code>boolean类型占用一个字节，八位二进制表示。boolean类型只有两个值true和flase。</code></pre><h3 id=\"float类型\"><a href=\"#float类型\" class=\"headerlink\" title=\"float类型\"></a>float类型</h3><p>  其他特殊表示:</p>\n<pre><code> 1.当指数部分和小数部分全为0时,表示0值,有+0和-0之分(符号位决定),0x00000000表示正0,0x80000000表示负0.\n2.指数部分全1,小数部分全0时,表示无穷大,有正无穷和负无穷,0x7f800000表示正无穷,0xff800000表示负无穷.\n3.指数部分全1,小数部分不全0时,表示NaN,分为QNaN和SNaN,Java中都是NaN.\n可以看出浮点数的取值范围是:2^(-149)~~(2-2^(-23))*2^127,也就是Float.MIN_VALUE和Float.MAX_VALUE.</code></pre><h3 id=\"double类型\"><a href=\"#double类型\" class=\"headerlink\" title=\"double类型\"></a>double类型</h3><pre><code>double类型占8个字节，一共是64位二进制表示。数符加尾数占48位，指数符加指数占16位.取值换算方式和float的换算方式一样。但是在使用float和double时最好先分析好目标数据的精度和性能要求，如果能够使用float满足的坚决不适用double，因为double类型使用内存占用是float的两倍，运算速度远不如float。</code></pre><h3 id=\"short类型-1\"><a href=\"#short类型-1\" class=\"headerlink\" title=\"short类型\"></a>short类型</h3><pre><code>    short类型，使用两个字节存放一个数据，两个字节16位二进制表示，那么它的取值范围就是：\n1000 0000 0000 0000 ~ 0111 1111 1111 1111(-32768-32767)换算同上述byte类型。short类型使用时除了要注意取值范围</code></pre><h3 id=\"int类型\"><a href=\"#int类型\" class=\"headerlink\" title=\"int类型\"></a>int类型</h3><pre><code>int类型，在Java中使用的是四个字节保存一个数据，一共是32为二进制表示，同上述的一样,取值范围:1000 0000 0000 0000 0000 0000 0000 0000 - 0111 1111 1111 1111 1111 1111 1111 1111\n(-2^32~2^31)</code></pre><h3 id=\"long类型\"><a href=\"#long类型\" class=\"headerlink\" title=\"long类型\"></a>long类型</h3><pre><code>long类型是Java的基础类型，使用8个字节存储一个数值，一共是64位二进制数。取值范围是（-2^64-2^63</code></pre><h3 id=\"八大常用类型的最大值与最小值\"><a href=\"#八大常用类型的最大值与最小值\" class=\"headerlink\" title=\"八大常用类型的最大值与最小值\"></a>八大常用类型的最大值与最小值</h3><p><code>float max=3.4028235E38\nfloat min=1.4E-45\ndouble max=1.7976931348623157E308\ndouble max=4.9E-324\nbyte max=127\nbyte min=-128\nchar max=?\nchar min=\nshort max=32767\nshort min=-32768\nint max=2147483647\nint min=-2147483648\nlong max=9223372036854775807\nlong min=-9223372036854775808</code></p>\n<p>| 数据类型      | byte类型 |  short类型  |char类型|boolean类型|float类型|int类型|double类型|long类型|<br>| :——– | ——–:| :——:<br>|所占字节数  | 2| 2|2|8|4|4|8|8|</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h2 id=\"java八大数据类型总结\"><a href=\"#java八大数据类型总结\" class=\"headerlink\" title=\"java八大数据类型总结\"></a>java八大数据类型总结</h2><p>[TOC]</p>\n<p><code>负数在电脑中的存储是用（该数值的绝对值的反码+1表示）</code><br><code>最高位是符号位，1表示负数，0表示正数</code><br><code>负数换算规则:负数的二进制=负数绝对值的二进制取反码+1。</code></p>\n<h3 id=\"byte类型\"><a href=\"#byte类型\" class=\"headerlink\" title=\"byte类型:\"></a>byte类型:</h3><pre><code>byte类型，使用一个字节存放一个数据，一个字节占八位，所以它取值范围是:1000 0000 ~ 0111 1111(-128-127)</code></pre><h3 id=\"short类型\"><a href=\"#short类型\" class=\"headerlink\" title=\"short类型\"></a>short类型</h3><pre><code>    short类型，使用两个字节存放一个数据，两个字节16位二进制表示，那么它的取值范围就是：\n1000 0000 0000 0000 ~ 0111 1111 1111 1111(-32768-32767)换算同上述byte类型</code></pre><hr>\n<h3 id=\"char类型\"><a href=\"#char类型\" class=\"headerlink\" title=\"char类型\"></a>char类型</h3><pre><code>char在java中占据两个字节，即用16位表示一个char类型的数据。由于char是无符号的所以其表示范围是0-65536.当计算超过其表示范围时，系统会自动将结果转换为int类型。</code></pre><h3 id=\"boolean类型\"><a href=\"#boolean类型\" class=\"headerlink\" title=\"boolean类型:\"></a>boolean类型:</h3><pre><code>boolean类型占用一个字节，八位二进制表示。boolean类型只有两个值true和flase。</code></pre><h3 id=\"float类型\"><a href=\"#float类型\" class=\"headerlink\" title=\"float类型\"></a>float类型</h3><p>  其他特殊表示:</p>\n<pre><code> 1.当指数部分和小数部分全为0时,表示0值,有+0和-0之分(符号位决定),0x00000000表示正0,0x80000000表示负0.\n2.指数部分全1,小数部分全0时,表示无穷大,有正无穷和负无穷,0x7f800000表示正无穷,0xff800000表示负无穷.\n3.指数部分全1,小数部分不全0时,表示NaN,分为QNaN和SNaN,Java中都是NaN.\n可以看出浮点数的取值范围是:2^(-149)~~(2-2^(-23))*2^127,也就是Float.MIN_VALUE和Float.MAX_VALUE.</code></pre><h3 id=\"double类型\"><a href=\"#double类型\" class=\"headerlink\" title=\"double类型\"></a>double类型</h3><pre><code>double类型占8个字节，一共是64位二进制表示。数符加尾数占48位，指数符加指数占16位.取值换算方式和float的换算方式一样。但是在使用float和double时最好先分析好目标数据的精度和性能要求，如果能够使用float满足的坚决不适用double，因为double类型使用内存占用是float的两倍，运算速度远不如float。</code></pre><h3 id=\"short类型-1\"><a href=\"#short类型-1\" class=\"headerlink\" title=\"short类型\"></a>short类型</h3><pre><code>    short类型，使用两个字节存放一个数据，两个字节16位二进制表示，那么它的取值范围就是：\n1000 0000 0000 0000 ~ 0111 1111 1111 1111(-32768-32767)换算同上述byte类型。short类型使用时除了要注意取值范围</code></pre><h3 id=\"int类型\"><a href=\"#int类型\" class=\"headerlink\" title=\"int类型\"></a>int类型</h3><pre><code>int类型，在Java中使用的是四个字节保存一个数据，一共是32为二进制表示，同上述的一样,取值范围:1000 0000 0000 0000 0000 0000 0000 0000 - 0111 1111 1111 1111 1111 1111 1111 1111\n(-2^32~2^31)</code></pre><h3 id=\"long类型\"><a href=\"#long类型\" class=\"headerlink\" title=\"long类型\"></a>long类型</h3><pre><code>long类型是Java的基础类型，使用8个字节存储一个数值，一共是64位二进制数。取值范围是（-2^64-2^63</code></pre><h3 id=\"八大常用类型的最大值与最小值\"><a href=\"#八大常用类型的最大值与最小值\" class=\"headerlink\" title=\"八大常用类型的最大值与最小值\"></a>八大常用类型的最大值与最小值</h3><p><code>float max=3.4028235E38\nfloat min=1.4E-45\ndouble max=1.7976931348623157E308\ndouble max=4.9E-324\nbyte max=127\nbyte min=-128\nchar max=?\nchar min=\nshort max=32767\nshort min=-32768\nint max=2147483647\nint min=-2147483648\nlong max=9223372036854775807\nlong min=-9223372036854775808</code></p>\n<p>| 数据类型      | byte类型 |  short类型  |char类型|boolean类型|float类型|int类型|double类型|long类型|<br>| :——– | ——–:| :——:<br>|所占字节数  | 2| 2|2|8|4|4|8|8|</p>"},{"title":"git小结","date":"2019-07-15T16:00:00.000Z","_content":"\n此处简介\n\n<!--more-->\n\n\n## git小结\n### 常用指令小结\n+ git init 初始化\n+ git add 将工作区的变更提交至暂存区\n+ git commit -m 将暂存区的内容提交至版本库\n+ git log 查看记录\n+ git reflog 操作记录\n+ git --hard commit id 回到对应的id版本下\n+ git status 查看状态\n+ git log --pretty=oneline 简约查看log\n+ git diff 查看文件与版本库中的差异\n+ git checkout -- file 文件在工作区的修改全部撤销\n+ git remote add origin git@github.com:michaelliao/learngit.git 添加远程库\n+ git push -u origin master 推送分支\n+ git clone git@github.com:michaelliao/gitskills.git 克隆\n+ git checkout -b dev 创建并切换分支\n+ git branch dev 创建分支\n+ git checkout dev  切换分支\n+ git merge dev 合并分支\n+ git branch -d  删除分支\n+ git stash 紧急切分支时，将工作区的变更内容暂存起来。\n+ git stash list 查看stash列表\n+ git stash apply 回复当前分支stash内容\n+ git stash pop 删除stash内容\n+ git branch -D 强行删除\n+ git checkout -b dev origin/dev 拉下远程分支\n+ git push origin branch-name 推送分支\n+ git pull 从远程库拉取更新\n+ git push origin branch-name 将本地库中的更新推送至远程库\n+ git branch --set-upstream branch-name origin/branch-name 建立本地分支与远程库的联系\n+ git tag <name>用于新建一个标签，默认为HEAD，也可以指定一个commit id；打标签\n+ git tag -a <tagname> -m \"blablabla...\"可以指定标签信息；\n+ git tag -s <tagname> -m \"blablabla...\"可以用PGP签名标签；\n+ git push origin <tagname>\n+ git push origin --tags\n+ git tag -d <tagname>\n+ git push origin :refs/tags/<tagname>","source":"_posts/技术/hexo/oldblog/blog12.md","raw":"---\n\ntitle: git小结\ndate: 2019-07-16\ntags:\n\n---\n\n此处简介\n\n<!--more-->\n\n\n## git小结\n### 常用指令小结\n+ git init 初始化\n+ git add 将工作区的变更提交至暂存区\n+ git commit -m 将暂存区的内容提交至版本库\n+ git log 查看记录\n+ git reflog 操作记录\n+ git --hard commit id 回到对应的id版本下\n+ git status 查看状态\n+ git log --pretty=oneline 简约查看log\n+ git diff 查看文件与版本库中的差异\n+ git checkout -- file 文件在工作区的修改全部撤销\n+ git remote add origin git@github.com:michaelliao/learngit.git 添加远程库\n+ git push -u origin master 推送分支\n+ git clone git@github.com:michaelliao/gitskills.git 克隆\n+ git checkout -b dev 创建并切换分支\n+ git branch dev 创建分支\n+ git checkout dev  切换分支\n+ git merge dev 合并分支\n+ git branch -d  删除分支\n+ git stash 紧急切分支时，将工作区的变更内容暂存起来。\n+ git stash list 查看stash列表\n+ git stash apply 回复当前分支stash内容\n+ git stash pop 删除stash内容\n+ git branch -D 强行删除\n+ git checkout -b dev origin/dev 拉下远程分支\n+ git push origin branch-name 推送分支\n+ git pull 从远程库拉取更新\n+ git push origin branch-name 将本地库中的更新推送至远程库\n+ git branch --set-upstream branch-name origin/branch-name 建立本地分支与远程库的联系\n+ git tag <name>用于新建一个标签，默认为HEAD，也可以指定一个commit id；打标签\n+ git tag -a <tagname> -m \"blablabla...\"可以指定标签信息；\n+ git tag -s <tagname> -m \"blablabla...\"可以用PGP签名标签；\n+ git push origin <tagname>\n+ git push origin --tags\n+ git tag -d <tagname>\n+ git push origin :refs/tags/<tagname>","slug":"技术/hexo/oldblog/blog12","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33a002r38pw4r0f2kax","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n\n<h2 id=\"git小结\"><a href=\"#git小结\" class=\"headerlink\" title=\"git小结\"></a>git小结</h2><h3 id=\"常用指令小结\"><a href=\"#常用指令小结\" class=\"headerlink\" title=\"常用指令小结\"></a>常用指令小结</h3><ul>\n<li>git init 初始化</li>\n<li>git add 将工作区的变更提交至暂存区</li>\n<li>git commit -m 将暂存区的内容提交至版本库</li>\n<li>git log 查看记录</li>\n<li>git reflog 操作记录</li>\n<li>git –hard commit id 回到对应的id版本下</li>\n<li>git status 查看状态</li>\n<li>git log –pretty=oneline 简约查看log</li>\n<li>git diff 查看文件与版本库中的差异</li>\n<li>git checkout – file 文件在工作区的修改全部撤销</li>\n<li>git remote add origin <a href=\"mailto:git@github.com\">git@github.com</a>:michaelliao/learngit.git 添加远程库</li>\n<li>git push -u origin master 推送分支</li>\n<li>git clone <a href=\"mailto:git@github.com\">git@github.com</a>:michaelliao/gitskills.git 克隆</li>\n<li>git checkout -b dev 创建并切换分支</li>\n<li>git branch dev 创建分支</li>\n<li>git checkout dev  切换分支</li>\n<li>git merge dev 合并分支</li>\n<li>git branch -d  删除分支</li>\n<li>git stash 紧急切分支时，将工作区的变更内容暂存起来。</li>\n<li>git stash list 查看stash列表</li>\n<li>git stash apply 回复当前分支stash内容</li>\n<li>git stash pop 删除stash内容</li>\n<li>git branch -D 强行删除</li>\n<li>git checkout -b dev origin/dev 拉下远程分支</li>\n<li>git push origin branch-name 推送分支</li>\n<li>git pull 从远程库拉取更新</li>\n<li>git push origin branch-name 将本地库中的更新推送至远程库</li>\n<li>git branch –set-upstream branch-name origin/branch-name 建立本地分支与远程库的联系</li>\n<li>git tag <name>用于新建一个标签，默认为HEAD，也可以指定一个commit id；打标签</li>\n<li>git tag -a <tagname> -m “blablabla…”可以指定标签信息；</li>\n<li>git tag -s <tagname> -m “blablabla…”可以用PGP签名标签；</li>\n<li>git push origin <tagname></li>\n<li>git push origin –tags</li>\n<li>git tag -d <tagname></li>\n<li>git push origin :refs/tags/<tagname></li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h2 id=\"git小结\"><a href=\"#git小结\" class=\"headerlink\" title=\"git小结\"></a>git小结</h2><h3 id=\"常用指令小结\"><a href=\"#常用指令小结\" class=\"headerlink\" title=\"常用指令小结\"></a>常用指令小结</h3><ul>\n<li>git init 初始化</li>\n<li>git add 将工作区的变更提交至暂存区</li>\n<li>git commit -m 将暂存区的内容提交至版本库</li>\n<li>git log 查看记录</li>\n<li>git reflog 操作记录</li>\n<li>git –hard commit id 回到对应的id版本下</li>\n<li>git status 查看状态</li>\n<li>git log –pretty=oneline 简约查看log</li>\n<li>git diff 查看文件与版本库中的差异</li>\n<li>git checkout – file 文件在工作区的修改全部撤销</li>\n<li>git remote add origin <a href=\"mailto:git@github.com\">git@github.com</a>:michaelliao/learngit.git 添加远程库</li>\n<li>git push -u origin master 推送分支</li>\n<li>git clone <a href=\"mailto:git@github.com\">git@github.com</a>:michaelliao/gitskills.git 克隆</li>\n<li>git checkout -b dev 创建并切换分支</li>\n<li>git branch dev 创建分支</li>\n<li>git checkout dev  切换分支</li>\n<li>git merge dev 合并分支</li>\n<li>git branch -d  删除分支</li>\n<li>git stash 紧急切分支时，将工作区的变更内容暂存起来。</li>\n<li>git stash list 查看stash列表</li>\n<li>git stash apply 回复当前分支stash内容</li>\n<li>git stash pop 删除stash内容</li>\n<li>git branch -D 强行删除</li>\n<li>git checkout -b dev origin/dev 拉下远程分支</li>\n<li>git push origin branch-name 推送分支</li>\n<li>git pull 从远程库拉取更新</li>\n<li>git push origin branch-name 将本地库中的更新推送至远程库</li>\n<li>git branch –set-upstream branch-name origin/branch-name 建立本地分支与远程库的联系</li>\n<li>git tag <name>用于新建一个标签，默认为HEAD，也可以指定一个commit id；打标签</li>\n<li>git tag -a <tagname> -m “blablabla…”可以指定标签信息；</li>\n<li>git tag -s <tagname> -m “blablabla…”可以用PGP签名标签；</li>\n<li>git push origin <tagname></li>\n<li>git push origin –tags</li>\n<li>git tag -d <tagname></li>\n<li>git push origin :refs/tags/<tagname></li>\n</ul>"},{"title":"文本处理小结","date":"2019-07-15T16:00:00.000Z","_content":"此处简介\n\n<!--more-->\n\n## 文本处理小结\n`此处的文本处理主要针对的是日常工作中主要遇到的一些场景小结，小结主要围绕技能展开，但不局限于某一技术点，`\n\n### 文本处理的主要类型\n1. 排序\n2. 去重\n3. 合并\n4. 切割\n4. 取集\n5. 打乱排序\n6. 模糊匹配\n7. 替换\n\n\n---\n\n```\n总得来说，目前主要的文本处理手段有linux指令、shell脚本、java脚本、mr脚本、excel、sublime等几类，下现主要也围绕这部份展开\n```\n### 排序\n目前主要涉及到的排序手段是linux中的sort指令，所以这里对sort进行展开小结\n####sort排序\nsort排序主要的操作有\n```sort命令是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出。sort命令既可以从特定的文件，也可以从stdin中获取输入。```\n\n\n#####sort 指令后面常接的Options\n\n![3baeaf03a24f4874818f343d3893d863-image.png](//img.wqkenqing.ren/file/2017/7/3baeaf03a24f4874818f343d3893d863-image.png)\n\n\n\n\n##### sort原理\nsort将文件/文本的每一行作为一个单位，相互比较，比较原则是从首字符向后，依次按ASCII码值进行比较，最后将他们按升序输出。\n\n排序有时往往伴随着去重，而sort则对应的有去重指令\n即sort -u file:忽略相同行(这里的-u其实以对应的是unique，而unique的主要作用还是去重，所以在去重部份再展开总结)\n\n##### sort的常用指令\n(-n、-r、-k、-t)\n-t:指定分隔符\n-n:指定以按数字的大小的形式进行排序\n-k:指定按那一列\n-r:-r是以相反顺序\n![5bf55dc045d846bcbe32e00202b80cbc-image.png](//img.wqkenqing.ren/file/2017/7/5bf55dc045d846bcbe32e00202b80cbc-image.png)\n\n\n```careful```\n-k 有一些复杂用法，即\n\n-k选项的语法格式： FStart.CStart Modifie,FEnd.CEnd Modifier -------Start--------,-------End-------- FStart.CStart 选项 , FEnd.CEnd 选项 这个语法格式可以被其中的逗号,分为两大部分，Start部分和End部分。Start部分也由三部分组成，其中的Modifier部分就是我们之前说过的类似n和r的选项部分。我们重点说说Start部分的FStart和C.Start。C.Start也是可以省略的，省略的话就表示从本域的开头部分开始。FStart.CStart，其中FStart就是表示使用的域，而CStart则表示在FStart域中从第几个字符开始算“排序首字符”。同理，在End部分中，你可以设定FEnd.CEnd，如果你省略.CEnd，则表示结尾到“域尾”，即本域的最后一个字符。或者，如果你将CEnd设定为0(零)，也是表示结尾到“域尾”。 从公司英文名称的第二个字母开始进行排序： $ sort -t ' ' -k 1.2 facebook.txt baidu 100 5000 sohu 100 4500 google 110 5000 guge 50 3000 使用了-k 1.2，表示对第一个域的第二个字符开始到本域的最后一个字符为止的字符串进行排序。你会发现baidu因为第二个字母是a而名列榜首。sohu和 google第二个字符都是o，但sohu的h在google的o前面，所以两者分别排在第二和第三。guge只能屈居第四了。 只针对公司英文名称的第二个字母进行排序，如果相同的按照员工工资进行降序排序： $ sort -t ' ' -k 1.2,1.2 -nrk 3,3 facebook.txt baidu 100 5000 google 110 5000 sohu 100 4500 guge 50 3000 由于只对第二个字母进行排序，所以我们使用了-k 1.2,1.2的表示方式，表示我们“只”对第二个字母进行排序。（如果你问“我使用-k 1.2怎么不行？”，当然不行，因为你省略了End部分，这就意味着你将对从第二个字母起到本域最后一个字符为止的字符串进行排序）。对于员工工资进行排 序，我们也使用了-k 3,3，这是最准确的表述，表示我们“只”对本域进行排序，因为如果你省略了后面的3，就变成了我们“对第3个域开始到最后一个域位置的内容进行排序” 了。\n\n---\n总得来说Linux的sort排序功能就能满足绝大部份应用场景\n\n---\n\n### 去重\n\n```排序完后，往往涉及到的场景更多的是去重```\n相较于排序而言，日常中去重的手段会更多一些\n\n#### 去重(linux方式)\n\nlinux中的去重指令首先是刚才在前文本提到的\n\nsort -u:对文本进行排序，去重，并对重复的只保留一份。\n\n而在日常中，结合去重可能会产生更多的应用场景，即取交集、并集等\n\n大致来讲linux 主要的去重指令是uniq\n\n##### uniq\n\nuniq的Options主要有\n![540273a5039941a39598a747888f4948-image.png](//img.wqkenqing.ren/file/2017/7/540273a5039941a39598a747888f4948-image.png)\n\n![10bff3119aaa49de976350596f94f4bf-image.png](//img.wqkenqing.ren/file/2017/7/10bff3119aaa49de976350596f94f4bf-image.png)\n\n\n\nuniq :默认只是将重复的保留一行\n\n而通过uniq实现交集与并集主要通过-d与-u实现\n\nuniq -d是只显示重复出现的行列\nuniq -u是只显示不重复的列\n\n这里要注意uniq -f -s的使用\nuniq -f nubmber :即指定忽略多少栏位开如计重\n\nuniq -s number:即指定忽略多少字任开始计重\n uniq -f -s :同时出现时则按先按栏位移，再按字符移。\n\n通过uniq实现去重要先排序\n\n---\n针对时常出现的应用场景提供一个思路\n两个文本中有重复内容，但只想去掉前一个文件与后一个文件中的重复内容，保留前一个文件中的非重复内容\n可以通过\n\nsort file1 file2 | uniq  -d >temp\n\nsort file1 temp|uniq -u\n\n```即思路是:先将两个文件中的重复内容找出并写入临时文件，再将前一个文件与临时文件合并，排序去重，保留只出现一次的文件内容```\n\n\n---\n##### 其它去重方式\n\n要去重，通过java方式也能轻松实现\n即主要利用set等的非重复内容的特性。进行实现\n\n\n### 切割\n\n即将文件进行切割，出于取样，或测试的需求考虑，可能需要从大文本中切割出一些小文件来。\n对文件的切割也有很多实现方式\n但从实现方式上更推荐linux指令式\n\n#### 切割linux指令式\n\n涉及到切割的linux指令主要有split\n\nsplit命令可以将一个大文件分割成很多个小文件，有时需要将文件分割成更小的片段，比如为提高可读性，生成日志等。\n\n![Alt text](./1493197221096.png)\n\n具体实现都是指令式，需要注意的地方较少，不记得时则翻阅相关文档\n\n#### 切割的其它方式\n\n主要擅长的还有java方式\n\n---\n\n### 打乱排序\n\n```\n打乱排序主要用的方式有awk与excel的方式\n```\n\n\n#### awk方式\n\n\nawk 'BEGIN{ 100000*srand();}{ printf \"%s %s\\n\", rand(), $0}'  t |sort -k1n | awk '{gsub($1FS,\"\"); print $0}'\n\n#### excel方式\n\n即通过在文本中再另加一列，生成随机数，然后对随机数列进行排序从列达到打乱的效果\n\n\n---\n\n\n### 模糊匹配\n\n```文本的模糊匹配有较多应该场景,可以再多总结```\n\n模糊匹配主要有在shell脚本中针对contains操作\n\n和vim中的匹配操作\n\n---","source":"_posts/技术/hexo/oldblog/blog14.md","raw":"\n---\n\ntitle: 文本处理小结\ndate: 2019-07-16\ntags:\n\n---\n此处简介\n\n<!--more-->\n\n## 文本处理小结\n`此处的文本处理主要针对的是日常工作中主要遇到的一些场景小结，小结主要围绕技能展开，但不局限于某一技术点，`\n\n### 文本处理的主要类型\n1. 排序\n2. 去重\n3. 合并\n4. 切割\n4. 取集\n5. 打乱排序\n6. 模糊匹配\n7. 替换\n\n\n---\n\n```\n总得来说，目前主要的文本处理手段有linux指令、shell脚本、java脚本、mr脚本、excel、sublime等几类，下现主要也围绕这部份展开\n```\n### 排序\n目前主要涉及到的排序手段是linux中的sort指令，所以这里对sort进行展开小结\n####sort排序\nsort排序主要的操作有\n```sort命令是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出。sort命令既可以从特定的文件，也可以从stdin中获取输入。```\n\n\n#####sort 指令后面常接的Options\n\n![3baeaf03a24f4874818f343d3893d863-image.png](//img.wqkenqing.ren/file/2017/7/3baeaf03a24f4874818f343d3893d863-image.png)\n\n\n\n\n##### sort原理\nsort将文件/文本的每一行作为一个单位，相互比较，比较原则是从首字符向后，依次按ASCII码值进行比较，最后将他们按升序输出。\n\n排序有时往往伴随着去重，而sort则对应的有去重指令\n即sort -u file:忽略相同行(这里的-u其实以对应的是unique，而unique的主要作用还是去重，所以在去重部份再展开总结)\n\n##### sort的常用指令\n(-n、-r、-k、-t)\n-t:指定分隔符\n-n:指定以按数字的大小的形式进行排序\n-k:指定按那一列\n-r:-r是以相反顺序\n![5bf55dc045d846bcbe32e00202b80cbc-image.png](//img.wqkenqing.ren/file/2017/7/5bf55dc045d846bcbe32e00202b80cbc-image.png)\n\n\n```careful```\n-k 有一些复杂用法，即\n\n-k选项的语法格式： FStart.CStart Modifie,FEnd.CEnd Modifier -------Start--------,-------End-------- FStart.CStart 选项 , FEnd.CEnd 选项 这个语法格式可以被其中的逗号,分为两大部分，Start部分和End部分。Start部分也由三部分组成，其中的Modifier部分就是我们之前说过的类似n和r的选项部分。我们重点说说Start部分的FStart和C.Start。C.Start也是可以省略的，省略的话就表示从本域的开头部分开始。FStart.CStart，其中FStart就是表示使用的域，而CStart则表示在FStart域中从第几个字符开始算“排序首字符”。同理，在End部分中，你可以设定FEnd.CEnd，如果你省略.CEnd，则表示结尾到“域尾”，即本域的最后一个字符。或者，如果你将CEnd设定为0(零)，也是表示结尾到“域尾”。 从公司英文名称的第二个字母开始进行排序： $ sort -t ' ' -k 1.2 facebook.txt baidu 100 5000 sohu 100 4500 google 110 5000 guge 50 3000 使用了-k 1.2，表示对第一个域的第二个字符开始到本域的最后一个字符为止的字符串进行排序。你会发现baidu因为第二个字母是a而名列榜首。sohu和 google第二个字符都是o，但sohu的h在google的o前面，所以两者分别排在第二和第三。guge只能屈居第四了。 只针对公司英文名称的第二个字母进行排序，如果相同的按照员工工资进行降序排序： $ sort -t ' ' -k 1.2,1.2 -nrk 3,3 facebook.txt baidu 100 5000 google 110 5000 sohu 100 4500 guge 50 3000 由于只对第二个字母进行排序，所以我们使用了-k 1.2,1.2的表示方式，表示我们“只”对第二个字母进行排序。（如果你问“我使用-k 1.2怎么不行？”，当然不行，因为你省略了End部分，这就意味着你将对从第二个字母起到本域最后一个字符为止的字符串进行排序）。对于员工工资进行排 序，我们也使用了-k 3,3，这是最准确的表述，表示我们“只”对本域进行排序，因为如果你省略了后面的3，就变成了我们“对第3个域开始到最后一个域位置的内容进行排序” 了。\n\n---\n总得来说Linux的sort排序功能就能满足绝大部份应用场景\n\n---\n\n### 去重\n\n```排序完后，往往涉及到的场景更多的是去重```\n相较于排序而言，日常中去重的手段会更多一些\n\n#### 去重(linux方式)\n\nlinux中的去重指令首先是刚才在前文本提到的\n\nsort -u:对文本进行排序，去重，并对重复的只保留一份。\n\n而在日常中，结合去重可能会产生更多的应用场景，即取交集、并集等\n\n大致来讲linux 主要的去重指令是uniq\n\n##### uniq\n\nuniq的Options主要有\n![540273a5039941a39598a747888f4948-image.png](//img.wqkenqing.ren/file/2017/7/540273a5039941a39598a747888f4948-image.png)\n\n![10bff3119aaa49de976350596f94f4bf-image.png](//img.wqkenqing.ren/file/2017/7/10bff3119aaa49de976350596f94f4bf-image.png)\n\n\n\nuniq :默认只是将重复的保留一行\n\n而通过uniq实现交集与并集主要通过-d与-u实现\n\nuniq -d是只显示重复出现的行列\nuniq -u是只显示不重复的列\n\n这里要注意uniq -f -s的使用\nuniq -f nubmber :即指定忽略多少栏位开如计重\n\nuniq -s number:即指定忽略多少字任开始计重\n uniq -f -s :同时出现时则按先按栏位移，再按字符移。\n\n通过uniq实现去重要先排序\n\n---\n针对时常出现的应用场景提供一个思路\n两个文本中有重复内容，但只想去掉前一个文件与后一个文件中的重复内容，保留前一个文件中的非重复内容\n可以通过\n\nsort file1 file2 | uniq  -d >temp\n\nsort file1 temp|uniq -u\n\n```即思路是:先将两个文件中的重复内容找出并写入临时文件，再将前一个文件与临时文件合并，排序去重，保留只出现一次的文件内容```\n\n\n---\n##### 其它去重方式\n\n要去重，通过java方式也能轻松实现\n即主要利用set等的非重复内容的特性。进行实现\n\n\n### 切割\n\n即将文件进行切割，出于取样，或测试的需求考虑，可能需要从大文本中切割出一些小文件来。\n对文件的切割也有很多实现方式\n但从实现方式上更推荐linux指令式\n\n#### 切割linux指令式\n\n涉及到切割的linux指令主要有split\n\nsplit命令可以将一个大文件分割成很多个小文件，有时需要将文件分割成更小的片段，比如为提高可读性，生成日志等。\n\n![Alt text](./1493197221096.png)\n\n具体实现都是指令式，需要注意的地方较少，不记得时则翻阅相关文档\n\n#### 切割的其它方式\n\n主要擅长的还有java方式\n\n---\n\n### 打乱排序\n\n```\n打乱排序主要用的方式有awk与excel的方式\n```\n\n\n#### awk方式\n\n\nawk 'BEGIN{ 100000*srand();}{ printf \"%s %s\\n\", rand(), $0}'  t |sort -k1n | awk '{gsub($1FS,\"\"); print $0}'\n\n#### excel方式\n\n即通过在文本中再另加一列，生成随机数，然后对随机数列进行排序从列达到打乱的效果\n\n\n---\n\n\n### 模糊匹配\n\n```文本的模糊匹配有较多应该场景,可以再多总结```\n\n模糊匹配主要有在shell脚本中针对contains操作\n\n和vim中的匹配操作\n\n---","slug":"技术/hexo/oldblog/blog14","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33a002s38pwcbe2edz0","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h2 id=\"文本处理小结\"><a href=\"#文本处理小结\" class=\"headerlink\" title=\"文本处理小结\"></a>文本处理小结</h2><p><code>此处的文本处理主要针对的是日常工作中主要遇到的一些场景小结，小结主要围绕技能展开，但不局限于某一技术点，</code></p>\n<h3 id=\"文本处理的主要类型\"><a href=\"#文本处理的主要类型\" class=\"headerlink\" title=\"文本处理的主要类型\"></a>文本处理的主要类型</h3><ol>\n<li>排序</li>\n<li>去重</li>\n<li>合并</li>\n<li>切割</li>\n<li>取集</li>\n<li>打乱排序</li>\n<li>模糊匹配</li>\n<li>替换</li>\n</ol>\n<hr>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">总得来说，目前主要的文本处理手段有linux指令、shell脚本、java脚本、mr脚本、excel、sublime等几类，下现主要也围绕这部份展开</span><br></pre></td></tr></table></figure>\n<h3 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h3><p>目前主要涉及到的排序手段是linux中的sort指令，所以这里对sort进行展开小结<br>####sort排序<br>sort排序主要的操作有</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">#####sort 指令后面常接的Options</span><br><span class=\"line\"></span><br><span class=\"line\">![3baeaf03a24f4874818f343d3893d863-image.png](&#x2F;&#x2F;img.wqkenqing.ren&#x2F;file&#x2F;2017&#x2F;7&#x2F;3baeaf03a24f4874818f343d3893d863-image.png)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">##### sort原理</span><br><span class=\"line\">sort将文件&#x2F;文本的每一行作为一个单位，相互比较，比较原则是从首字符向后，依次按ASCII码值进行比较，最后将他们按升序输出。</span><br><span class=\"line\"></span><br><span class=\"line\">排序有时往往伴随着去重，而sort则对应的有去重指令</span><br><span class=\"line\">即sort -u file:忽略相同行(这里的-u其实以对应的是unique，而unique的主要作用还是去重，所以在去重部份再展开总结)</span><br><span class=\"line\"></span><br><span class=\"line\">##### sort的常用指令</span><br><span class=\"line\">(-n、-r、-k、-t)</span><br><span class=\"line\">-t:指定分隔符</span><br><span class=\"line\">-n:指定以按数字的大小的形式进行排序</span><br><span class=\"line\">-k:指定按那一列</span><br><span class=\"line\">-r:-r是以相反顺序</span><br><span class=\"line\">![5bf55dc045d846bcbe32e00202b80cbc-image.png](&#x2F;&#x2F;img.wqkenqing.ren&#x2F;file&#x2F;2017&#x2F;7&#x2F;5bf55dc045d846bcbe32e00202b80cbc-image.png)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&#96;&#96;&#96;careful</span><br></pre></td></tr></table></figure>\n<p>-k 有一些复杂用法，即</p>\n<p>-k选项的语法格式： FStart.CStart Modifie,FEnd.CEnd Modifier ——-Start——–,——-End——– FStart.CStart 选项 , FEnd.CEnd 选项 这个语法格式可以被其中的逗号,分为两大部分，Start部分和End部分。Start部分也由三部分组成，其中的Modifier部分就是我们之前说过的类似n和r的选项部分。我们重点说说Start部分的FStart和C.Start。C.Start也是可以省略的，省略的话就表示从本域的开头部分开始。FStart.CStart，其中FStart就是表示使用的域，而CStart则表示在FStart域中从第几个字符开始算“排序首字符”。同理，在End部分中，你可以设定FEnd.CEnd，如果你省略.CEnd，则表示结尾到“域尾”，即本域的最后一个字符。或者，如果你将CEnd设定为0(零)，也是表示结尾到“域尾”。 从公司英文名称的第二个字母开始进行排序： $ sort -t ‘ ‘ -k 1.2 facebook.txt baidu 100 5000 sohu 100 4500 google 110 5000 guge 50 3000 使用了-k 1.2，表示对第一个域的第二个字符开始到本域的最后一个字符为止的字符串进行排序。你会发现baidu因为第二个字母是a而名列榜首。sohu和 google第二个字符都是o，但sohu的h在google的o前面，所以两者分别排在第二和第三。guge只能屈居第四了。 只针对公司英文名称的第二个字母进行排序，如果相同的按照员工工资进行降序排序： $ sort -t ‘ ‘ -k 1.2,1.2 -nrk 3,3 facebook.txt baidu 100 5000 google 110 5000 sohu 100 4500 guge 50 3000 由于只对第二个字母进行排序，所以我们使用了-k 1.2,1.2的表示方式，表示我们“只”对第二个字母进行排序。（如果你问“我使用-k 1.2怎么不行？”，当然不行，因为你省略了End部分，这就意味着你将对从第二个字母起到本域最后一个字符为止的字符串进行排序）。对于员工工资进行排 序，我们也使用了-k 3,3，这是最准确的表述，表示我们“只”对本域进行排序，因为如果你省略了后面的3，就变成了我们“对第3个域开始到最后一个域位置的内容进行排序” 了。</p>\n<hr>\n<p>总得来说Linux的sort排序功能就能满足绝大部份应用场景</p>\n<hr>\n<h3 id=\"去重\"><a href=\"#去重\" class=\"headerlink\" title=\"去重\"></a>去重</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">相较于排序而言，日常中去重的手段会更多一些</span><br><span class=\"line\"></span><br><span class=\"line\">#### 去重(linux方式)</span><br><span class=\"line\"></span><br><span class=\"line\">linux中的去重指令首先是刚才在前文本提到的</span><br><span class=\"line\"></span><br><span class=\"line\">sort -u:对文本进行排序，去重，并对重复的只保留一份。</span><br><span class=\"line\"></span><br><span class=\"line\">而在日常中，结合去重可能会产生更多的应用场景，即取交集、并集等</span><br><span class=\"line\"></span><br><span class=\"line\">大致来讲linux 主要的去重指令是uniq</span><br><span class=\"line\"></span><br><span class=\"line\">##### uniq</span><br><span class=\"line\"></span><br><span class=\"line\">uniq的Options主要有</span><br><span class=\"line\">![540273a5039941a39598a747888f4948-image.png](&#x2F;&#x2F;img.wqkenqing.ren&#x2F;file&#x2F;2017&#x2F;7&#x2F;540273a5039941a39598a747888f4948-image.png)</span><br><span class=\"line\"></span><br><span class=\"line\">![10bff3119aaa49de976350596f94f4bf-image.png](&#x2F;&#x2F;img.wqkenqing.ren&#x2F;file&#x2F;2017&#x2F;7&#x2F;10bff3119aaa49de976350596f94f4bf-image.png)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">uniq :默认只是将重复的保留一行</span><br><span class=\"line\"></span><br><span class=\"line\">而通过uniq实现交集与并集主要通过-d与-u实现</span><br><span class=\"line\"></span><br><span class=\"line\">uniq -d是只显示重复出现的行列</span><br><span class=\"line\">uniq -u是只显示不重复的列</span><br><span class=\"line\"></span><br><span class=\"line\">这里要注意uniq -f -s的使用</span><br><span class=\"line\">uniq -f nubmber :即指定忽略多少栏位开如计重</span><br><span class=\"line\"></span><br><span class=\"line\">uniq -s number:即指定忽略多少字任开始计重</span><br><span class=\"line\"> uniq -f -s :同时出现时则按先按栏位移，再按字符移。</span><br><span class=\"line\"></span><br><span class=\"line\">通过uniq实现去重要先排序</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\">针对时常出现的应用场景提供一个思路</span><br><span class=\"line\">两个文本中有重复内容，但只想去掉前一个文件与后一个文件中的重复内容，保留前一个文件中的非重复内容</span><br><span class=\"line\">可以通过</span><br><span class=\"line\"></span><br><span class=\"line\">sort file1 file2 | uniq  -d &gt;temp</span><br><span class=\"line\"></span><br><span class=\"line\">sort file1 temp|uniq -u</span><br><span class=\"line\"></span><br><span class=\"line\">&#96;&#96;&#96;即思路是:先将两个文件中的重复内容找出并写入临时文件，再将前一个文件与临时文件合并，排序去重，保留只出现一次的文件内容</span><br></pre></td></tr></table></figure>\n\n\n<hr>\n<h5 id=\"其它去重方式\"><a href=\"#其它去重方式\" class=\"headerlink\" title=\"其它去重方式\"></a>其它去重方式</h5><p>要去重，通过java方式也能轻松实现<br>即主要利用set等的非重复内容的特性。进行实现</p>\n<h3 id=\"切割\"><a href=\"#切割\" class=\"headerlink\" title=\"切割\"></a>切割</h3><p>即将文件进行切割，出于取样，或测试的需求考虑，可能需要从大文本中切割出一些小文件来。<br>对文件的切割也有很多实现方式<br>但从实现方式上更推荐linux指令式</p>\n<h4 id=\"切割linux指令式\"><a href=\"#切割linux指令式\" class=\"headerlink\" title=\"切割linux指令式\"></a>切割linux指令式</h4><p>涉及到切割的linux指令主要有split</p>\n<p>split命令可以将一个大文件分割成很多个小文件，有时需要将文件分割成更小的片段，比如为提高可读性，生成日志等。</p>\n<p><img src=\"./1493197221096.png\" alt=\"Alt text\"></p>\n<p>具体实现都是指令式，需要注意的地方较少，不记得时则翻阅相关文档</p>\n<h4 id=\"切割的其它方式\"><a href=\"#切割的其它方式\" class=\"headerlink\" title=\"切割的其它方式\"></a>切割的其它方式</h4><p>主要擅长的还有java方式</p>\n<hr>\n<h3 id=\"打乱排序\"><a href=\"#打乱排序\" class=\"headerlink\" title=\"打乱排序\"></a>打乱排序</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">打乱排序主要用的方式有awk与excel的方式</span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"awk方式\"><a href=\"#awk方式\" class=\"headerlink\" title=\"awk方式\"></a>awk方式</h4><p>awk ‘BEGIN{ 100000*srand();}{ printf “%s %s\\n”, rand(), $0}’  t |sort -k1n | awk ‘{gsub($1FS,””); print $0}’</p>\n<h4 id=\"excel方式\"><a href=\"#excel方式\" class=\"headerlink\" title=\"excel方式\"></a>excel方式</h4><p>即通过在文本中再另加一列，生成随机数，然后对随机数列进行排序从列达到打乱的效果</p>\n<hr>\n<h3 id=\"模糊匹配\"><a href=\"#模糊匹配\" class=\"headerlink\" title=\"模糊匹配\"></a>模糊匹配</h3><p><code>文本的模糊匹配有较多应该场景,可以再多总结</code></p>\n<p>模糊匹配主要有在shell脚本中针对contains操作</p>\n<p>和vim中的匹配操作</p>\n<hr>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h2 id=\"文本处理小结\"><a href=\"#文本处理小结\" class=\"headerlink\" title=\"文本处理小结\"></a>文本处理小结</h2><p><code>此处的文本处理主要针对的是日常工作中主要遇到的一些场景小结，小结主要围绕技能展开，但不局限于某一技术点，</code></p>\n<h3 id=\"文本处理的主要类型\"><a href=\"#文本处理的主要类型\" class=\"headerlink\" title=\"文本处理的主要类型\"></a>文本处理的主要类型</h3><ol>\n<li>排序</li>\n<li>去重</li>\n<li>合并</li>\n<li>切割</li>\n<li>取集</li>\n<li>打乱排序</li>\n<li>模糊匹配</li>\n<li>替换</li>\n</ol>\n<hr>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">总得来说，目前主要的文本处理手段有linux指令、shell脚本、java脚本、mr脚本、excel、sublime等几类，下现主要也围绕这部份展开</span><br></pre></td></tr></table></figure>\n<h3 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h3><p>目前主要涉及到的排序手段是linux中的sort指令，所以这里对sort进行展开小结<br>####sort排序<br>sort排序主要的操作有</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">#####sort 指令后面常接的Options</span><br><span class=\"line\"></span><br><span class=\"line\">![3baeaf03a24f4874818f343d3893d863-image.png](&#x2F;&#x2F;img.wqkenqing.ren&#x2F;file&#x2F;2017&#x2F;7&#x2F;3baeaf03a24f4874818f343d3893d863-image.png)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">##### sort原理</span><br><span class=\"line\">sort将文件&#x2F;文本的每一行作为一个单位，相互比较，比较原则是从首字符向后，依次按ASCII码值进行比较，最后将他们按升序输出。</span><br><span class=\"line\"></span><br><span class=\"line\">排序有时往往伴随着去重，而sort则对应的有去重指令</span><br><span class=\"line\">即sort -u file:忽略相同行(这里的-u其实以对应的是unique，而unique的主要作用还是去重，所以在去重部份再展开总结)</span><br><span class=\"line\"></span><br><span class=\"line\">##### sort的常用指令</span><br><span class=\"line\">(-n、-r、-k、-t)</span><br><span class=\"line\">-t:指定分隔符</span><br><span class=\"line\">-n:指定以按数字的大小的形式进行排序</span><br><span class=\"line\">-k:指定按那一列</span><br><span class=\"line\">-r:-r是以相反顺序</span><br><span class=\"line\">![5bf55dc045d846bcbe32e00202b80cbc-image.png](&#x2F;&#x2F;img.wqkenqing.ren&#x2F;file&#x2F;2017&#x2F;7&#x2F;5bf55dc045d846bcbe32e00202b80cbc-image.png)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">&#96;&#96;&#96;careful</span><br></pre></td></tr></table></figure>\n<p>-k 有一些复杂用法，即</p>\n<p>-k选项的语法格式： FStart.CStart Modifie,FEnd.CEnd Modifier ——-Start——–,——-End——– FStart.CStart 选项 , FEnd.CEnd 选项 这个语法格式可以被其中的逗号,分为两大部分，Start部分和End部分。Start部分也由三部分组成，其中的Modifier部分就是我们之前说过的类似n和r的选项部分。我们重点说说Start部分的FStart和C.Start。C.Start也是可以省略的，省略的话就表示从本域的开头部分开始。FStart.CStart，其中FStart就是表示使用的域，而CStart则表示在FStart域中从第几个字符开始算“排序首字符”。同理，在End部分中，你可以设定FEnd.CEnd，如果你省略.CEnd，则表示结尾到“域尾”，即本域的最后一个字符。或者，如果你将CEnd设定为0(零)，也是表示结尾到“域尾”。 从公司英文名称的第二个字母开始进行排序： $ sort -t ‘ ‘ -k 1.2 facebook.txt baidu 100 5000 sohu 100 4500 google 110 5000 guge 50 3000 使用了-k 1.2，表示对第一个域的第二个字符开始到本域的最后一个字符为止的字符串进行排序。你会发现baidu因为第二个字母是a而名列榜首。sohu和 google第二个字符都是o，但sohu的h在google的o前面，所以两者分别排在第二和第三。guge只能屈居第四了。 只针对公司英文名称的第二个字母进行排序，如果相同的按照员工工资进行降序排序： $ sort -t ‘ ‘ -k 1.2,1.2 -nrk 3,3 facebook.txt baidu 100 5000 google 110 5000 sohu 100 4500 guge 50 3000 由于只对第二个字母进行排序，所以我们使用了-k 1.2,1.2的表示方式，表示我们“只”对第二个字母进行排序。（如果你问“我使用-k 1.2怎么不行？”，当然不行，因为你省略了End部分，这就意味着你将对从第二个字母起到本域最后一个字符为止的字符串进行排序）。对于员工工资进行排 序，我们也使用了-k 3,3，这是最准确的表述，表示我们“只”对本域进行排序，因为如果你省略了后面的3，就变成了我们“对第3个域开始到最后一个域位置的内容进行排序” 了。</p>\n<hr>\n<p>总得来说Linux的sort排序功能就能满足绝大部份应用场景</p>\n<hr>\n<h3 id=\"去重\"><a href=\"#去重\" class=\"headerlink\" title=\"去重\"></a>去重</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">相较于排序而言，日常中去重的手段会更多一些</span><br><span class=\"line\"></span><br><span class=\"line\">#### 去重(linux方式)</span><br><span class=\"line\"></span><br><span class=\"line\">linux中的去重指令首先是刚才在前文本提到的</span><br><span class=\"line\"></span><br><span class=\"line\">sort -u:对文本进行排序，去重，并对重复的只保留一份。</span><br><span class=\"line\"></span><br><span class=\"line\">而在日常中，结合去重可能会产生更多的应用场景，即取交集、并集等</span><br><span class=\"line\"></span><br><span class=\"line\">大致来讲linux 主要的去重指令是uniq</span><br><span class=\"line\"></span><br><span class=\"line\">##### uniq</span><br><span class=\"line\"></span><br><span class=\"line\">uniq的Options主要有</span><br><span class=\"line\">![540273a5039941a39598a747888f4948-image.png](&#x2F;&#x2F;img.wqkenqing.ren&#x2F;file&#x2F;2017&#x2F;7&#x2F;540273a5039941a39598a747888f4948-image.png)</span><br><span class=\"line\"></span><br><span class=\"line\">![10bff3119aaa49de976350596f94f4bf-image.png](&#x2F;&#x2F;img.wqkenqing.ren&#x2F;file&#x2F;2017&#x2F;7&#x2F;10bff3119aaa49de976350596f94f4bf-image.png)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">uniq :默认只是将重复的保留一行</span><br><span class=\"line\"></span><br><span class=\"line\">而通过uniq实现交集与并集主要通过-d与-u实现</span><br><span class=\"line\"></span><br><span class=\"line\">uniq -d是只显示重复出现的行列</span><br><span class=\"line\">uniq -u是只显示不重复的列</span><br><span class=\"line\"></span><br><span class=\"line\">这里要注意uniq -f -s的使用</span><br><span class=\"line\">uniq -f nubmber :即指定忽略多少栏位开如计重</span><br><span class=\"line\"></span><br><span class=\"line\">uniq -s number:即指定忽略多少字任开始计重</span><br><span class=\"line\"> uniq -f -s :同时出现时则按先按栏位移，再按字符移。</span><br><span class=\"line\"></span><br><span class=\"line\">通过uniq实现去重要先排序</span><br><span class=\"line\"></span><br><span class=\"line\">---</span><br><span class=\"line\">针对时常出现的应用场景提供一个思路</span><br><span class=\"line\">两个文本中有重复内容，但只想去掉前一个文件与后一个文件中的重复内容，保留前一个文件中的非重复内容</span><br><span class=\"line\">可以通过</span><br><span class=\"line\"></span><br><span class=\"line\">sort file1 file2 | uniq  -d &gt;temp</span><br><span class=\"line\"></span><br><span class=\"line\">sort file1 temp|uniq -u</span><br><span class=\"line\"></span><br><span class=\"line\">&#96;&#96;&#96;即思路是:先将两个文件中的重复内容找出并写入临时文件，再将前一个文件与临时文件合并，排序去重，保留只出现一次的文件内容</span><br></pre></td></tr></table></figure>\n\n\n<hr>\n<h5 id=\"其它去重方式\"><a href=\"#其它去重方式\" class=\"headerlink\" title=\"其它去重方式\"></a>其它去重方式</h5><p>要去重，通过java方式也能轻松实现<br>即主要利用set等的非重复内容的特性。进行实现</p>\n<h3 id=\"切割\"><a href=\"#切割\" class=\"headerlink\" title=\"切割\"></a>切割</h3><p>即将文件进行切割，出于取样，或测试的需求考虑，可能需要从大文本中切割出一些小文件来。<br>对文件的切割也有很多实现方式<br>但从实现方式上更推荐linux指令式</p>\n<h4 id=\"切割linux指令式\"><a href=\"#切割linux指令式\" class=\"headerlink\" title=\"切割linux指令式\"></a>切割linux指令式</h4><p>涉及到切割的linux指令主要有split</p>\n<p>split命令可以将一个大文件分割成很多个小文件，有时需要将文件分割成更小的片段，比如为提高可读性，生成日志等。</p>\n<p><img src=\"./1493197221096.png\" alt=\"Alt text\"></p>\n<p>具体实现都是指令式，需要注意的地方较少，不记得时则翻阅相关文档</p>\n<h4 id=\"切割的其它方式\"><a href=\"#切割的其它方式\" class=\"headerlink\" title=\"切割的其它方式\"></a>切割的其它方式</h4><p>主要擅长的还有java方式</p>\n<hr>\n<h3 id=\"打乱排序\"><a href=\"#打乱排序\" class=\"headerlink\" title=\"打乱排序\"></a>打乱排序</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">打乱排序主要用的方式有awk与excel的方式</span><br></pre></td></tr></table></figure>\n\n\n<h4 id=\"awk方式\"><a href=\"#awk方式\" class=\"headerlink\" title=\"awk方式\"></a>awk方式</h4><p>awk ‘BEGIN{ 100000*srand();}{ printf “%s %s\\n”, rand(), $0}’  t |sort -k1n | awk ‘{gsub($1FS,””); print $0}’</p>\n<h4 id=\"excel方式\"><a href=\"#excel方式\" class=\"headerlink\" title=\"excel方式\"></a>excel方式</h4><p>即通过在文本中再另加一列，生成随机数，然后对随机数列进行排序从列达到打乱的效果</p>\n<hr>\n<h3 id=\"模糊匹配\"><a href=\"#模糊匹配\" class=\"headerlink\" title=\"模糊匹配\"></a>模糊匹配</h3><p><code>文本的模糊匹配有较多应该场景,可以再多总结</code></p>\n<p>模糊匹配主要有在shell脚本中针对contains操作</p>\n<p>和vim中的匹配操作</p>\n<hr>"},{"title":"Restful架构","date":"2019-07-15T16:00:00.000Z","_content":"此处简介\n<!--more-->\n\n## Restful架构\n### 什么是RESTful架构\n`Representational State Transfer:表现层状态转化`\n\nREST的名称\"表现层状态转化\"中，省略了主语。\"表现层\"其实指的是\"资源\"（Resources）的\"表现层\"。\n\n>所谓\"资源\"，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。\n\n\"资源\"是一种信息实体，它可以有多种外在表现形式。我们把\"资源\"具体呈现出来的形式，叫做它的\"表现层\"（Representation）。\n\n访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。\n\n互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生\"状态转化\"（State Transfer）。而这种转化是建立在表现层之上的，所以就是\"表现层状态转化\"。\n\n客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。\n\n\n---\n\n综合上面的解释，我们总结一下什么是RESTful架构：\n+ 每一个URI代表一种资源；\n+ 客户端和服务器之间，传递这种资源的某种表现层；\n+ 客户端通过四个HTTP动词，对服务器端资源进行操作，实现\"表现层状态转化\"。\n\n最常见的一种设计错误，就是URI包含动词。**因为\"资源\"表示一种实体，所以应该是名词**，**URI不应该有动词**，动词应该放在HTTP协议中。\n\n举例来说，某个URI是/posts/show/1，其中show是动词，这个URI就设计错了，正确的写法应该是/posts/1，然后用GET方法表示show。\n\n如果某些动作是HTTP动词表示不了的，你就应该把动作做成一种资源。比如网上汇款，从账户1向账户2汇款500元，错误的URI是：\n\n　　POST /accounts/1/transfer/500/to/2\n\n正确的写法是把动词transfer改成名词transaction，资源不能是动词，但是可以是一种服务：\n\n　　POST /transaction HTTP/1.1\n　　Host: 127.0.0.1\n　　\n　　from=1&to=2&amount=500.00\n\n\n\n另一个设计误区，就是在URI中加入版本号：\n\n　　http://www.example.com/app/1.0/foo\n\n　　http://www.example.com/app/1.1/foo\n\n　　http://www.example.com/app/2.0/foo\n\n因为不同的版本，可以理解成同一种资源的不同表现形式，所以应该采用同一个URI。版本号可以在HTTP请求头信息的Accept字段中进行区分（参见Versioning REST Services）：\nAccept: vnd.example-com.foo+json; version=1.0\n\n　　Accept: vnd.example-com.foo+json; version=1.1\n\n　　Accept: vnd.example-com.foo+json; version=2.0","source":"_posts/技术/hexo/oldblog/blog16.md","raw":"---\n\ntitle: Restful架构\ndate: 2019-07-16\ntags:\n\n---\n此处简介\n<!--more-->\n\n## Restful架构\n### 什么是RESTful架构\n`Representational State Transfer:表现层状态转化`\n\nREST的名称\"表现层状态转化\"中，省略了主语。\"表现层\"其实指的是\"资源\"（Resources）的\"表现层\"。\n\n>所谓\"资源\"，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。\n\n\"资源\"是一种信息实体，它可以有多种外在表现形式。我们把\"资源\"具体呈现出来的形式，叫做它的\"表现层\"（Representation）。\n\n访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。\n\n互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生\"状态转化\"（State Transfer）。而这种转化是建立在表现层之上的，所以就是\"表现层状态转化\"。\n\n客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。\n\n\n---\n\n综合上面的解释，我们总结一下什么是RESTful架构：\n+ 每一个URI代表一种资源；\n+ 客户端和服务器之间，传递这种资源的某种表现层；\n+ 客户端通过四个HTTP动词，对服务器端资源进行操作，实现\"表现层状态转化\"。\n\n最常见的一种设计错误，就是URI包含动词。**因为\"资源\"表示一种实体，所以应该是名词**，**URI不应该有动词**，动词应该放在HTTP协议中。\n\n举例来说，某个URI是/posts/show/1，其中show是动词，这个URI就设计错了，正确的写法应该是/posts/1，然后用GET方法表示show。\n\n如果某些动作是HTTP动词表示不了的，你就应该把动作做成一种资源。比如网上汇款，从账户1向账户2汇款500元，错误的URI是：\n\n　　POST /accounts/1/transfer/500/to/2\n\n正确的写法是把动词transfer改成名词transaction，资源不能是动词，但是可以是一种服务：\n\n　　POST /transaction HTTP/1.1\n　　Host: 127.0.0.1\n　　\n　　from=1&to=2&amount=500.00\n\n\n\n另一个设计误区，就是在URI中加入版本号：\n\n　　http://www.example.com/app/1.0/foo\n\n　　http://www.example.com/app/1.1/foo\n\n　　http://www.example.com/app/2.0/foo\n\n因为不同的版本，可以理解成同一种资源的不同表现形式，所以应该采用同一个URI。版本号可以在HTTP请求头信息的Accept字段中进行区分（参见Versioning REST Services）：\nAccept: vnd.example-com.foo+json; version=1.0\n\n　　Accept: vnd.example-com.foo+json; version=1.1\n\n　　Accept: vnd.example-com.foo+json; version=2.0","slug":"技术/hexo/oldblog/blog16","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33b002u38pwek7vhfpy","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h2 id=\"Restful架构\"><a href=\"#Restful架构\" class=\"headerlink\" title=\"Restful架构\"></a>Restful架构</h2><h3 id=\"什么是RESTful架构\"><a href=\"#什么是RESTful架构\" class=\"headerlink\" title=\"什么是RESTful架构\"></a>什么是RESTful架构</h3><p><code>Representational State Transfer:表现层状态转化</code></p>\n<p>REST的名称”表现层状态转化”中，省略了主语。”表现层”其实指的是”资源”（Resources）的”表现层”。</p>\n<blockquote>\n<p>所谓”资源”，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。</p>\n</blockquote>\n<p>“资源”是一种信息实体，它可以有多种外在表现形式。我们把”资源”具体呈现出来的形式，叫做它的”表现层”（Representation）。</p>\n<p>访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。</p>\n<p>互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生”状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是”表现层状态转化”。</p>\n<p>客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。</p>\n<hr>\n<p>综合上面的解释，我们总结一下什么是RESTful架构：</p>\n<ul>\n<li>每一个URI代表一种资源；</li>\n<li>客户端和服务器之间，传递这种资源的某种表现层；</li>\n<li>客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。</li>\n</ul>\n<p>最常见的一种设计错误，就是URI包含动词。<strong>因为”资源”表示一种实体，所以应该是名词</strong>，<strong>URI不应该有动词</strong>，动词应该放在HTTP协议中。</p>\n<p>举例来说，某个URI是/posts/show/1，其中show是动词，这个URI就设计错了，正确的写法应该是/posts/1，然后用GET方法表示show。</p>\n<p>如果某些动作是HTTP动词表示不了的，你就应该把动作做成一种资源。比如网上汇款，从账户1向账户2汇款500元，错误的URI是：</p>\n<p>　　POST /accounts/1/transfer/500/to/2</p>\n<p>正确的写法是把动词transfer改成名词transaction，资源不能是动词，但是可以是一种服务：</p>\n<p>　　POST /transaction HTTP/1.1<br>　　Host: 127.0.0.1<br>　　<br>　　from=1&amp;to=2&amp;amount=500.00</p>\n<p>另一个设计误区，就是在URI中加入版本号：</p>\n<p>　　<a href=\"http://www.example.com/app/1.0/foo\" target=\"_blank\" rel=\"noopener\">http://www.example.com/app/1.0/foo</a></p>\n<p>　　<a href=\"http://www.example.com/app/1.1/foo\" target=\"_blank\" rel=\"noopener\">http://www.example.com/app/1.1/foo</a></p>\n<p>　　<a href=\"http://www.example.com/app/2.0/foo\" target=\"_blank\" rel=\"noopener\">http://www.example.com/app/2.0/foo</a></p>\n<p>因为不同的版本，可以理解成同一种资源的不同表现形式，所以应该采用同一个URI。版本号可以在HTTP请求头信息的Accept字段中进行区分（参见Versioning REST Services）：<br>Accept: vnd.example-com.foo+json; version=1.0</p>\n<p>　　Accept: vnd.example-com.foo+json; version=1.1</p>\n<p>　　Accept: vnd.example-com.foo+json; version=2.0</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h2 id=\"Restful架构\"><a href=\"#Restful架构\" class=\"headerlink\" title=\"Restful架构\"></a>Restful架构</h2><h3 id=\"什么是RESTful架构\"><a href=\"#什么是RESTful架构\" class=\"headerlink\" title=\"什么是RESTful架构\"></a>什么是RESTful架构</h3><p><code>Representational State Transfer:表现层状态转化</code></p>\n<p>REST的名称”表现层状态转化”中，省略了主语。”表现层”其实指的是”资源”（Resources）的”表现层”。</p>\n<blockquote>\n<p>所谓”资源”，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。</p>\n</blockquote>\n<p>“资源”是一种信息实体，它可以有多种外在表现形式。我们把”资源”具体呈现出来的形式，叫做它的”表现层”（Representation）。</p>\n<p>访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。</p>\n<p>互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生”状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是”表现层状态转化”。</p>\n<p>客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。</p>\n<hr>\n<p>综合上面的解释，我们总结一下什么是RESTful架构：</p>\n<ul>\n<li>每一个URI代表一种资源；</li>\n<li>客户端和服务器之间，传递这种资源的某种表现层；</li>\n<li>客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。</li>\n</ul>\n<p>最常见的一种设计错误，就是URI包含动词。<strong>因为”资源”表示一种实体，所以应该是名词</strong>，<strong>URI不应该有动词</strong>，动词应该放在HTTP协议中。</p>\n<p>举例来说，某个URI是/posts/show/1，其中show是动词，这个URI就设计错了，正确的写法应该是/posts/1，然后用GET方法表示show。</p>\n<p>如果某些动作是HTTP动词表示不了的，你就应该把动作做成一种资源。比如网上汇款，从账户1向账户2汇款500元，错误的URI是：</p>\n<p>　　POST /accounts/1/transfer/500/to/2</p>\n<p>正确的写法是把动词transfer改成名词transaction，资源不能是动词，但是可以是一种服务：</p>\n<p>　　POST /transaction HTTP/1.1<br>　　Host: 127.0.0.1<br>　　<br>　　from=1&amp;to=2&amp;amount=500.00</p>\n<p>另一个设计误区，就是在URI中加入版本号：</p>\n<p>　　<a href=\"http://www.example.com/app/1.0/foo\" target=\"_blank\" rel=\"noopener\">http://www.example.com/app/1.0/foo</a></p>\n<p>　　<a href=\"http://www.example.com/app/1.1/foo\" target=\"_blank\" rel=\"noopener\">http://www.example.com/app/1.1/foo</a></p>\n<p>　　<a href=\"http://www.example.com/app/2.0/foo\" target=\"_blank\" rel=\"noopener\">http://www.example.com/app/2.0/foo</a></p>\n<p>因为不同的版本，可以理解成同一种资源的不同表现形式，所以应该采用同一个URI。版本号可以在HTTP请求头信息的Accept字段中进行区分（参见Versioning REST Services）：<br>Accept: vnd.example-com.foo+json; version=1.0</p>\n<p>　　Accept: vnd.example-com.foo+json; version=1.1</p>\n<p>　　Accept: vnd.example-com.foo+json; version=2.0</p>"},{"title":"Vim笔记","date":"2019-07-15T16:00:00.000Z","_content":"此处简介\n<!--more-->\n\n### Vim笔记\n\nvi有三种工作模式：\n* 命令模式\n* 插入模式\n* 和编辑模式。\n![d689c7d6faa6469a885abdb7ed9b9c52-image.png](//img.wqkenqing.ren/file/2017/7/d689c7d6faa6469a885abdb7ed9b9c52-image.png)\n\n\n\n#### 插入模式\n 命令          作用\na                 在光标后附加文本\nA                 在本行行末附加文本\ni                  在光标前插入文本\nI                  在本行开始插入文本\no                 在光标下插入新行\nO                 在光标上插入新行\n\n***\n#### 定位命令\n命令 作用\nh、方向左键 左移一个字符\nj、方向下键 下移一行\nk、方向上键 上移一行\nl、方向右键 右移一个字符\n$ 移至行尾\n0 移至行首\nH 移至屏幕上端\nM 移至屏幕中央\nL 移至屏幕下端\n:set nu 设置行号\n:set nonu 取消行号\ngg\nG 到第一行\n到最后一行\nnG 到第n行\n:n 到第n行\n***\n#### 删除命令\n命令 作用\nx 删除光标所在处字符\nnx 删除光标所在处后n个字符\ndd 删除光标所在行，ndd删除n行\ndG 删除光标所在行到文件末尾的内容\nD 删除从光标所在处到行尾的内容\n:n1,n2d 删除指定范围的行\n***\n复制和剪切命令\n命令 作用\nyy、Y 复制当前行\nnyy、nY 复制当前行一下n行\ndd 剪切当前行\nndd 剪切当前行以下n行\np、P 粘贴在当前光标所在行下或行上 注：在vi中，剪切就是删除之后再粘贴\n***\n#### 替换和取消命令\n命令 作用\nr 取代光标所在处字符\nR 从光标所在处开始替换字符，按Esc结束\nu 取消上一步操作 注：比如改变单个字符，先输入r，再输入需要更改的字符。比如将字符a改成b。这适合用于少量修改时使用\n***\n#### 搜索和替换命令\n命令 作用\n/string 向前搜索指定字符串\n搜索时忽略大小写 :set ic\nn 搜索指定字符串的下一个出现位置\n:%s/old/new/g  全文替换指定字符串\n:n1,n2s/old/new/g  在一定范围内替换指定字符串 注：n是从前往后，N是从后往前找\nset noic是设置大小写敏感\n:n1,n2s/old/new/c    替换时进行询问是否真的替换\n\nZZ与:wq的作用一样，都是保存退出\n对于readonly文件，如果是root或者改文件所有者，即使该文件没有写权限，使用:wq!也能保存该修改之后的文件。\n仅仅保存但不退出 :w\n另存为  :w /root/file.bak\n***\n#### 其它命令\n导入文件 :r 文件名\n在vi中执行命令 :! 命令\n定义快捷键 :map 快捷键  触发命令\n范例：  :map  ^P  I#<ESC>         注：^p是这样输入的 ctrl+v+v  --> ^p\n     :map  ^B  0x\n连续行注释 :n1,n2s/^/#/g              注：^表示行首\n    :n1,n2s/^#//g\n    :n1,n2s/^/\\/\\//g\n替换  :ab huhuimail    huhuics@gmail.com         取消ab命令    :unan huhuimail\n:r !date   在vi中加入命令执行的结果\n快捷键插入邮箱  :map ^e ihuhuics@gmail.com\n#### 修改用户vim设置\n修改用户vim设置，比如能永久保存快捷键\nvi ~/.vimrc\n缺省这个文件是空的，然后可以写入一些快捷键","source":"_posts/技术/hexo/oldblog/blog15.md","raw":"---\n\ntitle: Vim笔记\ndate: 2019-07-16\ntags:\n\n---\n此处简介\n<!--more-->\n\n### Vim笔记\n\nvi有三种工作模式：\n* 命令模式\n* 插入模式\n* 和编辑模式。\n![d689c7d6faa6469a885abdb7ed9b9c52-image.png](//img.wqkenqing.ren/file/2017/7/d689c7d6faa6469a885abdb7ed9b9c52-image.png)\n\n\n\n#### 插入模式\n 命令          作用\na                 在光标后附加文本\nA                 在本行行末附加文本\ni                  在光标前插入文本\nI                  在本行开始插入文本\no                 在光标下插入新行\nO                 在光标上插入新行\n\n***\n#### 定位命令\n命令 作用\nh、方向左键 左移一个字符\nj、方向下键 下移一行\nk、方向上键 上移一行\nl、方向右键 右移一个字符\n$ 移至行尾\n0 移至行首\nH 移至屏幕上端\nM 移至屏幕中央\nL 移至屏幕下端\n:set nu 设置行号\n:set nonu 取消行号\ngg\nG 到第一行\n到最后一行\nnG 到第n行\n:n 到第n行\n***\n#### 删除命令\n命令 作用\nx 删除光标所在处字符\nnx 删除光标所在处后n个字符\ndd 删除光标所在行，ndd删除n行\ndG 删除光标所在行到文件末尾的内容\nD 删除从光标所在处到行尾的内容\n:n1,n2d 删除指定范围的行\n***\n复制和剪切命令\n命令 作用\nyy、Y 复制当前行\nnyy、nY 复制当前行一下n行\ndd 剪切当前行\nndd 剪切当前行以下n行\np、P 粘贴在当前光标所在行下或行上 注：在vi中，剪切就是删除之后再粘贴\n***\n#### 替换和取消命令\n命令 作用\nr 取代光标所在处字符\nR 从光标所在处开始替换字符，按Esc结束\nu 取消上一步操作 注：比如改变单个字符，先输入r，再输入需要更改的字符。比如将字符a改成b。这适合用于少量修改时使用\n***\n#### 搜索和替换命令\n命令 作用\n/string 向前搜索指定字符串\n搜索时忽略大小写 :set ic\nn 搜索指定字符串的下一个出现位置\n:%s/old/new/g  全文替换指定字符串\n:n1,n2s/old/new/g  在一定范围内替换指定字符串 注：n是从前往后，N是从后往前找\nset noic是设置大小写敏感\n:n1,n2s/old/new/c    替换时进行询问是否真的替换\n\nZZ与:wq的作用一样，都是保存退出\n对于readonly文件，如果是root或者改文件所有者，即使该文件没有写权限，使用:wq!也能保存该修改之后的文件。\n仅仅保存但不退出 :w\n另存为  :w /root/file.bak\n***\n#### 其它命令\n导入文件 :r 文件名\n在vi中执行命令 :! 命令\n定义快捷键 :map 快捷键  触发命令\n范例：  :map  ^P  I#<ESC>         注：^p是这样输入的 ctrl+v+v  --> ^p\n     :map  ^B  0x\n连续行注释 :n1,n2s/^/#/g              注：^表示行首\n    :n1,n2s/^#//g\n    :n1,n2s/^/\\/\\//g\n替换  :ab huhuimail    huhuics@gmail.com         取消ab命令    :unan huhuimail\n:r !date   在vi中加入命令执行的结果\n快捷键插入邮箱  :map ^e ihuhuics@gmail.com\n#### 修改用户vim设置\n修改用户vim设置，比如能永久保存快捷键\nvi ~/.vimrc\n缺省这个文件是空的，然后可以写入一些快捷键","slug":"技术/hexo/oldblog/blog15","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33b002v38pwaihh45ml","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h3 id=\"Vim笔记\"><a href=\"#Vim笔记\" class=\"headerlink\" title=\"Vim笔记\"></a>Vim笔记</h3><p>vi有三种工作模式：</p>\n<ul>\n<li>命令模式</li>\n<li>插入模式</li>\n<li>和编辑模式。<br><img src=\"//img.wqkenqing.ren/file/2017/7/d689c7d6faa6469a885abdb7ed9b9c52-image.png\" alt=\"d689c7d6faa6469a885abdb7ed9b9c52-image.png\"></li>\n</ul>\n<h4 id=\"插入模式\"><a href=\"#插入模式\" class=\"headerlink\" title=\"插入模式\"></a>插入模式</h4><p> 命令          作用<br>a                 在光标后附加文本<br>A                 在本行行末附加文本<br>i                  在光标前插入文本<br>I                  在本行开始插入文本<br>o                 在光标下插入新行<br>O                 在光标上插入新行</p>\n<hr>\n<h4 id=\"定位命令\"><a href=\"#定位命令\" class=\"headerlink\" title=\"定位命令\"></a>定位命令</h4><p>命令 作用<br>h、方向左键 左移一个字符<br>j、方向下键 下移一行<br>k、方向上键 上移一行<br>l、方向右键 右移一个字符<br>$ 移至行尾<br>0 移至行首<br>H 移至屏幕上端<br>M 移至屏幕中央<br>L 移至屏幕下端<br>:set nu 设置行号<br>:set nonu 取消行号<br>gg<br>G 到第一行<br>到最后一行<br>nG 到第n行<br>:n 到第n行</p>\n<hr>\n<h4 id=\"删除命令\"><a href=\"#删除命令\" class=\"headerlink\" title=\"删除命令\"></a>删除命令</h4><p>命令 作用<br>x 删除光标所在处字符<br>nx 删除光标所在处后n个字符<br>dd 删除光标所在行，ndd删除n行<br>dG 删除光标所在行到文件末尾的内容<br>D 删除从光标所在处到行尾的内容<br>:n1,n2d 删除指定范围的行</p>\n<hr>\n<p>复制和剪切命令<br>命令 作用<br>yy、Y 复制当前行<br>nyy、nY 复制当前行一下n行<br>dd 剪切当前行<br>ndd 剪切当前行以下n行<br>p、P 粘贴在当前光标所在行下或行上 注：在vi中，剪切就是删除之后再粘贴</p>\n<hr>\n<h4 id=\"替换和取消命令\"><a href=\"#替换和取消命令\" class=\"headerlink\" title=\"替换和取消命令\"></a>替换和取消命令</h4><p>命令 作用<br>r 取代光标所在处字符<br>R 从光标所在处开始替换字符，按Esc结束<br>u 取消上一步操作 注：比如改变单个字符，先输入r，再输入需要更改的字符。比如将字符a改成b。这适合用于少量修改时使用</p>\n<hr>\n<h4 id=\"搜索和替换命令\"><a href=\"#搜索和替换命令\" class=\"headerlink\" title=\"搜索和替换命令\"></a>搜索和替换命令</h4><p>命令 作用<br>/string 向前搜索指定字符串<br>搜索时忽略大小写 :set ic<br>n 搜索指定字符串的下一个出现位置<br>:%s/old/new/g  全文替换指定字符串<br>:n1,n2s/old/new/g  在一定范围内替换指定字符串 注：n是从前往后，N是从后往前找<br>set noic是设置大小写敏感<br>:n1,n2s/old/new/c    替换时进行询问是否真的替换</p>\n<p>ZZ与:wq的作用一样，都是保存退出<br>对于readonly文件，如果是root或者改文件所有者，即使该文件没有写权限，使用:wq!也能保存该修改之后的文件。<br>仅仅保存但不退出 :w<br>另存为  :w /root/file.bak</p>\n<hr>\n<h4 id=\"其它命令\"><a href=\"#其它命令\" class=\"headerlink\" title=\"其它命令\"></a>其它命令</h4><p>导入文件 :r 文件名<br>在vi中执行命令 :! 命令<br>定义快捷键 :map 快捷键  触发命令<br>范例：  :map  ^P  I#<ESC>         注：^p是这样输入的 ctrl+v+v  –&gt; ^p<br>     :map  ^B  0x<br>连续行注释 :n1,n2s/^/#/g              注：^表示行首<br>    :n1,n2s/^#//g<br>    :n1,n2s/^////g<br>替换  :ab huhuimail    <a href=\"mailto:huhuics@gmail.com\">huhuics@gmail.com</a>         取消ab命令    :unan huhuimail<br>:r !date   在vi中加入命令执行的结果<br>快捷键插入邮箱  :map ^e <a href=\"mailto:ihuhuics@gmail.com\">ihuhuics@gmail.com</a></p>\n<h4 id=\"修改用户vim设置\"><a href=\"#修改用户vim设置\" class=\"headerlink\" title=\"修改用户vim设置\"></a>修改用户vim设置</h4><p>修改用户vim设置，比如能永久保存快捷键<br>vi ~/.vimrc<br>缺省这个文件是空的，然后可以写入一些快捷键</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h3 id=\"Vim笔记\"><a href=\"#Vim笔记\" class=\"headerlink\" title=\"Vim笔记\"></a>Vim笔记</h3><p>vi有三种工作模式：</p>\n<ul>\n<li>命令模式</li>\n<li>插入模式</li>\n<li>和编辑模式。<br><img src=\"//img.wqkenqing.ren/file/2017/7/d689c7d6faa6469a885abdb7ed9b9c52-image.png\" alt=\"d689c7d6faa6469a885abdb7ed9b9c52-image.png\"></li>\n</ul>\n<h4 id=\"插入模式\"><a href=\"#插入模式\" class=\"headerlink\" title=\"插入模式\"></a>插入模式</h4><p> 命令          作用<br>a                 在光标后附加文本<br>A                 在本行行末附加文本<br>i                  在光标前插入文本<br>I                  在本行开始插入文本<br>o                 在光标下插入新行<br>O                 在光标上插入新行</p>\n<hr>\n<h4 id=\"定位命令\"><a href=\"#定位命令\" class=\"headerlink\" title=\"定位命令\"></a>定位命令</h4><p>命令 作用<br>h、方向左键 左移一个字符<br>j、方向下键 下移一行<br>k、方向上键 上移一行<br>l、方向右键 右移一个字符<br>$ 移至行尾<br>0 移至行首<br>H 移至屏幕上端<br>M 移至屏幕中央<br>L 移至屏幕下端<br>:set nu 设置行号<br>:set nonu 取消行号<br>gg<br>G 到第一行<br>到最后一行<br>nG 到第n行<br>:n 到第n行</p>\n<hr>\n<h4 id=\"删除命令\"><a href=\"#删除命令\" class=\"headerlink\" title=\"删除命令\"></a>删除命令</h4><p>命令 作用<br>x 删除光标所在处字符<br>nx 删除光标所在处后n个字符<br>dd 删除光标所在行，ndd删除n行<br>dG 删除光标所在行到文件末尾的内容<br>D 删除从光标所在处到行尾的内容<br>:n1,n2d 删除指定范围的行</p>\n<hr>\n<p>复制和剪切命令<br>命令 作用<br>yy、Y 复制当前行<br>nyy、nY 复制当前行一下n行<br>dd 剪切当前行<br>ndd 剪切当前行以下n行<br>p、P 粘贴在当前光标所在行下或行上 注：在vi中，剪切就是删除之后再粘贴</p>\n<hr>\n<h4 id=\"替换和取消命令\"><a href=\"#替换和取消命令\" class=\"headerlink\" title=\"替换和取消命令\"></a>替换和取消命令</h4><p>命令 作用<br>r 取代光标所在处字符<br>R 从光标所在处开始替换字符，按Esc结束<br>u 取消上一步操作 注：比如改变单个字符，先输入r，再输入需要更改的字符。比如将字符a改成b。这适合用于少量修改时使用</p>\n<hr>\n<h4 id=\"搜索和替换命令\"><a href=\"#搜索和替换命令\" class=\"headerlink\" title=\"搜索和替换命令\"></a>搜索和替换命令</h4><p>命令 作用<br>/string 向前搜索指定字符串<br>搜索时忽略大小写 :set ic<br>n 搜索指定字符串的下一个出现位置<br>:%s/old/new/g  全文替换指定字符串<br>:n1,n2s/old/new/g  在一定范围内替换指定字符串 注：n是从前往后，N是从后往前找<br>set noic是设置大小写敏感<br>:n1,n2s/old/new/c    替换时进行询问是否真的替换</p>\n<p>ZZ与:wq的作用一样，都是保存退出<br>对于readonly文件，如果是root或者改文件所有者，即使该文件没有写权限，使用:wq!也能保存该修改之后的文件。<br>仅仅保存但不退出 :w<br>另存为  :w /root/file.bak</p>\n<hr>\n<h4 id=\"其它命令\"><a href=\"#其它命令\" class=\"headerlink\" title=\"其它命令\"></a>其它命令</h4><p>导入文件 :r 文件名<br>在vi中执行命令 :! 命令<br>定义快捷键 :map 快捷键  触发命令<br>范例：  :map  ^P  I#<ESC>         注：^p是这样输入的 ctrl+v+v  –&gt; ^p<br>     :map  ^B  0x<br>连续行注释 :n1,n2s/^/#/g              注：^表示行首<br>    :n1,n2s/^#//g<br>    :n1,n2s/^////g<br>替换  :ab huhuimail    <a href=\"mailto:huhuics@gmail.com\">huhuics@gmail.com</a>         取消ab命令    :unan huhuimail<br>:r !date   在vi中加入命令执行的结果<br>快捷键插入邮箱  :map ^e <a href=\"mailto:ihuhuics@gmail.com\">ihuhuics@gmail.com</a></p>\n<h4 id=\"修改用户vim设置\"><a href=\"#修改用户vim设置\" class=\"headerlink\" title=\"修改用户vim设置\"></a>修改用户vim设置</h4><p>修改用户vim设置，比如能永久保存快捷键<br>vi ~/.vimrc<br>缺省这个文件是空的，然后可以写入一些快捷键</p>"},{"title":"kafka小结","date":"2019-07-15T16:00:00.000Z","_content":"此处简介\n<!--more-->\n\n## kafka小结\n\n消息系统术语\nkafka特性\n+ 分布式的\n+ 可分区的\n+ 可复制的\n\n在普通的消息系统的功上，还有自己独特的设计\n\n\nKafka将消息以topic为单位进行归纳。\n将向Kafka topic发布消息的程序成为producers.\n将预订topics并消费消息的程序成为consumer.\nKafka以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个broker.\nproducers通过网络将消息发送到Kafka集群，集群向消费者提供消息，\n\n![827fdc820cae4619859042761c3b40a9-image.png](//img.wqkenqing.ren/file/2017/7/827fdc820cae4619859042761c3b40a9-image.png)\n\n\n\n客户端和服务端通过TCP协议通信。Kafka提供了Java客户端，并且对多种语言都提供了支持。\n\n\n---\n\nTopics 和Logs\n\n先来看一下Kafka提供的一个抽象概念:topic.\n一个topic是对一组消息的归纳。对每个topic，Kafka 对它的日志进行了分区\n![bf0d2ddee1d14cb29fd54483a622d67c-image.png](//img.wqkenqing.ren/file/2017/7/bf0d2ddee1d14cb29fd54483a622d67c-image.png)\n\n\n一个topic是对一组消息的归纳。\n对每个topic，Kafka 对它的日志进行了分区，\n\n\n每个分区都由一系列有序的、不可变的消息组成，这些消息被连续的追加到分区中。分区中的每个消息都有一个连续的序列号叫做offset,用来在分区中唯一的标识这个消息。\n\n\n\n---\n### kafka常用指令收集\n\n**查看topic的详细信息**\nkafka-topics.sh -zookeeper 127.0.0.1:2181 -describe -topic topic name\n\n**为topic增加副本**\nkafka-reassign-partitions.sh -zookeeper 127.0.0.1:2181 -reassignment-json-file json/partitions-to-move.json -execute\n**创建topic**\nkafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic name\n**为topic增加partition**\nkafka-topics.sh –zookeeper 127.0.0.1:2181 –alter –partitions 20 –topic name\n**kafka生产者客户端命令**\nkafka-console-producer.sh --broker-list localhost:9092 --topic name\n**kafka消费者客户端命令**\nkafka-console-consumer.sh -zookeeper localhost:2181 --from-beginning --topic name\n**kafka服务启动**\nkafka-server-start.sh -daemon ../config/server.properties\n**删除topic**\nkafka-run-class.sh kafka.admin.DeleteTopicCommand --topic testKJ1 --zookeeper 127.0.0.1:2181\nkafka-topics.sh --zookeeper localhost:2181 --delete --topic testKJ1\n**查看consumer组内消费的offset**\nkafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group test --topic name","source":"_posts/技术/hexo/oldblog/blog17.md","raw":"---\n\ntitle: kafka小结\ndate: 2019-07-16\ntags: \n\n---\n此处简介\n<!--more-->\n\n## kafka小结\n\n消息系统术语\nkafka特性\n+ 分布式的\n+ 可分区的\n+ 可复制的\n\n在普通的消息系统的功上，还有自己独特的设计\n\n\nKafka将消息以topic为单位进行归纳。\n将向Kafka topic发布消息的程序成为producers.\n将预订topics并消费消息的程序成为consumer.\nKafka以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个broker.\nproducers通过网络将消息发送到Kafka集群，集群向消费者提供消息，\n\n![827fdc820cae4619859042761c3b40a9-image.png](//img.wqkenqing.ren/file/2017/7/827fdc820cae4619859042761c3b40a9-image.png)\n\n\n\n客户端和服务端通过TCP协议通信。Kafka提供了Java客户端，并且对多种语言都提供了支持。\n\n\n---\n\nTopics 和Logs\n\n先来看一下Kafka提供的一个抽象概念:topic.\n一个topic是对一组消息的归纳。对每个topic，Kafka 对它的日志进行了分区\n![bf0d2ddee1d14cb29fd54483a622d67c-image.png](//img.wqkenqing.ren/file/2017/7/bf0d2ddee1d14cb29fd54483a622d67c-image.png)\n\n\n一个topic是对一组消息的归纳。\n对每个topic，Kafka 对它的日志进行了分区，\n\n\n每个分区都由一系列有序的、不可变的消息组成，这些消息被连续的追加到分区中。分区中的每个消息都有一个连续的序列号叫做offset,用来在分区中唯一的标识这个消息。\n\n\n\n---\n### kafka常用指令收集\n\n**查看topic的详细信息**\nkafka-topics.sh -zookeeper 127.0.0.1:2181 -describe -topic topic name\n\n**为topic增加副本**\nkafka-reassign-partitions.sh -zookeeper 127.0.0.1:2181 -reassignment-json-file json/partitions-to-move.json -execute\n**创建topic**\nkafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic name\n**为topic增加partition**\nkafka-topics.sh –zookeeper 127.0.0.1:2181 –alter –partitions 20 –topic name\n**kafka生产者客户端命令**\nkafka-console-producer.sh --broker-list localhost:9092 --topic name\n**kafka消费者客户端命令**\nkafka-console-consumer.sh -zookeeper localhost:2181 --from-beginning --topic name\n**kafka服务启动**\nkafka-server-start.sh -daemon ../config/server.properties\n**删除topic**\nkafka-run-class.sh kafka.admin.DeleteTopicCommand --topic testKJ1 --zookeeper 127.0.0.1:2181\nkafka-topics.sh --zookeeper localhost:2181 --delete --topic testKJ1\n**查看consumer组内消费的offset**\nkafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper localhost:2181 --group test --topic name","slug":"技术/hexo/oldblog/blog17","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33c002w38pw86v65ruu","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h2 id=\"kafka小结\"><a href=\"#kafka小结\" class=\"headerlink\" title=\"kafka小结\"></a>kafka小结</h2><p>消息系统术语<br>kafka特性</p>\n<ul>\n<li>分布式的</li>\n<li>可分区的</li>\n<li>可复制的</li>\n</ul>\n<p>在普通的消息系统的功上，还有自己独特的设计</p>\n<p>Kafka将消息以topic为单位进行归纳。<br>将向Kafka topic发布消息的程序成为producers.<br>将预订topics并消费消息的程序成为consumer.<br>Kafka以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个broker.<br>producers通过网络将消息发送到Kafka集群，集群向消费者提供消息，</p>\n<p><img src=\"//img.wqkenqing.ren/file/2017/7/827fdc820cae4619859042761c3b40a9-image.png\" alt=\"827fdc820cae4619859042761c3b40a9-image.png\"></p>\n<p>客户端和服务端通过TCP协议通信。Kafka提供了Java客户端，并且对多种语言都提供了支持。</p>\n<hr>\n<p>Topics 和Logs</p>\n<p>先来看一下Kafka提供的一个抽象概念:topic.<br>一个topic是对一组消息的归纳。对每个topic，Kafka 对它的日志进行了分区<br><img src=\"//img.wqkenqing.ren/file/2017/7/bf0d2ddee1d14cb29fd54483a622d67c-image.png\" alt=\"bf0d2ddee1d14cb29fd54483a622d67c-image.png\"></p>\n<p>一个topic是对一组消息的归纳。<br>对每个topic，Kafka 对它的日志进行了分区，</p>\n<p>每个分区都由一系列有序的、不可变的消息组成，这些消息被连续的追加到分区中。分区中的每个消息都有一个连续的序列号叫做offset,用来在分区中唯一的标识这个消息。</p>\n<hr>\n<h3 id=\"kafka常用指令收集\"><a href=\"#kafka常用指令收集\" class=\"headerlink\" title=\"kafka常用指令收集\"></a>kafka常用指令收集</h3><p><strong>查看topic的详细信息</strong><br>kafka-topics.sh -zookeeper 127.0.0.1:2181 -describe -topic topic name</p>\n<p><strong>为topic增加副本</strong><br>kafka-reassign-partitions.sh -zookeeper 127.0.0.1:2181 -reassignment-json-file json/partitions-to-move.json -execute<br><strong>创建topic</strong><br>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic name<br><strong>为topic增加partition</strong><br>kafka-topics.sh –zookeeper 127.0.0.1:2181 –alter –partitions 20 –topic name<br><strong>kafka生产者客户端命令</strong><br>kafka-console-producer.sh –broker-list localhost:9092 –topic name<br><strong>kafka消费者客户端命令</strong><br>kafka-console-consumer.sh -zookeeper localhost:2181 –from-beginning –topic name<br><strong>kafka服务启动</strong><br>kafka-server-start.sh -daemon ../config/server.properties<br><strong>删除topic</strong><br>kafka-run-class.sh kafka.admin.DeleteTopicCommand –topic testKJ1 –zookeeper 127.0.0.1:2181<br>kafka-topics.sh –zookeeper localhost:2181 –delete –topic testKJ1<br><strong>查看consumer组内消费的offset</strong><br>kafka-run-class.sh kafka.tools.ConsumerOffsetChecker –zookeeper localhost:2181 –group test –topic name</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h2 id=\"kafka小结\"><a href=\"#kafka小结\" class=\"headerlink\" title=\"kafka小结\"></a>kafka小结</h2><p>消息系统术语<br>kafka特性</p>\n<ul>\n<li>分布式的</li>\n<li>可分区的</li>\n<li>可复制的</li>\n</ul>\n<p>在普通的消息系统的功上，还有自己独特的设计</p>\n<p>Kafka将消息以topic为单位进行归纳。<br>将向Kafka topic发布消息的程序成为producers.<br>将预订topics并消费消息的程序成为consumer.<br>Kafka以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个broker.<br>producers通过网络将消息发送到Kafka集群，集群向消费者提供消息，</p>\n<p><img src=\"//img.wqkenqing.ren/file/2017/7/827fdc820cae4619859042761c3b40a9-image.png\" alt=\"827fdc820cae4619859042761c3b40a9-image.png\"></p>\n<p>客户端和服务端通过TCP协议通信。Kafka提供了Java客户端，并且对多种语言都提供了支持。</p>\n<hr>\n<p>Topics 和Logs</p>\n<p>先来看一下Kafka提供的一个抽象概念:topic.<br>一个topic是对一组消息的归纳。对每个topic，Kafka 对它的日志进行了分区<br><img src=\"//img.wqkenqing.ren/file/2017/7/bf0d2ddee1d14cb29fd54483a622d67c-image.png\" alt=\"bf0d2ddee1d14cb29fd54483a622d67c-image.png\"></p>\n<p>一个topic是对一组消息的归纳。<br>对每个topic，Kafka 对它的日志进行了分区，</p>\n<p>每个分区都由一系列有序的、不可变的消息组成，这些消息被连续的追加到分区中。分区中的每个消息都有一个连续的序列号叫做offset,用来在分区中唯一的标识这个消息。</p>\n<hr>\n<h3 id=\"kafka常用指令收集\"><a href=\"#kafka常用指令收集\" class=\"headerlink\" title=\"kafka常用指令收集\"></a>kafka常用指令收集</h3><p><strong>查看topic的详细信息</strong><br>kafka-topics.sh -zookeeper 127.0.0.1:2181 -describe -topic topic name</p>\n<p><strong>为topic增加副本</strong><br>kafka-reassign-partitions.sh -zookeeper 127.0.0.1:2181 -reassignment-json-file json/partitions-to-move.json -execute<br><strong>创建topic</strong><br>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic name<br><strong>为topic增加partition</strong><br>kafka-topics.sh –zookeeper 127.0.0.1:2181 –alter –partitions 20 –topic name<br><strong>kafka生产者客户端命令</strong><br>kafka-console-producer.sh –broker-list localhost:9092 –topic name<br><strong>kafka消费者客户端命令</strong><br>kafka-console-consumer.sh -zookeeper localhost:2181 –from-beginning –topic name<br><strong>kafka服务启动</strong><br>kafka-server-start.sh -daemon ../config/server.properties<br><strong>删除topic</strong><br>kafka-run-class.sh kafka.admin.DeleteTopicCommand –topic testKJ1 –zookeeper 127.0.0.1:2181<br>kafka-topics.sh –zookeeper localhost:2181 –delete –topic testKJ1<br><strong>查看consumer组内消费的offset</strong><br>kafka-run-class.sh kafka.tools.ConsumerOffsetChecker –zookeeper localhost:2181 –group test –topic name</p>"},{"title":"hbase","date":"2019-07-15T16:00:00.000Z","_content":"此处简介\n<!--more-->\n\nhbase shell命令            \t                描述\nalter\t    修改列族（column family）模式\ncount\t统计表中行的数量\ncreate\t创建表\ndescribe\t显示表相关的详细信息\ndelete\t删除指定对象的值（可以为表，行，列对应的值，另外也可以指定时间戳的值）\ndeleteall\t删除指定行的所有元素值\ndisable\t使表无效\ndrop\t删除表\nenable\t使表有效\nexists\t测试表是否存在\nexit\t退出hbase shell\nget\t获取行或单元（cell）的值\nincr\t增加指定表，行或列的值\nlist\t列出hbase中存在的所有表\nput\t向指向的表单元添加值\ntools\t列出hbase所支持的工具\nscan\t通过对表的扫描来获取对用的值\nstatus\t返回hbase集群的状态信息\nshutdown\t关闭hbase集群（与exit不同）\ntruncate\t重新创建指定表\nversion\t返回hbase版本信息","source":"_posts/技术/hexo/oldblog/blog20.md","raw":"---\n\ntitle: hbase\ndate: 2019-07-16\ntags: \n\n---\n此处简介\n<!--more-->\n\nhbase shell命令            \t                描述\nalter\t    修改列族（column family）模式\ncount\t统计表中行的数量\ncreate\t创建表\ndescribe\t显示表相关的详细信息\ndelete\t删除指定对象的值（可以为表，行，列对应的值，另外也可以指定时间戳的值）\ndeleteall\t删除指定行的所有元素值\ndisable\t使表无效\ndrop\t删除表\nenable\t使表有效\nexists\t测试表是否存在\nexit\t退出hbase shell\nget\t获取行或单元（cell）的值\nincr\t增加指定表，行或列的值\nlist\t列出hbase中存在的所有表\nput\t向指向的表单元添加值\ntools\t列出hbase所支持的工具\nscan\t通过对表的扫描来获取对用的值\nstatus\t返回hbase集群的状态信息\nshutdown\t关闭hbase集群（与exit不同）\ntruncate\t重新创建指定表\nversion\t返回hbase版本信息","slug":"技术/hexo/oldblog/blog20","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33c002x38pwdrdpgrfy","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<p>hbase shell命令                                描述<br>alter        修改列族（column family）模式<br>count    统计表中行的数量<br>create    创建表<br>describe    显示表相关的详细信息<br>delete    删除指定对象的值（可以为表，行，列对应的值，另外也可以指定时间戳的值）<br>deleteall    删除指定行的所有元素值<br>disable    使表无效<br>drop    删除表<br>enable    使表有效<br>exists    测试表是否存在<br>exit    退出hbase shell<br>get    获取行或单元（cell）的值<br>incr    增加指定表，行或列的值<br>list    列出hbase中存在的所有表<br>put    向指向的表单元添加值<br>tools    列出hbase所支持的工具<br>scan    通过对表的扫描来获取对用的值<br>status    返回hbase集群的状态信息<br>shutdown    关闭hbase集群（与exit不同）<br>truncate    重新创建指定表<br>version    返回hbase版本信息</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<p>hbase shell命令                                描述<br>alter        修改列族（column family）模式<br>count    统计表中行的数量<br>create    创建表<br>describe    显示表相关的详细信息<br>delete    删除指定对象的值（可以为表，行，列对应的值，另外也可以指定时间戳的值）<br>deleteall    删除指定行的所有元素值<br>disable    使表无效<br>drop    删除表<br>enable    使表有效<br>exists    测试表是否存在<br>exit    退出hbase shell<br>get    获取行或单元（cell）的值<br>incr    增加指定表，行或列的值<br>list    列出hbase中存在的所有表<br>put    向指向的表单元添加值<br>tools    列出hbase所支持的工具<br>scan    通过对表的扫描来获取对用的值<br>status    返回hbase集群的状态信息<br>shutdown    关闭hbase集群（与exit不同）<br>truncate    重新创建指定表<br>version    返回hbase版本信息</p>"},{"title":"Python","date":"2019-07-15T16:00:00.000Z","_content":"\n此处简介\n<!--more-->\n\n`the record for python`\n### Python 知识点\n\n#### 标识\b符号\n在 Python 里，标识符有字母、数字、下划线组成。\n在 Python 中，所有标识符可以包括英文、数字以及下划线(_)，但不能以数字开头。\n\n\n以下划线开头的标识符是有特殊意义的。以单下划线开头 _foo 的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用 from xxx import * 而导入；\n\n以双下划线开头的 __foo 代表类的私有成员；以双下划线开头和结尾的 __foo__ 代表 Python 里特殊方法专用的标识，如 __init__() 代表类的构造函数。\n\nPython 可以同一行显示多条语句，方法是用分号 ; 分开，\n\n#### 行和缩进\npython 最具特色的就是用缩进来写模块。\n缩进的空白数量是可变的，但是所有代码块语句必须包含相同的缩进空白数量，这个必须严格执行\n因此，在 Python 的代码块中必须使用相同数目的行首缩进空格数。\n建议你在每个缩进层次使用 单个制表符 或 两个空格 或 四个空格 , 切记不能混用\n\n####  多行语句\nPython语句中一般以新行作为为语句的结束符。\n但是我们可以使用斜杠（ \\）将一行的语句分为多行显示，\n\n#### Python 引号\n\nPython 可以使用引号( ' )、双引号( \" )、三引号( ''' 或 \"\"\" ) 来表示字符串，引号的开始与结束必须的相同类型的。\n其中三引号可以由多行组成，编写多行文本的快捷语法，常用语文档字符串，在文件的特定地点，被当做注释。\n\n#### Python注释\npython 中多行注释使用三个单引号(''')或三个双引号(\"\"\")。\n\n#### Python空行\n函数之间或类的方法之间用空行分隔，表示一段新的代码的开始。类和函数入口之间也用一行空行分隔，以突出函数入口的开始。\n空行与代码缩进不同，空行并不是Python语法的一部分。书写时不插入空行，Python解释器运行也不会出错。但是空行的作用在于分隔两段不同功能或含义的代码，便于日后代码的维护或重构。\n记住：空行也是程序代码的一部分。\n\n#### Python 变量类型\n\n变量存储在内存中的值。这就意味着在创建变量时会在内存中开辟一个空间。\n基于变量的数据类型，解释器会分配指定内存，并决定什么数据可以被存储在内存中。\n因此，变量可以指定不同的数据类型，这些变量可以存储整数，小数或字符。\n\n**变量赋值**\n\nPython 中的变量赋值不需要类型声明。\n每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。\n每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。\n等号（=）用来给变量赋值。\n\n#### python 标准数据类型\n\npython 定义了一些标准类型，用于存储各种类型的数据。\nPython有五个标准的数据类型：\nNumbers（数字）\nString（字符串）\nList（列表）\nTuple（元组）\nDictionary（字典）\n\n#### Python数字\n\n数字数据类型用于存储数值。\n他们是不可改变的数据类型，这意味着改变数字数据类型会分配一个新的对象。\n当你指定一个值时，Number对象就会被创建：\n您也可以使用del语句删除一些对象的引用。\ndel语句的语法是：\n\ndel var1[,var2[,var3[....,varN]]]]\n\n您可以通过使用del语句删除单个或多个对象的引用。例如：\n\ndel var\ndel var_a, var_b\n\nPython支持四种不同的数字类型：\n\nint（有符号整型）\n\nlong（长整型[也可以代表八进制和十六进制]）\n\nfloat（浮点型）\n\ncomplex（复数）\n\n#### Python字符串\n\n字符串或串(String)是由数字、字母、下划线组成的一串字符。\n\n一般记为 :\ns=\"a1a2···an\"(n>=0)\n它是编程语言中表示文本的数据类型。\n\n\npython的字串列表有2种取值顺序:\n\n从左到右索引默认0开始的，最大范围是字符串长度少1\n\n从右到左索引默认-1开始的，最大范围是字符串开头\n\n当使用以冒号分隔的字符串，python返回一个新的对象，结果包含了以这对偏移标识的连续的内容，左边的开始是包含了下边界。\n上面的结果包含了s[1]的值l，而取到的最大范围不包括上边界，就是s[5]的值p。\n\n加号（+）是字符串连接运算符，星号（*）是重复操作\n\n#### Python列表\nList（列表） 是 Python 中使用最频繁的数据类型。\n#### python元组\n\n元组是另一个数据类型，类似于List（列表）。\n\n元组用\"()\"标识。内部元素用逗号隔开。但是元组不能二次赋值，相当于只读列表\n\n#### Python 字典\n","source":"_posts/技术/hexo/oldblog/blog21.md","raw":"\n---\n\ntitle: Python\ndate: 2019-07-16\ntags:\n\n---\n\n此处简介\n<!--more-->\n\n`the record for python`\n### Python 知识点\n\n#### 标识\b符号\n在 Python 里，标识符有字母、数字、下划线组成。\n在 Python 中，所有标识符可以包括英文、数字以及下划线(_)，但不能以数字开头。\n\n\n以下划线开头的标识符是有特殊意义的。以单下划线开头 _foo 的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用 from xxx import * 而导入；\n\n以双下划线开头的 __foo 代表类的私有成员；以双下划线开头和结尾的 __foo__ 代表 Python 里特殊方法专用的标识，如 __init__() 代表类的构造函数。\n\nPython 可以同一行显示多条语句，方法是用分号 ; 分开，\n\n#### 行和缩进\npython 最具特色的就是用缩进来写模块。\n缩进的空白数量是可变的，但是所有代码块语句必须包含相同的缩进空白数量，这个必须严格执行\n因此，在 Python 的代码块中必须使用相同数目的行首缩进空格数。\n建议你在每个缩进层次使用 单个制表符 或 两个空格 或 四个空格 , 切记不能混用\n\n####  多行语句\nPython语句中一般以新行作为为语句的结束符。\n但是我们可以使用斜杠（ \\）将一行的语句分为多行显示，\n\n#### Python 引号\n\nPython 可以使用引号( ' )、双引号( \" )、三引号( ''' 或 \"\"\" ) 来表示字符串，引号的开始与结束必须的相同类型的。\n其中三引号可以由多行组成，编写多行文本的快捷语法，常用语文档字符串，在文件的特定地点，被当做注释。\n\n#### Python注释\npython 中多行注释使用三个单引号(''')或三个双引号(\"\"\")。\n\n#### Python空行\n函数之间或类的方法之间用空行分隔，表示一段新的代码的开始。类和函数入口之间也用一行空行分隔，以突出函数入口的开始。\n空行与代码缩进不同，空行并不是Python语法的一部分。书写时不插入空行，Python解释器运行也不会出错。但是空行的作用在于分隔两段不同功能或含义的代码，便于日后代码的维护或重构。\n记住：空行也是程序代码的一部分。\n\n#### Python 变量类型\n\n变量存储在内存中的值。这就意味着在创建变量时会在内存中开辟一个空间。\n基于变量的数据类型，解释器会分配指定内存，并决定什么数据可以被存储在内存中。\n因此，变量可以指定不同的数据类型，这些变量可以存储整数，小数或字符。\n\n**变量赋值**\n\nPython 中的变量赋值不需要类型声明。\n每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。\n每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。\n等号（=）用来给变量赋值。\n\n#### python 标准数据类型\n\npython 定义了一些标准类型，用于存储各种类型的数据。\nPython有五个标准的数据类型：\nNumbers（数字）\nString（字符串）\nList（列表）\nTuple（元组）\nDictionary（字典）\n\n#### Python数字\n\n数字数据类型用于存储数值。\n他们是不可改变的数据类型，这意味着改变数字数据类型会分配一个新的对象。\n当你指定一个值时，Number对象就会被创建：\n您也可以使用del语句删除一些对象的引用。\ndel语句的语法是：\n\ndel var1[,var2[,var3[....,varN]]]]\n\n您可以通过使用del语句删除单个或多个对象的引用。例如：\n\ndel var\ndel var_a, var_b\n\nPython支持四种不同的数字类型：\n\nint（有符号整型）\n\nlong（长整型[也可以代表八进制和十六进制]）\n\nfloat（浮点型）\n\ncomplex（复数）\n\n#### Python字符串\n\n字符串或串(String)是由数字、字母、下划线组成的一串字符。\n\n一般记为 :\ns=\"a1a2···an\"(n>=0)\n它是编程语言中表示文本的数据类型。\n\n\npython的字串列表有2种取值顺序:\n\n从左到右索引默认0开始的，最大范围是字符串长度少1\n\n从右到左索引默认-1开始的，最大范围是字符串开头\n\n当使用以冒号分隔的字符串，python返回一个新的对象，结果包含了以这对偏移标识的连续的内容，左边的开始是包含了下边界。\n上面的结果包含了s[1]的值l，而取到的最大范围不包括上边界，就是s[5]的值p。\n\n加号（+）是字符串连接运算符，星号（*）是重复操作\n\n#### Python列表\nList（列表） 是 Python 中使用最频繁的数据类型。\n#### python元组\n\n元组是另一个数据类型，类似于List（列表）。\n\n元组用\"()\"标识。内部元素用逗号隔开。但是元组不能二次赋值，相当于只读列表\n\n#### Python 字典\n","slug":"技术/hexo/oldblog/blog21","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33d002y38pwfho691jv","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<p><code>the record for python</code></p>\n<h3 id=\"Python-知识点\"><a href=\"#Python-知识点\" class=\"headerlink\" title=\"Python 知识点\"></a>Python 知识点</h3><h4 id=\"标识符号\"><a href=\"#标识符号\" class=\"headerlink\" title=\"标识\b符号\"></a>标识\b符号</h4><p>在 Python 里，标识符有字母、数字、下划线组成。<br>在 Python 中，所有标识符可以包括英文、数字以及下划线(_)，但不能以数字开头。</p>\n<p>以下划线开头的标识符是有特殊意义的。以单下划线开头 _foo 的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用 from xxx import * 而导入；</p>\n<p>以双下划线开头的 <strong>foo 代表类的私有成员；以双下划线开头和结尾的 __foo</strong> 代表 Python 里特殊方法专用的标识，如 <strong>init</strong>() 代表类的构造函数。</p>\n<p>Python 可以同一行显示多条语句，方法是用分号 ; 分开，</p>\n<h4 id=\"行和缩进\"><a href=\"#行和缩进\" class=\"headerlink\" title=\"行和缩进\"></a>行和缩进</h4><p>python 最具特色的就是用缩进来写模块。<br>缩进的空白数量是可变的，但是所有代码块语句必须包含相同的缩进空白数量，这个必须严格执行<br>因此，在 Python 的代码块中必须使用相同数目的行首缩进空格数。<br>建议你在每个缩进层次使用 单个制表符 或 两个空格 或 四个空格 , 切记不能混用</p>\n<h4 id=\"多行语句\"><a href=\"#多行语句\" class=\"headerlink\" title=\"多行语句\"></a>多行语句</h4><p>Python语句中一般以新行作为为语句的结束符。<br>但是我们可以使用斜杠（ \\）将一行的语句分为多行显示，</p>\n<h4 id=\"Python-引号\"><a href=\"#Python-引号\" class=\"headerlink\" title=\"Python 引号\"></a>Python 引号</h4><p>Python 可以使用引号( ‘ )、双引号( “ )、三引号( ‘’’ 或 “”” ) 来表示字符串，引号的开始与结束必须的相同类型的。<br>其中三引号可以由多行组成，编写多行文本的快捷语法，常用语文档字符串，在文件的特定地点，被当做注释。</p>\n<h4 id=\"Python注释\"><a href=\"#Python注释\" class=\"headerlink\" title=\"Python注释\"></a>Python注释</h4><p>python 中多行注释使用三个单引号(‘’’)或三个双引号(“””)。</p>\n<h4 id=\"Python空行\"><a href=\"#Python空行\" class=\"headerlink\" title=\"Python空行\"></a>Python空行</h4><p>函数之间或类的方法之间用空行分隔，表示一段新的代码的开始。类和函数入口之间也用一行空行分隔，以突出函数入口的开始。<br>空行与代码缩进不同，空行并不是Python语法的一部分。书写时不插入空行，Python解释器运行也不会出错。但是空行的作用在于分隔两段不同功能或含义的代码，便于日后代码的维护或重构。<br>记住：空行也是程序代码的一部分。</p>\n<h4 id=\"Python-变量类型\"><a href=\"#Python-变量类型\" class=\"headerlink\" title=\"Python 变量类型\"></a>Python 变量类型</h4><p>变量存储在内存中的值。这就意味着在创建变量时会在内存中开辟一个空间。<br>基于变量的数据类型，解释器会分配指定内存，并决定什么数据可以被存储在内存中。<br>因此，变量可以指定不同的数据类型，这些变量可以存储整数，小数或字符。</p>\n<p><strong>变量赋值</strong></p>\n<p>Python 中的变量赋值不需要类型声明。<br>每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。<br>每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。<br>等号（=）用来给变量赋值。</p>\n<h4 id=\"python-标准数据类型\"><a href=\"#python-标准数据类型\" class=\"headerlink\" title=\"python 标准数据类型\"></a>python 标准数据类型</h4><p>python 定义了一些标准类型，用于存储各种类型的数据。<br>Python有五个标准的数据类型：<br>Numbers（数字）<br>String（字符串）<br>List（列表）<br>Tuple（元组）<br>Dictionary（字典）</p>\n<h4 id=\"Python数字\"><a href=\"#Python数字\" class=\"headerlink\" title=\"Python数字\"></a>Python数字</h4><p>数字数据类型用于存储数值。<br>他们是不可改变的数据类型，这意味着改变数字数据类型会分配一个新的对象。<br>当你指定一个值时，Number对象就会被创建：<br>您也可以使用del语句删除一些对象的引用。<br>del语句的语法是：</p>\n<p>del var1[,var2[,var3[….,varN]]]]</p>\n<p>您可以通过使用del语句删除单个或多个对象的引用。例如：</p>\n<p>del var<br>del var_a, var_b</p>\n<p>Python支持四种不同的数字类型：</p>\n<p>int（有符号整型）</p>\n<p>long（长整型[也可以代表八进制和十六进制]）</p>\n<p>float（浮点型）</p>\n<p>complex（复数）</p>\n<h4 id=\"Python字符串\"><a href=\"#Python字符串\" class=\"headerlink\" title=\"Python字符串\"></a>Python字符串</h4><p>字符串或串(String)是由数字、字母、下划线组成的一串字符。</p>\n<p>一般记为 :<br>s=”a1a2···an”(n&gt;=0)<br>它是编程语言中表示文本的数据类型。</p>\n<p>python的字串列表有2种取值顺序:</p>\n<p>从左到右索引默认0开始的，最大范围是字符串长度少1</p>\n<p>从右到左索引默认-1开始的，最大范围是字符串开头</p>\n<p>当使用以冒号分隔的字符串，python返回一个新的对象，结果包含了以这对偏移标识的连续的内容，左边的开始是包含了下边界。<br>上面的结果包含了s[1]的值l，而取到的最大范围不包括上边界，就是s[5]的值p。</p>\n<p>加号（+）是字符串连接运算符，星号（*）是重复操作</p>\n<h4 id=\"Python列表\"><a href=\"#Python列表\" class=\"headerlink\" title=\"Python列表\"></a>Python列表</h4><p>List（列表） 是 Python 中使用最频繁的数据类型。</p>\n<h4 id=\"python元组\"><a href=\"#python元组\" class=\"headerlink\" title=\"python元组\"></a>python元组</h4><p>元组是另一个数据类型，类似于List（列表）。</p>\n<p>元组用”()”标识。内部元素用逗号隔开。但是元组不能二次赋值，相当于只读列表</p>\n<h4 id=\"Python-字典\"><a href=\"#Python-字典\" class=\"headerlink\" title=\"Python 字典\"></a>Python 字典</h4>","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<p><code>the record for python</code></p>\n<h3 id=\"Python-知识点\"><a href=\"#Python-知识点\" class=\"headerlink\" title=\"Python 知识点\"></a>Python 知识点</h3><h4 id=\"标识符号\"><a href=\"#标识符号\" class=\"headerlink\" title=\"标识\b符号\"></a>标识\b符号</h4><p>在 Python 里，标识符有字母、数字、下划线组成。<br>在 Python 中，所有标识符可以包括英文、数字以及下划线(_)，但不能以数字开头。</p>\n<p>以下划线开头的标识符是有特殊意义的。以单下划线开头 _foo 的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用 from xxx import * 而导入；</p>\n<p>以双下划线开头的 <strong>foo 代表类的私有成员；以双下划线开头和结尾的 __foo</strong> 代表 Python 里特殊方法专用的标识，如 <strong>init</strong>() 代表类的构造函数。</p>\n<p>Python 可以同一行显示多条语句，方法是用分号 ; 分开，</p>\n<h4 id=\"行和缩进\"><a href=\"#行和缩进\" class=\"headerlink\" title=\"行和缩进\"></a>行和缩进</h4><p>python 最具特色的就是用缩进来写模块。<br>缩进的空白数量是可变的，但是所有代码块语句必须包含相同的缩进空白数量，这个必须严格执行<br>因此，在 Python 的代码块中必须使用相同数目的行首缩进空格数。<br>建议你在每个缩进层次使用 单个制表符 或 两个空格 或 四个空格 , 切记不能混用</p>\n<h4 id=\"多行语句\"><a href=\"#多行语句\" class=\"headerlink\" title=\"多行语句\"></a>多行语句</h4><p>Python语句中一般以新行作为为语句的结束符。<br>但是我们可以使用斜杠（ \\）将一行的语句分为多行显示，</p>\n<h4 id=\"Python-引号\"><a href=\"#Python-引号\" class=\"headerlink\" title=\"Python 引号\"></a>Python 引号</h4><p>Python 可以使用引号( ‘ )、双引号( “ )、三引号( ‘’’ 或 “”” ) 来表示字符串，引号的开始与结束必须的相同类型的。<br>其中三引号可以由多行组成，编写多行文本的快捷语法，常用语文档字符串，在文件的特定地点，被当做注释。</p>\n<h4 id=\"Python注释\"><a href=\"#Python注释\" class=\"headerlink\" title=\"Python注释\"></a>Python注释</h4><p>python 中多行注释使用三个单引号(‘’’)或三个双引号(“””)。</p>\n<h4 id=\"Python空行\"><a href=\"#Python空行\" class=\"headerlink\" title=\"Python空行\"></a>Python空行</h4><p>函数之间或类的方法之间用空行分隔，表示一段新的代码的开始。类和函数入口之间也用一行空行分隔，以突出函数入口的开始。<br>空行与代码缩进不同，空行并不是Python语法的一部分。书写时不插入空行，Python解释器运行也不会出错。但是空行的作用在于分隔两段不同功能或含义的代码，便于日后代码的维护或重构。<br>记住：空行也是程序代码的一部分。</p>\n<h4 id=\"Python-变量类型\"><a href=\"#Python-变量类型\" class=\"headerlink\" title=\"Python 变量类型\"></a>Python 变量类型</h4><p>变量存储在内存中的值。这就意味着在创建变量时会在内存中开辟一个空间。<br>基于变量的数据类型，解释器会分配指定内存，并决定什么数据可以被存储在内存中。<br>因此，变量可以指定不同的数据类型，这些变量可以存储整数，小数或字符。</p>\n<p><strong>变量赋值</strong></p>\n<p>Python 中的变量赋值不需要类型声明。<br>每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。<br>每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。<br>等号（=）用来给变量赋值。</p>\n<h4 id=\"python-标准数据类型\"><a href=\"#python-标准数据类型\" class=\"headerlink\" title=\"python 标准数据类型\"></a>python 标准数据类型</h4><p>python 定义了一些标准类型，用于存储各种类型的数据。<br>Python有五个标准的数据类型：<br>Numbers（数字）<br>String（字符串）<br>List（列表）<br>Tuple（元组）<br>Dictionary（字典）</p>\n<h4 id=\"Python数字\"><a href=\"#Python数字\" class=\"headerlink\" title=\"Python数字\"></a>Python数字</h4><p>数字数据类型用于存储数值。<br>他们是不可改变的数据类型，这意味着改变数字数据类型会分配一个新的对象。<br>当你指定一个值时，Number对象就会被创建：<br>您也可以使用del语句删除一些对象的引用。<br>del语句的语法是：</p>\n<p>del var1[,var2[,var3[….,varN]]]]</p>\n<p>您可以通过使用del语句删除单个或多个对象的引用。例如：</p>\n<p>del var<br>del var_a, var_b</p>\n<p>Python支持四种不同的数字类型：</p>\n<p>int（有符号整型）</p>\n<p>long（长整型[也可以代表八进制和十六进制]）</p>\n<p>float（浮点型）</p>\n<p>complex（复数）</p>\n<h4 id=\"Python字符串\"><a href=\"#Python字符串\" class=\"headerlink\" title=\"Python字符串\"></a>Python字符串</h4><p>字符串或串(String)是由数字、字母、下划线组成的一串字符。</p>\n<p>一般记为 :<br>s=”a1a2···an”(n&gt;=0)<br>它是编程语言中表示文本的数据类型。</p>\n<p>python的字串列表有2种取值顺序:</p>\n<p>从左到右索引默认0开始的，最大范围是字符串长度少1</p>\n<p>从右到左索引默认-1开始的，最大范围是字符串开头</p>\n<p>当使用以冒号分隔的字符串，python返回一个新的对象，结果包含了以这对偏移标识的连续的内容，左边的开始是包含了下边界。<br>上面的结果包含了s[1]的值l，而取到的最大范围不包括上边界，就是s[5]的值p。</p>\n<p>加号（+）是字符串连接运算符，星号（*）是重复操作</p>\n<h4 id=\"Python列表\"><a href=\"#Python列表\" class=\"headerlink\" title=\"Python列表\"></a>Python列表</h4><p>List（列表） 是 Python 中使用最频繁的数据类型。</p>\n<h4 id=\"python元组\"><a href=\"#python元组\" class=\"headerlink\" title=\"python元组\"></a>python元组</h4><p>元组是另一个数据类型，类似于List（列表）。</p>\n<p>元组用”()”标识。内部元素用逗号隔开。但是元组不能二次赋值，相当于只读列表</p>\n<h4 id=\"Python-字典\"><a href=\"#Python-字典\" class=\"headerlink\" title=\"Python 字典\"></a>Python 字典</h4>"},{"title":"flume小结","date":"2019-07-15T16:00:00.000Z","_content":"此处简介\n<!--more-->\n\n## flume小结\n`此次flume环境的搭建是针对实际日志业务，整个过程还算顺利`\n针对flume的引入更多的偏向应用层面。所以更多的要熟悉相关配置与参数的设置\n\n### flume的整体构思\n采用的是flume框架中的flume-ng。整体架构如下图\n![c6589103d8ba4dffaf21c52b37cc7e17-image.png](//img.wqkenqing.ren/file/2017/7/c6589103d8ba4dffaf21c52b37cc7e17-image.png)\nlog_product环节尚有争议，主要针对flume环节进行小结。\n原从效率上考虑，打算在跳板机上搭建直接接入hadoop的单flume节点，因为网络权限等问题，无法直接写入所以放弃。转而改为在hadoop环境中也引入一个flume节点(flume-server)。因client是单节点，所以没有必要引入fail-over机制。因此flume-server也是单节点\n。\n\n---\n\n###\n写入hdfs时有三个参数要注意\nrollSize\nrollCount\nrollInterval\n这三个参数对写入单个hdfs文件时的大小，行，时间。\n\n---\nflume-ng agent -n agent1 -c conf -f flume-client.properties -Dflume.root.logger=DEBUG,console\n\nflume-ng agent -n agent1 -c conf -f flume-client.properties -Dflume.root.logger=DEBUG,console &\n\nflume-ng agent -n a1 -c ../conf -f flume-server.properties -Dflume.root.logger=DEBUG,console &","source":"_posts/技术/hexo/oldblog/blog18.md","raw":"---\n\ntitle: flume小结\ndate: 2019-07-16\ntags: \n\n---\n此处简介\n<!--more-->\n\n## flume小结\n`此次flume环境的搭建是针对实际日志业务，整个过程还算顺利`\n针对flume的引入更多的偏向应用层面。所以更多的要熟悉相关配置与参数的设置\n\n### flume的整体构思\n采用的是flume框架中的flume-ng。整体架构如下图\n![c6589103d8ba4dffaf21c52b37cc7e17-image.png](//img.wqkenqing.ren/file/2017/7/c6589103d8ba4dffaf21c52b37cc7e17-image.png)\nlog_product环节尚有争议，主要针对flume环节进行小结。\n原从效率上考虑，打算在跳板机上搭建直接接入hadoop的单flume节点，因为网络权限等问题，无法直接写入所以放弃。转而改为在hadoop环境中也引入一个flume节点(flume-server)。因client是单节点，所以没有必要引入fail-over机制。因此flume-server也是单节点\n。\n\n---\n\n###\n写入hdfs时有三个参数要注意\nrollSize\nrollCount\nrollInterval\n这三个参数对写入单个hdfs文件时的大小，行，时间。\n\n---\nflume-ng agent -n agent1 -c conf -f flume-client.properties -Dflume.root.logger=DEBUG,console\n\nflume-ng agent -n agent1 -c conf -f flume-client.properties -Dflume.root.logger=DEBUG,console &\n\nflume-ng agent -n a1 -c ../conf -f flume-server.properties -Dflume.root.logger=DEBUG,console &","slug":"技术/hexo/oldblog/blog18","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33e002z38pwbte52xtd","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h2 id=\"flume小结\"><a href=\"#flume小结\" class=\"headerlink\" title=\"flume小结\"></a>flume小结</h2><p><code>此次flume环境的搭建是针对实际日志业务，整个过程还算顺利</code><br>针对flume的引入更多的偏向应用层面。所以更多的要熟悉相关配置与参数的设置</p>\n<h3 id=\"flume的整体构思\"><a href=\"#flume的整体构思\" class=\"headerlink\" title=\"flume的整体构思\"></a>flume的整体构思</h3><p>采用的是flume框架中的flume-ng。整体架构如下图<br><img src=\"//img.wqkenqing.ren/file/2017/7/c6589103d8ba4dffaf21c52b37cc7e17-image.png\" alt=\"c6589103d8ba4dffaf21c52b37cc7e17-image.png\"><br>log_product环节尚有争议，主要针对flume环节进行小结。<br>原从效率上考虑，打算在跳板机上搭建直接接入hadoop的单flume节点，因为网络权限等问题，无法直接写入所以放弃。转而改为在hadoop环境中也引入一个flume节点(flume-server)。因client是单节点，所以没有必要引入fail-over机制。因此flume-server也是单节点<br>。</p>\n<hr>\n<p>###<br>写入hdfs时有三个参数要注意<br>rollSize<br>rollCount<br>rollInterval<br>这三个参数对写入单个hdfs文件时的大小，行，时间。</p>\n<hr>\n<p>flume-ng agent -n agent1 -c conf -f flume-client.properties -Dflume.root.logger=DEBUG,console</p>\n<p>flume-ng agent -n agent1 -c conf -f flume-client.properties -Dflume.root.logger=DEBUG,console &amp;</p>\n<p>flume-ng agent -n a1 -c ../conf -f flume-server.properties -Dflume.root.logger=DEBUG,console &amp;</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h2 id=\"flume小结\"><a href=\"#flume小结\" class=\"headerlink\" title=\"flume小结\"></a>flume小结</h2><p><code>此次flume环境的搭建是针对实际日志业务，整个过程还算顺利</code><br>针对flume的引入更多的偏向应用层面。所以更多的要熟悉相关配置与参数的设置</p>\n<h3 id=\"flume的整体构思\"><a href=\"#flume的整体构思\" class=\"headerlink\" title=\"flume的整体构思\"></a>flume的整体构思</h3><p>采用的是flume框架中的flume-ng。整体架构如下图<br><img src=\"//img.wqkenqing.ren/file/2017/7/c6589103d8ba4dffaf21c52b37cc7e17-image.png\" alt=\"c6589103d8ba4dffaf21c52b37cc7e17-image.png\"><br>log_product环节尚有争议，主要针对flume环节进行小结。<br>原从效率上考虑，打算在跳板机上搭建直接接入hadoop的单flume节点，因为网络权限等问题，无法直接写入所以放弃。转而改为在hadoop环境中也引入一个flume节点(flume-server)。因client是单节点，所以没有必要引入fail-over机制。因此flume-server也是单节点<br>。</p>\n<hr>\n<p>###<br>写入hdfs时有三个参数要注意<br>rollSize<br>rollCount<br>rollInterval<br>这三个参数对写入单个hdfs文件时的大小，行，时间。</p>\n<hr>\n<p>flume-ng agent -n agent1 -c conf -f flume-client.properties -Dflume.root.logger=DEBUG,console</p>\n<p>flume-ng agent -n agent1 -c conf -f flume-client.properties -Dflume.root.logger=DEBUG,console &amp;</p>\n<p>flume-ng agent -n a1 -c ../conf -f flume-server.properties -Dflume.root.logger=DEBUG,console &amp;</p>"},{"title":"JVM问题","date":"2019-07-15T16:00:00.000Z","_content":"此处简介\n<!--more-->\n\nJVM问题\n1、堆内存溢出\n2、持久代内存溢出\n3、系统频繁FGC\n\n框架使用不当\n4、错误使用框架提供API\n5、日志框架使用不当\nOS内存溢出\n6、某系统物理内存溢出\n数据库问题\n7、慢SQL问题\n\n案例1、堆内存溢出\nJVM基础知识\n1、Jvm内存分为三个大区，young区，old区和perm区；其中young区又包含三个区：Edgn区、S0、S1区\n2、young区和old区属于heap区，占据堆内存；perm区称为持久代，不占据堆内存。\n堆内存溢出\n性能问题发现过程\n\n查看服务器上报错日志，发现有如下报错信息［java.lang.OutOfMemoryError: Java heap space］；根据报错信息确定是jvm 堆内存空间不够导致，于是使用jvm命令（下图）查看，发现此时old区内存空间已经被占满了，同时使用jvisualvm监控工具也\n发现old区空间被占满（右图），整个heap区空间已经无法再容纳新对象进入。\n建议\n考虑大量数据一次性写入内存场景\n\n持久代内存溢出\n现象\n\n压测某系统接口，压测前1分钟左右tps 400多，之后\nTps直降为零，后台报错日志：java.lang.OutOfMemoryError:PermGen space，通过jvm监控工具查看持久代（perm区）空间被占满，Old区空闲；\n\n问题定位过程\n通过注释代码块定位问题，考虑到perm区溢出大部分跟类对象大量创建有关，故锁定问题在序列化框架使用可能有问题；\n对于比较棘手难解决的perm溢出问题，作者构建了一个perm区溢出的场景，可以采用如下定位方案\n1、添加jvm dump配置\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/data/dump.bin\n2、安装eclipse mat分析工具\n3、将dump文件导入eclipse，点击［Leak Suspects］，找到跟公司有关的代码进行分析\n此处不过多讲解，大家可以去网上查阅资料学习\n解决办法\n跟开发沟通后选择去掉msgpack0.6版本框架，采用java原成序列化框架，修改后系统tps稳定在400多，gc情况正常\n修改前后gc情况对比\n修复前\n类似问题如何避免\n\n1、去掉项目无用jar包\n2、避免大量使用类对象、大量使用反射\n案例3、频繁FGC\n（1）系统某接口频繁FGC\n问题排查：\n先查JVM内存信息找可疑对象\n从内存对象实例信息中发现跟mysql连接有关，然后检测mysql配置信息\n <bean id=\"dataSource\" class=\"org.springframework.jdbc.datasource.DriverManagerDataSource\">\n\n发现系统采用的是 spring框架的数据源，没有用连接池；\n\n思考\n使用连接池有什么好处？\n连接复用、减少连接重复建立和销毁造成的大量资源消耗\n\n\n然后换做hikaricp连接池做对比测试\n <bean id=\"dataSource\" class=\"com.zaxxer.hikari.HikariDataSource\"\n压测半小时未出现fgc，问题得到解决\n类似问题如何避免\n\n1、研发规范统一DB连接池，避免研发误用\n2、减少大对象、临时对象使用\n\n案例4、错误使用框架提供API\n现象\n某系统本身业务逻辑处理能力很快（研发本机自测tps可以到达2w多），但是接入到framework框架后，TPS最高只能到达300笔/S左右，而且系统负载很低\n\n问题排查\n根据这种现象说明系统可能是堵在了某块方法上，根据这种情况一般采用线程dump的方式来查看系统具体哪些线程出现异常情况，通过线程dump 发现 ［TIMED_WAITING］状态的业务线程占比很高\n根据线程dump信息，找到公司包名开头的信息，然后从下往上查看\n线程dump信息，从信息中我们可以看到\n\nframework.servlet.fServlet.doPost：框架api封装了servlet dopost方法做了某些操作\nframework.servlet.fServlet.execute：框架api执行servelt\nframework.process.fProcessor.process：框架api进行自身逻辑处理\nframework.filter.impl.AuthFilter.before：框架使用过滤器进行用户权限过滤\n\n 。。。。。。然后就是进行http请求操作\n由此我们断定，就是在框架进行权限校验这块堵住了。之后跟开发沟通这块的\n问题即可\n\n分析思路\n压测端 ：net  服务器  jvm\n服务端：net  服务器 nginx tomcat jvm（应用程序）算法 db（mysql redis）","source":"_posts/技术/hexo/oldblog/blog19.md","raw":"---\n\ntitle: JVM问题\ndate: 2019-07-16\ntags: \n\n---\n此处简介\n<!--more-->\n\nJVM问题\n1、堆内存溢出\n2、持久代内存溢出\n3、系统频繁FGC\n\n框架使用不当\n4、错误使用框架提供API\n5、日志框架使用不当\nOS内存溢出\n6、某系统物理内存溢出\n数据库问题\n7、慢SQL问题\n\n案例1、堆内存溢出\nJVM基础知识\n1、Jvm内存分为三个大区，young区，old区和perm区；其中young区又包含三个区：Edgn区、S0、S1区\n2、young区和old区属于heap区，占据堆内存；perm区称为持久代，不占据堆内存。\n堆内存溢出\n性能问题发现过程\n\n查看服务器上报错日志，发现有如下报错信息［java.lang.OutOfMemoryError: Java heap space］；根据报错信息确定是jvm 堆内存空间不够导致，于是使用jvm命令（下图）查看，发现此时old区内存空间已经被占满了，同时使用jvisualvm监控工具也\n发现old区空间被占满（右图），整个heap区空间已经无法再容纳新对象进入。\n建议\n考虑大量数据一次性写入内存场景\n\n持久代内存溢出\n现象\n\n压测某系统接口，压测前1分钟左右tps 400多，之后\nTps直降为零，后台报错日志：java.lang.OutOfMemoryError:PermGen space，通过jvm监控工具查看持久代（perm区）空间被占满，Old区空闲；\n\n问题定位过程\n通过注释代码块定位问题，考虑到perm区溢出大部分跟类对象大量创建有关，故锁定问题在序列化框架使用可能有问题；\n对于比较棘手难解决的perm溢出问题，作者构建了一个perm区溢出的场景，可以采用如下定位方案\n1、添加jvm dump配置\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/data/dump.bin\n2、安装eclipse mat分析工具\n3、将dump文件导入eclipse，点击［Leak Suspects］，找到跟公司有关的代码进行分析\n此处不过多讲解，大家可以去网上查阅资料学习\n解决办法\n跟开发沟通后选择去掉msgpack0.6版本框架，采用java原成序列化框架，修改后系统tps稳定在400多，gc情况正常\n修改前后gc情况对比\n修复前\n类似问题如何避免\n\n1、去掉项目无用jar包\n2、避免大量使用类对象、大量使用反射\n案例3、频繁FGC\n（1）系统某接口频繁FGC\n问题排查：\n先查JVM内存信息找可疑对象\n从内存对象实例信息中发现跟mysql连接有关，然后检测mysql配置信息\n <bean id=\"dataSource\" class=\"org.springframework.jdbc.datasource.DriverManagerDataSource\">\n\n发现系统采用的是 spring框架的数据源，没有用连接池；\n\n思考\n使用连接池有什么好处？\n连接复用、减少连接重复建立和销毁造成的大量资源消耗\n\n\n然后换做hikaricp连接池做对比测试\n <bean id=\"dataSource\" class=\"com.zaxxer.hikari.HikariDataSource\"\n压测半小时未出现fgc，问题得到解决\n类似问题如何避免\n\n1、研发规范统一DB连接池，避免研发误用\n2、减少大对象、临时对象使用\n\n案例4、错误使用框架提供API\n现象\n某系统本身业务逻辑处理能力很快（研发本机自测tps可以到达2w多），但是接入到framework框架后，TPS最高只能到达300笔/S左右，而且系统负载很低\n\n问题排查\n根据这种现象说明系统可能是堵在了某块方法上，根据这种情况一般采用线程dump的方式来查看系统具体哪些线程出现异常情况，通过线程dump 发现 ［TIMED_WAITING］状态的业务线程占比很高\n根据线程dump信息，找到公司包名开头的信息，然后从下往上查看\n线程dump信息，从信息中我们可以看到\n\nframework.servlet.fServlet.doPost：框架api封装了servlet dopost方法做了某些操作\nframework.servlet.fServlet.execute：框架api执行servelt\nframework.process.fProcessor.process：框架api进行自身逻辑处理\nframework.filter.impl.AuthFilter.before：框架使用过滤器进行用户权限过滤\n\n 。。。。。。然后就是进行http请求操作\n由此我们断定，就是在框架进行权限校验这块堵住了。之后跟开发沟通这块的\n问题即可\n\n分析思路\n压测端 ：net  服务器  jvm\n服务端：net  服务器 nginx tomcat jvm（应用程序）算法 db（mysql redis）","slug":"技术/hexo/oldblog/blog19","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33f003038pw957c69re","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<p>JVM问题<br>1、堆内存溢出<br>2、持久代内存溢出<br>3、系统频繁FGC</p>\n<p>框架使用不当<br>4、错误使用框架提供API<br>5、日志框架使用不当<br>OS内存溢出<br>6、某系统物理内存溢出<br>数据库问题<br>7、慢SQL问题</p>\n<p>案例1、堆内存溢出<br>JVM基础知识<br>1、Jvm内存分为三个大区，young区，old区和perm区；其中young区又包含三个区：Edgn区、S0、S1区<br>2、young区和old区属于heap区，占据堆内存；perm区称为持久代，不占据堆内存。<br>堆内存溢出<br>性能问题发现过程</p>\n<p>查看服务器上报错日志，发现有如下报错信息［java.lang.OutOfMemoryError: Java heap space］；根据报错信息确定是jvm 堆内存空间不够导致，于是使用jvm命令（下图）查看，发现此时old区内存空间已经被占满了，同时使用jvisualvm监控工具也<br>发现old区空间被占满（右图），整个heap区空间已经无法再容纳新对象进入。<br>建议<br>考虑大量数据一次性写入内存场景</p>\n<p>持久代内存溢出<br>现象</p>\n<p>压测某系统接口，压测前1分钟左右tps 400多，之后<br>Tps直降为零，后台报错日志：java.lang.OutOfMemoryError:PermGen space，通过jvm监控工具查看持久代（perm区）空间被占满，Old区空闲；</p>\n<p>问题定位过程<br>通过注释代码块定位问题，考虑到perm区溢出大部分跟类对象大量创建有关，故锁定问题在序列化框架使用可能有问题；<br>对于比较棘手难解决的perm溢出问题，作者构建了一个perm区溢出的场景，可以采用如下定位方案<br>1、添加jvm dump配置<br>-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/data/dump.bin<br>2、安装eclipse mat分析工具<br>3、将dump文件导入eclipse，点击［Leak Suspects］，找到跟公司有关的代码进行分析<br>此处不过多讲解，大家可以去网上查阅资料学习<br>解决办法<br>跟开发沟通后选择去掉msgpack0.6版本框架，采用java原成序列化框架，修改后系统tps稳定在400多，gc情况正常<br>修改前后gc情况对比<br>修复前<br>类似问题如何避免</p>\n<p>1、去掉项目无用jar包<br>2、避免大量使用类对象、大量使用反射<br>案例3、频繁FGC<br>（1）系统某接口频繁FGC<br>问题排查：<br>先查JVM内存信息找可疑对象<br>从内存对象实例信息中发现跟mysql连接有关，然后检测mysql配置信息<br> <bean id=\"dataSource\" class=\"org.springframework.jdbc.datasource.DriverManagerDataSource\"></p>\n<p>发现系统采用的是 spring框架的数据源，没有用连接池；</p>\n<p>思考<br>使用连接池有什么好处？<br>连接复用、减少连接重复建立和销毁造成的大量资源消耗</p>\n<p>然后换做hikaricp连接池做对比测试<br> &lt;bean id=”dataSource” class=”com.zaxxer.hikari.HikariDataSource”<br>压测半小时未出现fgc，问题得到解决<br>类似问题如何避免</p>\n<p>1、研发规范统一DB连接池，避免研发误用<br>2、减少大对象、临时对象使用</p>\n<p>案例4、错误使用框架提供API<br>现象<br>某系统本身业务逻辑处理能力很快（研发本机自测tps可以到达2w多），但是接入到framework框架后，TPS最高只能到达300笔/S左右，而且系统负载很低</p>\n<p>问题排查<br>根据这种现象说明系统可能是堵在了某块方法上，根据这种情况一般采用线程dump的方式来查看系统具体哪些线程出现异常情况，通过线程dump 发现 ［TIMED_WAITING］状态的业务线程占比很高<br>根据线程dump信息，找到公司包名开头的信息，然后从下往上查看<br>线程dump信息，从信息中我们可以看到</p>\n<p>framework.servlet.fServlet.doPost：框架api封装了servlet dopost方法做了某些操作<br>framework.servlet.fServlet.execute：框架api执行servelt<br>framework.process.fProcessor.process：框架api进行自身逻辑处理<br>framework.filter.impl.AuthFilter.before：框架使用过滤器进行用户权限过滤</p>\n<p> 。。。。。。然后就是进行http请求操作<br>由此我们断定，就是在框架进行权限校验这块堵住了。之后跟开发沟通这块的<br>问题即可</p>\n<p>分析思路<br>压测端 ：net  服务器  jvm<br>服务端：net  服务器 nginx tomcat jvm（应用程序）算法 db（mysql redis）</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<p>JVM问题<br>1、堆内存溢出<br>2、持久代内存溢出<br>3、系统频繁FGC</p>\n<p>框架使用不当<br>4、错误使用框架提供API<br>5、日志框架使用不当<br>OS内存溢出<br>6、某系统物理内存溢出<br>数据库问题<br>7、慢SQL问题</p>\n<p>案例1、堆内存溢出<br>JVM基础知识<br>1、Jvm内存分为三个大区，young区，old区和perm区；其中young区又包含三个区：Edgn区、S0、S1区<br>2、young区和old区属于heap区，占据堆内存；perm区称为持久代，不占据堆内存。<br>堆内存溢出<br>性能问题发现过程</p>\n<p>查看服务器上报错日志，发现有如下报错信息［java.lang.OutOfMemoryError: Java heap space］；根据报错信息确定是jvm 堆内存空间不够导致，于是使用jvm命令（下图）查看，发现此时old区内存空间已经被占满了，同时使用jvisualvm监控工具也<br>发现old区空间被占满（右图），整个heap区空间已经无法再容纳新对象进入。<br>建议<br>考虑大量数据一次性写入内存场景</p>\n<p>持久代内存溢出<br>现象</p>\n<p>压测某系统接口，压测前1分钟左右tps 400多，之后<br>Tps直降为零，后台报错日志：java.lang.OutOfMemoryError:PermGen space，通过jvm监控工具查看持久代（perm区）空间被占满，Old区空闲；</p>\n<p>问题定位过程<br>通过注释代码块定位问题，考虑到perm区溢出大部分跟类对象大量创建有关，故锁定问题在序列化框架使用可能有问题；<br>对于比较棘手难解决的perm溢出问题，作者构建了一个perm区溢出的场景，可以采用如下定位方案<br>1、添加jvm dump配置<br>-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/data/dump.bin<br>2、安装eclipse mat分析工具<br>3、将dump文件导入eclipse，点击［Leak Suspects］，找到跟公司有关的代码进行分析<br>此处不过多讲解，大家可以去网上查阅资料学习<br>解决办法<br>跟开发沟通后选择去掉msgpack0.6版本框架，采用java原成序列化框架，修改后系统tps稳定在400多，gc情况正常<br>修改前后gc情况对比<br>修复前<br>类似问题如何避免</p>\n<p>1、去掉项目无用jar包<br>2、避免大量使用类对象、大量使用反射<br>案例3、频繁FGC<br>（1）系统某接口频繁FGC<br>问题排查：<br>先查JVM内存信息找可疑对象<br>从内存对象实例信息中发现跟mysql连接有关，然后检测mysql配置信息<br> <bean id=\"dataSource\" class=\"org.springframework.jdbc.datasource.DriverManagerDataSource\"></p>\n<p>发现系统采用的是 spring框架的数据源，没有用连接池；</p>\n<p>思考<br>使用连接池有什么好处？<br>连接复用、减少连接重复建立和销毁造成的大量资源消耗</p>\n<p>然后换做hikaricp连接池做对比测试<br> &lt;bean id=”dataSource” class=”com.zaxxer.hikari.HikariDataSource”<br>压测半小时未出现fgc，问题得到解决<br>类似问题如何避免</p>\n<p>1、研发规范统一DB连接池，避免研发误用<br>2、减少大对象、临时对象使用</p>\n<p>案例4、错误使用框架提供API<br>现象<br>某系统本身业务逻辑处理能力很快（研发本机自测tps可以到达2w多），但是接入到framework框架后，TPS最高只能到达300笔/S左右，而且系统负载很低</p>\n<p>问题排查<br>根据这种现象说明系统可能是堵在了某块方法上，根据这种情况一般采用线程dump的方式来查看系统具体哪些线程出现异常情况，通过线程dump 发现 ［TIMED_WAITING］状态的业务线程占比很高<br>根据线程dump信息，找到公司包名开头的信息，然后从下往上查看<br>线程dump信息，从信息中我们可以看到</p>\n<p>framework.servlet.fServlet.doPost：框架api封装了servlet dopost方法做了某些操作<br>framework.servlet.fServlet.execute：框架api执行servelt<br>framework.process.fProcessor.process：框架api进行自身逻辑处理<br>framework.filter.impl.AuthFilter.before：框架使用过滤器进行用户权限过滤</p>\n<p> 。。。。。。然后就是进行http请求操作<br>由此我们断定，就是在框架进行权限校验这块堵住了。之后跟开发沟通这块的<br>问题即可</p>\n<p>分析思路<br>压测端 ：net  服务器  jvm<br>服务端：net  服务器 nginx tomcat jvm（应用程序）算法 db（mysql redis）</p>"},{"title":"String StringBuffer StringBuilder","date":"2019-07-15T16:00:00.000Z","_content":"此处简介\n\n<!--more-->\n### String StringBuffer StringBuilder\n* String s =new String(\"ok\")\n*\n（1）String ok1=new String(“ok”);\n（2）String ok2=“ok”;\n我相信很多人都知道这两种方式定义字符串，但他们之间的差别又有多少人清楚呢。画出这两个字符串的内存示意图：\n\n```\nString ok1=new String(“ok”)。首先会在堆内存申请一块内存存储字符串ok,ok1指向其内存块对象。同时还会检查字符串常量池中是否含有ok字符串,若没有则添加ok到字符串常量池中。所以 new String()可能会创建两个对象.\n```\n\n     String ok2=“ok”。 先检查字符串常量池中是否含有ok字符串,如果有则直接指向, 没有则在字符串常量池添加ok字符串并指向它.所以这种方法最多创建一个对象，有可能不创建对象所以String ok1=new String(“ok”);//创建了两个对象String ok2=“ok”;//没有创建对象\n\n`比较类中的数值是否相等使用equals(),比较两个包装类的引用是否指向同一个对象时用==`\n\n\n```\nString ok=\"ok\";\nString ok1=new String(\"ok\");\nSystem.out.println(ok==ok1);//fasle\n```\n明显不是同一个对象，一个指向字符串常量池，一个指向new出来的堆内存块，new的字符串在编译期是无法确定的。所以输出false\n\n```\nString ok=\"apple1\";\nString ok1=\"apple\"+1;\nSystem.out.println(ok==ok1);//true\n```\n\n    String ok=\"apple1\";\n    int temp=1;\n    String ok1=\"apple\"+temp;\n    System.out.println(ok==ok1)\n\n#### Intern()方法\n但我们可以通过intern()方法扩展常量池。\n         intern()是扩充常量池的一个方法,当一个String实例str调用intern()方法时,java会检查常量池中是否有相同的字符串,如果有则返回其引用,如果没有则在常量池中增加一个str字符串并返回它的引用。\n\n`String类具有immutable(不能改变)性质,当String变量需要经常变换时,会产生很多变量值,应考虑使用StringBuffer提高效率。在开发时，注意String的创建方法`\n\n`使用System.out.println(obj.hashcode())输出的时对象的哈希码，\n而非内存地址。在Java中是不可能得到对象真正的内存地址的，因为Java中堆是由JVM管理的不能直接操作。\n只能说此时打印出的Hash码表示了该对象在JAVA虚拟机中的内存位置，\nJava虚拟机会根据该hash码最终在真正的的堆空间中给该对象分配一个地址.\n但是该地址 是不能通过java提供的api获取的\n`\n+ String变量连接新字符串会改变hashCode值，变量是在JVM中“连接——断开”；\n+ StringBuffer变量连接新字符串不会改变hashCode值，因为变量的堆地址不变。\n+ StringBuilder变量连接新字符串不会改变hashCode值，因为变量的堆地址不变。\n\n#### 比较String、StringBuffer、StringBuilder性能\n\n+ String类由于Java中的共享设计，在修改变量值时使其反复改变栈中的对于堆的引用地址，所以性能低。\n+ StringBuilder是线性不安全的，适合于单线程操作，其性能比StringBuffer略高。\n+ StringBuffer和StringBuilder类设计时改变其值，其堆内存的地址不变，避免了反复修改栈引用的地址，其性能高。\n\n当String使用引号创建字符串时，会先去字符串池中找，找到了就返回，找不到就在字符串池中增加一个然后返回，这样由于共享提高了性能。\n\n 而new String()无论内容是否已经存在，都会开辟新的堆空间，栈中的堆内存也会改变。\n\n性能简介\nStringBuilder>StringBuffer>String\n\n\nhttp://www.jb51.net/article/78057.htm\n\n\nStringBuffer中的setLength与delete的效率比较\n+ 前者主要是通过将底层的storage数组长度设置为0\n+ 后者则是另复制一份至另一空间，长度设为0\n所以后则的效率会相对慢一点","source":"_posts/技术/hexo/oldblog/blog2.md","raw":"---\n\ntitle: String StringBuffer StringBuilder\ndate: 2019-07-16\ntags:\n\n---\n此处简介\n\n<!--more-->\n### String StringBuffer StringBuilder\n* String s =new String(\"ok\")\n*\n（1）String ok1=new String(“ok”);\n（2）String ok2=“ok”;\n我相信很多人都知道这两种方式定义字符串，但他们之间的差别又有多少人清楚呢。画出这两个字符串的内存示意图：\n\n```\nString ok1=new String(“ok”)。首先会在堆内存申请一块内存存储字符串ok,ok1指向其内存块对象。同时还会检查字符串常量池中是否含有ok字符串,若没有则添加ok到字符串常量池中。所以 new String()可能会创建两个对象.\n```\n\n     String ok2=“ok”。 先检查字符串常量池中是否含有ok字符串,如果有则直接指向, 没有则在字符串常量池添加ok字符串并指向它.所以这种方法最多创建一个对象，有可能不创建对象所以String ok1=new String(“ok”);//创建了两个对象String ok2=“ok”;//没有创建对象\n\n`比较类中的数值是否相等使用equals(),比较两个包装类的引用是否指向同一个对象时用==`\n\n\n```\nString ok=\"ok\";\nString ok1=new String(\"ok\");\nSystem.out.println(ok==ok1);//fasle\n```\n明显不是同一个对象，一个指向字符串常量池，一个指向new出来的堆内存块，new的字符串在编译期是无法确定的。所以输出false\n\n```\nString ok=\"apple1\";\nString ok1=\"apple\"+1;\nSystem.out.println(ok==ok1);//true\n```\n\n    String ok=\"apple1\";\n    int temp=1;\n    String ok1=\"apple\"+temp;\n    System.out.println(ok==ok1)\n\n#### Intern()方法\n但我们可以通过intern()方法扩展常量池。\n         intern()是扩充常量池的一个方法,当一个String实例str调用intern()方法时,java会检查常量池中是否有相同的字符串,如果有则返回其引用,如果没有则在常量池中增加一个str字符串并返回它的引用。\n\n`String类具有immutable(不能改变)性质,当String变量需要经常变换时,会产生很多变量值,应考虑使用StringBuffer提高效率。在开发时，注意String的创建方法`\n\n`使用System.out.println(obj.hashcode())输出的时对象的哈希码，\n而非内存地址。在Java中是不可能得到对象真正的内存地址的，因为Java中堆是由JVM管理的不能直接操作。\n只能说此时打印出的Hash码表示了该对象在JAVA虚拟机中的内存位置，\nJava虚拟机会根据该hash码最终在真正的的堆空间中给该对象分配一个地址.\n但是该地址 是不能通过java提供的api获取的\n`\n+ String变量连接新字符串会改变hashCode值，变量是在JVM中“连接——断开”；\n+ StringBuffer变量连接新字符串不会改变hashCode值，因为变量的堆地址不变。\n+ StringBuilder变量连接新字符串不会改变hashCode值，因为变量的堆地址不变。\n\n#### 比较String、StringBuffer、StringBuilder性能\n\n+ String类由于Java中的共享设计，在修改变量值时使其反复改变栈中的对于堆的引用地址，所以性能低。\n+ StringBuilder是线性不安全的，适合于单线程操作，其性能比StringBuffer略高。\n+ StringBuffer和StringBuilder类设计时改变其值，其堆内存的地址不变，避免了反复修改栈引用的地址，其性能高。\n\n当String使用引号创建字符串时，会先去字符串池中找，找到了就返回，找不到就在字符串池中增加一个然后返回，这样由于共享提高了性能。\n\n 而new String()无论内容是否已经存在，都会开辟新的堆空间，栈中的堆内存也会改变。\n\n性能简介\nStringBuilder>StringBuffer>String\n\n\nhttp://www.jb51.net/article/78057.htm\n\n\nStringBuffer中的setLength与delete的效率比较\n+ 前者主要是通过将底层的storage数组长度设置为0\n+ 后者则是另复制一份至另一空间，长度设为0\n所以后则的效率会相对慢一点","slug":"技术/hexo/oldblog/blog2","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33f003138pw6e8dcor2","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n<h3 id=\"String-StringBuffer-StringBuilder\"><a href=\"#String-StringBuffer-StringBuilder\" class=\"headerlink\" title=\"String StringBuffer StringBuilder\"></a>String StringBuffer StringBuilder</h3><ul>\n<li>String s =new String(“ok”)</li>\n<li>（1）String ok1=new String(“ok”);<br>（2）String ok2=“ok”;<br>我相信很多人都知道这两种方式定义字符串，但他们之间的差别又有多少人清楚呢。画出这两个字符串的内存示意图：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">String ok1&#x3D;new String(“ok”)。首先会在堆内存申请一块内存存储字符串ok,ok1指向其内存块对象。同时还会检查字符串常量池中是否含有ok字符串,若没有则添加ok到字符串常量池中。所以 new String()可能会创建两个对象.</span><br></pre></td></tr></table></figure>\n\n<pre><code>String ok2=“ok”。 先检查字符串常量池中是否含有ok字符串,如果有则直接指向, 没有则在字符串常量池添加ok字符串并指向它.所以这种方法最多创建一个对象，有可能不创建对象所以String ok1=new String(“ok”);//创建了两个对象String ok2=“ok”;//没有创建对象</code></pre><p><code>比较类中的数值是否相等使用equals(),比较两个包装类的引用是否指向同一个对象时用==</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">String ok&#x3D;&quot;ok&quot;;</span><br><span class=\"line\">String ok1&#x3D;new String(&quot;ok&quot;);</span><br><span class=\"line\">System.out.println(ok&#x3D;&#x3D;ok1);&#x2F;&#x2F;fasle</span><br></pre></td></tr></table></figure>\n<p>明显不是同一个对象，一个指向字符串常量池，一个指向new出来的堆内存块，new的字符串在编译期是无法确定的。所以输出false</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">String ok&#x3D;&quot;apple1&quot;;</span><br><span class=\"line\">String ok1&#x3D;&quot;apple&quot;+1;</span><br><span class=\"line\">System.out.println(ok&#x3D;&#x3D;ok1);&#x2F;&#x2F;true</span><br></pre></td></tr></table></figure>\n\n<pre><code>String ok=&quot;apple1&quot;;\nint temp=1;\nString ok1=&quot;apple&quot;+temp;\nSystem.out.println(ok==ok1)</code></pre><h4 id=\"Intern-方法\"><a href=\"#Intern-方法\" class=\"headerlink\" title=\"Intern()方法\"></a>Intern()方法</h4><p>但我们可以通过intern()方法扩展常量池。<br>         intern()是扩充常量池的一个方法,当一个String实例str调用intern()方法时,java会检查常量池中是否有相同的字符串,如果有则返回其引用,如果没有则在常量池中增加一个str字符串并返回它的引用。</p>\n<p><code>String类具有immutable(不能改变)性质,当String变量需要经常变换时,会产生很多变量值,应考虑使用StringBuffer提高效率。在开发时，注意String的创建方法</code></p>\n<p><code>使用System.out.println(obj.hashcode())输出的时对象的哈希码，\n而非内存地址。在Java中是不可能得到对象真正的内存地址的，因为Java中堆是由JVM管理的不能直接操作。\n只能说此时打印出的Hash码表示了该对象在JAVA虚拟机中的内存位置，\nJava虚拟机会根据该hash码最终在真正的的堆空间中给该对象分配一个地址.\n但是该地址 是不能通过java提供的api获取的</code></p>\n<ul>\n<li>String变量连接新字符串会改变hashCode值，变量是在JVM中“连接——断开”；</li>\n<li>StringBuffer变量连接新字符串不会改变hashCode值，因为变量的堆地址不变。</li>\n<li>StringBuilder变量连接新字符串不会改变hashCode值，因为变量的堆地址不变。</li>\n</ul>\n<h4 id=\"比较String、StringBuffer、StringBuilder性能\"><a href=\"#比较String、StringBuffer、StringBuilder性能\" class=\"headerlink\" title=\"比较String、StringBuffer、StringBuilder性能\"></a>比较String、StringBuffer、StringBuilder性能</h4><ul>\n<li>String类由于Java中的共享设计，在修改变量值时使其反复改变栈中的对于堆的引用地址，所以性能低。</li>\n<li>StringBuilder是线性不安全的，适合于单线程操作，其性能比StringBuffer略高。</li>\n<li>StringBuffer和StringBuilder类设计时改变其值，其堆内存的地址不变，避免了反复修改栈引用的地址，其性能高。</li>\n</ul>\n<p>当String使用引号创建字符串时，会先去字符串池中找，找到了就返回，找不到就在字符串池中增加一个然后返回，这样由于共享提高了性能。</p>\n<p> 而new String()无论内容是否已经存在，都会开辟新的堆空间，栈中的堆内存也会改变。</p>\n<p>性能简介<br>StringBuilder&gt;StringBuffer&gt;String</p>\n<p><a href=\"http://www.jb51.net/article/78057.htm\" target=\"_blank\" rel=\"noopener\">http://www.jb51.net/article/78057.htm</a></p>\n<p>StringBuffer中的setLength与delete的效率比较</p>\n<ul>\n<li>前者主要是通过将底层的storage数组长度设置为0</li>\n<li>后者则是另复制一份至另一空间，长度设为0<br>所以后则的效率会相对慢一点</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h3 id=\"String-StringBuffer-StringBuilder\"><a href=\"#String-StringBuffer-StringBuilder\" class=\"headerlink\" title=\"String StringBuffer StringBuilder\"></a>String StringBuffer StringBuilder</h3><ul>\n<li>String s =new String(“ok”)</li>\n<li>（1）String ok1=new String(“ok”);<br>（2）String ok2=“ok”;<br>我相信很多人都知道这两种方式定义字符串，但他们之间的差别又有多少人清楚呢。画出这两个字符串的内存示意图：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">String ok1&#x3D;new String(“ok”)。首先会在堆内存申请一块内存存储字符串ok,ok1指向其内存块对象。同时还会检查字符串常量池中是否含有ok字符串,若没有则添加ok到字符串常量池中。所以 new String()可能会创建两个对象.</span><br></pre></td></tr></table></figure>\n\n<pre><code>String ok2=“ok”。 先检查字符串常量池中是否含有ok字符串,如果有则直接指向, 没有则在字符串常量池添加ok字符串并指向它.所以这种方法最多创建一个对象，有可能不创建对象所以String ok1=new String(“ok”);//创建了两个对象String ok2=“ok”;//没有创建对象</code></pre><p><code>比较类中的数值是否相等使用equals(),比较两个包装类的引用是否指向同一个对象时用==</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">String ok&#x3D;&quot;ok&quot;;</span><br><span class=\"line\">String ok1&#x3D;new String(&quot;ok&quot;);</span><br><span class=\"line\">System.out.println(ok&#x3D;&#x3D;ok1);&#x2F;&#x2F;fasle</span><br></pre></td></tr></table></figure>\n<p>明显不是同一个对象，一个指向字符串常量池，一个指向new出来的堆内存块，new的字符串在编译期是无法确定的。所以输出false</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">String ok&#x3D;&quot;apple1&quot;;</span><br><span class=\"line\">String ok1&#x3D;&quot;apple&quot;+1;</span><br><span class=\"line\">System.out.println(ok&#x3D;&#x3D;ok1);&#x2F;&#x2F;true</span><br></pre></td></tr></table></figure>\n\n<pre><code>String ok=&quot;apple1&quot;;\nint temp=1;\nString ok1=&quot;apple&quot;+temp;\nSystem.out.println(ok==ok1)</code></pre><h4 id=\"Intern-方法\"><a href=\"#Intern-方法\" class=\"headerlink\" title=\"Intern()方法\"></a>Intern()方法</h4><p>但我们可以通过intern()方法扩展常量池。<br>         intern()是扩充常量池的一个方法,当一个String实例str调用intern()方法时,java会检查常量池中是否有相同的字符串,如果有则返回其引用,如果没有则在常量池中增加一个str字符串并返回它的引用。</p>\n<p><code>String类具有immutable(不能改变)性质,当String变量需要经常变换时,会产生很多变量值,应考虑使用StringBuffer提高效率。在开发时，注意String的创建方法</code></p>\n<p><code>使用System.out.println(obj.hashcode())输出的时对象的哈希码，\n而非内存地址。在Java中是不可能得到对象真正的内存地址的，因为Java中堆是由JVM管理的不能直接操作。\n只能说此时打印出的Hash码表示了该对象在JAVA虚拟机中的内存位置，\nJava虚拟机会根据该hash码最终在真正的的堆空间中给该对象分配一个地址.\n但是该地址 是不能通过java提供的api获取的</code></p>\n<ul>\n<li>String变量连接新字符串会改变hashCode值，变量是在JVM中“连接——断开”；</li>\n<li>StringBuffer变量连接新字符串不会改变hashCode值，因为变量的堆地址不变。</li>\n<li>StringBuilder变量连接新字符串不会改变hashCode值，因为变量的堆地址不变。</li>\n</ul>\n<h4 id=\"比较String、StringBuffer、StringBuilder性能\"><a href=\"#比较String、StringBuffer、StringBuilder性能\" class=\"headerlink\" title=\"比较String、StringBuffer、StringBuilder性能\"></a>比较String、StringBuffer、StringBuilder性能</h4><ul>\n<li>String类由于Java中的共享设计，在修改变量值时使其反复改变栈中的对于堆的引用地址，所以性能低。</li>\n<li>StringBuilder是线性不安全的，适合于单线程操作，其性能比StringBuffer略高。</li>\n<li>StringBuffer和StringBuilder类设计时改变其值，其堆内存的地址不变，避免了反复修改栈引用的地址，其性能高。</li>\n</ul>\n<p>当String使用引号创建字符串时，会先去字符串池中找，找到了就返回，找不到就在字符串池中增加一个然后返回，这样由于共享提高了性能。</p>\n<p> 而new String()无论内容是否已经存在，都会开辟新的堆空间，栈中的堆内存也会改变。</p>\n<p>性能简介<br>StringBuilder&gt;StringBuffer&gt;String</p>\n<p><a href=\"http://www.jb51.net/article/78057.htm\" target=\"_blank\" rel=\"noopener\">http://www.jb51.net/article/78057.htm</a></p>\n<p>StringBuffer中的setLength与delete的效率比较</p>\n<ul>\n<li>前者主要是通过将底层的storage数组长度设置为0</li>\n<li>后者则是另复制一份至另一空间，长度设为0<br>所以后则的效率会相对慢一点</li>\n</ul>"},{"_content":"#测试环境搭建小结\n`因一些原因，最近协助搭建测试服务器，主要涉及到了一服务器系统安装，环境配置，参数调优，软件使用，自动化建设等内容，因为主要是协助，所以着重小结我参与的部份`\n## 参数调优\n这里的参数调优主要针对的是服务器的调优，主要是针对出现问题后的调优，这次软件使用上大致问题有\n+ oom\n+ ioStream\n+ 端口数不足\n+ pid分配不够等问题\n\n### om\n对oom的调优主要的动作有调大分配给jvm的内存，但光调大内存不一定能解决问题，当遇到大量创建线程，但linux服务器允许该用户的执行的线程数不够时，会报无法创建的问题，严重会致使无法执行。所以还需要调大用户的进程数，查看用户信息通过ulimit -a\n### ioStream\n这个是io问题，经分析出现这个问题的主要原因应该该用户下限制了最大文件打开数，所以通过ulimit -n numbers即可调大该值\n但有时退出后可能会重新复原，所以需要长久变更可设置在.bashrc文件中\n### 端口数不足\n这是并发测试web接口时，发送的请求动作的完成需要时间，本机设置的端口区间可能无法满足大批量的端口需求，所以需要进行重新设置，主要方式是调大区间，调低端口被释放时间等\n\n 第一步，修改/etc/sysctl.conf文件，在文件中添加如下行：\n   net.ipv4.ip_local_port_range = 1024 65000\n   这表明将系统对本地端口范围限制设置为1024~65000之间。请注意，本地端口范围的最小值必须大于或等于1024；而端口范围的最大值则应小于或等于65535。修改完后保存此文件。\n[解决方案]([http://blog.sina.com.cn/s/blog_658c8cea0101l2sw.html)][1]\n[解决方案2](http://blog.chinaunix.net/uid-24907956-id-3428052.html)\n### pid分配不够等问题\n[pid分配不够](http://blog.csdn.net/cfaster/article/details/53065738)\n\n\n  [1]: http://blog.sina.com.cn/s/blog_658c8cea0101l2sw.html%29\n  [2]: http://blog.sina.com.cn/s/blog_658c8cea0101l2sw.html%29","source":"_posts/技术/hexo/oldblog/blog22.md","raw":"#测试环境搭建小结\n`因一些原因，最近协助搭建测试服务器，主要涉及到了一服务器系统安装，环境配置，参数调优，软件使用，自动化建设等内容，因为主要是协助，所以着重小结我参与的部份`\n## 参数调优\n这里的参数调优主要针对的是服务器的调优，主要是针对出现问题后的调优，这次软件使用上大致问题有\n+ oom\n+ ioStream\n+ 端口数不足\n+ pid分配不够等问题\n\n### om\n对oom的调优主要的动作有调大分配给jvm的内存，但光调大内存不一定能解决问题，当遇到大量创建线程，但linux服务器允许该用户的执行的线程数不够时，会报无法创建的问题，严重会致使无法执行。所以还需要调大用户的进程数，查看用户信息通过ulimit -a\n### ioStream\n这个是io问题，经分析出现这个问题的主要原因应该该用户下限制了最大文件打开数，所以通过ulimit -n numbers即可调大该值\n但有时退出后可能会重新复原，所以需要长久变更可设置在.bashrc文件中\n### 端口数不足\n这是并发测试web接口时，发送的请求动作的完成需要时间，本机设置的端口区间可能无法满足大批量的端口需求，所以需要进行重新设置，主要方式是调大区间，调低端口被释放时间等\n\n 第一步，修改/etc/sysctl.conf文件，在文件中添加如下行：\n   net.ipv4.ip_local_port_range = 1024 65000\n   这表明将系统对本地端口范围限制设置为1024~65000之间。请注意，本地端口范围的最小值必须大于或等于1024；而端口范围的最大值则应小于或等于65535。修改完后保存此文件。\n[解决方案]([http://blog.sina.com.cn/s/blog_658c8cea0101l2sw.html)][1]\n[解决方案2](http://blog.chinaunix.net/uid-24907956-id-3428052.html)\n### pid分配不够等问题\n[pid分配不够](http://blog.csdn.net/cfaster/article/details/53065738)\n\n\n  [1]: http://blog.sina.com.cn/s/blog_658c8cea0101l2sw.html%29\n  [2]: http://blog.sina.com.cn/s/blog_658c8cea0101l2sw.html%29","slug":"技术/hexo/oldblog/blog22","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/hexo/oldblog/blog22","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33g003238pw98ab8v6m","content":"<p>#测试环境搭建小结<br><code>因一些原因，最近协助搭建测试服务器，主要涉及到了一服务器系统安装，环境配置，参数调优，软件使用，自动化建设等内容，因为主要是协助，所以着重小结我参与的部份</code></p>\n<h2 id=\"参数调优\"><a href=\"#参数调优\" class=\"headerlink\" title=\"参数调优\"></a>参数调优</h2><p>这里的参数调优主要针对的是服务器的调优，主要是针对出现问题后的调优，这次软件使用上大致问题有</p>\n<ul>\n<li>oom</li>\n<li>ioStream</li>\n<li>端口数不足</li>\n<li>pid分配不够等问题</li>\n</ul>\n<h3 id=\"om\"><a href=\"#om\" class=\"headerlink\" title=\"om\"></a>om</h3><p>对oom的调优主要的动作有调大分配给jvm的内存，但光调大内存不一定能解决问题，当遇到大量创建线程，但linux服务器允许该用户的执行的线程数不够时，会报无法创建的问题，严重会致使无法执行。所以还需要调大用户的进程数，查看用户信息通过ulimit -a</p>\n<h3 id=\"ioStream\"><a href=\"#ioStream\" class=\"headerlink\" title=\"ioStream\"></a>ioStream</h3><p>这个是io问题，经分析出现这个问题的主要原因应该该用户下限制了最大文件打开数，所以通过ulimit -n numbers即可调大该值<br>但有时退出后可能会重新复原，所以需要长久变更可设置在.bashrc文件中</p>\n<h3 id=\"端口数不足\"><a href=\"#端口数不足\" class=\"headerlink\" title=\"端口数不足\"></a>端口数不足</h3><p>这是并发测试web接口时，发送的请求动作的完成需要时间，本机设置的端口区间可能无法满足大批量的端口需求，所以需要进行重新设置，主要方式是调大区间，调低端口被释放时间等</p>\n<p> 第一步，修改/etc/sysctl.conf文件，在文件中添加如下行：<br>   net.ipv4.ip_local_port_range = 1024 65000<br>   这表明将系统对本地端口范围限制设置为1024~65000之间。请注意，本地端口范围的最小值必须大于或等于1024；而端口范围的最大值则应小于或等于65535。修改完后保存此文件。<br><a href=\"[http://blog.sina.com.cn/s/blog_658c8cea0101l2sw.html\">解决方案</a>]<a href=\"http://blog.sina.com.cn/s/blog_658c8cea0101l2sw.html%29\" target=\"_blank\" rel=\"noopener\">1</a><br><a href=\"http://blog.chinaunix.net/uid-24907956-id-3428052.html\" target=\"_blank\" rel=\"noopener\">解决方案2</a></p>\n<h3 id=\"pid分配不够等问题\"><a href=\"#pid分配不够等问题\" class=\"headerlink\" title=\"pid分配不够等问题\"></a>pid分配不够等问题</h3><p><a href=\"http://blog.csdn.net/cfaster/article/details/53065738\" target=\"_blank\" rel=\"noopener\">pid分配不够</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>#测试环境搭建小结<br><code>因一些原因，最近协助搭建测试服务器，主要涉及到了一服务器系统安装，环境配置，参数调优，软件使用，自动化建设等内容，因为主要是协助，所以着重小结我参与的部份</code></p>\n<h2 id=\"参数调优\"><a href=\"#参数调优\" class=\"headerlink\" title=\"参数调优\"></a>参数调优</h2><p>这里的参数调优主要针对的是服务器的调优，主要是针对出现问题后的调优，这次软件使用上大致问题有</p>\n<ul>\n<li>oom</li>\n<li>ioStream</li>\n<li>端口数不足</li>\n<li>pid分配不够等问题</li>\n</ul>\n<h3 id=\"om\"><a href=\"#om\" class=\"headerlink\" title=\"om\"></a>om</h3><p>对oom的调优主要的动作有调大分配给jvm的内存，但光调大内存不一定能解决问题，当遇到大量创建线程，但linux服务器允许该用户的执行的线程数不够时，会报无法创建的问题，严重会致使无法执行。所以还需要调大用户的进程数，查看用户信息通过ulimit -a</p>\n<h3 id=\"ioStream\"><a href=\"#ioStream\" class=\"headerlink\" title=\"ioStream\"></a>ioStream</h3><p>这个是io问题，经分析出现这个问题的主要原因应该该用户下限制了最大文件打开数，所以通过ulimit -n numbers即可调大该值<br>但有时退出后可能会重新复原，所以需要长久变更可设置在.bashrc文件中</p>\n<h3 id=\"端口数不足\"><a href=\"#端口数不足\" class=\"headerlink\" title=\"端口数不足\"></a>端口数不足</h3><p>这是并发测试web接口时，发送的请求动作的完成需要时间，本机设置的端口区间可能无法满足大批量的端口需求，所以需要进行重新设置，主要方式是调大区间，调低端口被释放时间等</p>\n<p> 第一步，修改/etc/sysctl.conf文件，在文件中添加如下行：<br>   net.ipv4.ip_local_port_range = 1024 65000<br>   这表明将系统对本地端口范围限制设置为1024~65000之间。请注意，本地端口范围的最小值必须大于或等于1024；而端口范围的最大值则应小于或等于65535。修改完后保存此文件。<br><a href=\"[http://blog.sina.com.cn/s/blog_658c8cea0101l2sw.html\">解决方案</a>]<a href=\"http://blog.sina.com.cn/s/blog_658c8cea0101l2sw.html%29\" target=\"_blank\" rel=\"noopener\">1</a><br><a href=\"http://blog.chinaunix.net/uid-24907956-id-3428052.html\" target=\"_blank\" rel=\"noopener\">解决方案2</a></p>\n<h3 id=\"pid分配不够等问题\"><a href=\"#pid分配不够等问题\" class=\"headerlink\" title=\"pid分配不够等问题\"></a>pid分配不够等问题</h3><p><a href=\"http://blog.csdn.net/cfaster/article/details/53065738\" target=\"_blank\" rel=\"noopener\">pid分配不够</a></p>\n"},{"title":"java中枚举的使用","date":"2019-07-15T16:00:00.000Z","_content":"\n此处简介\n<!--more-->\n### java中枚举的使用\n**enum** 的全称为 enumeration 是 JDK 1.5  中引入的新特性，存放在 java.lang 包中。\n1. 原始的接口定义常量\n\n2. 语法（定义）\n\n3. 遍历、switch 等常用操作\n\n4. enum 对象的常用方法介绍\n\n5. 给 enum 自定义属性和方法\n\n6. EnumSet，EnumMap 的应用\n\n7. enum 的原理分析\n\n8. 总结\n\n***\n原始的接口定义常量\n\n```\npublic interface IConstants {\n    String MON = \"Mon\";\n    String TUE = \"Tue\";\n    String WED = \"Wed\";\n    String THU = \"Thu\";\n    String FRI = \"Fri\";\n    String SAT = \"Sat\";\n    String SUN = \"Sun\";\n}\n```\n***\n**语法（定义）**\n 创建枚举类型要使用 enum 关键字，隐含了所创建的类型都是 java.lang.Enum 类的子类（java.lang.Enum 是一个抽象类）。枚举类型符合通用模式 Class Enum<E extends Enum<E>>，而 E 表示枚举类型的名称。枚举类型的每一个值都将映射到 protected Enum(String name, int ordinal) 构造函数中，在这里，每个值的名称都被转换成一个字符串，并且序数设置表示了此设置被创建的顺序。\n\n\n\n```\npackage com.hmw.test;\n/**\n * 枚举测试类\n * @author <a href=\"mailto:hemingwang0902@126.com\">何明旺</a>\n */\npublic enum EnumTest {\n    MON, TUE, WED, THU, FRI, SAT, SUN;\n}\n这段代码实际上调用了7次 Enum(String name, int ordinal)：\n\nnew Enum<EnumTest>(\"MON\",0);\nnew Enum<EnumTest>(\"TUE\",1);\nnew Enum<EnumTest>(\"WED\",2);\n```\n***\n**遍历、switch 等常用操作**\n\n\npublic class Test {\n    public static void main(String[] args) {\n        for (EnumTest e : EnumTest.values()) {\n            System.out.println(e.toString());\n        }\n\n        System.out.println(\"----------------我是分隔线------------------\");\n\n        EnumTest test = EnumTest.TUE;\n        switch (test) {\n        case MON:\n            System.out.println(\"今天是星期一\");\n            break;\n        case TUE:\n            System.out.println(\"今天是星期二\");\n            break;\n        // ... ...\n        default:\n            System.out.println(test);\n            break;\n        }\n    }\n    }\n***\n**enum 对象的常用方法介绍**\nint compareTo(E o)\n          比较此枚举与指定对象的顺序。\n\nClass<E> getDeclaringClass()\n          返回与此枚举常量的枚举类型相对应的 Class 对象。\n\nString name()\n          返回此枚举常量的名称，在其枚举声明中对其进行声明。\n\nint ordinal()\n          返回枚举常量的序数（它在枚举声明中的位置，其中初始常量序数为零）。\n\nString toString()\n\n           返回枚举常量的名称，它包含在声明中。\n\nstatic <T extends Enum<T>> T valueOf(Class<T> enumType, String name)\n          返回带指定名称的指定枚举类型的枚举常量。\n\n***\n**给 enum 自定义属性和方法**\n\n```\n给 enum 对象加一下 value 的属性和 getValue() 的方法：\n\npackage com.hmw.test;\n\n/**\n * 枚举测试类\n *\n * @author <a href=\"mailto:hemingwang0902@126.com\">何明旺</a>\n */\npublic enum EnumTest {\n    MON(1), TUE(2), WED(3), THU(4), FRI(5), SAT(6) {\n        @Override\n        public boolean isRest() {\n            return true;\n        }\n    },\n    SUN(0) {\n        @Override\n        public boolean isRest() {\n            return true;\n        }\n    };\n\n    private int value;\n\n    private EnumTest(int value) {\n        this.value = value;\n    }\n\n    public int getValue() {\n        return value;\n    }\n\n    public boolean isRest() {\n        return false;\n    }\n}\npublic class Test {\n    public static void main(String[] args) {\n        System.out.println(\"EnumTest.FRI 的 value = \" + EnumTest.FRI.getValue());\n    }\n}\n输出结果：\nEnumTest.FRI 的 value = 5\n```\n***\n**EnumSet，EnumMap 的应用**\n\n```\npublic class Test {\n    public static void main(String[] args) {\n        // EnumSet的使用\n        EnumSet<EnumTest> weekSet = EnumSet.allOf(EnumTest.class);\n        for (EnumTest day : weekSet) {\n            System.out.println(day);\n        }\n\n        // EnumMap的使用\n        EnumMap<EnumTest, String> weekMap = new EnumMap(EnumTest.class);\n        weekMap.put(EnumTest.MON, \"星期一\");\n        weekMap.put(EnumTest.TUE, \"星期二\");\n        // ... ...\n        for (Iterator<Entry<EnumTest, String>> iter = weekMap.entrySet().iterator(); iter.hasNext();) {\n            Entry<EnumTest, String> entry = iter.next();\n            System.out.println(entry.getKey().name() + \":\" + entry.getValue());\n        }\n    }\n}\n```\n***\n原理分析\n        enum 的语法结构尽管和 class 的语法不一样，但是经过编译器编译之后产生的是一个class文件。该class文件经过反编译可以看到实际上是生成了一个类，该类继承了java.lang.Enum<E>。EnumTest 经过反编译(javap com.hmw.test.EnumTest 命令)之后得到的内容如下：\n\n```\npublic class com.hmw.test.EnumTest extends java.lang.Enum{\n    public static final com.hmw.test.EnumTest MON;\n    public static final com.hmw.test.EnumTest TUE;\n    public static final com.hmw.test.EnumTest WED;\n    public static final com.hmw.test.EnumTest THU;\n    public static final com.hmw.test.EnumTest FRI;\n    public static final com.hmw.test.EnumTest SAT;\n    public static final com.hmw.test.EnumTest SUN;\n    static {};\n    public int getValue();\n    public boolean isRest();\n    public static com.hmw.test.EnumTest[] values();\n    public static com.hmw.test.EnumTest valueOf(java.lang.String);\n    com.hmw.test.EnumTest(java.lang.String, int, int, com.hmw.test.EnumTest);\n}\n```\n\n所以，实际上 enum 就是一个 class，只不过 java 编译器帮我们做了语法的解析和编译而已。\n总结\n    可以把 enum 看成是一个普通的 class，它们都可以定义一些属性和方法，不同之处是：enum 不能使用 extends 关键字继承其他类，因为 enum 已经继承了 java.lang.Enum（java是单一继承）。","source":"_posts/技术/hexo/oldblog/blog3.md","raw":"---\n\ntitle: java中枚举的使用\ndate: 2019-07-16\ntags:\n\n---\n\n此处简介\n<!--more-->\n### java中枚举的使用\n**enum** 的全称为 enumeration 是 JDK 1.5  中引入的新特性，存放在 java.lang 包中。\n1. 原始的接口定义常量\n\n2. 语法（定义）\n\n3. 遍历、switch 等常用操作\n\n4. enum 对象的常用方法介绍\n\n5. 给 enum 自定义属性和方法\n\n6. EnumSet，EnumMap 的应用\n\n7. enum 的原理分析\n\n8. 总结\n\n***\n原始的接口定义常量\n\n```\npublic interface IConstants {\n    String MON = \"Mon\";\n    String TUE = \"Tue\";\n    String WED = \"Wed\";\n    String THU = \"Thu\";\n    String FRI = \"Fri\";\n    String SAT = \"Sat\";\n    String SUN = \"Sun\";\n}\n```\n***\n**语法（定义）**\n 创建枚举类型要使用 enum 关键字，隐含了所创建的类型都是 java.lang.Enum 类的子类（java.lang.Enum 是一个抽象类）。枚举类型符合通用模式 Class Enum<E extends Enum<E>>，而 E 表示枚举类型的名称。枚举类型的每一个值都将映射到 protected Enum(String name, int ordinal) 构造函数中，在这里，每个值的名称都被转换成一个字符串，并且序数设置表示了此设置被创建的顺序。\n\n\n\n```\npackage com.hmw.test;\n/**\n * 枚举测试类\n * @author <a href=\"mailto:hemingwang0902@126.com\">何明旺</a>\n */\npublic enum EnumTest {\n    MON, TUE, WED, THU, FRI, SAT, SUN;\n}\n这段代码实际上调用了7次 Enum(String name, int ordinal)：\n\nnew Enum<EnumTest>(\"MON\",0);\nnew Enum<EnumTest>(\"TUE\",1);\nnew Enum<EnumTest>(\"WED\",2);\n```\n***\n**遍历、switch 等常用操作**\n\n\npublic class Test {\n    public static void main(String[] args) {\n        for (EnumTest e : EnumTest.values()) {\n            System.out.println(e.toString());\n        }\n\n        System.out.println(\"----------------我是分隔线------------------\");\n\n        EnumTest test = EnumTest.TUE;\n        switch (test) {\n        case MON:\n            System.out.println(\"今天是星期一\");\n            break;\n        case TUE:\n            System.out.println(\"今天是星期二\");\n            break;\n        // ... ...\n        default:\n            System.out.println(test);\n            break;\n        }\n    }\n    }\n***\n**enum 对象的常用方法介绍**\nint compareTo(E o)\n          比较此枚举与指定对象的顺序。\n\nClass<E> getDeclaringClass()\n          返回与此枚举常量的枚举类型相对应的 Class 对象。\n\nString name()\n          返回此枚举常量的名称，在其枚举声明中对其进行声明。\n\nint ordinal()\n          返回枚举常量的序数（它在枚举声明中的位置，其中初始常量序数为零）。\n\nString toString()\n\n           返回枚举常量的名称，它包含在声明中。\n\nstatic <T extends Enum<T>> T valueOf(Class<T> enumType, String name)\n          返回带指定名称的指定枚举类型的枚举常量。\n\n***\n**给 enum 自定义属性和方法**\n\n```\n给 enum 对象加一下 value 的属性和 getValue() 的方法：\n\npackage com.hmw.test;\n\n/**\n * 枚举测试类\n *\n * @author <a href=\"mailto:hemingwang0902@126.com\">何明旺</a>\n */\npublic enum EnumTest {\n    MON(1), TUE(2), WED(3), THU(4), FRI(5), SAT(6) {\n        @Override\n        public boolean isRest() {\n            return true;\n        }\n    },\n    SUN(0) {\n        @Override\n        public boolean isRest() {\n            return true;\n        }\n    };\n\n    private int value;\n\n    private EnumTest(int value) {\n        this.value = value;\n    }\n\n    public int getValue() {\n        return value;\n    }\n\n    public boolean isRest() {\n        return false;\n    }\n}\npublic class Test {\n    public static void main(String[] args) {\n        System.out.println(\"EnumTest.FRI 的 value = \" + EnumTest.FRI.getValue());\n    }\n}\n输出结果：\nEnumTest.FRI 的 value = 5\n```\n***\n**EnumSet，EnumMap 的应用**\n\n```\npublic class Test {\n    public static void main(String[] args) {\n        // EnumSet的使用\n        EnumSet<EnumTest> weekSet = EnumSet.allOf(EnumTest.class);\n        for (EnumTest day : weekSet) {\n            System.out.println(day);\n        }\n\n        // EnumMap的使用\n        EnumMap<EnumTest, String> weekMap = new EnumMap(EnumTest.class);\n        weekMap.put(EnumTest.MON, \"星期一\");\n        weekMap.put(EnumTest.TUE, \"星期二\");\n        // ... ...\n        for (Iterator<Entry<EnumTest, String>> iter = weekMap.entrySet().iterator(); iter.hasNext();) {\n            Entry<EnumTest, String> entry = iter.next();\n            System.out.println(entry.getKey().name() + \":\" + entry.getValue());\n        }\n    }\n}\n```\n***\n原理分析\n        enum 的语法结构尽管和 class 的语法不一样，但是经过编译器编译之后产生的是一个class文件。该class文件经过反编译可以看到实际上是生成了一个类，该类继承了java.lang.Enum<E>。EnumTest 经过反编译(javap com.hmw.test.EnumTest 命令)之后得到的内容如下：\n\n```\npublic class com.hmw.test.EnumTest extends java.lang.Enum{\n    public static final com.hmw.test.EnumTest MON;\n    public static final com.hmw.test.EnumTest TUE;\n    public static final com.hmw.test.EnumTest WED;\n    public static final com.hmw.test.EnumTest THU;\n    public static final com.hmw.test.EnumTest FRI;\n    public static final com.hmw.test.EnumTest SAT;\n    public static final com.hmw.test.EnumTest SUN;\n    static {};\n    public int getValue();\n    public boolean isRest();\n    public static com.hmw.test.EnumTest[] values();\n    public static com.hmw.test.EnumTest valueOf(java.lang.String);\n    com.hmw.test.EnumTest(java.lang.String, int, int, com.hmw.test.EnumTest);\n}\n```\n\n所以，实际上 enum 就是一个 class，只不过 java 编译器帮我们做了语法的解析和编译而已。\n总结\n    可以把 enum 看成是一个普通的 class，它们都可以定义一些属性和方法，不同之处是：enum 不能使用 extends 关键字继承其他类，因为 enum 已经继承了 java.lang.Enum（java是单一继承）。","slug":"技术/hexo/oldblog/blog3","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33g003338pwduz47k36","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n<h3 id=\"java中枚举的使用\"><a href=\"#java中枚举的使用\" class=\"headerlink\" title=\"java中枚举的使用\"></a>java中枚举的使用</h3><p><strong>enum</strong> 的全称为 enumeration 是 JDK 1.5  中引入的新特性，存放在 java.lang 包中。</p>\n<ol>\n<li><p>原始的接口定义常量</p>\n</li>\n<li><p>语法（定义）</p>\n</li>\n<li><p>遍历、switch 等常用操作</p>\n</li>\n<li><p>enum 对象的常用方法介绍</p>\n</li>\n<li><p>给 enum 自定义属性和方法</p>\n</li>\n<li><p>EnumSet，EnumMap 的应用</p>\n</li>\n<li><p>enum 的原理分析</p>\n</li>\n<li><p>总结</p>\n</li>\n</ol>\n<hr>\n<p>原始的接口定义常量</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public interface IConstants &#123;</span><br><span class=\"line\">    String MON &#x3D; &quot;Mon&quot;;</span><br><span class=\"line\">    String TUE &#x3D; &quot;Tue&quot;;</span><br><span class=\"line\">    String WED &#x3D; &quot;Wed&quot;;</span><br><span class=\"line\">    String THU &#x3D; &quot;Thu&quot;;</span><br><span class=\"line\">    String FRI &#x3D; &quot;Fri&quot;;</span><br><span class=\"line\">    String SAT &#x3D; &quot;Sat&quot;;</span><br><span class=\"line\">    String SUN &#x3D; &quot;Sun&quot;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<p><strong>语法（定义）</strong><br> 创建枚举类型要使用 enum 关键字，隐含了所创建的类型都是 java.lang.Enum 类的子类（java.lang.Enum 是一个抽象类）。枚举类型符合通用模式 Class Enum&lt;E extends Enum<E>&gt;，而 E 表示枚举类型的名称。枚举类型的每一个值都将映射到 protected Enum(String name, int ordinal) 构造函数中，在这里，每个值的名称都被转换成一个字符串，并且序数设置表示了此设置被创建的顺序。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package com.hmw.test;</span><br><span class=\"line\">&#x2F;**</span><br><span class=\"line\"> * 枚举测试类</span><br><span class=\"line\"> * @author &lt;a href&#x3D;&quot;mailto:hemingwang0902@126.com&quot;&gt;何明旺&lt;&#x2F;a&gt;</span><br><span class=\"line\"> *&#x2F;</span><br><span class=\"line\">public enum EnumTest &#123;</span><br><span class=\"line\">    MON, TUE, WED, THU, FRI, SAT, SUN;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">这段代码实际上调用了7次 Enum(String name, int ordinal)：</span><br><span class=\"line\"></span><br><span class=\"line\">new Enum&lt;EnumTest&gt;(&quot;MON&quot;,0);</span><br><span class=\"line\">new Enum&lt;EnumTest&gt;(&quot;TUE&quot;,1);</span><br><span class=\"line\">new Enum&lt;EnumTest&gt;(&quot;WED&quot;,2);</span><br></pre></td></tr></table></figure>\n<hr>\n<p><strong>遍历、switch 等常用操作</strong></p>\n<p>public class Test {<br>    public static void main(String[] args) {<br>        for (EnumTest e : EnumTest.values()) {<br>            System.out.println(e.toString());<br>        }</p>\n<pre><code>    System.out.println(&quot;----------------我是分隔线------------------&quot;);\n\n    EnumTest test = EnumTest.TUE;\n    switch (test) {\n    case MON:\n        System.out.println(&quot;今天是星期一&quot;);\n        break;\n    case TUE:\n        System.out.println(&quot;今天是星期二&quot;);\n        break;\n    // ... ...\n    default:\n        System.out.println(test);\n        break;\n    }\n}\n}</code></pre><hr>\n<p><strong>enum 对象的常用方法介绍</strong><br>int compareTo(E o)<br>          比较此枚举与指定对象的顺序。</p>\n<p>Class<E> getDeclaringClass()<br>          返回与此枚举常量的枚举类型相对应的 Class 对象。</p>\n<p>String name()<br>          返回此枚举常量的名称，在其枚举声明中对其进行声明。</p>\n<p>int ordinal()<br>          返回枚举常量的序数（它在枚举声明中的位置，其中初始常量序数为零）。</p>\n<p>String toString()</p>\n<pre><code>返回枚举常量的名称，它包含在声明中。</code></pre><p>static &lt;T extends Enum<T>&gt; T valueOf(Class<T> enumType, String name)<br>          返回带指定名称的指定枚举类型的枚举常量。</p>\n<hr>\n<p><strong>给 enum 自定义属性和方法</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">给 enum 对象加一下 value 的属性和 getValue() 的方法：</span><br><span class=\"line\"></span><br><span class=\"line\">package com.hmw.test;</span><br><span class=\"line\"></span><br><span class=\"line\">&#x2F;**</span><br><span class=\"line\"> * 枚举测试类</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * @author &lt;a href&#x3D;&quot;mailto:hemingwang0902@126.com&quot;&gt;何明旺&lt;&#x2F;a&gt;</span><br><span class=\"line\"> *&#x2F;</span><br><span class=\"line\">public enum EnumTest &#123;</span><br><span class=\"line\">    MON(1), TUE(2), WED(3), THU(4), FRI(5), SAT(6) &#123;</span><br><span class=\"line\">        @Override</span><br><span class=\"line\">        public boolean isRest() &#123;</span><br><span class=\"line\">            return true;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    SUN(0) &#123;</span><br><span class=\"line\">        @Override</span><br><span class=\"line\">        public boolean isRest() &#123;</span><br><span class=\"line\">            return true;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    private int value;</span><br><span class=\"line\"></span><br><span class=\"line\">    private EnumTest(int value) &#123;</span><br><span class=\"line\">        this.value &#x3D; value;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public int getValue() &#123;</span><br><span class=\"line\">        return value;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public boolean isRest() &#123;</span><br><span class=\"line\">        return false;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">public class Test &#123;</span><br><span class=\"line\">    public static void main(String[] args) &#123;</span><br><span class=\"line\">        System.out.println(&quot;EnumTest.FRI 的 value &#x3D; &quot; + EnumTest.FRI.getValue());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">输出结果：</span><br><span class=\"line\">EnumTest.FRI 的 value &#x3D; 5</span><br></pre></td></tr></table></figure>\n<hr>\n<p><strong>EnumSet，EnumMap 的应用</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class Test &#123;</span><br><span class=\"line\">    public static void main(String[] args) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; EnumSet的使用</span><br><span class=\"line\">        EnumSet&lt;EnumTest&gt; weekSet &#x3D; EnumSet.allOf(EnumTest.class);</span><br><span class=\"line\">        for (EnumTest day : weekSet) &#123;</span><br><span class=\"line\">            System.out.println(day);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#x2F;&#x2F; EnumMap的使用</span><br><span class=\"line\">        EnumMap&lt;EnumTest, String&gt; weekMap &#x3D; new EnumMap(EnumTest.class);</span><br><span class=\"line\">        weekMap.put(EnumTest.MON, &quot;星期一&quot;);</span><br><span class=\"line\">        weekMap.put(EnumTest.TUE, &quot;星期二&quot;);</span><br><span class=\"line\">        &#x2F;&#x2F; ... ...</span><br><span class=\"line\">        for (Iterator&lt;Entry&lt;EnumTest, String&gt;&gt; iter &#x3D; weekMap.entrySet().iterator(); iter.hasNext();) &#123;</span><br><span class=\"line\">            Entry&lt;EnumTest, String&gt; entry &#x3D; iter.next();</span><br><span class=\"line\">            System.out.println(entry.getKey().name() + &quot;:&quot; + entry.getValue());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<p>原理分析<br>        enum 的语法结构尽管和 class 的语法不一样，但是经过编译器编译之后产生的是一个class文件。该class文件经过反编译可以看到实际上是生成了一个类，该类继承了java.lang.Enum<E>。EnumTest 经过反编译(javap com.hmw.test.EnumTest 命令)之后得到的内容如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class com.hmw.test.EnumTest extends java.lang.Enum&#123;</span><br><span class=\"line\">    public static final com.hmw.test.EnumTest MON;</span><br><span class=\"line\">    public static final com.hmw.test.EnumTest TUE;</span><br><span class=\"line\">    public static final com.hmw.test.EnumTest WED;</span><br><span class=\"line\">    public static final com.hmw.test.EnumTest THU;</span><br><span class=\"line\">    public static final com.hmw.test.EnumTest FRI;</span><br><span class=\"line\">    public static final com.hmw.test.EnumTest SAT;</span><br><span class=\"line\">    public static final com.hmw.test.EnumTest SUN;</span><br><span class=\"line\">    static &#123;&#125;;</span><br><span class=\"line\">    public int getValue();</span><br><span class=\"line\">    public boolean isRest();</span><br><span class=\"line\">    public static com.hmw.test.EnumTest[] values();</span><br><span class=\"line\">    public static com.hmw.test.EnumTest valueOf(java.lang.String);</span><br><span class=\"line\">    com.hmw.test.EnumTest(java.lang.String, int, int, com.hmw.test.EnumTest);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>所以，实际上 enum 就是一个 class，只不过 java 编译器帮我们做了语法的解析和编译而已。<br>总结<br>    可以把 enum 看成是一个普通的 class，它们都可以定义一些属性和方法，不同之处是：enum 不能使用 extends 关键字继承其他类，因为 enum 已经继承了 java.lang.Enum（java是单一继承）。</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h3 id=\"java中枚举的使用\"><a href=\"#java中枚举的使用\" class=\"headerlink\" title=\"java中枚举的使用\"></a>java中枚举的使用</h3><p><strong>enum</strong> 的全称为 enumeration 是 JDK 1.5  中引入的新特性，存放在 java.lang 包中。</p>\n<ol>\n<li><p>原始的接口定义常量</p>\n</li>\n<li><p>语法（定义）</p>\n</li>\n<li><p>遍历、switch 等常用操作</p>\n</li>\n<li><p>enum 对象的常用方法介绍</p>\n</li>\n<li><p>给 enum 自定义属性和方法</p>\n</li>\n<li><p>EnumSet，EnumMap 的应用</p>\n</li>\n<li><p>enum 的原理分析</p>\n</li>\n<li><p>总结</p>\n</li>\n</ol>\n<hr>\n<p>原始的接口定义常量</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public interface IConstants &#123;</span><br><span class=\"line\">    String MON &#x3D; &quot;Mon&quot;;</span><br><span class=\"line\">    String TUE &#x3D; &quot;Tue&quot;;</span><br><span class=\"line\">    String WED &#x3D; &quot;Wed&quot;;</span><br><span class=\"line\">    String THU &#x3D; &quot;Thu&quot;;</span><br><span class=\"line\">    String FRI &#x3D; &quot;Fri&quot;;</span><br><span class=\"line\">    String SAT &#x3D; &quot;Sat&quot;;</span><br><span class=\"line\">    String SUN &#x3D; &quot;Sun&quot;;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<p><strong>语法（定义）</strong><br> 创建枚举类型要使用 enum 关键字，隐含了所创建的类型都是 java.lang.Enum 类的子类（java.lang.Enum 是一个抽象类）。枚举类型符合通用模式 Class Enum&lt;E extends Enum<E>&gt;，而 E 表示枚举类型的名称。枚举类型的每一个值都将映射到 protected Enum(String name, int ordinal) 构造函数中，在这里，每个值的名称都被转换成一个字符串，并且序数设置表示了此设置被创建的顺序。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package com.hmw.test;</span><br><span class=\"line\">&#x2F;**</span><br><span class=\"line\"> * 枚举测试类</span><br><span class=\"line\"> * @author &lt;a href&#x3D;&quot;mailto:hemingwang0902@126.com&quot;&gt;何明旺&lt;&#x2F;a&gt;</span><br><span class=\"line\"> *&#x2F;</span><br><span class=\"line\">public enum EnumTest &#123;</span><br><span class=\"line\">    MON, TUE, WED, THU, FRI, SAT, SUN;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">这段代码实际上调用了7次 Enum(String name, int ordinal)：</span><br><span class=\"line\"></span><br><span class=\"line\">new Enum&lt;EnumTest&gt;(&quot;MON&quot;,0);</span><br><span class=\"line\">new Enum&lt;EnumTest&gt;(&quot;TUE&quot;,1);</span><br><span class=\"line\">new Enum&lt;EnumTest&gt;(&quot;WED&quot;,2);</span><br></pre></td></tr></table></figure>\n<hr>\n<p><strong>遍历、switch 等常用操作</strong></p>\n<p>public class Test {<br>    public static void main(String[] args) {<br>        for (EnumTest e : EnumTest.values()) {<br>            System.out.println(e.toString());<br>        }</p>\n<pre><code>    System.out.println(&quot;----------------我是分隔线------------------&quot;);\n\n    EnumTest test = EnumTest.TUE;\n    switch (test) {\n    case MON:\n        System.out.println(&quot;今天是星期一&quot;);\n        break;\n    case TUE:\n        System.out.println(&quot;今天是星期二&quot;);\n        break;\n    // ... ...\n    default:\n        System.out.println(test);\n        break;\n    }\n}\n}</code></pre><hr>\n<p><strong>enum 对象的常用方法介绍</strong><br>int compareTo(E o)<br>          比较此枚举与指定对象的顺序。</p>\n<p>Class<E> getDeclaringClass()<br>          返回与此枚举常量的枚举类型相对应的 Class 对象。</p>\n<p>String name()<br>          返回此枚举常量的名称，在其枚举声明中对其进行声明。</p>\n<p>int ordinal()<br>          返回枚举常量的序数（它在枚举声明中的位置，其中初始常量序数为零）。</p>\n<p>String toString()</p>\n<pre><code>返回枚举常量的名称，它包含在声明中。</code></pre><p>static &lt;T extends Enum<T>&gt; T valueOf(Class<T> enumType, String name)<br>          返回带指定名称的指定枚举类型的枚举常量。</p>\n<hr>\n<p><strong>给 enum 自定义属性和方法</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">给 enum 对象加一下 value 的属性和 getValue() 的方法：</span><br><span class=\"line\"></span><br><span class=\"line\">package com.hmw.test;</span><br><span class=\"line\"></span><br><span class=\"line\">&#x2F;**</span><br><span class=\"line\"> * 枚举测试类</span><br><span class=\"line\"> *</span><br><span class=\"line\"> * @author &lt;a href&#x3D;&quot;mailto:hemingwang0902@126.com&quot;&gt;何明旺&lt;&#x2F;a&gt;</span><br><span class=\"line\"> *&#x2F;</span><br><span class=\"line\">public enum EnumTest &#123;</span><br><span class=\"line\">    MON(1), TUE(2), WED(3), THU(4), FRI(5), SAT(6) &#123;</span><br><span class=\"line\">        @Override</span><br><span class=\"line\">        public boolean isRest() &#123;</span><br><span class=\"line\">            return true;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    SUN(0) &#123;</span><br><span class=\"line\">        @Override</span><br><span class=\"line\">        public boolean isRest() &#123;</span><br><span class=\"line\">            return true;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"></span><br><span class=\"line\">    private int value;</span><br><span class=\"line\"></span><br><span class=\"line\">    private EnumTest(int value) &#123;</span><br><span class=\"line\">        this.value &#x3D; value;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public int getValue() &#123;</span><br><span class=\"line\">        return value;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public boolean isRest() &#123;</span><br><span class=\"line\">        return false;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">public class Test &#123;</span><br><span class=\"line\">    public static void main(String[] args) &#123;</span><br><span class=\"line\">        System.out.println(&quot;EnumTest.FRI 的 value &#x3D; &quot; + EnumTest.FRI.getValue());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">输出结果：</span><br><span class=\"line\">EnumTest.FRI 的 value &#x3D; 5</span><br></pre></td></tr></table></figure>\n<hr>\n<p><strong>EnumSet，EnumMap 的应用</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class Test &#123;</span><br><span class=\"line\">    public static void main(String[] args) &#123;</span><br><span class=\"line\">        &#x2F;&#x2F; EnumSet的使用</span><br><span class=\"line\">        EnumSet&lt;EnumTest&gt; weekSet &#x3D; EnumSet.allOf(EnumTest.class);</span><br><span class=\"line\">        for (EnumTest day : weekSet) &#123;</span><br><span class=\"line\">            System.out.println(day);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#x2F;&#x2F; EnumMap的使用</span><br><span class=\"line\">        EnumMap&lt;EnumTest, String&gt; weekMap &#x3D; new EnumMap(EnumTest.class);</span><br><span class=\"line\">        weekMap.put(EnumTest.MON, &quot;星期一&quot;);</span><br><span class=\"line\">        weekMap.put(EnumTest.TUE, &quot;星期二&quot;);</span><br><span class=\"line\">        &#x2F;&#x2F; ... ...</span><br><span class=\"line\">        for (Iterator&lt;Entry&lt;EnumTest, String&gt;&gt; iter &#x3D; weekMap.entrySet().iterator(); iter.hasNext();) &#123;</span><br><span class=\"line\">            Entry&lt;EnumTest, String&gt; entry &#x3D; iter.next();</span><br><span class=\"line\">            System.out.println(entry.getKey().name() + &quot;:&quot; + entry.getValue());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<p>原理分析<br>        enum 的语法结构尽管和 class 的语法不一样，但是经过编译器编译之后产生的是一个class文件。该class文件经过反编译可以看到实际上是生成了一个类，该类继承了java.lang.Enum<E>。EnumTest 经过反编译(javap com.hmw.test.EnumTest 命令)之后得到的内容如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class com.hmw.test.EnumTest extends java.lang.Enum&#123;</span><br><span class=\"line\">    public static final com.hmw.test.EnumTest MON;</span><br><span class=\"line\">    public static final com.hmw.test.EnumTest TUE;</span><br><span class=\"line\">    public static final com.hmw.test.EnumTest WED;</span><br><span class=\"line\">    public static final com.hmw.test.EnumTest THU;</span><br><span class=\"line\">    public static final com.hmw.test.EnumTest FRI;</span><br><span class=\"line\">    public static final com.hmw.test.EnumTest SAT;</span><br><span class=\"line\">    public static final com.hmw.test.EnumTest SUN;</span><br><span class=\"line\">    static &#123;&#125;;</span><br><span class=\"line\">    public int getValue();</span><br><span class=\"line\">    public boolean isRest();</span><br><span class=\"line\">    public static com.hmw.test.EnumTest[] values();</span><br><span class=\"line\">    public static com.hmw.test.EnumTest valueOf(java.lang.String);</span><br><span class=\"line\">    com.hmw.test.EnumTest(java.lang.String, int, int, com.hmw.test.EnumTest);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>所以，实际上 enum 就是一个 class，只不过 java 编译器帮我们做了语法的解析和编译而已。<br>总结<br>    可以把 enum 看成是一个普通的 class，它们都可以定义一些属性和方法，不同之处是：enum 不能使用 extends 关键字继承其他类，因为 enum 已经继承了 java.lang.Enum（java是单一继承）。</p>"},{"title":"Hadoop总结(第一版---HDFS篇)","date":"2019-07-15T16:00:00.000Z","_content":"此处简介\n<!--more-->\n\n# Hadoop总结(第一版---HDFS篇)\n```接触hadoop生态也有大半年了，一直碎片化的查阅，学习了一些博客和书籍。随着使用的深入，对一些常用的模块有较熟练的使用，也有写过一些日常小结，但对hadoop本身背后的原理没有系统性的学习，回顾来看，就提升曲线来说，现在亟需总结这环节，这也是本文的由来```\n## 什么是hadoop？\n>hadoop是稳定的，高容错的，可大规模布署的分布式文件，存储，并行编程框架。\n```本文默认是已经有hadoop使用经验,所以暂不涉及具体的hadoop生态的各组件的部署和调优细节，后续单开文章来总结。但在具体讲解时会涉及说参数配置会对相关组件参生影响```\n具体而言，hadoop核心组件内容有：**hdfs**、**mapredcue**。所以接下来的总结主要针对这两在核心组件展开\n## HDFS篇\n### 什么是HDFS？\n**分布式文件系统**：分布式文件系统是一种允许文件通过网络在多台主机上分享的 文件的系统，可让多机器上的多用户分享文件和存储空间\nHDFS:（Hadoop Distribute File System）即hadoop分布式文件系统  主要用于适合运行在通用硬件上的分布式文件系统，特点是高度容错，适合布署在廉价服务器上，具有高吞吐量的数据访问等特点  1\\. 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。  2\\. 运行在廉价的机器上。  3\\. 适合大数据的处理。多大？多小？HDFS默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。  4\\. 适用于一次写入、多次查询的情况  5\\. 不支持并发写情况，小文件不合适。因为小文件也占用一个块，小文件越多（1000个1k文件）块越 多，NameNode压力越大。\n``hdfs得部署在linux系统上``\n### HDFS的具体内容\n文件、节点、数据块 HDFS主要是是围绕着这三个关键词设计的.\n#### **数据块**\n* Block：在HDFS中，每个文件都是采用的分块的方式存储，每个block放在不同的dataNode节点上，每个block的标识是一个三元组(block id , numBytes,generationStamp),block id 具有唯一性，具体分配是由namenode节点设置，然后在datanode上建立对应的Block文件，同时建立对应的block meta文件(问题是block meta文件存放位，block size可以通过配置文件设置，所以修改block size会对以前持续化的数据有何影响?）\n* Packet:是HDFS文件在DFSClient与DataNode之间通信的过程中文件的形式(一般一个Block对应多个Packet)\n* Chunk:是通过程中具体传输的文件单位，发送过程以Packet的方式进行，但 一个packet包含多个Chunk,同时对于每个chunk进行checksum计算，生成checksum bytes。\n\n**Packet**\nPacket的结构：数据包和heatbeat包  一个Packet数据包的组成结构主要分为 Packet Header 、PacketData  Packet Header 中又分为：\n![10eed6bcc563463ea0b8cd5adf99adec-image.png](//img.wqkenqing.ren.qiniudns.com/file/2017/7/10eed6bcc563463ea0b8cd5adf99adec-image.png)\nPacket Data部分是一个Packet的实际数据部分。\n主要内容有\n\n* 一个4字节校验\n* Checksum\n* Chunk部分，Chunk部分最大为512字节\n![c68e61da1e8148ab99b43fe8e3f5408e-image.png](//img.wqkenqing.ren/file/2017/7/c68e61da1e8148ab99b43fe8e3f5408e-image.png)\nPacket创建过程：首先将字节流数据写入一个buffer缓冲区中，也就是从偏移量为25的位置（checksumStart）开 始写Packet数据Chunk的Checksum部分，从偏移量为533的位置（dataStart）开始写Packet数据的Chunk Data部分，直到一个Packet创建完成为止。\n\n```注意：当写一个文件的最后一个Block的最后一个Packet时，如果一个Packet的大小未能达到最大长度，也就是上图对应的缓冲区 中，Checksum与Chunk Data之间还保留了一段未被写过的缓冲区位置，在发送这个Packet之前，会检查Chunksum与Chunk Data之间的缓冲区是否为空白缓冲区（gap），如果有则将Chunk Data部分向前移动，使得Chunk Data 1与Chunk Checksum N相邻，然后才会被发送到DataNode节点```\n\n#### hdsf架构(主要组成是节点）\n主要的构成角色有：Client、NameNode、SecondayNameNode、DataNode\n![5ba6f1ec6b9c4b0982dc2ce73ff9c444-image.png](//img.wqkenqing.ren/file/2017/7/5ba6f1ec6b9c4b0982dc2ce73ff9c444-image.png)\n\n+ Client：系统使用者，调用HDFS API操作文件；与NN交互获取文件元数据;与DN交互进行数据读写, 注意：写数据时文件切分由Client完成\n+ Namenode：Master节点 （也称元数据节点）是系统唯一的管理者。负责元数据的管理(名称空间和数据块映射信息);配置副本策略；处理客户端请求\n+ Datanode：数据存储节点(也称Slave节点)，存储实际的数据；执行数据块的读写；汇报存储信息给NN\n+ Secondary NameNode：备胎，namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode\n\n```careful:``` 注意：在hadoop 2.x 版本，当启用 hdfs ha 时，将没有这一角色\n**热冷备份说明**：\n热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作  冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失\n**hdfs构架原则**\n\n1. 元数据与数据分离：文件本身的属性（即元数据）与文件所持有的数据分离\n2. 主/从架构：一个HDFS集群是由一个NameNode和一定数目的DataNode组成\n3. 一次写入多次读取：HDFS中的文件在任何时间只能有一个Writer。当文件被创建，接着写入数据，最后，一旦文件被关闭，就不能再修改。\n4. 移动计算比移动数据更划算：数据运算，越靠近数据，执行运算的性能就越好，由于hdfs数据分布在不同机器上，要让网络的消耗最低，并提高系统的吞吐量，最佳方式是将运算的执行移到离它要处理的数据更近的地方，而不是移动数据\n\n**针对第四条的解释**：\n在上文中交代到hdfs中的文件是以block的形式存放在集群中，所以一个文件可以是被切分成很多block存放在集群的机器中针对第四条，移动计算比移动数据更划算，是因为，从理论上讲，集群的计算能力是很方便扩展的，如服务器的硬件提升，或增加服务器等。但网络带宽等资源却很容易达到瓶颈或增加经济负担，所以将计算转移至每个block就近的机器进行计算，会比将所有的block合到一个机器上再进行计算要划算，所以，叫移动计算要比移动数据划算\n\n##### NameNode\nNameNode是整个文件系统的管理与计算节点，是HDFS中最复杂的一个实体，它与管理着HDFS文件系统中最重要的两个关系\n\n\n1. HDFS文件系统中的文件目录树，以及文件的数据块索引，即每个文件对应的数据块列表  数据块和数据节点的对应关系，即某一块数据块保存在哪些数据节点的信息  **第一个关系**即目录树、元数据和数据块的索引信息持久化到物理存储中，具体的实现是保存在命名空间的镜像fsimage和编辑日志edits中，**careful**：在fsimage中，并没有记录每一个block对应到那几个Datanodes的对应表信息\n2. **第二个关系是**在NameNode启动后，每个DataNode对本地的磁盘进行扫描，将本DataNode上保存的block信息上报至NameNode,Namenode在接收到每个Datanode的块信息汇报后，将接收到的块信息，以及其所在的Datanode信息等保存在内存中。HDFS就是通过这种块信息汇报的方式来完成 block -> Datanodes list的对应表构建（**careful**）\n类似于数据库中的检查点，为了避免edits日志过大，在Hadoop1.X 中，SecondaryNameNode会按照时间阈值（比如24小时）或者edits大小阈值（比如1G），周期性的将fsimage和edits的合 并，然后将最新的fsimage推送给NameNode。而在Hadoop2.X中，这个动作是由Standby NameNode来完成.\n由此可看出，这两个文件一旦损坏或丢失，将导致整个HDFS文件系统不可用\n在hadoop1.X为了保证这两种元数据文件的高可用性，一般的做法，将dfs.namenode.name.dir设置成以逗号分隔的多个目录，这多个目录至少不要在一块磁盘上，最好放在不同的机器上，比如：挂载一个共享文件系统\nfsimage\\edits 是序列化后的文件，想要查看或编辑里面的内容，可通过 hdfs 提供的 oiv\\oev 命令，\n命令: hdfs oiv （offline image viewer） 用于将fsimage文件的内容转储到指定文件中以便于阅读,，如文本文件、XML文件，该命令需要以下参数：\n-i (必填参数) –inputFile <arg> 输入FSImage文件\n-o (必填参数) –outputFile <arg> 输出转换后的文件，如果存在，则会覆盖\n-p (可选参数） –processor <arg> 将FSImage文件转换成哪种格式： (Ls|XML|FileDistribution).默认为Ls\n示例：hdfs oiv -i /data1/hadoop/dfs/name/current/fsimage_0000000000019372521 -o /home/hadoop/fsimage.txt\n命令：hdfs oev (offline edits viewer 离线edits查看器）的缩写， 该工具只操作文件因而并不需要hadoop集群处于运行状态。\n示例: hdfs oev -i edits_0000000000000042778-0000000000000042779 -o edits.xml\n支持的输出格式有binary（hadoop使用的二进制格式）、xml（在不使用参数p时的默认输出格式）和stats（输出edits文件的统计信息）\n由此可以总结到：NameNode管理着DataNode，接收DataNode的注册、心跳、数据块提交等信息的上报，并且在心跳中发送数据块**复制**、**删除**、**恢复**等指令；同时，NameNode还为客户端对**文件系统目录树的操作**和对**文件数据读写**、对**HDFS系统进行管理提供支持**\n另 Namenode 启动后会进入一个称为**安全模式**的特殊状态。处于安全模式 的 Namenode 是不会进行数据块的复制的。 Namenode 从所有的 Datanode 接收心跳信号和块状态报告。 块状态报告包括了某个 Datanode 所有的数据 块列表。每个数据块都有一个指定的最小副本数。当 Namenode 检测确认某 个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全 (safely replicated) 的；在一定百分比（这个参数可配置）的数据块被 Namenode 检测确认是安全之后（加上一个额外的 30 秒等待时间）， Namenode 将退出安全模式状态。接下来它会确定还有哪些数据块的副本没 有达到指定数目，并将这些数据块复制到其他 Datanode 上。  ##### Secondary NameNode  在HA cluster中又称为standby node\n主要作用：  1.如上文提到的合并fsimage和eits日志，将eits日志文件大小控制在一个限度下  大致流程如下  namenode 响应 Secondary namenode 请求，将 edit log 推送给 Secondary namenode ， 开始重新写一个新的 edit log  Secondary namenode 收到来自 namenode 的 fsimage 文件和 edit log  Secondary namenode 将 fsimage 加载到内存，应用 edit log ， 并生成一 个新的 fsimage 文件  Secondary namenode 将新的 fsimage 推送给 Namenode  Namenode 用新的 fsimage 取代旧的 fsimage ， 在 fstime 文件中记下检查 点发生的时\n![aed541bebcc04fb79e2de3c504b7ee46-image.png](//img.wqkenqing.ren/file/2017/7/aed541bebcc04fb79e2de3c504b7ee46-image.png)\n\n\n#### HDFS写文件  1.x 默认的block大小是64M 2.X版本默认block的大小是 128M ![d0c33b33bfbc41e8a493553395e52ef0-image.png](//img.wqkenqing.ren/file/2017/7/d0c33b33bfbc41e8a493553395e52ef0-image.png)\n如上图所示  + Client预先设置的block参数切分FIle\n\n+ CLient向NameNode发送写数据请求，\n+ NameNode，记录block信息，并返回可用的DataNode(具体的返回规则参考下文)\n+ client向DataNode发送block1；发送过程是以流式写入具体流程是\n\n1. 将64M的block1按64k的packet划分\n2. 然后将第一个packet发送给host2\n3. host2接收完后，将第一个packet发送给host1，同时client想host2发送第二个packet\n4. host1接收完第一个packet后，发送给host3，同时接收host2发来的第二个packet\n5. 以此类推，如图红线实线所示，直到将block1发送完毕\n6. host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。\n7. client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。\n8. 发送完block1后，再向host7，host8，host4发送block2  当客户端向 HDFS 文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副 本系数设置为 3 ，当本地临时文件累积到一个数据块的大小时，客户端会从 Namenode 获取一个 Datanode 列表用于存放副本。然后客户端开始向第一个 Datanode 传输数据，第一个 Datanode 一小部分一小部分 (4 KB) 地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中 第二个 Datanode 节点。第二个 Datanode 也是这样，一小部分一小部分地接收数据，写入本地 仓库，并同时传给第三个 Datanode 。最后，第三个 Datanode 接收数据并存储在本地。因此， Datanode 能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的 方式从前一个 Datanode 复制到下一个\n![2c77aaad2f684820bf8a6b475b79d2f1-image.png](//img.wqkenqing.ren/file/2017/7/2c77aaad2f684820bf8a6b475b79d2f1-image.png)\n\n  写入的过程，按hdsf默认设置，1T文件，我们需要3T的存储，3T的网络流量  在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去  挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份\n##### hdfs读文件\n\n  ![8fb62c84f57a4203a2dbedf5f68920d9-image.png](//img.wqkenqing.ren/file/2017/7/8fb62c84f57a4203a2dbedf5f68920d9-image.png)\n\n  客户端通过调用FileSystem对象的open()方法来打开希望读取的文件，对于HDFS来说，这个对象时分布文件系统的一个实例；  DistributedFileSystem通过使用RPC来调用NameNode以确定文件起始块的位置，同一Block按照重复数会返回多个位置，这些位置按照Hadoop集群拓扑结构排序，距离客户端近的排在前面  前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流，客户端对这个输入流调用read()方法  存储着文件起始块的DataNode地址的DFSInputStream随即连接距离最近的DataNode，通过对数据流反复调用read()方法，将数据从DataNode传输到客户端  到达块的末端时，DFSInputStream会关闭与该DataNode的连接，然后寻找下一个块的最佳DataNode，这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流  一旦客户端完成读取，就对FSDataInputStream调用close()方法关闭文件读取\n  ##### block持续化结构\n  DataNode节点上一个Block持久化到磁盘上的物理存储结构，如下图所示：  ![df10f0ac586f4baebc2587842aec5675-image.png](//img.wqkenqing.ren/file/2017/7/df10f0ac586f4baebc2587842aec5675-image.png)\n\n 每个Block文件（如上图中blk_1084013198文件）都对应一个meta文件（如上图中blk_1084013198_10273532.meta文件），Block文件是一个一个Chunk的二进制数据（每个Chunk的大小是512字节），而meta文件是与每一个Chunk对应的Checksum数据，是序列化形式存储  ---  至上我们大致了解了HDFS。正如上文提到的Hadoop的特点，高可能，高容错性。若光从上文提到的特性可能还不足以说明，如NameNode环节就提到了NameNode的重要作用，但若NameNode出现了故障，对整个机集会是毁灭性的打击，于是Hadoop也引入其它的一些手段来保存高可用，高容错。接下来我们就来探讨下\n### Hadoop HA的引入\n HA：High Available即高可用性集群，是保证业务连续性的有效解决方案，一般有两个或两个以上的节点，且分为活动节点及备用节点。  在HA具体实现方法不同的情况下，HA框架的流程是一致的, 不一致的就是如何存储和管理日志。在Active NN和Standby NN之间要有个共享的存储日志的地方，Active NN把EditLog写到这个共享的存储日志的地方，Standby NN去读取日志然后执行，这样Active和Standby NN内存中的HDFS元数据保持着同步。一旦发生主从切换Standby NN可以尽快接管Active NN的工作; 默认并未启用 hdfs ha。\n SPOF方案回顾：\n\n1. Secondary NameNode：它不是HA，它只是阶段性的合并edits和fsimage，以缩短集群启动的时间。当NN失效的时候，Secondary NN并无法立刻提供服务，Secondary NN甚至无法保证数据完整性：如果NN数据丢失的话，在上一次合并后的文件系统的改动会丢失\n2. Backup NameNode (HADOOP-4539)：它在内存中复制了NN的当前状态，算是Warm Standby，可也就仅限于此，并没有failover等。它同样是阶段性的做checkpoint，也无法保证数据完整性\n3\\. 手动把name.dir指向NFS（Network File System），这是安全的Cold Standby，可以保证元数据不丢失，但集群的恢复则完全靠手动\n4\\. Facebook AvatarNode：Facebook有强大的运维做后盾，所以Avatarnode只是Hot Standby，并没有自动切换，当主NN失效的时候，需要管理员确认，然后手动把对外提供服务的虚拟IP映射到Standby NN，这样做的好处是确保不会发生脑裂的场景。其某些设计思想和Hadoop 2.0里的HA非常相似，从时间上来看，Hadoop 2.0应该是借鉴了Facebook的做法 ![32ec8d45379b4e4d99505b03b1b33e61-image.png](//img.wqkenqing.ren/file/2017/7/32ec8d45379b4e4d99505b03b1b33e61-image.png)\n\n5\\. PrimaryNN 与StandbyNN之间通过NFS来共享FsEdits、FsImage文件，这样主备NN之间就拥有了一致的目录树和block信息；而block的 位置信息，可以根据DN向两个NN上报的信息过程中构建起来。这样再辅以虚IP，可以较好达到主备NN快速热切的目的。但是显然，这里的NFS又引入了新的SPOF\n6\\. 在主备NN共享元数据的过程中，也有方案通过主NN将FsEdits的内容通过与备NN建立的网络IO流，实时写入备NN，并且保证整个过程的原子性。这种方案，解决了NFS共享元数据引入的SPOF，但是主备NN之间的网络连接又会成为新的问题\n#### hadoop2.X ha 原理:  hadoop2.x之后，Clouera提出了QJM/Qurom Journal Manager，这是一个基于Paxos算法实现的HDFS HA方案，它给出了一种较好的解决思路和方案,示意图如下：\n![bc057fbb4b4047928bf06b7a43364335-image.png](//img.wqkenqing.ren/file/2017/7/bc057fbb4b4047928bf06b7a43364335-image.png)\n\n  + 基本原理就是用2N+1台 JN 存储EditLog，每次写数据操作有大多数（>=N+1）返回成功时即认为该次写成功，数据不会丢失了。当然这个算法所能容忍的是最多有N台机器挂掉，如果多于N台挂掉，这个算法就失效了。这个原理是基于Paxos算法\n  + 在HA架构里面SecondaryNameNode这个冷备角色已经不存在了，为了保持standby NN时时的与主Active NN的元数据保持一致，他们之间交互通过一系列守护的轻量级进程JournalNode  + 任何修改操作在 Active NN上执行时，JN进程同时也会记录修改log到至少半数以上的JN中，这时 Standby NN 监测到JN 里面的同步log发生变化了会读取 JN 里面的修改log，然后同步到自己的的目录镜像树里面，\n  ![608621d2ab52466db89e8a95d165f6e7-image.png](//img.wqkenqing.ren/file/2017/7/608621d2ab52466db89e8a95d165f6e7-image.png)\n\n 当发生故障时，Active的 NN 挂掉后，Standby NN 会在它成为Active NN 前，读取所有的JN里面的修改日志，这样就能高可靠的保证与挂掉的NN的目录镜像树一致，然后无缝的接替它的职责，维护来自客户端请求，从而达到一个高可用的目的  QJM方式来实现HA的主要优势：\n 1\\. 不需要配置额外的高共享存储，降低了复杂度和维护成本\n 2\\. 消除spof\n 3\\. 系统鲁棒性(Robust:健壮)的程度是可配置\n 4\\. JN不会因为其中一台的延迟而影响整体的延迟，而且也不会因为JN的数量增多而影响性能（因为NN向JN发送日志是并行的）  datanode的fencing: 确保只有一个NN能命令DN。HDFS-1972中详细描述了DN如何实现fencing\n 1\\. 每个NN改变状态的时候，向DN发送自己的状态和一个序列号\n 2\\. DN在运行过程中维护此序列号，当failover时，新的NN在返回DN心跳时会返回自己的active状态和一个更大的序列号。DN接收到这个返回则认为该NN为新的active  3\\. 如果这时原来的active NN恢复，返回给DN的心跳信息包含active状态和原来的序列号，这时DN就会拒绝这个NN的命令  客户端fencing：确保只有一个NN能响应客户端请求，让访问standby nn的客户端直接失败。在RPC层封装了一层，通过FailoverProxyProvider以重试的方式连接NN。通过若干次连接一个NN失败后尝试连接新的NN，对客户端的影响是重试的时候增加一定的延迟。客户端可以设置重试此时和时间  Hadoop提供了ZKFailoverController角色，部署在每个NameNode的节点上，作为一个deamon进程, 简称zkfc，\n ![3cfc6fc17a8e47509465442f6eaf7c14-image.png](//img.wqkenqing.ren/file/2017/7/3cfc6fc17a8e47509465442f6eaf7c14-image.png)\n\n   FailoverController主要包括三个组件:\n   1\\. HealthMonitor: 监控NameNode是否处于unavailable或unhealthy状态。当前通过RPC调用NN相应的方法完成\n   2\\. ActiveStandbyElector: 管理和监控自己在ZK中的状态\n   3\\. ZKFailoverController 它订阅HealthMonitor 和ActiveStandbyElector 的事件，并管理NameNode的状态  ZKFailoverController主要职责：      1\\. 健康监测：周期性的向它监控的NN发送健康探测命令，从而来确定某个NameNode是否处于健康状态，如果机器宕机，心跳失败，那么zkfc就会标记它处于一个不健康的状态\n   2\\. 会话管理：如 果NN是健康的，zkfc就会在zookeeper中保持一个打开的会话，如果NameNode同时还是Active状态的，那么zkfc还会在 Zookeeper中占有一个类型为短暂类型的znode，当这个NN挂掉时，这个znode将会被删除，然后备用的NN，将会得到这把锁，升级为主 NN，同时标记状态为Active\n   3\\. 当宕机的NN新启动时，它会再次注册zookeper，发现已经有znode锁了，便会自动变为Standby状态，如此往复循环，保证高可靠，需要注意，目前仅仅支持最多配置2个NN\n   4\\. master选举：如上所述，通过在zookeeper中维持一个短暂类型的znode，来实现抢占式的锁机制，从而判断那个NameNode为Active状态  **hadoop2.x Federation**：  单Active NN的架构使得HDFS在集群扩展性和性能上都有潜在的问题，当集群大到一定程度后，NN进程使用的内存可能会达到上百G，NN成为了性能的瓶颈  常用的估算公式为1G对应1百万个块，按缺省块大小计算的话，大概是64T (这个估算比例是有比较大的富裕的，其实，即使是每个文件只有一个块，所有元数据信息也不会有1KB/block)  为了解决这个问题,Hadoop 2.x提供了HDFS Federation, 示意图如下：\n   ![e5015b90accf4fe6a7729afe692ffa64-image.png](//img.wqkenqing.ren/file/2017/7/e5015b90accf4fe6a7729afe692ffa64-image.png)\n\n  多个NN共用一个集群里的存储资源，每个NN都可以单独对外提供服务  每个NN都会定义一个存储池，有单独的id，每个DN都为所有存储池提供存储  DN会按照存储池id向其对应的NN汇报块信息，同时，DN会向所有NN汇报本地存储可用资源情况  如果需要在客户端方便的访问若干个NN上的资源，可以使用客户端挂载表，把不同的目录映射到不同的NN，但NN上必须存在相应的目录  设计优势：  改动最小，向前兼容；现有的NN无需任何配置改动；如果现有的客户端只连某台NN的话  分离命名空间管理和块存储管理  客户端挂载表：通过路径自动对应NN、使Federation的配置改动对应用透明  (与上面ha方案中介绍的最多2个NN冲突？)  至此hadoop中的hdfs高可用特性，高容错的实现又有了更深理解，但针对hdfs还一层设计实现**机架感知**\n#### 机架感知\n分布式的集群通常包含非常多的机器，由于受到机架槽位和交换机网口的限制，通常大型的分布式集群都会跨好几个机架，由多个机架上的机器共同组成一个分布 式集群。机架内的机器之间的网络速度通常都会高于跨机架机器之间的网络速度，并且机架之间机器的网络通信通常受到上层交换机间网络带宽的限制。  具体到hadoop集群，由于hadoop的HDFS对数据文件的分布式存放是按照分块block存储，每个block会有多个副本(默认为3)，并且为了数据的安全和高效，所以hadoop默认对3个副本的存放策略为：\n在本地机器的hdfs目录下存储一个block  在另外一个rack的某个datanode上存储一个block\n在该机器的同一个rack下的某台机器上存储最后一个block\n这样的策略可以保证对该block所属文件的访问能够优先在本rack下找到，如果整个rack发生了异常，也可以在另外的rack上找到该block的副本。这样足够的高效，并且同时做到了数据的容错。\nhadoop对机架的感知并非是自适应的，亦即，hadoop集群分辨某台slave机器是属于哪个rack并非是只能的感知的，而是需要 hadoop的管理者人为的告知hadoop哪台机器属于哪个rack，这样在hadoop的namenode启动初始化时，会将这些机器与rack的对 应信息保存在内存中，用来作为对接下来所有的HDFS的写块操作分配datanode列表时（比如3个block对应三台datanode）的选择 datanode策略，做到hadoop allocate block的策略：尽量将三个副本分布到不同的rack。\n具体实现本文不在深究，在此附上网上的一些解决方式\n[机架感知实现1](http://www.cnblogs.com/cloudma/articles/hadoop-topology.html)  [机架感知实现2](http://blog.csdn.net/magicdreaming/article/details/7629773)\n\n---\n\n至此对hdfs的理解与总结告一段落，后续有了新的理解再进行补充\n\n\n\n","source":"_posts/技术/hexo/oldblog/blog24.md","raw":"---\n\ntitle: Hadoop总结(第一版---HDFS篇)\ndate: 2019-07-16\ntags: \n\n---\n此处简介\n<!--more-->\n\n# Hadoop总结(第一版---HDFS篇)\n```接触hadoop生态也有大半年了，一直碎片化的查阅，学习了一些博客和书籍。随着使用的深入，对一些常用的模块有较熟练的使用，也有写过一些日常小结，但对hadoop本身背后的原理没有系统性的学习，回顾来看，就提升曲线来说，现在亟需总结这环节，这也是本文的由来```\n## 什么是hadoop？\n>hadoop是稳定的，高容错的，可大规模布署的分布式文件，存储，并行编程框架。\n```本文默认是已经有hadoop使用经验,所以暂不涉及具体的hadoop生态的各组件的部署和调优细节，后续单开文章来总结。但在具体讲解时会涉及说参数配置会对相关组件参生影响```\n具体而言，hadoop核心组件内容有：**hdfs**、**mapredcue**。所以接下来的总结主要针对这两在核心组件展开\n## HDFS篇\n### 什么是HDFS？\n**分布式文件系统**：分布式文件系统是一种允许文件通过网络在多台主机上分享的 文件的系统，可让多机器上的多用户分享文件和存储空间\nHDFS:（Hadoop Distribute File System）即hadoop分布式文件系统  主要用于适合运行在通用硬件上的分布式文件系统，特点是高度容错，适合布署在廉价服务器上，具有高吞吐量的数据访问等特点  1\\. 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。  2\\. 运行在廉价的机器上。  3\\. 适合大数据的处理。多大？多小？HDFS默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。  4\\. 适用于一次写入、多次查询的情况  5\\. 不支持并发写情况，小文件不合适。因为小文件也占用一个块，小文件越多（1000个1k文件）块越 多，NameNode压力越大。\n``hdfs得部署在linux系统上``\n### HDFS的具体内容\n文件、节点、数据块 HDFS主要是是围绕着这三个关键词设计的.\n#### **数据块**\n* Block：在HDFS中，每个文件都是采用的分块的方式存储，每个block放在不同的dataNode节点上，每个block的标识是一个三元组(block id , numBytes,generationStamp),block id 具有唯一性，具体分配是由namenode节点设置，然后在datanode上建立对应的Block文件，同时建立对应的block meta文件(问题是block meta文件存放位，block size可以通过配置文件设置，所以修改block size会对以前持续化的数据有何影响?）\n* Packet:是HDFS文件在DFSClient与DataNode之间通信的过程中文件的形式(一般一个Block对应多个Packet)\n* Chunk:是通过程中具体传输的文件单位，发送过程以Packet的方式进行，但 一个packet包含多个Chunk,同时对于每个chunk进行checksum计算，生成checksum bytes。\n\n**Packet**\nPacket的结构：数据包和heatbeat包  一个Packet数据包的组成结构主要分为 Packet Header 、PacketData  Packet Header 中又分为：\n![10eed6bcc563463ea0b8cd5adf99adec-image.png](//img.wqkenqing.ren.qiniudns.com/file/2017/7/10eed6bcc563463ea0b8cd5adf99adec-image.png)\nPacket Data部分是一个Packet的实际数据部分。\n主要内容有\n\n* 一个4字节校验\n* Checksum\n* Chunk部分，Chunk部分最大为512字节\n![c68e61da1e8148ab99b43fe8e3f5408e-image.png](//img.wqkenqing.ren/file/2017/7/c68e61da1e8148ab99b43fe8e3f5408e-image.png)\nPacket创建过程：首先将字节流数据写入一个buffer缓冲区中，也就是从偏移量为25的位置（checksumStart）开 始写Packet数据Chunk的Checksum部分，从偏移量为533的位置（dataStart）开始写Packet数据的Chunk Data部分，直到一个Packet创建完成为止。\n\n```注意：当写一个文件的最后一个Block的最后一个Packet时，如果一个Packet的大小未能达到最大长度，也就是上图对应的缓冲区 中，Checksum与Chunk Data之间还保留了一段未被写过的缓冲区位置，在发送这个Packet之前，会检查Chunksum与Chunk Data之间的缓冲区是否为空白缓冲区（gap），如果有则将Chunk Data部分向前移动，使得Chunk Data 1与Chunk Checksum N相邻，然后才会被发送到DataNode节点```\n\n#### hdsf架构(主要组成是节点）\n主要的构成角色有：Client、NameNode、SecondayNameNode、DataNode\n![5ba6f1ec6b9c4b0982dc2ce73ff9c444-image.png](//img.wqkenqing.ren/file/2017/7/5ba6f1ec6b9c4b0982dc2ce73ff9c444-image.png)\n\n+ Client：系统使用者，调用HDFS API操作文件；与NN交互获取文件元数据;与DN交互进行数据读写, 注意：写数据时文件切分由Client完成\n+ Namenode：Master节点 （也称元数据节点）是系统唯一的管理者。负责元数据的管理(名称空间和数据块映射信息);配置副本策略；处理客户端请求\n+ Datanode：数据存储节点(也称Slave节点)，存储实际的数据；执行数据块的读写；汇报存储信息给NN\n+ Secondary NameNode：备胎，namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode\n\n```careful:``` 注意：在hadoop 2.x 版本，当启用 hdfs ha 时，将没有这一角色\n**热冷备份说明**：\n热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作  冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失\n**hdfs构架原则**\n\n1. 元数据与数据分离：文件本身的属性（即元数据）与文件所持有的数据分离\n2. 主/从架构：一个HDFS集群是由一个NameNode和一定数目的DataNode组成\n3. 一次写入多次读取：HDFS中的文件在任何时间只能有一个Writer。当文件被创建，接着写入数据，最后，一旦文件被关闭，就不能再修改。\n4. 移动计算比移动数据更划算：数据运算，越靠近数据，执行运算的性能就越好，由于hdfs数据分布在不同机器上，要让网络的消耗最低，并提高系统的吞吐量，最佳方式是将运算的执行移到离它要处理的数据更近的地方，而不是移动数据\n\n**针对第四条的解释**：\n在上文中交代到hdfs中的文件是以block的形式存放在集群中，所以一个文件可以是被切分成很多block存放在集群的机器中针对第四条，移动计算比移动数据更划算，是因为，从理论上讲，集群的计算能力是很方便扩展的，如服务器的硬件提升，或增加服务器等。但网络带宽等资源却很容易达到瓶颈或增加经济负担，所以将计算转移至每个block就近的机器进行计算，会比将所有的block合到一个机器上再进行计算要划算，所以，叫移动计算要比移动数据划算\n\n##### NameNode\nNameNode是整个文件系统的管理与计算节点，是HDFS中最复杂的一个实体，它与管理着HDFS文件系统中最重要的两个关系\n\n\n1. HDFS文件系统中的文件目录树，以及文件的数据块索引，即每个文件对应的数据块列表  数据块和数据节点的对应关系，即某一块数据块保存在哪些数据节点的信息  **第一个关系**即目录树、元数据和数据块的索引信息持久化到物理存储中，具体的实现是保存在命名空间的镜像fsimage和编辑日志edits中，**careful**：在fsimage中，并没有记录每一个block对应到那几个Datanodes的对应表信息\n2. **第二个关系是**在NameNode启动后，每个DataNode对本地的磁盘进行扫描，将本DataNode上保存的block信息上报至NameNode,Namenode在接收到每个Datanode的块信息汇报后，将接收到的块信息，以及其所在的Datanode信息等保存在内存中。HDFS就是通过这种块信息汇报的方式来完成 block -> Datanodes list的对应表构建（**careful**）\n类似于数据库中的检查点，为了避免edits日志过大，在Hadoop1.X 中，SecondaryNameNode会按照时间阈值（比如24小时）或者edits大小阈值（比如1G），周期性的将fsimage和edits的合 并，然后将最新的fsimage推送给NameNode。而在Hadoop2.X中，这个动作是由Standby NameNode来完成.\n由此可看出，这两个文件一旦损坏或丢失，将导致整个HDFS文件系统不可用\n在hadoop1.X为了保证这两种元数据文件的高可用性，一般的做法，将dfs.namenode.name.dir设置成以逗号分隔的多个目录，这多个目录至少不要在一块磁盘上，最好放在不同的机器上，比如：挂载一个共享文件系统\nfsimage\\edits 是序列化后的文件，想要查看或编辑里面的内容，可通过 hdfs 提供的 oiv\\oev 命令，\n命令: hdfs oiv （offline image viewer） 用于将fsimage文件的内容转储到指定文件中以便于阅读,，如文本文件、XML文件，该命令需要以下参数：\n-i (必填参数) –inputFile <arg> 输入FSImage文件\n-o (必填参数) –outputFile <arg> 输出转换后的文件，如果存在，则会覆盖\n-p (可选参数） –processor <arg> 将FSImage文件转换成哪种格式： (Ls|XML|FileDistribution).默认为Ls\n示例：hdfs oiv -i /data1/hadoop/dfs/name/current/fsimage_0000000000019372521 -o /home/hadoop/fsimage.txt\n命令：hdfs oev (offline edits viewer 离线edits查看器）的缩写， 该工具只操作文件因而并不需要hadoop集群处于运行状态。\n示例: hdfs oev -i edits_0000000000000042778-0000000000000042779 -o edits.xml\n支持的输出格式有binary（hadoop使用的二进制格式）、xml（在不使用参数p时的默认输出格式）和stats（输出edits文件的统计信息）\n由此可以总结到：NameNode管理着DataNode，接收DataNode的注册、心跳、数据块提交等信息的上报，并且在心跳中发送数据块**复制**、**删除**、**恢复**等指令；同时，NameNode还为客户端对**文件系统目录树的操作**和对**文件数据读写**、对**HDFS系统进行管理提供支持**\n另 Namenode 启动后会进入一个称为**安全模式**的特殊状态。处于安全模式 的 Namenode 是不会进行数据块的复制的。 Namenode 从所有的 Datanode 接收心跳信号和块状态报告。 块状态报告包括了某个 Datanode 所有的数据 块列表。每个数据块都有一个指定的最小副本数。当 Namenode 检测确认某 个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全 (safely replicated) 的；在一定百分比（这个参数可配置）的数据块被 Namenode 检测确认是安全之后（加上一个额外的 30 秒等待时间）， Namenode 将退出安全模式状态。接下来它会确定还有哪些数据块的副本没 有达到指定数目，并将这些数据块复制到其他 Datanode 上。  ##### Secondary NameNode  在HA cluster中又称为standby node\n主要作用：  1.如上文提到的合并fsimage和eits日志，将eits日志文件大小控制在一个限度下  大致流程如下  namenode 响应 Secondary namenode 请求，将 edit log 推送给 Secondary namenode ， 开始重新写一个新的 edit log  Secondary namenode 收到来自 namenode 的 fsimage 文件和 edit log  Secondary namenode 将 fsimage 加载到内存，应用 edit log ， 并生成一 个新的 fsimage 文件  Secondary namenode 将新的 fsimage 推送给 Namenode  Namenode 用新的 fsimage 取代旧的 fsimage ， 在 fstime 文件中记下检查 点发生的时\n![aed541bebcc04fb79e2de3c504b7ee46-image.png](//img.wqkenqing.ren/file/2017/7/aed541bebcc04fb79e2de3c504b7ee46-image.png)\n\n\n#### HDFS写文件  1.x 默认的block大小是64M 2.X版本默认block的大小是 128M ![d0c33b33bfbc41e8a493553395e52ef0-image.png](//img.wqkenqing.ren/file/2017/7/d0c33b33bfbc41e8a493553395e52ef0-image.png)\n如上图所示  + Client预先设置的block参数切分FIle\n\n+ CLient向NameNode发送写数据请求，\n+ NameNode，记录block信息，并返回可用的DataNode(具体的返回规则参考下文)\n+ client向DataNode发送block1；发送过程是以流式写入具体流程是\n\n1. 将64M的block1按64k的packet划分\n2. 然后将第一个packet发送给host2\n3. host2接收完后，将第一个packet发送给host1，同时client想host2发送第二个packet\n4. host1接收完第一个packet后，发送给host3，同时接收host2发来的第二个packet\n5. 以此类推，如图红线实线所示，直到将block1发送完毕\n6. host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。\n7. client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。\n8. 发送完block1后，再向host7，host8，host4发送block2  当客户端向 HDFS 文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副 本系数设置为 3 ，当本地临时文件累积到一个数据块的大小时，客户端会从 Namenode 获取一个 Datanode 列表用于存放副本。然后客户端开始向第一个 Datanode 传输数据，第一个 Datanode 一小部分一小部分 (4 KB) 地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中 第二个 Datanode 节点。第二个 Datanode 也是这样，一小部分一小部分地接收数据，写入本地 仓库，并同时传给第三个 Datanode 。最后，第三个 Datanode 接收数据并存储在本地。因此， Datanode 能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的 方式从前一个 Datanode 复制到下一个\n![2c77aaad2f684820bf8a6b475b79d2f1-image.png](//img.wqkenqing.ren/file/2017/7/2c77aaad2f684820bf8a6b475b79d2f1-image.png)\n\n  写入的过程，按hdsf默认设置，1T文件，我们需要3T的存储，3T的网络流量  在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去  挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份\n##### hdfs读文件\n\n  ![8fb62c84f57a4203a2dbedf5f68920d9-image.png](//img.wqkenqing.ren/file/2017/7/8fb62c84f57a4203a2dbedf5f68920d9-image.png)\n\n  客户端通过调用FileSystem对象的open()方法来打开希望读取的文件，对于HDFS来说，这个对象时分布文件系统的一个实例；  DistributedFileSystem通过使用RPC来调用NameNode以确定文件起始块的位置，同一Block按照重复数会返回多个位置，这些位置按照Hadoop集群拓扑结构排序，距离客户端近的排在前面  前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流，客户端对这个输入流调用read()方法  存储着文件起始块的DataNode地址的DFSInputStream随即连接距离最近的DataNode，通过对数据流反复调用read()方法，将数据从DataNode传输到客户端  到达块的末端时，DFSInputStream会关闭与该DataNode的连接，然后寻找下一个块的最佳DataNode，这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流  一旦客户端完成读取，就对FSDataInputStream调用close()方法关闭文件读取\n  ##### block持续化结构\n  DataNode节点上一个Block持久化到磁盘上的物理存储结构，如下图所示：  ![df10f0ac586f4baebc2587842aec5675-image.png](//img.wqkenqing.ren/file/2017/7/df10f0ac586f4baebc2587842aec5675-image.png)\n\n 每个Block文件（如上图中blk_1084013198文件）都对应一个meta文件（如上图中blk_1084013198_10273532.meta文件），Block文件是一个一个Chunk的二进制数据（每个Chunk的大小是512字节），而meta文件是与每一个Chunk对应的Checksum数据，是序列化形式存储  ---  至上我们大致了解了HDFS。正如上文提到的Hadoop的特点，高可能，高容错性。若光从上文提到的特性可能还不足以说明，如NameNode环节就提到了NameNode的重要作用，但若NameNode出现了故障，对整个机集会是毁灭性的打击，于是Hadoop也引入其它的一些手段来保存高可用，高容错。接下来我们就来探讨下\n### Hadoop HA的引入\n HA：High Available即高可用性集群，是保证业务连续性的有效解决方案，一般有两个或两个以上的节点，且分为活动节点及备用节点。  在HA具体实现方法不同的情况下，HA框架的流程是一致的, 不一致的就是如何存储和管理日志。在Active NN和Standby NN之间要有个共享的存储日志的地方，Active NN把EditLog写到这个共享的存储日志的地方，Standby NN去读取日志然后执行，这样Active和Standby NN内存中的HDFS元数据保持着同步。一旦发生主从切换Standby NN可以尽快接管Active NN的工作; 默认并未启用 hdfs ha。\n SPOF方案回顾：\n\n1. Secondary NameNode：它不是HA，它只是阶段性的合并edits和fsimage，以缩短集群启动的时间。当NN失效的时候，Secondary NN并无法立刻提供服务，Secondary NN甚至无法保证数据完整性：如果NN数据丢失的话，在上一次合并后的文件系统的改动会丢失\n2. Backup NameNode (HADOOP-4539)：它在内存中复制了NN的当前状态，算是Warm Standby，可也就仅限于此，并没有failover等。它同样是阶段性的做checkpoint，也无法保证数据完整性\n3\\. 手动把name.dir指向NFS（Network File System），这是安全的Cold Standby，可以保证元数据不丢失，但集群的恢复则完全靠手动\n4\\. Facebook AvatarNode：Facebook有强大的运维做后盾，所以Avatarnode只是Hot Standby，并没有自动切换，当主NN失效的时候，需要管理员确认，然后手动把对外提供服务的虚拟IP映射到Standby NN，这样做的好处是确保不会发生脑裂的场景。其某些设计思想和Hadoop 2.0里的HA非常相似，从时间上来看，Hadoop 2.0应该是借鉴了Facebook的做法 ![32ec8d45379b4e4d99505b03b1b33e61-image.png](//img.wqkenqing.ren/file/2017/7/32ec8d45379b4e4d99505b03b1b33e61-image.png)\n\n5\\. PrimaryNN 与StandbyNN之间通过NFS来共享FsEdits、FsImage文件，这样主备NN之间就拥有了一致的目录树和block信息；而block的 位置信息，可以根据DN向两个NN上报的信息过程中构建起来。这样再辅以虚IP，可以较好达到主备NN快速热切的目的。但是显然，这里的NFS又引入了新的SPOF\n6\\. 在主备NN共享元数据的过程中，也有方案通过主NN将FsEdits的内容通过与备NN建立的网络IO流，实时写入备NN，并且保证整个过程的原子性。这种方案，解决了NFS共享元数据引入的SPOF，但是主备NN之间的网络连接又会成为新的问题\n#### hadoop2.X ha 原理:  hadoop2.x之后，Clouera提出了QJM/Qurom Journal Manager，这是一个基于Paxos算法实现的HDFS HA方案，它给出了一种较好的解决思路和方案,示意图如下：\n![bc057fbb4b4047928bf06b7a43364335-image.png](//img.wqkenqing.ren/file/2017/7/bc057fbb4b4047928bf06b7a43364335-image.png)\n\n  + 基本原理就是用2N+1台 JN 存储EditLog，每次写数据操作有大多数（>=N+1）返回成功时即认为该次写成功，数据不会丢失了。当然这个算法所能容忍的是最多有N台机器挂掉，如果多于N台挂掉，这个算法就失效了。这个原理是基于Paxos算法\n  + 在HA架构里面SecondaryNameNode这个冷备角色已经不存在了，为了保持standby NN时时的与主Active NN的元数据保持一致，他们之间交互通过一系列守护的轻量级进程JournalNode  + 任何修改操作在 Active NN上执行时，JN进程同时也会记录修改log到至少半数以上的JN中，这时 Standby NN 监测到JN 里面的同步log发生变化了会读取 JN 里面的修改log，然后同步到自己的的目录镜像树里面，\n  ![608621d2ab52466db89e8a95d165f6e7-image.png](//img.wqkenqing.ren/file/2017/7/608621d2ab52466db89e8a95d165f6e7-image.png)\n\n 当发生故障时，Active的 NN 挂掉后，Standby NN 会在它成为Active NN 前，读取所有的JN里面的修改日志，这样就能高可靠的保证与挂掉的NN的目录镜像树一致，然后无缝的接替它的职责，维护来自客户端请求，从而达到一个高可用的目的  QJM方式来实现HA的主要优势：\n 1\\. 不需要配置额外的高共享存储，降低了复杂度和维护成本\n 2\\. 消除spof\n 3\\. 系统鲁棒性(Robust:健壮)的程度是可配置\n 4\\. JN不会因为其中一台的延迟而影响整体的延迟，而且也不会因为JN的数量增多而影响性能（因为NN向JN发送日志是并行的）  datanode的fencing: 确保只有一个NN能命令DN。HDFS-1972中详细描述了DN如何实现fencing\n 1\\. 每个NN改变状态的时候，向DN发送自己的状态和一个序列号\n 2\\. DN在运行过程中维护此序列号，当failover时，新的NN在返回DN心跳时会返回自己的active状态和一个更大的序列号。DN接收到这个返回则认为该NN为新的active  3\\. 如果这时原来的active NN恢复，返回给DN的心跳信息包含active状态和原来的序列号，这时DN就会拒绝这个NN的命令  客户端fencing：确保只有一个NN能响应客户端请求，让访问standby nn的客户端直接失败。在RPC层封装了一层，通过FailoverProxyProvider以重试的方式连接NN。通过若干次连接一个NN失败后尝试连接新的NN，对客户端的影响是重试的时候增加一定的延迟。客户端可以设置重试此时和时间  Hadoop提供了ZKFailoverController角色，部署在每个NameNode的节点上，作为一个deamon进程, 简称zkfc，\n ![3cfc6fc17a8e47509465442f6eaf7c14-image.png](//img.wqkenqing.ren/file/2017/7/3cfc6fc17a8e47509465442f6eaf7c14-image.png)\n\n   FailoverController主要包括三个组件:\n   1\\. HealthMonitor: 监控NameNode是否处于unavailable或unhealthy状态。当前通过RPC调用NN相应的方法完成\n   2\\. ActiveStandbyElector: 管理和监控自己在ZK中的状态\n   3\\. ZKFailoverController 它订阅HealthMonitor 和ActiveStandbyElector 的事件，并管理NameNode的状态  ZKFailoverController主要职责：      1\\. 健康监测：周期性的向它监控的NN发送健康探测命令，从而来确定某个NameNode是否处于健康状态，如果机器宕机，心跳失败，那么zkfc就会标记它处于一个不健康的状态\n   2\\. 会话管理：如 果NN是健康的，zkfc就会在zookeeper中保持一个打开的会话，如果NameNode同时还是Active状态的，那么zkfc还会在 Zookeeper中占有一个类型为短暂类型的znode，当这个NN挂掉时，这个znode将会被删除，然后备用的NN，将会得到这把锁，升级为主 NN，同时标记状态为Active\n   3\\. 当宕机的NN新启动时，它会再次注册zookeper，发现已经有znode锁了，便会自动变为Standby状态，如此往复循环，保证高可靠，需要注意，目前仅仅支持最多配置2个NN\n   4\\. master选举：如上所述，通过在zookeeper中维持一个短暂类型的znode，来实现抢占式的锁机制，从而判断那个NameNode为Active状态  **hadoop2.x Federation**：  单Active NN的架构使得HDFS在集群扩展性和性能上都有潜在的问题，当集群大到一定程度后，NN进程使用的内存可能会达到上百G，NN成为了性能的瓶颈  常用的估算公式为1G对应1百万个块，按缺省块大小计算的话，大概是64T (这个估算比例是有比较大的富裕的，其实，即使是每个文件只有一个块，所有元数据信息也不会有1KB/block)  为了解决这个问题,Hadoop 2.x提供了HDFS Federation, 示意图如下：\n   ![e5015b90accf4fe6a7729afe692ffa64-image.png](//img.wqkenqing.ren/file/2017/7/e5015b90accf4fe6a7729afe692ffa64-image.png)\n\n  多个NN共用一个集群里的存储资源，每个NN都可以单独对外提供服务  每个NN都会定义一个存储池，有单独的id，每个DN都为所有存储池提供存储  DN会按照存储池id向其对应的NN汇报块信息，同时，DN会向所有NN汇报本地存储可用资源情况  如果需要在客户端方便的访问若干个NN上的资源，可以使用客户端挂载表，把不同的目录映射到不同的NN，但NN上必须存在相应的目录  设计优势：  改动最小，向前兼容；现有的NN无需任何配置改动；如果现有的客户端只连某台NN的话  分离命名空间管理和块存储管理  客户端挂载表：通过路径自动对应NN、使Federation的配置改动对应用透明  (与上面ha方案中介绍的最多2个NN冲突？)  至此hadoop中的hdfs高可用特性，高容错的实现又有了更深理解，但针对hdfs还一层设计实现**机架感知**\n#### 机架感知\n分布式的集群通常包含非常多的机器，由于受到机架槽位和交换机网口的限制，通常大型的分布式集群都会跨好几个机架，由多个机架上的机器共同组成一个分布 式集群。机架内的机器之间的网络速度通常都会高于跨机架机器之间的网络速度，并且机架之间机器的网络通信通常受到上层交换机间网络带宽的限制。  具体到hadoop集群，由于hadoop的HDFS对数据文件的分布式存放是按照分块block存储，每个block会有多个副本(默认为3)，并且为了数据的安全和高效，所以hadoop默认对3个副本的存放策略为：\n在本地机器的hdfs目录下存储一个block  在另外一个rack的某个datanode上存储一个block\n在该机器的同一个rack下的某台机器上存储最后一个block\n这样的策略可以保证对该block所属文件的访问能够优先在本rack下找到，如果整个rack发生了异常，也可以在另外的rack上找到该block的副本。这样足够的高效，并且同时做到了数据的容错。\nhadoop对机架的感知并非是自适应的，亦即，hadoop集群分辨某台slave机器是属于哪个rack并非是只能的感知的，而是需要 hadoop的管理者人为的告知hadoop哪台机器属于哪个rack，这样在hadoop的namenode启动初始化时，会将这些机器与rack的对 应信息保存在内存中，用来作为对接下来所有的HDFS的写块操作分配datanode列表时（比如3个block对应三台datanode）的选择 datanode策略，做到hadoop allocate block的策略：尽量将三个副本分布到不同的rack。\n具体实现本文不在深究，在此附上网上的一些解决方式\n[机架感知实现1](http://www.cnblogs.com/cloudma/articles/hadoop-topology.html)  [机架感知实现2](http://blog.csdn.net/magicdreaming/article/details/7629773)\n\n---\n\n至此对hdfs的理解与总结告一段落，后续有了新的理解再进行补充\n\n\n\n","slug":"技术/hexo/oldblog/blog24","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33h003438pwhb8s4b7f","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h1 id=\"Hadoop总结-第一版—HDFS篇\"><a href=\"#Hadoop总结-第一版—HDFS篇\" class=\"headerlink\" title=\"Hadoop总结(第一版—HDFS篇)\"></a>Hadoop总结(第一版—HDFS篇)</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">## 什么是hadoop？</span><br><span class=\"line\">&gt;hadoop是稳定的，高容错的，可大规模布署的分布式文件，存储，并行编程框架。</span><br><span class=\"line\">&#96;&#96;&#96;本文默认是已经有hadoop使用经验,所以暂不涉及具体的hadoop生态的各组件的部署和调优细节，后续单开文章来总结。但在具体讲解时会涉及说参数配置会对相关组件参生影响</span><br></pre></td></tr></table></figure>\n<p>具体而言，hadoop核心组件内容有：<strong>hdfs</strong>、<strong>mapredcue</strong>。所以接下来的总结主要针对这两在核心组件展开</p>\n<h2 id=\"HDFS篇\"><a href=\"#HDFS篇\" class=\"headerlink\" title=\"HDFS篇\"></a>HDFS篇</h2><h3 id=\"什么是HDFS？\"><a href=\"#什么是HDFS？\" class=\"headerlink\" title=\"什么是HDFS？\"></a>什么是HDFS？</h3><p><strong>分布式文件系统</strong>：分布式文件系统是一种允许文件通过网络在多台主机上分享的 文件的系统，可让多机器上的多用户分享文件和存储空间<br>HDFS:（Hadoop Distribute File System）即hadoop分布式文件系统  主要用于适合运行在通用硬件上的分布式文件系统，特点是高度容错，适合布署在廉价服务器上，具有高吞吐量的数据访问等特点  1. 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。  2. 运行在廉价的机器上。  3. 适合大数据的处理。多大？多小？HDFS默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。  4. 适用于一次写入、多次查询的情况  5. 不支持并发写情况，小文件不合适。因为小文件也占用一个块，小文件越多（1000个1k文件）块越 多，NameNode压力越大。<br><code>hdfs得部署在linux系统上</code></p>\n<h3 id=\"HDFS的具体内容\"><a href=\"#HDFS的具体内容\" class=\"headerlink\" title=\"HDFS的具体内容\"></a>HDFS的具体内容</h3><p>文件、节点、数据块 HDFS主要是是围绕着这三个关键词设计的.</p>\n<h4 id=\"数据块\"><a href=\"#数据块\" class=\"headerlink\" title=\"数据块\"></a><strong>数据块</strong></h4><ul>\n<li>Block：在HDFS中，每个文件都是采用的分块的方式存储，每个block放在不同的dataNode节点上，每个block的标识是一个三元组(block id , numBytes,generationStamp),block id 具有唯一性，具体分配是由namenode节点设置，然后在datanode上建立对应的Block文件，同时建立对应的block meta文件(问题是block meta文件存放位，block size可以通过配置文件设置，所以修改block size会对以前持续化的数据有何影响?）</li>\n<li>Packet:是HDFS文件在DFSClient与DataNode之间通信的过程中文件的形式(一般一个Block对应多个Packet)</li>\n<li>Chunk:是通过程中具体传输的文件单位，发送过程以Packet的方式进行，但 一个packet包含多个Chunk,同时对于每个chunk进行checksum计算，生成checksum bytes。</li>\n</ul>\n<p><strong>Packet</strong><br>Packet的结构：数据包和heatbeat包  一个Packet数据包的组成结构主要分为 Packet Header 、PacketData  Packet Header 中又分为：<br><img src=\"//img.wqkenqing.ren.qiniudns.com/file/2017/7/10eed6bcc563463ea0b8cd5adf99adec-image.png\" alt=\"10eed6bcc563463ea0b8cd5adf99adec-image.png\"><br>Packet Data部分是一个Packet的实际数据部分。<br>主要内容有</p>\n<ul>\n<li>一个4字节校验</li>\n<li>Checksum</li>\n<li>Chunk部分，Chunk部分最大为512字节<br><img src=\"//img.wqkenqing.ren/file/2017/7/c68e61da1e8148ab99b43fe8e3f5408e-image.png\" alt=\"c68e61da1e8148ab99b43fe8e3f5408e-image.png\"><br>Packet创建过程：首先将字节流数据写入一个buffer缓冲区中，也就是从偏移量为25的位置（checksumStart）开 始写Packet数据Chunk的Checksum部分，从偏移量为533的位置（dataStart）开始写Packet数据的Chunk Data部分，直到一个Packet创建完成为止。</li>\n</ul>\n<p><code>注意：当写一个文件的最后一个Block的最后一个Packet时，如果一个Packet的大小未能达到最大长度，也就是上图对应的缓冲区 中，Checksum与Chunk Data之间还保留了一段未被写过的缓冲区位置，在发送这个Packet之前，会检查Chunksum与Chunk Data之间的缓冲区是否为空白缓冲区（gap），如果有则将Chunk Data部分向前移动，使得Chunk Data 1与Chunk Checksum N相邻，然后才会被发送到DataNode节点</code></p>\n<h4 id=\"hdsf架构-主要组成是节点）\"><a href=\"#hdsf架构-主要组成是节点）\" class=\"headerlink\" title=\"hdsf架构(主要组成是节点）\"></a>hdsf架构(主要组成是节点）</h4><p>主要的构成角色有：Client、NameNode、SecondayNameNode、DataNode<br><img src=\"//img.wqkenqing.ren/file/2017/7/5ba6f1ec6b9c4b0982dc2ce73ff9c444-image.png\" alt=\"5ba6f1ec6b9c4b0982dc2ce73ff9c444-image.png\"></p>\n<ul>\n<li>Client：系统使用者，调用HDFS API操作文件；与NN交互获取文件元数据;与DN交互进行数据读写, 注意：写数据时文件切分由Client完成</li>\n<li>Namenode：Master节点 （也称元数据节点）是系统唯一的管理者。负责元数据的管理(名称空间和数据块映射信息);配置副本策略；处理客户端请求</li>\n<li>Datanode：数据存储节点(也称Slave节点)，存储实际的数据；执行数据块的读写；汇报存储信息给NN</li>\n<li>Secondary NameNode：备胎，namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode</li>\n</ul>\n<p><code>careful:</code> 注意：在hadoop 2.x 版本，当启用 hdfs ha 时，将没有这一角色<br><strong>热冷备份说明</strong>：<br>热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作  冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失<br><strong>hdfs构架原则</strong></p>\n<ol>\n<li>元数据与数据分离：文件本身的属性（即元数据）与文件所持有的数据分离</li>\n<li>主/从架构：一个HDFS集群是由一个NameNode和一定数目的DataNode组成</li>\n<li>一次写入多次读取：HDFS中的文件在任何时间只能有一个Writer。当文件被创建，接着写入数据，最后，一旦文件被关闭，就不能再修改。</li>\n<li>移动计算比移动数据更划算：数据运算，越靠近数据，执行运算的性能就越好，由于hdfs数据分布在不同机器上，要让网络的消耗最低，并提高系统的吞吐量，最佳方式是将运算的执行移到离它要处理的数据更近的地方，而不是移动数据</li>\n</ol>\n<p><strong>针对第四条的解释</strong>：<br>在上文中交代到hdfs中的文件是以block的形式存放在集群中，所以一个文件可以是被切分成很多block存放在集群的机器中针对第四条，移动计算比移动数据更划算，是因为，从理论上讲，集群的计算能力是很方便扩展的，如服务器的硬件提升，或增加服务器等。但网络带宽等资源却很容易达到瓶颈或增加经济负担，所以将计算转移至每个block就近的机器进行计算，会比将所有的block合到一个机器上再进行计算要划算，所以，叫移动计算要比移动数据划算</p>\n<h5 id=\"NameNode\"><a href=\"#NameNode\" class=\"headerlink\" title=\"NameNode\"></a>NameNode</h5><p>NameNode是整个文件系统的管理与计算节点，是HDFS中最复杂的一个实体，它与管理着HDFS文件系统中最重要的两个关系</p>\n<ol>\n<li>HDFS文件系统中的文件目录树，以及文件的数据块索引，即每个文件对应的数据块列表  数据块和数据节点的对应关系，即某一块数据块保存在哪些数据节点的信息  <strong>第一个关系</strong>即目录树、元数据和数据块的索引信息持久化到物理存储中，具体的实现是保存在命名空间的镜像fsimage和编辑日志edits中，<strong>careful</strong>：在fsimage中，并没有记录每一个block对应到那几个Datanodes的对应表信息</li>\n<li><strong>第二个关系是</strong>在NameNode启动后，每个DataNode对本地的磁盘进行扫描，将本DataNode上保存的block信息上报至NameNode,Namenode在接收到每个Datanode的块信息汇报后，将接收到的块信息，以及其所在的Datanode信息等保存在内存中。HDFS就是通过这种块信息汇报的方式来完成 block -&gt; Datanodes list的对应表构建（<strong>careful</strong>）<br>类似于数据库中的检查点，为了避免edits日志过大，在Hadoop1.X 中，SecondaryNameNode会按照时间阈值（比如24小时）或者edits大小阈值（比如1G），周期性的将fsimage和edits的合 并，然后将最新的fsimage推送给NameNode。而在Hadoop2.X中，这个动作是由Standby NameNode来完成.<br>由此可看出，这两个文件一旦损坏或丢失，将导致整个HDFS文件系统不可用<br>在hadoop1.X为了保证这两种元数据文件的高可用性，一般的做法，将dfs.namenode.name.dir设置成以逗号分隔的多个目录，这多个目录至少不要在一块磁盘上，最好放在不同的机器上，比如：挂载一个共享文件系统<br>fsimage\\edits 是序列化后的文件，想要查看或编辑里面的内容，可通过 hdfs 提供的 oiv\\oev 命令，<br>命令: hdfs oiv （offline image viewer） 用于将fsimage文件的内容转储到指定文件中以便于阅读,，如文本文件、XML文件，该命令需要以下参数：</li>\n</ol>\n<p>-i (必填参数) –inputFile <arg> 输入FSImage文件<br>-o (必填参数) –outputFile <arg> 输出转换后的文件，如果存在，则会覆盖<br>-p (可选参数） –processor <arg> 将FSImage文件转换成哪种格式： (Ls|XML|FileDistribution).默认为Ls<br>示例：hdfs oiv -i /data1/hadoop/dfs/name/current/fsimage_0000000000019372521 -o /home/hadoop/fsimage.txt<br>命令：hdfs oev (offline edits viewer 离线edits查看器）的缩写， 该工具只操作文件因而并不需要hadoop集群处于运行状态。<br>示例: hdfs oev -i edits_0000000000000042778-0000000000000042779 -o edits.xml<br>支持的输出格式有binary（hadoop使用的二进制格式）、xml（在不使用参数p时的默认输出格式）和stats（输出edits文件的统计信息）<br>由此可以总结到：NameNode管理着DataNode，接收DataNode的注册、心跳、数据块提交等信息的上报，并且在心跳中发送数据块<strong>复制</strong>、<strong>删除</strong>、<strong>恢复</strong>等指令；同时，NameNode还为客户端对<strong>文件系统目录树的操作</strong>和对<strong>文件数据读写</strong>、对<strong>HDFS系统进行管理提供支持</strong><br>另 Namenode 启动后会进入一个称为<strong>安全模式</strong>的特殊状态。处于安全模式 的 Namenode 是不会进行数据块的复制的。 Namenode 从所有的 Datanode 接收心跳信号和块状态报告。 块状态报告包括了某个 Datanode 所有的数据 块列表。每个数据块都有一个指定的最小副本数。当 Namenode 检测确认某 个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全 (safely replicated) 的；在一定百分比（这个参数可配置）的数据块被 Namenode 检测确认是安全之后（加上一个额外的 30 秒等待时间）， Namenode 将退出安全模式状态。接下来它会确定还有哪些数据块的副本没 有达到指定数目，并将这些数据块复制到其他 Datanode 上。  ##### Secondary NameNode  在HA cluster中又称为standby node<br>主要作用：  1.如上文提到的合并fsimage和eits日志，将eits日志文件大小控制在一个限度下  大致流程如下  namenode 响应 Secondary namenode 请求，将 edit log 推送给 Secondary namenode ， 开始重新写一个新的 edit log  Secondary namenode 收到来自 namenode 的 fsimage 文件和 edit log  Secondary namenode 将 fsimage 加载到内存，应用 edit log ， 并生成一 个新的 fsimage 文件  Secondary namenode 将新的 fsimage 推送给 Namenode  Namenode 用新的 fsimage 取代旧的 fsimage ， 在 fstime 文件中记下检查 点发生的时<br><img src=\"//img.wqkenqing.ren/file/2017/7/aed541bebcc04fb79e2de3c504b7ee46-image.png\" alt=\"aed541bebcc04fb79e2de3c504b7ee46-image.png\"></p>\n<h4 id=\"HDFS写文件-1-x-默认的block大小是64M-2-X版本默认block的大小是-128M\"><a href=\"#HDFS写文件-1-x-默认的block大小是64M-2-X版本默认block的大小是-128M\" class=\"headerlink\" title=\"HDFS写文件  1.x 默认的block大小是64M 2.X版本默认block的大小是 128M \"></a>HDFS写文件  1.x 默认的block大小是64M 2.X版本默认block的大小是 128M <img src=\"//img.wqkenqing.ren/file/2017/7/d0c33b33bfbc41e8a493553395e52ef0-image.png\" alt=\"d0c33b33bfbc41e8a493553395e52ef0-image.png\"></h4><p>如上图所示  + Client预先设置的block参数切分FIle</p>\n<ul>\n<li>CLient向NameNode发送写数据请求，</li>\n<li>NameNode，记录block信息，并返回可用的DataNode(具体的返回规则参考下文)</li>\n<li>client向DataNode发送block1；发送过程是以流式写入具体流程是</li>\n</ul>\n<ol>\n<li><p>将64M的block1按64k的packet划分</p>\n</li>\n<li><p>然后将第一个packet发送给host2</p>\n</li>\n<li><p>host2接收完后，将第一个packet发送给host1，同时client想host2发送第二个packet</p>\n</li>\n<li><p>host1接收完第一个packet后，发送给host3，同时接收host2发来的第二个packet</p>\n</li>\n<li><p>以此类推，如图红线实线所示，直到将block1发送完毕</p>\n</li>\n<li><p>host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。</p>\n</li>\n<li><p>client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。</p>\n</li>\n<li><p>发送完block1后，再向host7，host8，host4发送block2  当客户端向 HDFS 文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副 本系数设置为 3 ，当本地临时文件累积到一个数据块的大小时，客户端会从 Namenode 获取一个 Datanode 列表用于存放副本。然后客户端开始向第一个 Datanode 传输数据，第一个 Datanode 一小部分一小部分 (4 KB) 地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中 第二个 Datanode 节点。第二个 Datanode 也是这样，一小部分一小部分地接收数据，写入本地 仓库，并同时传给第三个 Datanode 。最后，第三个 Datanode 接收数据并存储在本地。因此， Datanode 能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的 方式从前一个 Datanode 复制到下一个<br><img src=\"//img.wqkenqing.ren/file/2017/7/2c77aaad2f684820bf8a6b475b79d2f1-image.png\" alt=\"2c77aaad2f684820bf8a6b475b79d2f1-image.png\"></p>\n<p>写入的过程，按hdsf默认设置，1T文件，我们需要3T的存储，3T的网络流量  在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去  挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份</p>\n<h5 id=\"hdfs读文件\"><a href=\"#hdfs读文件\" class=\"headerlink\" title=\"hdfs读文件\"></a>hdfs读文件</h5><p><img src=\"//img.wqkenqing.ren/file/2017/7/8fb62c84f57a4203a2dbedf5f68920d9-image.png\" alt=\"8fb62c84f57a4203a2dbedf5f68920d9-image.png\"></p>\n<p>客户端通过调用FileSystem对象的open()方法来打开希望读取的文件，对于HDFS来说，这个对象时分布文件系统的一个实例；  DistributedFileSystem通过使用RPC来调用NameNode以确定文件起始块的位置，同一Block按照重复数会返回多个位置，这些位置按照Hadoop集群拓扑结构排序，距离客户端近的排在前面  前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流，客户端对这个输入流调用read()方法  存储着文件起始块的DataNode地址的DFSInputStream随即连接距离最近的DataNode，通过对数据流反复调用read()方法，将数据从DataNode传输到客户端  到达块的末端时，DFSInputStream会关闭与该DataNode的连接，然后寻找下一个块的最佳DataNode，这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流  一旦客户端完成读取，就对FSDataInputStream调用close()方法关闭文件读取</p>\n<h5 id=\"block持续化结构\"><a href=\"#block持续化结构\" class=\"headerlink\" title=\"block持续化结构\"></a>block持续化结构</h5><p>DataNode节点上一个Block持久化到磁盘上的物理存储结构，如下图所示：  <img src=\"//img.wqkenqing.ren/file/2017/7/df10f0ac586f4baebc2587842aec5675-image.png\" alt=\"df10f0ac586f4baebc2587842aec5675-image.png\"></p>\n<p>每个Block文件（如上图中blk_1084013198文件）都对应一个meta文件（如上图中blk_1084013198_10273532.meta文件），Block文件是一个一个Chunk的二进制数据（每个Chunk的大小是512字节），而meta文件是与每一个Chunk对应的Checksum数据，是序列化形式存储  —  至上我们大致了解了HDFS。正如上文提到的Hadoop的特点，高可能，高容错性。若光从上文提到的特性可能还不足以说明，如NameNode环节就提到了NameNode的重要作用，但若NameNode出现了故障，对整个机集会是毁灭性的打击，于是Hadoop也引入其它的一些手段来保存高可用，高容错。接下来我们就来探讨下</p>\n<h3 id=\"Hadoop-HA的引入\"><a href=\"#Hadoop-HA的引入\" class=\"headerlink\" title=\"Hadoop HA的引入\"></a>Hadoop HA的引入</h3><p>HA：High Available即高可用性集群，是保证业务连续性的有效解决方案，一般有两个或两个以上的节点，且分为活动节点及备用节点。  在HA具体实现方法不同的情况下，HA框架的流程是一致的, 不一致的就是如何存储和管理日志。在Active NN和Standby NN之间要有个共享的存储日志的地方，Active NN把EditLog写到这个共享的存储日志的地方，Standby NN去读取日志然后执行，这样Active和Standby NN内存中的HDFS元数据保持着同步。一旦发生主从切换Standby NN可以尽快接管Active NN的工作; 默认并未启用 hdfs ha。<br>SPOF方案回顾：</p>\n</li>\n<li><p>Secondary NameNode：它不是HA，它只是阶段性的合并edits和fsimage，以缩短集群启动的时间。当NN失效的时候，Secondary NN并无法立刻提供服务，Secondary NN甚至无法保证数据完整性：如果NN数据丢失的话，在上一次合并后的文件系统的改动会丢失</p>\n</li>\n<li><p>Backup NameNode (HADOOP-4539)：它在内存中复制了NN的当前状态，算是Warm Standby，可也就仅限于此，并没有failover等。它同样是阶段性的做checkpoint，也无法保证数据完整性<br>3. 手动把name.dir指向NFS（Network File System），这是安全的Cold Standby，可以保证元数据不丢失，但集群的恢复则完全靠手动<br>4. Facebook AvatarNode：Facebook有强大的运维做后盾，所以Avatarnode只是Hot Standby，并没有自动切换，当主NN失效的时候，需要管理员确认，然后手动把对外提供服务的虚拟IP映射到Standby NN，这样做的好处是确保不会发生脑裂的场景。其某些设计思想和Hadoop 2.0里的HA非常相似，从时间上来看，Hadoop 2.0应该是借鉴了Facebook的做法 <img src=\"//img.wqkenqing.ren/file/2017/7/32ec8d45379b4e4d99505b03b1b33e61-image.png\" alt=\"32ec8d45379b4e4d99505b03b1b33e61-image.png\"></p>\n</li>\n</ol>\n<p>5. PrimaryNN 与StandbyNN之间通过NFS来共享FsEdits、FsImage文件，这样主备NN之间就拥有了一致的目录树和block信息；而block的 位置信息，可以根据DN向两个NN上报的信息过程中构建起来。这样再辅以虚IP，可以较好达到主备NN快速热切的目的。但是显然，这里的NFS又引入了新的SPOF<br>6. 在主备NN共享元数据的过程中，也有方案通过主NN将FsEdits的内容通过与备NN建立的网络IO流，实时写入备NN，并且保证整个过程的原子性。这种方案，解决了NFS共享元数据引入的SPOF，但是主备NN之间的网络连接又会成为新的问题</p>\n<h4 id=\"hadoop2-X-ha-原理-hadoop2-x之后，Clouera提出了QJM-Qurom-Journal-Manager，这是一个基于Paxos算法实现的HDFS-HA方案，它给出了一种较好的解决思路和方案-示意图如下：\"><a href=\"#hadoop2-X-ha-原理-hadoop2-x之后，Clouera提出了QJM-Qurom-Journal-Manager，这是一个基于Paxos算法实现的HDFS-HA方案，它给出了一种较好的解决思路和方案-示意图如下：\" class=\"headerlink\" title=\"hadoop2.X ha 原理:  hadoop2.x之后，Clouera提出了QJM/Qurom Journal Manager，这是一个基于Paxos算法实现的HDFS HA方案，它给出了一种较好的解决思路和方案,示意图如下：\"></a>hadoop2.X ha 原理:  hadoop2.x之后，Clouera提出了QJM/Qurom Journal Manager，这是一个基于Paxos算法实现的HDFS HA方案，它给出了一种较好的解决思路和方案,示意图如下：</h4><p><img src=\"//img.wqkenqing.ren/file/2017/7/bc057fbb4b4047928bf06b7a43364335-image.png\" alt=\"bc057fbb4b4047928bf06b7a43364335-image.png\"></p>\n<ul>\n<li><p>基本原理就是用2N+1台 JN 存储EditLog，每次写数据操作有大多数（&gt;=N+1）返回成功时即认为该次写成功，数据不会丢失了。当然这个算法所能容忍的是最多有N台机器挂掉，如果多于N台挂掉，这个算法就失效了。这个原理是基于Paxos算法</p>\n</li>\n<li><p>在HA架构里面SecondaryNameNode这个冷备角色已经不存在了，为了保持standby NN时时的与主Active NN的元数据保持一致，他们之间交互通过一系列守护的轻量级进程JournalNode  + 任何修改操作在 Active NN上执行时，JN进程同时也会记录修改log到至少半数以上的JN中，这时 Standby NN 监测到JN 里面的同步log发生变化了会读取 JN 里面的修改log，然后同步到自己的的目录镜像树里面，<br><img src=\"//img.wqkenqing.ren/file/2017/7/608621d2ab52466db89e8a95d165f6e7-image.png\" alt=\"608621d2ab52466db89e8a95d165f6e7-image.png\"></p>\n<p>当发生故障时，Active的 NN 挂掉后，Standby NN 会在它成为Active NN 前，读取所有的JN里面的修改日志，这样就能高可靠的保证与挂掉的NN的目录镜像树一致，然后无缝的接替它的职责，维护来自客户端请求，从而达到一个高可用的目的  QJM方式来实现HA的主要优势：<br>1. 不需要配置额外的高共享存储，降低了复杂度和维护成本<br>2. 消除spof<br>3. 系统鲁棒性(Robust:健壮)的程度是可配置<br>4. JN不会因为其中一台的延迟而影响整体的延迟，而且也不会因为JN的数量增多而影响性能（因为NN向JN发送日志是并行的）  datanode的fencing: 确保只有一个NN能命令DN。HDFS-1972中详细描述了DN如何实现fencing<br>1. 每个NN改变状态的时候，向DN发送自己的状态和一个序列号<br>2. DN在运行过程中维护此序列号，当failover时，新的NN在返回DN心跳时会返回自己的active状态和一个更大的序列号。DN接收到这个返回则认为该NN为新的active  3. 如果这时原来的active NN恢复，返回给DN的心跳信息包含active状态和原来的序列号，这时DN就会拒绝这个NN的命令  客户端fencing：确保只有一个NN能响应客户端请求，让访问standby nn的客户端直接失败。在RPC层封装了一层，通过FailoverProxyProvider以重试的方式连接NN。通过若干次连接一个NN失败后尝试连接新的NN，对客户端的影响是重试的时候增加一定的延迟。客户端可以设置重试此时和时间  Hadoop提供了ZKFailoverController角色，部署在每个NameNode的节点上，作为一个deamon进程, 简称zkfc，<br><img src=\"//img.wqkenqing.ren/file/2017/7/3cfc6fc17a8e47509465442f6eaf7c14-image.png\" alt=\"3cfc6fc17a8e47509465442f6eaf7c14-image.png\"></p>\n<p>FailoverController主要包括三个组件:<br>1. HealthMonitor: 监控NameNode是否处于unavailable或unhealthy状态。当前通过RPC调用NN相应的方法完成<br>2. ActiveStandbyElector: 管理和监控自己在ZK中的状态<br>3. ZKFailoverController 它订阅HealthMonitor 和ActiveStandbyElector 的事件，并管理NameNode的状态  ZKFailoverController主要职责：      1. 健康监测：周期性的向它监控的NN发送健康探测命令，从而来确定某个NameNode是否处于健康状态，如果机器宕机，心跳失败，那么zkfc就会标记它处于一个不健康的状态<br>2. 会话管理：如 果NN是健康的，zkfc就会在zookeeper中保持一个打开的会话，如果NameNode同时还是Active状态的，那么zkfc还会在 Zookeeper中占有一个类型为短暂类型的znode，当这个NN挂掉时，这个znode将会被删除，然后备用的NN，将会得到这把锁，升级为主 NN，同时标记状态为Active<br>3. 当宕机的NN新启动时，它会再次注册zookeper，发现已经有znode锁了，便会自动变为Standby状态，如此往复循环，保证高可靠，需要注意，目前仅仅支持最多配置2个NN<br>4. master选举：如上所述，通过在zookeeper中维持一个短暂类型的znode，来实现抢占式的锁机制，从而判断那个NameNode为Active状态  <strong>hadoop2.x Federation</strong>：  单Active NN的架构使得HDFS在集群扩展性和性能上都有潜在的问题，当集群大到一定程度后，NN进程使用的内存可能会达到上百G，NN成为了性能的瓶颈  常用的估算公式为1G对应1百万个块，按缺省块大小计算的话，大概是64T (这个估算比例是有比较大的富裕的，其实，即使是每个文件只有一个块，所有元数据信息也不会有1KB/block)  为了解决这个问题,Hadoop 2.x提供了HDFS Federation, 示意图如下：<br><img src=\"//img.wqkenqing.ren/file/2017/7/e5015b90accf4fe6a7729afe692ffa64-image.png\" alt=\"e5015b90accf4fe6a7729afe692ffa64-image.png\"></p>\n<p>多个NN共用一个集群里的存储资源，每个NN都可以单独对外提供服务  每个NN都会定义一个存储池，有单独的id，每个DN都为所有存储池提供存储  DN会按照存储池id向其对应的NN汇报块信息，同时，DN会向所有NN汇报本地存储可用资源情况  如果需要在客户端方便的访问若干个NN上的资源，可以使用客户端挂载表，把不同的目录映射到不同的NN，但NN上必须存在相应的目录  设计优势：  改动最小，向前兼容；现有的NN无需任何配置改动；如果现有的客户端只连某台NN的话  分离命名空间管理和块存储管理  客户端挂载表：通过路径自动对应NN、使Federation的配置改动对应用透明  (与上面ha方案中介绍的最多2个NN冲突？)  至此hadoop中的hdfs高可用特性，高容错的实现又有了更深理解，但针对hdfs还一层设计实现<strong>机架感知</strong></p>\n<h4 id=\"机架感知\"><a href=\"#机架感知\" class=\"headerlink\" title=\"机架感知\"></a>机架感知</h4><p>分布式的集群通常包含非常多的机器，由于受到机架槽位和交换机网口的限制，通常大型的分布式集群都会跨好几个机架，由多个机架上的机器共同组成一个分布 式集群。机架内的机器之间的网络速度通常都会高于跨机架机器之间的网络速度，并且机架之间机器的网络通信通常受到上层交换机间网络带宽的限制。  具体到hadoop集群，由于hadoop的HDFS对数据文件的分布式存放是按照分块block存储，每个block会有多个副本(默认为3)，并且为了数据的安全和高效，所以hadoop默认对3个副本的存放策略为：<br>在本地机器的hdfs目录下存储一个block  在另外一个rack的某个datanode上存储一个block<br>在该机器的同一个rack下的某台机器上存储最后一个block<br>这样的策略可以保证对该block所属文件的访问能够优先在本rack下找到，如果整个rack发生了异常，也可以在另外的rack上找到该block的副本。这样足够的高效，并且同时做到了数据的容错。<br>hadoop对机架的感知并非是自适应的，亦即，hadoop集群分辨某台slave机器是属于哪个rack并非是只能的感知的，而是需要 hadoop的管理者人为的告知hadoop哪台机器属于哪个rack，这样在hadoop的namenode启动初始化时，会将这些机器与rack的对 应信息保存在内存中，用来作为对接下来所有的HDFS的写块操作分配datanode列表时（比如3个block对应三台datanode）的选择 datanode策略，做到hadoop allocate block的策略：尽量将三个副本分布到不同的rack。<br>具体实现本文不在深究，在此附上网上的一些解决方式<br><a href=\"http://www.cnblogs.com/cloudma/articles/hadoop-topology.html\" target=\"_blank\" rel=\"noopener\">机架感知实现1</a>  <a href=\"http://blog.csdn.net/magicdreaming/article/details/7629773\" target=\"_blank\" rel=\"noopener\">机架感知实现2</a></p>\n</li>\n</ul>\n<hr>\n<p>至此对hdfs的理解与总结告一段落，后续有了新的理解再进行补充</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h1 id=\"Hadoop总结-第一版—HDFS篇\"><a href=\"#Hadoop总结-第一版—HDFS篇\" class=\"headerlink\" title=\"Hadoop总结(第一版—HDFS篇)\"></a>Hadoop总结(第一版—HDFS篇)</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">## 什么是hadoop？</span><br><span class=\"line\">&gt;hadoop是稳定的，高容错的，可大规模布署的分布式文件，存储，并行编程框架。</span><br><span class=\"line\">&#96;&#96;&#96;本文默认是已经有hadoop使用经验,所以暂不涉及具体的hadoop生态的各组件的部署和调优细节，后续单开文章来总结。但在具体讲解时会涉及说参数配置会对相关组件参生影响</span><br></pre></td></tr></table></figure>\n<p>具体而言，hadoop核心组件内容有：<strong>hdfs</strong>、<strong>mapredcue</strong>。所以接下来的总结主要针对这两在核心组件展开</p>\n<h2 id=\"HDFS篇\"><a href=\"#HDFS篇\" class=\"headerlink\" title=\"HDFS篇\"></a>HDFS篇</h2><h3 id=\"什么是HDFS？\"><a href=\"#什么是HDFS？\" class=\"headerlink\" title=\"什么是HDFS？\"></a>什么是HDFS？</h3><p><strong>分布式文件系统</strong>：分布式文件系统是一种允许文件通过网络在多台主机上分享的 文件的系统，可让多机器上的多用户分享文件和存储空间<br>HDFS:（Hadoop Distribute File System）即hadoop分布式文件系统  主要用于适合运行在通用硬件上的分布式文件系统，特点是高度容错，适合布署在廉价服务器上，具有高吞吐量的数据访问等特点  1. 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。  2. 运行在廉价的机器上。  3. 适合大数据的处理。多大？多小？HDFS默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。  4. 适用于一次写入、多次查询的情况  5. 不支持并发写情况，小文件不合适。因为小文件也占用一个块，小文件越多（1000个1k文件）块越 多，NameNode压力越大。<br><code>hdfs得部署在linux系统上</code></p>\n<h3 id=\"HDFS的具体内容\"><a href=\"#HDFS的具体内容\" class=\"headerlink\" title=\"HDFS的具体内容\"></a>HDFS的具体内容</h3><p>文件、节点、数据块 HDFS主要是是围绕着这三个关键词设计的.</p>\n<h4 id=\"数据块\"><a href=\"#数据块\" class=\"headerlink\" title=\"数据块\"></a><strong>数据块</strong></h4><ul>\n<li>Block：在HDFS中，每个文件都是采用的分块的方式存储，每个block放在不同的dataNode节点上，每个block的标识是一个三元组(block id , numBytes,generationStamp),block id 具有唯一性，具体分配是由namenode节点设置，然后在datanode上建立对应的Block文件，同时建立对应的block meta文件(问题是block meta文件存放位，block size可以通过配置文件设置，所以修改block size会对以前持续化的数据有何影响?）</li>\n<li>Packet:是HDFS文件在DFSClient与DataNode之间通信的过程中文件的形式(一般一个Block对应多个Packet)</li>\n<li>Chunk:是通过程中具体传输的文件单位，发送过程以Packet的方式进行，但 一个packet包含多个Chunk,同时对于每个chunk进行checksum计算，生成checksum bytes。</li>\n</ul>\n<p><strong>Packet</strong><br>Packet的结构：数据包和heatbeat包  一个Packet数据包的组成结构主要分为 Packet Header 、PacketData  Packet Header 中又分为：<br><img src=\"//img.wqkenqing.ren.qiniudns.com/file/2017/7/10eed6bcc563463ea0b8cd5adf99adec-image.png\" alt=\"10eed6bcc563463ea0b8cd5adf99adec-image.png\"><br>Packet Data部分是一个Packet的实际数据部分。<br>主要内容有</p>\n<ul>\n<li>一个4字节校验</li>\n<li>Checksum</li>\n<li>Chunk部分，Chunk部分最大为512字节<br><img src=\"//img.wqkenqing.ren/file/2017/7/c68e61da1e8148ab99b43fe8e3f5408e-image.png\" alt=\"c68e61da1e8148ab99b43fe8e3f5408e-image.png\"><br>Packet创建过程：首先将字节流数据写入一个buffer缓冲区中，也就是从偏移量为25的位置（checksumStart）开 始写Packet数据Chunk的Checksum部分，从偏移量为533的位置（dataStart）开始写Packet数据的Chunk Data部分，直到一个Packet创建完成为止。</li>\n</ul>\n<p><code>注意：当写一个文件的最后一个Block的最后一个Packet时，如果一个Packet的大小未能达到最大长度，也就是上图对应的缓冲区 中，Checksum与Chunk Data之间还保留了一段未被写过的缓冲区位置，在发送这个Packet之前，会检查Chunksum与Chunk Data之间的缓冲区是否为空白缓冲区（gap），如果有则将Chunk Data部分向前移动，使得Chunk Data 1与Chunk Checksum N相邻，然后才会被发送到DataNode节点</code></p>\n<h4 id=\"hdsf架构-主要组成是节点）\"><a href=\"#hdsf架构-主要组成是节点）\" class=\"headerlink\" title=\"hdsf架构(主要组成是节点）\"></a>hdsf架构(主要组成是节点）</h4><p>主要的构成角色有：Client、NameNode、SecondayNameNode、DataNode<br><img src=\"//img.wqkenqing.ren/file/2017/7/5ba6f1ec6b9c4b0982dc2ce73ff9c444-image.png\" alt=\"5ba6f1ec6b9c4b0982dc2ce73ff9c444-image.png\"></p>\n<ul>\n<li>Client：系统使用者，调用HDFS API操作文件；与NN交互获取文件元数据;与DN交互进行数据读写, 注意：写数据时文件切分由Client完成</li>\n<li>Namenode：Master节点 （也称元数据节点）是系统唯一的管理者。负责元数据的管理(名称空间和数据块映射信息);配置副本策略；处理客户端请求</li>\n<li>Datanode：数据存储节点(也称Slave节点)，存储实际的数据；执行数据块的读写；汇报存储信息给NN</li>\n<li>Secondary NameNode：备胎，namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode</li>\n</ul>\n<p><code>careful:</code> 注意：在hadoop 2.x 版本，当启用 hdfs ha 时，将没有这一角色<br><strong>热冷备份说明</strong>：<br>热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作  冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失<br><strong>hdfs构架原则</strong></p>\n<ol>\n<li>元数据与数据分离：文件本身的属性（即元数据）与文件所持有的数据分离</li>\n<li>主/从架构：一个HDFS集群是由一个NameNode和一定数目的DataNode组成</li>\n<li>一次写入多次读取：HDFS中的文件在任何时间只能有一个Writer。当文件被创建，接着写入数据，最后，一旦文件被关闭，就不能再修改。</li>\n<li>移动计算比移动数据更划算：数据运算，越靠近数据，执行运算的性能就越好，由于hdfs数据分布在不同机器上，要让网络的消耗最低，并提高系统的吞吐量，最佳方式是将运算的执行移到离它要处理的数据更近的地方，而不是移动数据</li>\n</ol>\n<p><strong>针对第四条的解释</strong>：<br>在上文中交代到hdfs中的文件是以block的形式存放在集群中，所以一个文件可以是被切分成很多block存放在集群的机器中针对第四条，移动计算比移动数据更划算，是因为，从理论上讲，集群的计算能力是很方便扩展的，如服务器的硬件提升，或增加服务器等。但网络带宽等资源却很容易达到瓶颈或增加经济负担，所以将计算转移至每个block就近的机器进行计算，会比将所有的block合到一个机器上再进行计算要划算，所以，叫移动计算要比移动数据划算</p>\n<h5 id=\"NameNode\"><a href=\"#NameNode\" class=\"headerlink\" title=\"NameNode\"></a>NameNode</h5><p>NameNode是整个文件系统的管理与计算节点，是HDFS中最复杂的一个实体，它与管理着HDFS文件系统中最重要的两个关系</p>\n<ol>\n<li>HDFS文件系统中的文件目录树，以及文件的数据块索引，即每个文件对应的数据块列表  数据块和数据节点的对应关系，即某一块数据块保存在哪些数据节点的信息  <strong>第一个关系</strong>即目录树、元数据和数据块的索引信息持久化到物理存储中，具体的实现是保存在命名空间的镜像fsimage和编辑日志edits中，<strong>careful</strong>：在fsimage中，并没有记录每一个block对应到那几个Datanodes的对应表信息</li>\n<li><strong>第二个关系是</strong>在NameNode启动后，每个DataNode对本地的磁盘进行扫描，将本DataNode上保存的block信息上报至NameNode,Namenode在接收到每个Datanode的块信息汇报后，将接收到的块信息，以及其所在的Datanode信息等保存在内存中。HDFS就是通过这种块信息汇报的方式来完成 block -&gt; Datanodes list的对应表构建（<strong>careful</strong>）<br>类似于数据库中的检查点，为了避免edits日志过大，在Hadoop1.X 中，SecondaryNameNode会按照时间阈值（比如24小时）或者edits大小阈值（比如1G），周期性的将fsimage和edits的合 并，然后将最新的fsimage推送给NameNode。而在Hadoop2.X中，这个动作是由Standby NameNode来完成.<br>由此可看出，这两个文件一旦损坏或丢失，将导致整个HDFS文件系统不可用<br>在hadoop1.X为了保证这两种元数据文件的高可用性，一般的做法，将dfs.namenode.name.dir设置成以逗号分隔的多个目录，这多个目录至少不要在一块磁盘上，最好放在不同的机器上，比如：挂载一个共享文件系统<br>fsimage\\edits 是序列化后的文件，想要查看或编辑里面的内容，可通过 hdfs 提供的 oiv\\oev 命令，<br>命令: hdfs oiv （offline image viewer） 用于将fsimage文件的内容转储到指定文件中以便于阅读,，如文本文件、XML文件，该命令需要以下参数：</li>\n</ol>\n<p>-i (必填参数) –inputFile <arg> 输入FSImage文件<br>-o (必填参数) –outputFile <arg> 输出转换后的文件，如果存在，则会覆盖<br>-p (可选参数） –processor <arg> 将FSImage文件转换成哪种格式： (Ls|XML|FileDistribution).默认为Ls<br>示例：hdfs oiv -i /data1/hadoop/dfs/name/current/fsimage_0000000000019372521 -o /home/hadoop/fsimage.txt<br>命令：hdfs oev (offline edits viewer 离线edits查看器）的缩写， 该工具只操作文件因而并不需要hadoop集群处于运行状态。<br>示例: hdfs oev -i edits_0000000000000042778-0000000000000042779 -o edits.xml<br>支持的输出格式有binary（hadoop使用的二进制格式）、xml（在不使用参数p时的默认输出格式）和stats（输出edits文件的统计信息）<br>由此可以总结到：NameNode管理着DataNode，接收DataNode的注册、心跳、数据块提交等信息的上报，并且在心跳中发送数据块<strong>复制</strong>、<strong>删除</strong>、<strong>恢复</strong>等指令；同时，NameNode还为客户端对<strong>文件系统目录树的操作</strong>和对<strong>文件数据读写</strong>、对<strong>HDFS系统进行管理提供支持</strong><br>另 Namenode 启动后会进入一个称为<strong>安全模式</strong>的特殊状态。处于安全模式 的 Namenode 是不会进行数据块的复制的。 Namenode 从所有的 Datanode 接收心跳信号和块状态报告。 块状态报告包括了某个 Datanode 所有的数据 块列表。每个数据块都有一个指定的最小副本数。当 Namenode 检测确认某 个数据块的副本数目达到这个最小值，那么该数据块就会被认为是副本安全 (safely replicated) 的；在一定百分比（这个参数可配置）的数据块被 Namenode 检测确认是安全之后（加上一个额外的 30 秒等待时间）， Namenode 将退出安全模式状态。接下来它会确定还有哪些数据块的副本没 有达到指定数目，并将这些数据块复制到其他 Datanode 上。  ##### Secondary NameNode  在HA cluster中又称为standby node<br>主要作用：  1.如上文提到的合并fsimage和eits日志，将eits日志文件大小控制在一个限度下  大致流程如下  namenode 响应 Secondary namenode 请求，将 edit log 推送给 Secondary namenode ， 开始重新写一个新的 edit log  Secondary namenode 收到来自 namenode 的 fsimage 文件和 edit log  Secondary namenode 将 fsimage 加载到内存，应用 edit log ， 并生成一 个新的 fsimage 文件  Secondary namenode 将新的 fsimage 推送给 Namenode  Namenode 用新的 fsimage 取代旧的 fsimage ， 在 fstime 文件中记下检查 点发生的时<br><img src=\"//img.wqkenqing.ren/file/2017/7/aed541bebcc04fb79e2de3c504b7ee46-image.png\" alt=\"aed541bebcc04fb79e2de3c504b7ee46-image.png\"></p>\n<h4 id=\"HDFS写文件-1-x-默认的block大小是64M-2-X版本默认block的大小是-128M\"><a href=\"#HDFS写文件-1-x-默认的block大小是64M-2-X版本默认block的大小是-128M\" class=\"headerlink\" title=\"HDFS写文件  1.x 默认的block大小是64M 2.X版本默认block的大小是 128M \"></a>HDFS写文件  1.x 默认的block大小是64M 2.X版本默认block的大小是 128M <img src=\"//img.wqkenqing.ren/file/2017/7/d0c33b33bfbc41e8a493553395e52ef0-image.png\" alt=\"d0c33b33bfbc41e8a493553395e52ef0-image.png\"></h4><p>如上图所示  + Client预先设置的block参数切分FIle</p>\n<ul>\n<li>CLient向NameNode发送写数据请求，</li>\n<li>NameNode，记录block信息，并返回可用的DataNode(具体的返回规则参考下文)</li>\n<li>client向DataNode发送block1；发送过程是以流式写入具体流程是</li>\n</ul>\n<ol>\n<li><p>将64M的block1按64k的packet划分</p>\n</li>\n<li><p>然后将第一个packet发送给host2</p>\n</li>\n<li><p>host2接收完后，将第一个packet发送给host1，同时client想host2发送第二个packet</p>\n</li>\n<li><p>host1接收完第一个packet后，发送给host3，同时接收host2发来的第二个packet</p>\n</li>\n<li><p>以此类推，如图红线实线所示，直到将block1发送完毕</p>\n</li>\n<li><p>host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。</p>\n</li>\n<li><p>client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。</p>\n</li>\n<li><p>发送完block1后，再向host7，host8，host4发送block2  当客户端向 HDFS 文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副 本系数设置为 3 ，当本地临时文件累积到一个数据块的大小时，客户端会从 Namenode 获取一个 Datanode 列表用于存放副本。然后客户端开始向第一个 Datanode 传输数据，第一个 Datanode 一小部分一小部分 (4 KB) 地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中 第二个 Datanode 节点。第二个 Datanode 也是这样，一小部分一小部分地接收数据，写入本地 仓库，并同时传给第三个 Datanode 。最后，第三个 Datanode 接收数据并存储在本地。因此， Datanode 能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的 方式从前一个 Datanode 复制到下一个<br><img src=\"//img.wqkenqing.ren/file/2017/7/2c77aaad2f684820bf8a6b475b79d2f1-image.png\" alt=\"2c77aaad2f684820bf8a6b475b79d2f1-image.png\"></p>\n<p>写入的过程，按hdsf默认设置，1T文件，我们需要3T的存储，3T的网络流量  在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去  挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份</p>\n<h5 id=\"hdfs读文件\"><a href=\"#hdfs读文件\" class=\"headerlink\" title=\"hdfs读文件\"></a>hdfs读文件</h5><p><img src=\"//img.wqkenqing.ren/file/2017/7/8fb62c84f57a4203a2dbedf5f68920d9-image.png\" alt=\"8fb62c84f57a4203a2dbedf5f68920d9-image.png\"></p>\n<p>客户端通过调用FileSystem对象的open()方法来打开希望读取的文件，对于HDFS来说，这个对象时分布文件系统的一个实例；  DistributedFileSystem通过使用RPC来调用NameNode以确定文件起始块的位置，同一Block按照重复数会返回多个位置，这些位置按照Hadoop集群拓扑结构排序，距离客户端近的排在前面  前两步会返回一个FSDataInputStream对象，该对象会被封装成DFSInputStream对象，DFSInputStream可以方便的管理datanode和namenode数据流，客户端对这个输入流调用read()方法  存储着文件起始块的DataNode地址的DFSInputStream随即连接距离最近的DataNode，通过对数据流反复调用read()方法，将数据从DataNode传输到客户端  到达块的末端时，DFSInputStream会关闭与该DataNode的连接，然后寻找下一个块的最佳DataNode，这些操作对客户端来说是透明的，客户端的角度看来只是读一个持续不断的流  一旦客户端完成读取，就对FSDataInputStream调用close()方法关闭文件读取</p>\n<h5 id=\"block持续化结构\"><a href=\"#block持续化结构\" class=\"headerlink\" title=\"block持续化结构\"></a>block持续化结构</h5><p>DataNode节点上一个Block持久化到磁盘上的物理存储结构，如下图所示：  <img src=\"//img.wqkenqing.ren/file/2017/7/df10f0ac586f4baebc2587842aec5675-image.png\" alt=\"df10f0ac586f4baebc2587842aec5675-image.png\"></p>\n<p>每个Block文件（如上图中blk_1084013198文件）都对应一个meta文件（如上图中blk_1084013198_10273532.meta文件），Block文件是一个一个Chunk的二进制数据（每个Chunk的大小是512字节），而meta文件是与每一个Chunk对应的Checksum数据，是序列化形式存储  —  至上我们大致了解了HDFS。正如上文提到的Hadoop的特点，高可能，高容错性。若光从上文提到的特性可能还不足以说明，如NameNode环节就提到了NameNode的重要作用，但若NameNode出现了故障，对整个机集会是毁灭性的打击，于是Hadoop也引入其它的一些手段来保存高可用，高容错。接下来我们就来探讨下</p>\n<h3 id=\"Hadoop-HA的引入\"><a href=\"#Hadoop-HA的引入\" class=\"headerlink\" title=\"Hadoop HA的引入\"></a>Hadoop HA的引入</h3><p>HA：High Available即高可用性集群，是保证业务连续性的有效解决方案，一般有两个或两个以上的节点，且分为活动节点及备用节点。  在HA具体实现方法不同的情况下，HA框架的流程是一致的, 不一致的就是如何存储和管理日志。在Active NN和Standby NN之间要有个共享的存储日志的地方，Active NN把EditLog写到这个共享的存储日志的地方，Standby NN去读取日志然后执行，这样Active和Standby NN内存中的HDFS元数据保持着同步。一旦发生主从切换Standby NN可以尽快接管Active NN的工作; 默认并未启用 hdfs ha。<br>SPOF方案回顾：</p>\n</li>\n<li><p>Secondary NameNode：它不是HA，它只是阶段性的合并edits和fsimage，以缩短集群启动的时间。当NN失效的时候，Secondary NN并无法立刻提供服务，Secondary NN甚至无法保证数据完整性：如果NN数据丢失的话，在上一次合并后的文件系统的改动会丢失</p>\n</li>\n<li><p>Backup NameNode (HADOOP-4539)：它在内存中复制了NN的当前状态，算是Warm Standby，可也就仅限于此，并没有failover等。它同样是阶段性的做checkpoint，也无法保证数据完整性<br>3. 手动把name.dir指向NFS（Network File System），这是安全的Cold Standby，可以保证元数据不丢失，但集群的恢复则完全靠手动<br>4. Facebook AvatarNode：Facebook有强大的运维做后盾，所以Avatarnode只是Hot Standby，并没有自动切换，当主NN失效的时候，需要管理员确认，然后手动把对外提供服务的虚拟IP映射到Standby NN，这样做的好处是确保不会发生脑裂的场景。其某些设计思想和Hadoop 2.0里的HA非常相似，从时间上来看，Hadoop 2.0应该是借鉴了Facebook的做法 <img src=\"//img.wqkenqing.ren/file/2017/7/32ec8d45379b4e4d99505b03b1b33e61-image.png\" alt=\"32ec8d45379b4e4d99505b03b1b33e61-image.png\"></p>\n</li>\n</ol>\n<p>5. PrimaryNN 与StandbyNN之间通过NFS来共享FsEdits、FsImage文件，这样主备NN之间就拥有了一致的目录树和block信息；而block的 位置信息，可以根据DN向两个NN上报的信息过程中构建起来。这样再辅以虚IP，可以较好达到主备NN快速热切的目的。但是显然，这里的NFS又引入了新的SPOF<br>6. 在主备NN共享元数据的过程中，也有方案通过主NN将FsEdits的内容通过与备NN建立的网络IO流，实时写入备NN，并且保证整个过程的原子性。这种方案，解决了NFS共享元数据引入的SPOF，但是主备NN之间的网络连接又会成为新的问题</p>\n<h4 id=\"hadoop2-X-ha-原理-hadoop2-x之后，Clouera提出了QJM-Qurom-Journal-Manager，这是一个基于Paxos算法实现的HDFS-HA方案，它给出了一种较好的解决思路和方案-示意图如下：\"><a href=\"#hadoop2-X-ha-原理-hadoop2-x之后，Clouera提出了QJM-Qurom-Journal-Manager，这是一个基于Paxos算法实现的HDFS-HA方案，它给出了一种较好的解决思路和方案-示意图如下：\" class=\"headerlink\" title=\"hadoop2.X ha 原理:  hadoop2.x之后，Clouera提出了QJM/Qurom Journal Manager，这是一个基于Paxos算法实现的HDFS HA方案，它给出了一种较好的解决思路和方案,示意图如下：\"></a>hadoop2.X ha 原理:  hadoop2.x之后，Clouera提出了QJM/Qurom Journal Manager，这是一个基于Paxos算法实现的HDFS HA方案，它给出了一种较好的解决思路和方案,示意图如下：</h4><p><img src=\"//img.wqkenqing.ren/file/2017/7/bc057fbb4b4047928bf06b7a43364335-image.png\" alt=\"bc057fbb4b4047928bf06b7a43364335-image.png\"></p>\n<ul>\n<li><p>基本原理就是用2N+1台 JN 存储EditLog，每次写数据操作有大多数（&gt;=N+1）返回成功时即认为该次写成功，数据不会丢失了。当然这个算法所能容忍的是最多有N台机器挂掉，如果多于N台挂掉，这个算法就失效了。这个原理是基于Paxos算法</p>\n</li>\n<li><p>在HA架构里面SecondaryNameNode这个冷备角色已经不存在了，为了保持standby NN时时的与主Active NN的元数据保持一致，他们之间交互通过一系列守护的轻量级进程JournalNode  + 任何修改操作在 Active NN上执行时，JN进程同时也会记录修改log到至少半数以上的JN中，这时 Standby NN 监测到JN 里面的同步log发生变化了会读取 JN 里面的修改log，然后同步到自己的的目录镜像树里面，<br><img src=\"//img.wqkenqing.ren/file/2017/7/608621d2ab52466db89e8a95d165f6e7-image.png\" alt=\"608621d2ab52466db89e8a95d165f6e7-image.png\"></p>\n<p>当发生故障时，Active的 NN 挂掉后，Standby NN 会在它成为Active NN 前，读取所有的JN里面的修改日志，这样就能高可靠的保证与挂掉的NN的目录镜像树一致，然后无缝的接替它的职责，维护来自客户端请求，从而达到一个高可用的目的  QJM方式来实现HA的主要优势：<br>1. 不需要配置额外的高共享存储，降低了复杂度和维护成本<br>2. 消除spof<br>3. 系统鲁棒性(Robust:健壮)的程度是可配置<br>4. JN不会因为其中一台的延迟而影响整体的延迟，而且也不会因为JN的数量增多而影响性能（因为NN向JN发送日志是并行的）  datanode的fencing: 确保只有一个NN能命令DN。HDFS-1972中详细描述了DN如何实现fencing<br>1. 每个NN改变状态的时候，向DN发送自己的状态和一个序列号<br>2. DN在运行过程中维护此序列号，当failover时，新的NN在返回DN心跳时会返回自己的active状态和一个更大的序列号。DN接收到这个返回则认为该NN为新的active  3. 如果这时原来的active NN恢复，返回给DN的心跳信息包含active状态和原来的序列号，这时DN就会拒绝这个NN的命令  客户端fencing：确保只有一个NN能响应客户端请求，让访问standby nn的客户端直接失败。在RPC层封装了一层，通过FailoverProxyProvider以重试的方式连接NN。通过若干次连接一个NN失败后尝试连接新的NN，对客户端的影响是重试的时候增加一定的延迟。客户端可以设置重试此时和时间  Hadoop提供了ZKFailoverController角色，部署在每个NameNode的节点上，作为一个deamon进程, 简称zkfc，<br><img src=\"//img.wqkenqing.ren/file/2017/7/3cfc6fc17a8e47509465442f6eaf7c14-image.png\" alt=\"3cfc6fc17a8e47509465442f6eaf7c14-image.png\"></p>\n<p>FailoverController主要包括三个组件:<br>1. HealthMonitor: 监控NameNode是否处于unavailable或unhealthy状态。当前通过RPC调用NN相应的方法完成<br>2. ActiveStandbyElector: 管理和监控自己在ZK中的状态<br>3. ZKFailoverController 它订阅HealthMonitor 和ActiveStandbyElector 的事件，并管理NameNode的状态  ZKFailoverController主要职责：      1. 健康监测：周期性的向它监控的NN发送健康探测命令，从而来确定某个NameNode是否处于健康状态，如果机器宕机，心跳失败，那么zkfc就会标记它处于一个不健康的状态<br>2. 会话管理：如 果NN是健康的，zkfc就会在zookeeper中保持一个打开的会话，如果NameNode同时还是Active状态的，那么zkfc还会在 Zookeeper中占有一个类型为短暂类型的znode，当这个NN挂掉时，这个znode将会被删除，然后备用的NN，将会得到这把锁，升级为主 NN，同时标记状态为Active<br>3. 当宕机的NN新启动时，它会再次注册zookeper，发现已经有znode锁了，便会自动变为Standby状态，如此往复循环，保证高可靠，需要注意，目前仅仅支持最多配置2个NN<br>4. master选举：如上所述，通过在zookeeper中维持一个短暂类型的znode，来实现抢占式的锁机制，从而判断那个NameNode为Active状态  <strong>hadoop2.x Federation</strong>：  单Active NN的架构使得HDFS在集群扩展性和性能上都有潜在的问题，当集群大到一定程度后，NN进程使用的内存可能会达到上百G，NN成为了性能的瓶颈  常用的估算公式为1G对应1百万个块，按缺省块大小计算的话，大概是64T (这个估算比例是有比较大的富裕的，其实，即使是每个文件只有一个块，所有元数据信息也不会有1KB/block)  为了解决这个问题,Hadoop 2.x提供了HDFS Federation, 示意图如下：<br><img src=\"//img.wqkenqing.ren/file/2017/7/e5015b90accf4fe6a7729afe692ffa64-image.png\" alt=\"e5015b90accf4fe6a7729afe692ffa64-image.png\"></p>\n<p>多个NN共用一个集群里的存储资源，每个NN都可以单独对外提供服务  每个NN都会定义一个存储池，有单独的id，每个DN都为所有存储池提供存储  DN会按照存储池id向其对应的NN汇报块信息，同时，DN会向所有NN汇报本地存储可用资源情况  如果需要在客户端方便的访问若干个NN上的资源，可以使用客户端挂载表，把不同的目录映射到不同的NN，但NN上必须存在相应的目录  设计优势：  改动最小，向前兼容；现有的NN无需任何配置改动；如果现有的客户端只连某台NN的话  分离命名空间管理和块存储管理  客户端挂载表：通过路径自动对应NN、使Federation的配置改动对应用透明  (与上面ha方案中介绍的最多2个NN冲突？)  至此hadoop中的hdfs高可用特性，高容错的实现又有了更深理解，但针对hdfs还一层设计实现<strong>机架感知</strong></p>\n<h4 id=\"机架感知\"><a href=\"#机架感知\" class=\"headerlink\" title=\"机架感知\"></a>机架感知</h4><p>分布式的集群通常包含非常多的机器，由于受到机架槽位和交换机网口的限制，通常大型的分布式集群都会跨好几个机架，由多个机架上的机器共同组成一个分布 式集群。机架内的机器之间的网络速度通常都会高于跨机架机器之间的网络速度，并且机架之间机器的网络通信通常受到上层交换机间网络带宽的限制。  具体到hadoop集群，由于hadoop的HDFS对数据文件的分布式存放是按照分块block存储，每个block会有多个副本(默认为3)，并且为了数据的安全和高效，所以hadoop默认对3个副本的存放策略为：<br>在本地机器的hdfs目录下存储一个block  在另外一个rack的某个datanode上存储一个block<br>在该机器的同一个rack下的某台机器上存储最后一个block<br>这样的策略可以保证对该block所属文件的访问能够优先在本rack下找到，如果整个rack发生了异常，也可以在另外的rack上找到该block的副本。这样足够的高效，并且同时做到了数据的容错。<br>hadoop对机架的感知并非是自适应的，亦即，hadoop集群分辨某台slave机器是属于哪个rack并非是只能的感知的，而是需要 hadoop的管理者人为的告知hadoop哪台机器属于哪个rack，这样在hadoop的namenode启动初始化时，会将这些机器与rack的对 应信息保存在内存中，用来作为对接下来所有的HDFS的写块操作分配datanode列表时（比如3个block对应三台datanode）的选择 datanode策略，做到hadoop allocate block的策略：尽量将三个副本分布到不同的rack。<br>具体实现本文不在深究，在此附上网上的一些解决方式<br><a href=\"http://www.cnblogs.com/cloudma/articles/hadoop-topology.html\" target=\"_blank\" rel=\"noopener\">机架感知实现1</a>  <a href=\"http://blog.csdn.net/magicdreaming/article/details/7629773\" target=\"_blank\" rel=\"noopener\">机架感知实现2</a></p>\n</li>\n</ul>\n<hr>\n<p>至此对hdfs的理解与总结告一段落，后续有了新的理解再进行补充</p>"},{"title":"MapReduce的核心思想","date":"2019-07-15T16:00:00.000Z","_content":"此处简介\n<!--more-->\n\n`mapreduce在hadoop中更多的承担的是计算的角色`\n\n## 什么是MapReduce？\n\nmapreduce源于谷歌公司为研究大规模数据处理而研发出的一种并行计算模型和方法。\n它将并行编程中的难点和有相对门槛的方法进行高度封装，而给开发者一套接口，让开发者理专注于自己的业务逻辑，即可让自己的代码运行在分布式集群中，大大降低了开发一些并发程序的门槛。从字面上就可以知道，MapReduce分为Map(映射)、Reduce(规约)\n\n## MapReduce的核心思想\n\nMapReduce主要是两种经典函数：\n\n*   映射（Mapping）将一个整体按某种规则映射成N份。并对这N份进行同一种操作。\n*   规约（Reducing）将N份文件，按某种策略进行合并。\n\n## MapReduce的角色与动作\n\n　 MapReduce包含四个组成部分，分别为**Client**、**JobTracker**、**TaskTracker**和**Task**，下面我们详细介绍这四个组成部分。\n\n*   Client：作业提交的发起者， 每一个 Job 都会在用户端通过 Client 类将应用程序以及配置参数 Configuration 打包成 JAR 文件存储在 HDFS，并把路径提交到 JobTracker 的 master 服务，然后由 master 创建每一个 Task（即 MapTask 和 ReduceTask） 将它们分发到各个 TaskTracker 服务中去执行。\n*   JobTracker：初始化作业，分配作业，与TaskTracker通信，协调整个作业 JobTracke负责资源监控和作业调度。JobTracker 监控所有TaskTracker 与job的健康状况，一旦发现失败，就将相应的任务转移到其他节点；同时，JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器，而调度器会在资源出现空闲时，选择合适的任务使用这些资源。在Hadoop 中，任务调度器是一个可插拔的模块，用户可以根据自己的需要设计相应的调度器。\n*   TaskTracker：TaskTracker 会周期性地通过Heartbeat 将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）。TaskTracker 使用“slot”等量划分本节点上的资源量。“slot”代表计算资源（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop 调度器的作用就是将各个TaskTracker 上的空闲slot 分配给Task 使用。slot 分为Map slot 和Reduce slot 两种，分别供Map Task 和Reduce Task 使用。TaskTracker 通过slot 数目（可配置参数）限定Task 的并发度。\n*   Task ： Task 分为Map Task 和Reduce Task 两种，均由TaskTracker 启动。HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。\n*   Map Task 执行过程如下图 所示：由该图可知，Map Task 先将对应的split 迭代解析成一个个key/value 对，依次调用用户 自定义的map() 函数进行处理，最终将临时结果存放到本地磁盘上, 其中临时数据被分成若干个partition，每个partition 将被一个Reduce Task 处理。\n   ![09d902f773ff4e30b1cf995705d4f03c-image.png](http://img.wqkenqing.ren//file/2017/10/09d902f773ff4e30b1cf995705d4f03c-image.png)\n\n\n    Reduce Task 执行过程下图所示。该过程分为三个阶段\n    ![2f9ff84d09cf424cb8a1c4d24db637c0-image.png](http://img.wqkenqing.ren//file/2017/10/2f9ff84d09cf424cb8a1c4d24db637c0-image.png)\n\n\n\n**提交作业**\n\n*   在作业提交之前，需要对作业进行配置\n*   程序代码，主要是自己书写的MapReduce程序\n*   输入输出路径\n*   其他配置，如输出压缩等\n*   配置完成后，通过JobClient来提交\n\n**作业的初始化**\n\n*   客户端提交完成后，JobTracker会将作业加入队列，然后进行调度，默认的调度方法是FIFO调试方式\n    **任务的分配**\n*   TaskTracker和JobTracker之间的通信与任务的分配是通过心跳机制完成的。\n*   TaskTracker会主动向JobTracker询问是否有作业要做，如果自己能做，那么就会申请作业任务，这个任务可以使Map，也可能是Reduce任务\n    **任务的执行**\n*   申请到任务后，TaskTracker会做如下事情：\n*   拷贝代码到本地\n*   拷贝任务的信息到本地\n*   启动Jvm运行任务\n    **状态与任务的更新**\n*   任务在运行过程中，首先会将自己的状态汇报给TaskTracker，然后由TaskTracker汇总报给JobTracker\n*   任务进度是通过计数器来实现的。\n\n**作业的完成**\n\n*   JobTracker 是在接受到最后一个任务运行完成后，才会将任务标志为成功\n*   此时会做删除中间结果等善后处理工作\n  ![8fba570574f6491c96b51698743466b2-image.png](http://img.wqkenqing.ren//file/2017/10/8fba570574f6491c96b51698743466b2-image.png)\n\n\n### MapReduce任务执行流程与数据处理流程详解\n\n`就任务流程而言，上文中有些或已经涉及，有些也介绍的比较详细了。但就整体而言，不是特别具体，或这个整体性，所以在此我们再集中总结一下，即使重复内容，就也当加深印象吧。`\n\n#### 任务执行流程详解\n\n1.  通过jobClient提交当mapreduce的job提交至JobTracker\n    ,提交的具体信息大致会有，conf配置内容，path，相关的Map，Reduce函数等。（数据的切片会在client上完成）\n2.  JobTracker中的Master服务会完成创建一个Task（MapTask与ReduceTask），并将这个任务加载至任务队列中，等待TaskTracker来获取。同进JobTracker会处于一种监听状态，监听所有TaskTracker与Job的健康情况。面对故障时，提供服务转移等。同时它还会记录任务的执行进度，资源的使用情况，提交至任务调度器，由调度器进行资源的调度。\n3.  TaskTracker会主动向jobTacker去询问是否有任务，如果有，就会申请到作业，作业可以是map,也可以是reduce任务。TaskTracker获取的不仅是任务，还有相关的处理代码也会copy一份至本地，然后TaskTracker再针对所分配的split进行处理。TaskTracker还会周期性地通过HearBeat将本节点上的资源的使用情况和任务的进行进度汇报给JobTracker，同时接收JobTracker发过来的相关命令并执行，\n4.  当TaskTracker中的任务完成后会上报给JobTracker,而当最后一个TaskTracker完成后，JobTracker才会将任务标志为成功，并执行一些如删除中间结果等善后工作。\n\n* * *\n\n`从这里我们知道了mapreduce中任务执行流程，但mapreduce作为hadoop的计算框架，它对数据的处理流程我们还没明确涉及，所以接下来我们再深入了解下具体的数据处理流程`\n\n#### 数据处理流程详解\n\n这里的数据处理流程，主要指的是mapreduce执行后，Task中对split的任务具体处理的这一流程，其中还包括，数据的切片，mapTask完成后将中间结果上传等动作。下面我们来具体讨论\n当jobClient向JobTracker提交了任务后，数据处理流程也随之开始\n\n*   在Client端将输入源的数据进行切片(split)，具体的切片机制参考后面\n*   JobTracker中的MRAppMaster将每个Split信息计算出需要的MapTask的实例数量，然后向集群申请机器启动相应数量的mapTask进程\n*   进程启动后，根据给定的数据切片范围进行数据处理，主要流程为\n    a)通过inputFormat来获取RecordReader读取数据，并形成输入的KV对\n    b)将输入KV对传递给用户定义的map（）方法，做逻辑处理，并map()方法的输出的kv对手机到缓存中(这里用到了缓存机制)\n    简而言之map中输入时要做的事是\n    1.反射构造InputFormat.\n    2.反射构造InputSplit.\n    3.创建RecordReader.\n    4.反射创建MapperRunner\n\n![3a461434b0604118aad45a4d38cf3649-image.png](http://img.wqkenqing.ren//file/2017/10/3a461434b0604118aad45a4d38cf3649-image.png)\n\n\n而map输出时相对复杂，主要涉及到的有Partitioner，shuffle，sort，combiner等概念，我们就来一一讨论。\n在map（）方法执行后，map阶段是会有处理的数据输出，正常来说，就是每个split对应的每一行。如上图MapRunner的next为false时，对输入数据的map完成，这时对存内中这些map的数据，会对其进行sort,如果我们事先设置的有combiner那么，还会对sort完的数据执行combiner(**一个类reduce操作，不过是对本地数据的reduce，使用它的原则是combiner的输入不会影响到reduce计算的最终输入，例如：如果计算只是求总数，最大值，最小值可以使用combiner，但是做平均值计算使用combiner的话，最终的reduce计算结果就会出错。**)，然后开始map的内容spill到磁盘中，如果频繁的spill对磁盘会带来较大的损耗和效率影响。所以引入了一个写缓冲区的概念，即每一个Map Task都拥有一个“环形缓冲区”作为Mapper输出的写缓冲区。写缓冲区大小默认为100MB（通过属性io.sort.mb调整），当写缓冲区的数据量达到一定的容量限额时（默认为80%，通过属性io.sort.spill.percent调整），后台线程开始将写缓冲区的数据溢写到本地磁盘。在数据溢写的过程中，只要写缓冲区没有被写满，Mappper依然可以将数据写出到缓冲区中；否则Mapper的执行过程将被阻塞，直到溢写结束。\n溢写以循环的方式进行（即写缓冲区的数据量大致限额时就会执行溢写），可以通过属性mapred.local.dir指定写出的目录。\nspill结束前 溢写线程将数据最终写出到本地磁盘之前，首先根据Reducer的数目对这部分数据进行分区（即每一个分区中的数据会被传送至同一个Reducer进行处理，分区数目与Reducer数据保持一致）即 **partition**，partition是一个类inputSplit()的操作，即根据有多少个ReduceTask生成多少个partition,并通过jobTracker指定给相应的ReduceTask。\n当spill完成后，本地磁盘中会有多个溢出文件存在。在MapTask结束前，这些文件会根据相应的分区进行合并，并排序，合并可能发生多次，具体由**io.sort.factor控制一次最多合并多少个文件。**\n\n如果溢写文件个数超过3（通过属性min.num.spills.for.combine设置），会对合并且分区排序后的结果执行Combine过程（如果MapReduce有设置Combiner），而且combine过程在不影响最终结果的前提下可能会被执行多次；否则不会执行Combine过程（相对而言，Combine开销过大）。\n\n注意：Map Task执行过程中，Combine可能出现在两个地方：写缓冲区溢写过程中、溢写文件合并过程中。\n\n注意：Mapper的一条输出结果（由key、value表示）写出到写缓冲区之前，已经提前计算好相应的分区信息，即分区的过程在数据写入写缓冲区之前就已经完成，溢写过程实际是写缓冲区数据排序的过程（先按分区排序，如果分区相同时，再按键值排序）。\n\n这里涉及到MapReduce的两个组件：Comparator、Partitioner。\n(由于篇幅的原因，这里暂不引入对这两个组件的源码分析，和自定义方式，后面有机会则单开文章讨论)\n\n在将map输出结果作为reducTask中的输入时，会涉及到磁盘写入，网络传输等资源的限制，所以对出于节省资源的考虑，可以在对map的输出结果进行压缩。默认情况下，压缩是不被开启的，可以通过属性**mapred.compress.map.output**、**mapred.map.output.compression.codec**进行相应设置。\n\n当MapTask任务结束后，被指定的分区ReduceTask会立即开始执行，即开始拷贝对应MapTask分区中的输出结果。\nReduce Task的这个阶段被称为“Copy Phase”。Reduce Task拥有少量的线程用于并行地获取Map Tasks的输出结果，默认线程数为5，可以通过属性mapred.reduce.parallel.copies进行设置。\n同样：如果Map Task的输出结果足够小，它会被拷贝至Reduce Task的缓冲区中；否则拷贝至磁盘。当缓冲区中的数据达到一定量（由属性mapred.job.shuffle.merge.percent、mapred.inmem.merge.threshold），这些数据将被合并且溢写到磁盘。如果Combine过程被指定，它将在合并过程被执行，用来减少需要写出到磁盘的数据量。\n\n随着拷贝文件中磁盘上的不断积累，一个后台线程会将它们合并为更大地、有序的文件，用来节省后期的合并时间。如果Map Tasks的输出结合使用了压缩机制，则在合并的过程中需要对数据进行解压处理。\n\n当Reduce Task的所有Map Tasks输出结果均完成拷贝，Reduce Task进入“Sort Phase”（更为合适地应该被称为“Merge Phase”，排序在Map阶段已经被执行），该阶段在保持原有顺序的情况下进行合并。这种合并是以循环方式进行的，循环次数与合并因子（io.sort.factor）有关。\nsort phase 通常不是合并成一个文件，而是略过磁盘操作，直接将数据合并输入至Reduce方法中 这次合并的数据可以结合内存、磁盘两部分进行操作），即“Reduce Phase”。\n\n通常这里还有一个“Group”的阶段，这个阶段决定着哪些键值对属于同一个键。如果没有特殊设置，只有在Map Task输出时那些键完全一样的数据属于同一个键，但这是可以被改变的。\n\n描述至这里终于能引用mapreduce中相当重要的一个概念，即shuffle。这个词在这里该怎么定义，我暂未找到个一个比较满意的答案，但我比较喜欢有人把这个比作是搓完牌一桌子人，在下一局开始前的整个过程。\n即Shuffle操作，涉及到数据的partition、sort、[combine]、spill、[comress]、[merge]、copy、[combine]、merge、group，而这些操作不但决定着程序逻辑的正确性，也决定着MapReduce的运行效率。\n\nshuffle完后进入了ReduceTask的reduce()方法中\n在Reduce Phase的过程中，它处理的是所有Map Tasks输出结果中某一个分区中的所有数据，这些数据整体表现为一个根据键有序的输入，对于每一个键都会相应地调用一次Reduce Function（同一个键对应的值可能有多个，这些值将作为Reduce Function的参数）\n\n至此MapReduce的逻辑过程基本描述完成，虽然洋洋洒洒可能会有数千字，但本文的出发点就不是简析，而更多是自我概念原理部份的总结，所以力求整个流程完整详细。后面我配上一些网络图片，方便大家快速理解，结合文字加深印象。\n![b26c54ff10ad4e2b9832a960ef4aab90-image.png](http://img.wqkenqing.ren//file/2017/10/b26c54ff10ad4e2b9832a960ef4aab90-image.png)\n\n![a179f15b88cc403b8bd84d7963823762-image.png](http://img.wqkenqing.ren//file/2017/10/a179f15b88cc403b8bd84d7963823762-image.png)\n\n![4a5b5498dd764087ade380db394e6f84-image.png](http://img.wqkenqing.ren//file/2017/10/4a5b5498dd764087ade380db394e6f84-image.png)\n\n![74ed215c305747a49348430782e5636a-image.png](http://img.wqkenqing.ren//file/2017/10/74ed215c305747a49348430782e5636a-image.png)\n\n\n![b55be177bd514ce79b7444c2dd3ddcfb-image.png](http://img.wqkenqing.ren//file/2017/10/b55be177bd514ce79b7444c2dd3ddcfb-image.png)\n\n![286bb24da172471793924b2b9b7c857c-image.png](http://img.wqkenqing.ren//file/2017/10/286bb24da172471793924b2b9b7c857c-image.png)\n![68bbc5f114f3496d886402cbb0da8fc1-image.png](http://img.wqkenqing.ren//file/2017/10/68bbc5f114f3496d886402cbb0da8fc1-image.png)\n\n![38d8bf9498274367b22856f62a8f7fcf-image.png](http://img.wqkenqing.ren//file/2017/10/38d8bf9498274367b22856f62a8f7fcf-image.png)\n\n![7b7e5da815064fdfa0d060910f8dfb9b-image.png](http://img.wqkenqing.ren//file/2017/10/7b7e5da815064fdfa0d060910f8dfb9b-image.png)\n\n![a4ac8a6f758b4bdfb5e1a752a02f0654-image.png](http://img.wqkenqing.ren//file/2017/10/a4ac8a6f758b4bdfb5e1a752a02f0654-image.png)\n\n![9e91f8a6b0ab488abbad0714487fe10f-image.png](http://img.wqkenqing.ren//file/2017/10/9e91f8a6b0ab488abbad0714487fe10f-image.png)\n\n![87a4d55fff15408c87f94029b8fb2aea-image.png](http://img.wqkenqing.ren//file/2017/10/87a4d55fff15408c87f94029b8fb2aea-image.png)\n\n![eb099e53d6fa44a5b76410040f41f3ae-image.png](http://img.wqkenqing.ren//file/2017/10/eb099e53d6fa44a5b76410040f41f3ae-image.png)\n\n![98aac8b77be64d53b276f7a112aba12d-image.png](http://img.wqkenqing.ren//file/2017/10/98aac8b77be64d53b276f7a112aba12d-image.png)\n\n\n\n","source":"_posts/技术/hexo/oldblog/blog26.md","raw":"\n---\n\ntitle: MapReduce的核心思想\ndate: 2019-07-16\ntags: \n\n---\n此处简介\n<!--more-->\n\n`mapreduce在hadoop中更多的承担的是计算的角色`\n\n## 什么是MapReduce？\n\nmapreduce源于谷歌公司为研究大规模数据处理而研发出的一种并行计算模型和方法。\n它将并行编程中的难点和有相对门槛的方法进行高度封装，而给开发者一套接口，让开发者理专注于自己的业务逻辑，即可让自己的代码运行在分布式集群中，大大降低了开发一些并发程序的门槛。从字面上就可以知道，MapReduce分为Map(映射)、Reduce(规约)\n\n## MapReduce的核心思想\n\nMapReduce主要是两种经典函数：\n\n*   映射（Mapping）将一个整体按某种规则映射成N份。并对这N份进行同一种操作。\n*   规约（Reducing）将N份文件，按某种策略进行合并。\n\n## MapReduce的角色与动作\n\n　 MapReduce包含四个组成部分，分别为**Client**、**JobTracker**、**TaskTracker**和**Task**，下面我们详细介绍这四个组成部分。\n\n*   Client：作业提交的发起者， 每一个 Job 都会在用户端通过 Client 类将应用程序以及配置参数 Configuration 打包成 JAR 文件存储在 HDFS，并把路径提交到 JobTracker 的 master 服务，然后由 master 创建每一个 Task（即 MapTask 和 ReduceTask） 将它们分发到各个 TaskTracker 服务中去执行。\n*   JobTracker：初始化作业，分配作业，与TaskTracker通信，协调整个作业 JobTracke负责资源监控和作业调度。JobTracker 监控所有TaskTracker 与job的健康状况，一旦发现失败，就将相应的任务转移到其他节点；同时，JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器，而调度器会在资源出现空闲时，选择合适的任务使用这些资源。在Hadoop 中，任务调度器是一个可插拔的模块，用户可以根据自己的需要设计相应的调度器。\n*   TaskTracker：TaskTracker 会周期性地通过Heartbeat 将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）。TaskTracker 使用“slot”等量划分本节点上的资源量。“slot”代表计算资源（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop 调度器的作用就是将各个TaskTracker 上的空闲slot 分配给Task 使用。slot 分为Map slot 和Reduce slot 两种，分别供Map Task 和Reduce Task 使用。TaskTracker 通过slot 数目（可配置参数）限定Task 的并发度。\n*   Task ： Task 分为Map Task 和Reduce Task 两种，均由TaskTracker 启动。HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。\n*   Map Task 执行过程如下图 所示：由该图可知，Map Task 先将对应的split 迭代解析成一个个key/value 对，依次调用用户 自定义的map() 函数进行处理，最终将临时结果存放到本地磁盘上, 其中临时数据被分成若干个partition，每个partition 将被一个Reduce Task 处理。\n   ![09d902f773ff4e30b1cf995705d4f03c-image.png](http://img.wqkenqing.ren//file/2017/10/09d902f773ff4e30b1cf995705d4f03c-image.png)\n\n\n    Reduce Task 执行过程下图所示。该过程分为三个阶段\n    ![2f9ff84d09cf424cb8a1c4d24db637c0-image.png](http://img.wqkenqing.ren//file/2017/10/2f9ff84d09cf424cb8a1c4d24db637c0-image.png)\n\n\n\n**提交作业**\n\n*   在作业提交之前，需要对作业进行配置\n*   程序代码，主要是自己书写的MapReduce程序\n*   输入输出路径\n*   其他配置，如输出压缩等\n*   配置完成后，通过JobClient来提交\n\n**作业的初始化**\n\n*   客户端提交完成后，JobTracker会将作业加入队列，然后进行调度，默认的调度方法是FIFO调试方式\n    **任务的分配**\n*   TaskTracker和JobTracker之间的通信与任务的分配是通过心跳机制完成的。\n*   TaskTracker会主动向JobTracker询问是否有作业要做，如果自己能做，那么就会申请作业任务，这个任务可以使Map，也可能是Reduce任务\n    **任务的执行**\n*   申请到任务后，TaskTracker会做如下事情：\n*   拷贝代码到本地\n*   拷贝任务的信息到本地\n*   启动Jvm运行任务\n    **状态与任务的更新**\n*   任务在运行过程中，首先会将自己的状态汇报给TaskTracker，然后由TaskTracker汇总报给JobTracker\n*   任务进度是通过计数器来实现的。\n\n**作业的完成**\n\n*   JobTracker 是在接受到最后一个任务运行完成后，才会将任务标志为成功\n*   此时会做删除中间结果等善后处理工作\n  ![8fba570574f6491c96b51698743466b2-image.png](http://img.wqkenqing.ren//file/2017/10/8fba570574f6491c96b51698743466b2-image.png)\n\n\n### MapReduce任务执行流程与数据处理流程详解\n\n`就任务流程而言，上文中有些或已经涉及，有些也介绍的比较详细了。但就整体而言，不是特别具体，或这个整体性，所以在此我们再集中总结一下，即使重复内容，就也当加深印象吧。`\n\n#### 任务执行流程详解\n\n1.  通过jobClient提交当mapreduce的job提交至JobTracker\n    ,提交的具体信息大致会有，conf配置内容，path，相关的Map，Reduce函数等。（数据的切片会在client上完成）\n2.  JobTracker中的Master服务会完成创建一个Task（MapTask与ReduceTask），并将这个任务加载至任务队列中，等待TaskTracker来获取。同进JobTracker会处于一种监听状态，监听所有TaskTracker与Job的健康情况。面对故障时，提供服务转移等。同时它还会记录任务的执行进度，资源的使用情况，提交至任务调度器，由调度器进行资源的调度。\n3.  TaskTracker会主动向jobTacker去询问是否有任务，如果有，就会申请到作业，作业可以是map,也可以是reduce任务。TaskTracker获取的不仅是任务，还有相关的处理代码也会copy一份至本地，然后TaskTracker再针对所分配的split进行处理。TaskTracker还会周期性地通过HearBeat将本节点上的资源的使用情况和任务的进行进度汇报给JobTracker，同时接收JobTracker发过来的相关命令并执行，\n4.  当TaskTracker中的任务完成后会上报给JobTracker,而当最后一个TaskTracker完成后，JobTracker才会将任务标志为成功，并执行一些如删除中间结果等善后工作。\n\n* * *\n\n`从这里我们知道了mapreduce中任务执行流程，但mapreduce作为hadoop的计算框架，它对数据的处理流程我们还没明确涉及，所以接下来我们再深入了解下具体的数据处理流程`\n\n#### 数据处理流程详解\n\n这里的数据处理流程，主要指的是mapreduce执行后，Task中对split的任务具体处理的这一流程，其中还包括，数据的切片，mapTask完成后将中间结果上传等动作。下面我们来具体讨论\n当jobClient向JobTracker提交了任务后，数据处理流程也随之开始\n\n*   在Client端将输入源的数据进行切片(split)，具体的切片机制参考后面\n*   JobTracker中的MRAppMaster将每个Split信息计算出需要的MapTask的实例数量，然后向集群申请机器启动相应数量的mapTask进程\n*   进程启动后，根据给定的数据切片范围进行数据处理，主要流程为\n    a)通过inputFormat来获取RecordReader读取数据，并形成输入的KV对\n    b)将输入KV对传递给用户定义的map（）方法，做逻辑处理，并map()方法的输出的kv对手机到缓存中(这里用到了缓存机制)\n    简而言之map中输入时要做的事是\n    1.反射构造InputFormat.\n    2.反射构造InputSplit.\n    3.创建RecordReader.\n    4.反射创建MapperRunner\n\n![3a461434b0604118aad45a4d38cf3649-image.png](http://img.wqkenqing.ren//file/2017/10/3a461434b0604118aad45a4d38cf3649-image.png)\n\n\n而map输出时相对复杂，主要涉及到的有Partitioner，shuffle，sort，combiner等概念，我们就来一一讨论。\n在map（）方法执行后，map阶段是会有处理的数据输出，正常来说，就是每个split对应的每一行。如上图MapRunner的next为false时，对输入数据的map完成，这时对存内中这些map的数据，会对其进行sort,如果我们事先设置的有combiner那么，还会对sort完的数据执行combiner(**一个类reduce操作，不过是对本地数据的reduce，使用它的原则是combiner的输入不会影响到reduce计算的最终输入，例如：如果计算只是求总数，最大值，最小值可以使用combiner，但是做平均值计算使用combiner的话，最终的reduce计算结果就会出错。**)，然后开始map的内容spill到磁盘中，如果频繁的spill对磁盘会带来较大的损耗和效率影响。所以引入了一个写缓冲区的概念，即每一个Map Task都拥有一个“环形缓冲区”作为Mapper输出的写缓冲区。写缓冲区大小默认为100MB（通过属性io.sort.mb调整），当写缓冲区的数据量达到一定的容量限额时（默认为80%，通过属性io.sort.spill.percent调整），后台线程开始将写缓冲区的数据溢写到本地磁盘。在数据溢写的过程中，只要写缓冲区没有被写满，Mappper依然可以将数据写出到缓冲区中；否则Mapper的执行过程将被阻塞，直到溢写结束。\n溢写以循环的方式进行（即写缓冲区的数据量大致限额时就会执行溢写），可以通过属性mapred.local.dir指定写出的目录。\nspill结束前 溢写线程将数据最终写出到本地磁盘之前，首先根据Reducer的数目对这部分数据进行分区（即每一个分区中的数据会被传送至同一个Reducer进行处理，分区数目与Reducer数据保持一致）即 **partition**，partition是一个类inputSplit()的操作，即根据有多少个ReduceTask生成多少个partition,并通过jobTracker指定给相应的ReduceTask。\n当spill完成后，本地磁盘中会有多个溢出文件存在。在MapTask结束前，这些文件会根据相应的分区进行合并，并排序，合并可能发生多次，具体由**io.sort.factor控制一次最多合并多少个文件。**\n\n如果溢写文件个数超过3（通过属性min.num.spills.for.combine设置），会对合并且分区排序后的结果执行Combine过程（如果MapReduce有设置Combiner），而且combine过程在不影响最终结果的前提下可能会被执行多次；否则不会执行Combine过程（相对而言，Combine开销过大）。\n\n注意：Map Task执行过程中，Combine可能出现在两个地方：写缓冲区溢写过程中、溢写文件合并过程中。\n\n注意：Mapper的一条输出结果（由key、value表示）写出到写缓冲区之前，已经提前计算好相应的分区信息，即分区的过程在数据写入写缓冲区之前就已经完成，溢写过程实际是写缓冲区数据排序的过程（先按分区排序，如果分区相同时，再按键值排序）。\n\n这里涉及到MapReduce的两个组件：Comparator、Partitioner。\n(由于篇幅的原因，这里暂不引入对这两个组件的源码分析，和自定义方式，后面有机会则单开文章讨论)\n\n在将map输出结果作为reducTask中的输入时，会涉及到磁盘写入，网络传输等资源的限制，所以对出于节省资源的考虑，可以在对map的输出结果进行压缩。默认情况下，压缩是不被开启的，可以通过属性**mapred.compress.map.output**、**mapred.map.output.compression.codec**进行相应设置。\n\n当MapTask任务结束后，被指定的分区ReduceTask会立即开始执行，即开始拷贝对应MapTask分区中的输出结果。\nReduce Task的这个阶段被称为“Copy Phase”。Reduce Task拥有少量的线程用于并行地获取Map Tasks的输出结果，默认线程数为5，可以通过属性mapred.reduce.parallel.copies进行设置。\n同样：如果Map Task的输出结果足够小，它会被拷贝至Reduce Task的缓冲区中；否则拷贝至磁盘。当缓冲区中的数据达到一定量（由属性mapred.job.shuffle.merge.percent、mapred.inmem.merge.threshold），这些数据将被合并且溢写到磁盘。如果Combine过程被指定，它将在合并过程被执行，用来减少需要写出到磁盘的数据量。\n\n随着拷贝文件中磁盘上的不断积累，一个后台线程会将它们合并为更大地、有序的文件，用来节省后期的合并时间。如果Map Tasks的输出结合使用了压缩机制，则在合并的过程中需要对数据进行解压处理。\n\n当Reduce Task的所有Map Tasks输出结果均完成拷贝，Reduce Task进入“Sort Phase”（更为合适地应该被称为“Merge Phase”，排序在Map阶段已经被执行），该阶段在保持原有顺序的情况下进行合并。这种合并是以循环方式进行的，循环次数与合并因子（io.sort.factor）有关。\nsort phase 通常不是合并成一个文件，而是略过磁盘操作，直接将数据合并输入至Reduce方法中 这次合并的数据可以结合内存、磁盘两部分进行操作），即“Reduce Phase”。\n\n通常这里还有一个“Group”的阶段，这个阶段决定着哪些键值对属于同一个键。如果没有特殊设置，只有在Map Task输出时那些键完全一样的数据属于同一个键，但这是可以被改变的。\n\n描述至这里终于能引用mapreduce中相当重要的一个概念，即shuffle。这个词在这里该怎么定义，我暂未找到个一个比较满意的答案，但我比较喜欢有人把这个比作是搓完牌一桌子人，在下一局开始前的整个过程。\n即Shuffle操作，涉及到数据的partition、sort、[combine]、spill、[comress]、[merge]、copy、[combine]、merge、group，而这些操作不但决定着程序逻辑的正确性，也决定着MapReduce的运行效率。\n\nshuffle完后进入了ReduceTask的reduce()方法中\n在Reduce Phase的过程中，它处理的是所有Map Tasks输出结果中某一个分区中的所有数据，这些数据整体表现为一个根据键有序的输入，对于每一个键都会相应地调用一次Reduce Function（同一个键对应的值可能有多个，这些值将作为Reduce Function的参数）\n\n至此MapReduce的逻辑过程基本描述完成，虽然洋洋洒洒可能会有数千字，但本文的出发点就不是简析，而更多是自我概念原理部份的总结，所以力求整个流程完整详细。后面我配上一些网络图片，方便大家快速理解，结合文字加深印象。\n![b26c54ff10ad4e2b9832a960ef4aab90-image.png](http://img.wqkenqing.ren//file/2017/10/b26c54ff10ad4e2b9832a960ef4aab90-image.png)\n\n![a179f15b88cc403b8bd84d7963823762-image.png](http://img.wqkenqing.ren//file/2017/10/a179f15b88cc403b8bd84d7963823762-image.png)\n\n![4a5b5498dd764087ade380db394e6f84-image.png](http://img.wqkenqing.ren//file/2017/10/4a5b5498dd764087ade380db394e6f84-image.png)\n\n![74ed215c305747a49348430782e5636a-image.png](http://img.wqkenqing.ren//file/2017/10/74ed215c305747a49348430782e5636a-image.png)\n\n\n![b55be177bd514ce79b7444c2dd3ddcfb-image.png](http://img.wqkenqing.ren//file/2017/10/b55be177bd514ce79b7444c2dd3ddcfb-image.png)\n\n![286bb24da172471793924b2b9b7c857c-image.png](http://img.wqkenqing.ren//file/2017/10/286bb24da172471793924b2b9b7c857c-image.png)\n![68bbc5f114f3496d886402cbb0da8fc1-image.png](http://img.wqkenqing.ren//file/2017/10/68bbc5f114f3496d886402cbb0da8fc1-image.png)\n\n![38d8bf9498274367b22856f62a8f7fcf-image.png](http://img.wqkenqing.ren//file/2017/10/38d8bf9498274367b22856f62a8f7fcf-image.png)\n\n![7b7e5da815064fdfa0d060910f8dfb9b-image.png](http://img.wqkenqing.ren//file/2017/10/7b7e5da815064fdfa0d060910f8dfb9b-image.png)\n\n![a4ac8a6f758b4bdfb5e1a752a02f0654-image.png](http://img.wqkenqing.ren//file/2017/10/a4ac8a6f758b4bdfb5e1a752a02f0654-image.png)\n\n![9e91f8a6b0ab488abbad0714487fe10f-image.png](http://img.wqkenqing.ren//file/2017/10/9e91f8a6b0ab488abbad0714487fe10f-image.png)\n\n![87a4d55fff15408c87f94029b8fb2aea-image.png](http://img.wqkenqing.ren//file/2017/10/87a4d55fff15408c87f94029b8fb2aea-image.png)\n\n![eb099e53d6fa44a5b76410040f41f3ae-image.png](http://img.wqkenqing.ren//file/2017/10/eb099e53d6fa44a5b76410040f41f3ae-image.png)\n\n![98aac8b77be64d53b276f7a112aba12d-image.png](http://img.wqkenqing.ren//file/2017/10/98aac8b77be64d53b276f7a112aba12d-image.png)\n\n\n\n","slug":"技术/hexo/oldblog/blog26","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33i003538pwecaqb9hj","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<p><code>mapreduce在hadoop中更多的承担的是计算的角色</code></p>\n<h2 id=\"什么是MapReduce？\"><a href=\"#什么是MapReduce？\" class=\"headerlink\" title=\"什么是MapReduce？\"></a>什么是MapReduce？</h2><p>mapreduce源于谷歌公司为研究大规模数据处理而研发出的一种并行计算模型和方法。<br>它将并行编程中的难点和有相对门槛的方法进行高度封装，而给开发者一套接口，让开发者理专注于自己的业务逻辑，即可让自己的代码运行在分布式集群中，大大降低了开发一些并发程序的门槛。从字面上就可以知道，MapReduce分为Map(映射)、Reduce(规约)</p>\n<h2 id=\"MapReduce的核心思想\"><a href=\"#MapReduce的核心思想\" class=\"headerlink\" title=\"MapReduce的核心思想\"></a>MapReduce的核心思想</h2><p>MapReduce主要是两种经典函数：</p>\n<ul>\n<li>映射（Mapping）将一个整体按某种规则映射成N份。并对这N份进行同一种操作。</li>\n<li>规约（Reducing）将N份文件，按某种策略进行合并。</li>\n</ul>\n<h2 id=\"MapReduce的角色与动作\"><a href=\"#MapReduce的角色与动作\" class=\"headerlink\" title=\"MapReduce的角色与动作\"></a>MapReduce的角色与动作</h2><p>　 MapReduce包含四个组成部分，分别为<strong>Client</strong>、<strong>JobTracker</strong>、<strong>TaskTracker</strong>和<strong>Task</strong>，下面我们详细介绍这四个组成部分。</p>\n<ul>\n<li>Client：作业提交的发起者， 每一个 Job 都会在用户端通过 Client 类将应用程序以及配置参数 Configuration 打包成 JAR 文件存储在 HDFS，并把路径提交到 JobTracker 的 master 服务，然后由 master 创建每一个 Task（即 MapTask 和 ReduceTask） 将它们分发到各个 TaskTracker 服务中去执行。</li>\n<li>JobTracker：初始化作业，分配作业，与TaskTracker通信，协调整个作业 JobTracke负责资源监控和作业调度。JobTracker 监控所有TaskTracker 与job的健康状况，一旦发现失败，就将相应的任务转移到其他节点；同时，JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器，而调度器会在资源出现空闲时，选择合适的任务使用这些资源。在Hadoop 中，任务调度器是一个可插拔的模块，用户可以根据自己的需要设计相应的调度器。</li>\n<li>TaskTracker：TaskTracker 会周期性地通过Heartbeat 将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）。TaskTracker 使用“slot”等量划分本节点上的资源量。“slot”代表计算资源（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop 调度器的作用就是将各个TaskTracker 上的空闲slot 分配给Task 使用。slot 分为Map slot 和Reduce slot 两种，分别供Map Task 和Reduce Task 使用。TaskTracker 通过slot 数目（可配置参数）限定Task 的并发度。</li>\n<li>Task ： Task 分为Map Task 和Reduce Task 两种，均由TaskTracker 启动。HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。</li>\n<li>Map Task 执行过程如下图 所示：由该图可知，Map Task 先将对应的split 迭代解析成一个个key/value 对，依次调用用户 自定义的map() 函数进行处理，最终将临时结果存放到本地磁盘上, 其中临时数据被分成若干个partition，每个partition 将被一个Reduce Task 处理。<br><img src=\"http://img.wqkenqing.ren//file/2017/10/09d902f773ff4e30b1cf995705d4f03c-image.png\" alt=\"09d902f773ff4e30b1cf995705d4f03c-image.png\"></li>\n</ul>\n<pre><code>Reduce Task 执行过程下图所示。该过程分为三个阶段\n![2f9ff84d09cf424cb8a1c4d24db637c0-image.png](http://img.wqkenqing.ren//file/2017/10/2f9ff84d09cf424cb8a1c4d24db637c0-image.png)</code></pre><p><strong>提交作业</strong></p>\n<ul>\n<li>在作业提交之前，需要对作业进行配置</li>\n<li>程序代码，主要是自己书写的MapReduce程序</li>\n<li>输入输出路径</li>\n<li>其他配置，如输出压缩等</li>\n<li>配置完成后，通过JobClient来提交</li>\n</ul>\n<p><strong>作业的初始化</strong></p>\n<ul>\n<li>客户端提交完成后，JobTracker会将作业加入队列，然后进行调度，默认的调度方法是FIFO调试方式<br><strong>任务的分配</strong></li>\n<li>TaskTracker和JobTracker之间的通信与任务的分配是通过心跳机制完成的。</li>\n<li>TaskTracker会主动向JobTracker询问是否有作业要做，如果自己能做，那么就会申请作业任务，这个任务可以使Map，也可能是Reduce任务<br><strong>任务的执行</strong></li>\n<li>申请到任务后，TaskTracker会做如下事情：</li>\n<li>拷贝代码到本地</li>\n<li>拷贝任务的信息到本地</li>\n<li>启动Jvm运行任务<br><strong>状态与任务的更新</strong></li>\n<li>任务在运行过程中，首先会将自己的状态汇报给TaskTracker，然后由TaskTracker汇总报给JobTracker</li>\n<li>任务进度是通过计数器来实现的。</li>\n</ul>\n<p><strong>作业的完成</strong></p>\n<ul>\n<li>JobTracker 是在接受到最后一个任务运行完成后，才会将任务标志为成功</li>\n<li>此时会做删除中间结果等善后处理工作<br><img src=\"http://img.wqkenqing.ren//file/2017/10/8fba570574f6491c96b51698743466b2-image.png\" alt=\"8fba570574f6491c96b51698743466b2-image.png\"></li>\n</ul>\n<h3 id=\"MapReduce任务执行流程与数据处理流程详解\"><a href=\"#MapReduce任务执行流程与数据处理流程详解\" class=\"headerlink\" title=\"MapReduce任务执行流程与数据处理流程详解\"></a>MapReduce任务执行流程与数据处理流程详解</h3><p><code>就任务流程而言，上文中有些或已经涉及，有些也介绍的比较详细了。但就整体而言，不是特别具体，或这个整体性，所以在此我们再集中总结一下，即使重复内容，就也当加深印象吧。</code></p>\n<h4 id=\"任务执行流程详解\"><a href=\"#任务执行流程详解\" class=\"headerlink\" title=\"任务执行流程详解\"></a>任务执行流程详解</h4><ol>\n<li>通过jobClient提交当mapreduce的job提交至JobTracker<br>,提交的具体信息大致会有，conf配置内容，path，相关的Map，Reduce函数等。（数据的切片会在client上完成）</li>\n<li>JobTracker中的Master服务会完成创建一个Task（MapTask与ReduceTask），并将这个任务加载至任务队列中，等待TaskTracker来获取。同进JobTracker会处于一种监听状态，监听所有TaskTracker与Job的健康情况。面对故障时，提供服务转移等。同时它还会记录任务的执行进度，资源的使用情况，提交至任务调度器，由调度器进行资源的调度。</li>\n<li>TaskTracker会主动向jobTacker去询问是否有任务，如果有，就会申请到作业，作业可以是map,也可以是reduce任务。TaskTracker获取的不仅是任务，还有相关的处理代码也会copy一份至本地，然后TaskTracker再针对所分配的split进行处理。TaskTracker还会周期性地通过HearBeat将本节点上的资源的使用情况和任务的进行进度汇报给JobTracker，同时接收JobTracker发过来的相关命令并执行，</li>\n<li>当TaskTracker中的任务完成后会上报给JobTracker,而当最后一个TaskTracker完成后，JobTracker才会将任务标志为成功，并执行一些如删除中间结果等善后工作。</li>\n</ol>\n<hr>\n<p><code>从这里我们知道了mapreduce中任务执行流程，但mapreduce作为hadoop的计算框架，它对数据的处理流程我们还没明确涉及，所以接下来我们再深入了解下具体的数据处理流程</code></p>\n<h4 id=\"数据处理流程详解\"><a href=\"#数据处理流程详解\" class=\"headerlink\" title=\"数据处理流程详解\"></a>数据处理流程详解</h4><p>这里的数据处理流程，主要指的是mapreduce执行后，Task中对split的任务具体处理的这一流程，其中还包括，数据的切片，mapTask完成后将中间结果上传等动作。下面我们来具体讨论<br>当jobClient向JobTracker提交了任务后，数据处理流程也随之开始</p>\n<ul>\n<li>在Client端将输入源的数据进行切片(split)，具体的切片机制参考后面</li>\n<li>JobTracker中的MRAppMaster将每个Split信息计算出需要的MapTask的实例数量，然后向集群申请机器启动相应数量的mapTask进程</li>\n<li>进程启动后，根据给定的数据切片范围进行数据处理，主要流程为<br>a)通过inputFormat来获取RecordReader读取数据，并形成输入的KV对<br>b)将输入KV对传递给用户定义的map（）方法，做逻辑处理，并map()方法的输出的kv对手机到缓存中(这里用到了缓存机制)<br>简而言之map中输入时要做的事是<br>1.反射构造InputFormat.<br>2.反射构造InputSplit.<br>3.创建RecordReader.<br>4.反射创建MapperRunner</li>\n</ul>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/3a461434b0604118aad45a4d38cf3649-image.png\" alt=\"3a461434b0604118aad45a4d38cf3649-image.png\"></p>\n<p>而map输出时相对复杂，主要涉及到的有Partitioner，shuffle，sort，combiner等概念，我们就来一一讨论。<br>在map（）方法执行后，map阶段是会有处理的数据输出，正常来说，就是每个split对应的每一行。如上图MapRunner的next为false时，对输入数据的map完成，这时对存内中这些map的数据，会对其进行sort,如果我们事先设置的有combiner那么，还会对sort完的数据执行combiner(<strong>一个类reduce操作，不过是对本地数据的reduce，使用它的原则是combiner的输入不会影响到reduce计算的最终输入，例如：如果计算只是求总数，最大值，最小值可以使用combiner，但是做平均值计算使用combiner的话，最终的reduce计算结果就会出错。</strong>)，然后开始map的内容spill到磁盘中，如果频繁的spill对磁盘会带来较大的损耗和效率影响。所以引入了一个写缓冲区的概念，即每一个Map Task都拥有一个“环形缓冲区”作为Mapper输出的写缓冲区。写缓冲区大小默认为100MB（通过属性io.sort.mb调整），当写缓冲区的数据量达到一定的容量限额时（默认为80%，通过属性io.sort.spill.percent调整），后台线程开始将写缓冲区的数据溢写到本地磁盘。在数据溢写的过程中，只要写缓冲区没有被写满，Mappper依然可以将数据写出到缓冲区中；否则Mapper的执行过程将被阻塞，直到溢写结束。<br>溢写以循环的方式进行（即写缓冲区的数据量大致限额时就会执行溢写），可以通过属性mapred.local.dir指定写出的目录。<br>spill结束前 溢写线程将数据最终写出到本地磁盘之前，首先根据Reducer的数目对这部分数据进行分区（即每一个分区中的数据会被传送至同一个Reducer进行处理，分区数目与Reducer数据保持一致）即 <strong>partition</strong>，partition是一个类inputSplit()的操作，即根据有多少个ReduceTask生成多少个partition,并通过jobTracker指定给相应的ReduceTask。<br>当spill完成后，本地磁盘中会有多个溢出文件存在。在MapTask结束前，这些文件会根据相应的分区进行合并，并排序，合并可能发生多次，具体由<strong>io.sort.factor控制一次最多合并多少个文件。</strong></p>\n<p>如果溢写文件个数超过3（通过属性min.num.spills.for.combine设置），会对合并且分区排序后的结果执行Combine过程（如果MapReduce有设置Combiner），而且combine过程在不影响最终结果的前提下可能会被执行多次；否则不会执行Combine过程（相对而言，Combine开销过大）。</p>\n<p>注意：Map Task执行过程中，Combine可能出现在两个地方：写缓冲区溢写过程中、溢写文件合并过程中。</p>\n<p>注意：Mapper的一条输出结果（由key、value表示）写出到写缓冲区之前，已经提前计算好相应的分区信息，即分区的过程在数据写入写缓冲区之前就已经完成，溢写过程实际是写缓冲区数据排序的过程（先按分区排序，如果分区相同时，再按键值排序）。</p>\n<p>这里涉及到MapReduce的两个组件：Comparator、Partitioner。<br>(由于篇幅的原因，这里暂不引入对这两个组件的源码分析，和自定义方式，后面有机会则单开文章讨论)</p>\n<p>在将map输出结果作为reducTask中的输入时，会涉及到磁盘写入，网络传输等资源的限制，所以对出于节省资源的考虑，可以在对map的输出结果进行压缩。默认情况下，压缩是不被开启的，可以通过属性<strong>mapred.compress.map.output</strong>、<strong>mapred.map.output.compression.codec</strong>进行相应设置。</p>\n<p>当MapTask任务结束后，被指定的分区ReduceTask会立即开始执行，即开始拷贝对应MapTask分区中的输出结果。<br>Reduce Task的这个阶段被称为“Copy Phase”。Reduce Task拥有少量的线程用于并行地获取Map Tasks的输出结果，默认线程数为5，可以通过属性mapred.reduce.parallel.copies进行设置。<br>同样：如果Map Task的输出结果足够小，它会被拷贝至Reduce Task的缓冲区中；否则拷贝至磁盘。当缓冲区中的数据达到一定量（由属性mapred.job.shuffle.merge.percent、mapred.inmem.merge.threshold），这些数据将被合并且溢写到磁盘。如果Combine过程被指定，它将在合并过程被执行，用来减少需要写出到磁盘的数据量。</p>\n<p>随着拷贝文件中磁盘上的不断积累，一个后台线程会将它们合并为更大地、有序的文件，用来节省后期的合并时间。如果Map Tasks的输出结合使用了压缩机制，则在合并的过程中需要对数据进行解压处理。</p>\n<p>当Reduce Task的所有Map Tasks输出结果均完成拷贝，Reduce Task进入“Sort Phase”（更为合适地应该被称为“Merge Phase”，排序在Map阶段已经被执行），该阶段在保持原有顺序的情况下进行合并。这种合并是以循环方式进行的，循环次数与合并因子（io.sort.factor）有关。<br>sort phase 通常不是合并成一个文件，而是略过磁盘操作，直接将数据合并输入至Reduce方法中 这次合并的数据可以结合内存、磁盘两部分进行操作），即“Reduce Phase”。</p>\n<p>通常这里还有一个“Group”的阶段，这个阶段决定着哪些键值对属于同一个键。如果没有特殊设置，只有在Map Task输出时那些键完全一样的数据属于同一个键，但这是可以被改变的。</p>\n<p>描述至这里终于能引用mapreduce中相当重要的一个概念，即shuffle。这个词在这里该怎么定义，我暂未找到个一个比较满意的答案，但我比较喜欢有人把这个比作是搓完牌一桌子人，在下一局开始前的整个过程。<br>即Shuffle操作，涉及到数据的partition、sort、[combine]、spill、[comress]、[merge]、copy、[combine]、merge、group，而这些操作不但决定着程序逻辑的正确性，也决定着MapReduce的运行效率。</p>\n<p>shuffle完后进入了ReduceTask的reduce()方法中<br>在Reduce Phase的过程中，它处理的是所有Map Tasks输出结果中某一个分区中的所有数据，这些数据整体表现为一个根据键有序的输入，对于每一个键都会相应地调用一次Reduce Function（同一个键对应的值可能有多个，这些值将作为Reduce Function的参数）</p>\n<p>至此MapReduce的逻辑过程基本描述完成，虽然洋洋洒洒可能会有数千字，但本文的出发点就不是简析，而更多是自我概念原理部份的总结，所以力求整个流程完整详细。后面我配上一些网络图片，方便大家快速理解，结合文字加深印象。<br><img src=\"http://img.wqkenqing.ren//file/2017/10/b26c54ff10ad4e2b9832a960ef4aab90-image.png\" alt=\"b26c54ff10ad4e2b9832a960ef4aab90-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/a179f15b88cc403b8bd84d7963823762-image.png\" alt=\"a179f15b88cc403b8bd84d7963823762-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/4a5b5498dd764087ade380db394e6f84-image.png\" alt=\"4a5b5498dd764087ade380db394e6f84-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/74ed215c305747a49348430782e5636a-image.png\" alt=\"74ed215c305747a49348430782e5636a-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/b55be177bd514ce79b7444c2dd3ddcfb-image.png\" alt=\"b55be177bd514ce79b7444c2dd3ddcfb-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/286bb24da172471793924b2b9b7c857c-image.png\" alt=\"286bb24da172471793924b2b9b7c857c-image.png\"><br><img src=\"http://img.wqkenqing.ren//file/2017/10/68bbc5f114f3496d886402cbb0da8fc1-image.png\" alt=\"68bbc5f114f3496d886402cbb0da8fc1-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/38d8bf9498274367b22856f62a8f7fcf-image.png\" alt=\"38d8bf9498274367b22856f62a8f7fcf-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/7b7e5da815064fdfa0d060910f8dfb9b-image.png\" alt=\"7b7e5da815064fdfa0d060910f8dfb9b-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/a4ac8a6f758b4bdfb5e1a752a02f0654-image.png\" alt=\"a4ac8a6f758b4bdfb5e1a752a02f0654-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/9e91f8a6b0ab488abbad0714487fe10f-image.png\" alt=\"9e91f8a6b0ab488abbad0714487fe10f-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/87a4d55fff15408c87f94029b8fb2aea-image.png\" alt=\"87a4d55fff15408c87f94029b8fb2aea-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/eb099e53d6fa44a5b76410040f41f3ae-image.png\" alt=\"eb099e53d6fa44a5b76410040f41f3ae-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/98aac8b77be64d53b276f7a112aba12d-image.png\" alt=\"98aac8b77be64d53b276f7a112aba12d-image.png\"></p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<p><code>mapreduce在hadoop中更多的承担的是计算的角色</code></p>\n<h2 id=\"什么是MapReduce？\"><a href=\"#什么是MapReduce？\" class=\"headerlink\" title=\"什么是MapReduce？\"></a>什么是MapReduce？</h2><p>mapreduce源于谷歌公司为研究大规模数据处理而研发出的一种并行计算模型和方法。<br>它将并行编程中的难点和有相对门槛的方法进行高度封装，而给开发者一套接口，让开发者理专注于自己的业务逻辑，即可让自己的代码运行在分布式集群中，大大降低了开发一些并发程序的门槛。从字面上就可以知道，MapReduce分为Map(映射)、Reduce(规约)</p>\n<h2 id=\"MapReduce的核心思想\"><a href=\"#MapReduce的核心思想\" class=\"headerlink\" title=\"MapReduce的核心思想\"></a>MapReduce的核心思想</h2><p>MapReduce主要是两种经典函数：</p>\n<ul>\n<li>映射（Mapping）将一个整体按某种规则映射成N份。并对这N份进行同一种操作。</li>\n<li>规约（Reducing）将N份文件，按某种策略进行合并。</li>\n</ul>\n<h2 id=\"MapReduce的角色与动作\"><a href=\"#MapReduce的角色与动作\" class=\"headerlink\" title=\"MapReduce的角色与动作\"></a>MapReduce的角色与动作</h2><p>　 MapReduce包含四个组成部分，分别为<strong>Client</strong>、<strong>JobTracker</strong>、<strong>TaskTracker</strong>和<strong>Task</strong>，下面我们详细介绍这四个组成部分。</p>\n<ul>\n<li>Client：作业提交的发起者， 每一个 Job 都会在用户端通过 Client 类将应用程序以及配置参数 Configuration 打包成 JAR 文件存储在 HDFS，并把路径提交到 JobTracker 的 master 服务，然后由 master 创建每一个 Task（即 MapTask 和 ReduceTask） 将它们分发到各个 TaskTracker 服务中去执行。</li>\n<li>JobTracker：初始化作业，分配作业，与TaskTracker通信，协调整个作业 JobTracke负责资源监控和作业调度。JobTracker 监控所有TaskTracker 与job的健康状况，一旦发现失败，就将相应的任务转移到其他节点；同时，JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器，而调度器会在资源出现空闲时，选择合适的任务使用这些资源。在Hadoop 中，任务调度器是一个可插拔的模块，用户可以根据自己的需要设计相应的调度器。</li>\n<li>TaskTracker：TaskTracker 会周期性地通过Heartbeat 将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）。TaskTracker 使用“slot”等量划分本节点上的资源量。“slot”代表计算资源（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop 调度器的作用就是将各个TaskTracker 上的空闲slot 分配给Task 使用。slot 分为Map slot 和Reduce slot 两种，分别供Map Task 和Reduce Task 使用。TaskTracker 通过slot 数目（可配置参数）限定Task 的并发度。</li>\n<li>Task ： Task 分为Map Task 和Reduce Task 两种，均由TaskTracker 启动。HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。</li>\n<li>Map Task 执行过程如下图 所示：由该图可知，Map Task 先将对应的split 迭代解析成一个个key/value 对，依次调用用户 自定义的map() 函数进行处理，最终将临时结果存放到本地磁盘上, 其中临时数据被分成若干个partition，每个partition 将被一个Reduce Task 处理。<br><img src=\"http://img.wqkenqing.ren//file/2017/10/09d902f773ff4e30b1cf995705d4f03c-image.png\" alt=\"09d902f773ff4e30b1cf995705d4f03c-image.png\"></li>\n</ul>\n<pre><code>Reduce Task 执行过程下图所示。该过程分为三个阶段\n![2f9ff84d09cf424cb8a1c4d24db637c0-image.png](http://img.wqkenqing.ren//file/2017/10/2f9ff84d09cf424cb8a1c4d24db637c0-image.png)</code></pre><p><strong>提交作业</strong></p>\n<ul>\n<li>在作业提交之前，需要对作业进行配置</li>\n<li>程序代码，主要是自己书写的MapReduce程序</li>\n<li>输入输出路径</li>\n<li>其他配置，如输出压缩等</li>\n<li>配置完成后，通过JobClient来提交</li>\n</ul>\n<p><strong>作业的初始化</strong></p>\n<ul>\n<li>客户端提交完成后，JobTracker会将作业加入队列，然后进行调度，默认的调度方法是FIFO调试方式<br><strong>任务的分配</strong></li>\n<li>TaskTracker和JobTracker之间的通信与任务的分配是通过心跳机制完成的。</li>\n<li>TaskTracker会主动向JobTracker询问是否有作业要做，如果自己能做，那么就会申请作业任务，这个任务可以使Map，也可能是Reduce任务<br><strong>任务的执行</strong></li>\n<li>申请到任务后，TaskTracker会做如下事情：</li>\n<li>拷贝代码到本地</li>\n<li>拷贝任务的信息到本地</li>\n<li>启动Jvm运行任务<br><strong>状态与任务的更新</strong></li>\n<li>任务在运行过程中，首先会将自己的状态汇报给TaskTracker，然后由TaskTracker汇总报给JobTracker</li>\n<li>任务进度是通过计数器来实现的。</li>\n</ul>\n<p><strong>作业的完成</strong></p>\n<ul>\n<li>JobTracker 是在接受到最后一个任务运行完成后，才会将任务标志为成功</li>\n<li>此时会做删除中间结果等善后处理工作<br><img src=\"http://img.wqkenqing.ren//file/2017/10/8fba570574f6491c96b51698743466b2-image.png\" alt=\"8fba570574f6491c96b51698743466b2-image.png\"></li>\n</ul>\n<h3 id=\"MapReduce任务执行流程与数据处理流程详解\"><a href=\"#MapReduce任务执行流程与数据处理流程详解\" class=\"headerlink\" title=\"MapReduce任务执行流程与数据处理流程详解\"></a>MapReduce任务执行流程与数据处理流程详解</h3><p><code>就任务流程而言，上文中有些或已经涉及，有些也介绍的比较详细了。但就整体而言，不是特别具体，或这个整体性，所以在此我们再集中总结一下，即使重复内容，就也当加深印象吧。</code></p>\n<h4 id=\"任务执行流程详解\"><a href=\"#任务执行流程详解\" class=\"headerlink\" title=\"任务执行流程详解\"></a>任务执行流程详解</h4><ol>\n<li>通过jobClient提交当mapreduce的job提交至JobTracker<br>,提交的具体信息大致会有，conf配置内容，path，相关的Map，Reduce函数等。（数据的切片会在client上完成）</li>\n<li>JobTracker中的Master服务会完成创建一个Task（MapTask与ReduceTask），并将这个任务加载至任务队列中，等待TaskTracker来获取。同进JobTracker会处于一种监听状态，监听所有TaskTracker与Job的健康情况。面对故障时，提供服务转移等。同时它还会记录任务的执行进度，资源的使用情况，提交至任务调度器，由调度器进行资源的调度。</li>\n<li>TaskTracker会主动向jobTacker去询问是否有任务，如果有，就会申请到作业，作业可以是map,也可以是reduce任务。TaskTracker获取的不仅是任务，还有相关的处理代码也会copy一份至本地，然后TaskTracker再针对所分配的split进行处理。TaskTracker还会周期性地通过HearBeat将本节点上的资源的使用情况和任务的进行进度汇报给JobTracker，同时接收JobTracker发过来的相关命令并执行，</li>\n<li>当TaskTracker中的任务完成后会上报给JobTracker,而当最后一个TaskTracker完成后，JobTracker才会将任务标志为成功，并执行一些如删除中间结果等善后工作。</li>\n</ol>\n<hr>\n<p><code>从这里我们知道了mapreduce中任务执行流程，但mapreduce作为hadoop的计算框架，它对数据的处理流程我们还没明确涉及，所以接下来我们再深入了解下具体的数据处理流程</code></p>\n<h4 id=\"数据处理流程详解\"><a href=\"#数据处理流程详解\" class=\"headerlink\" title=\"数据处理流程详解\"></a>数据处理流程详解</h4><p>这里的数据处理流程，主要指的是mapreduce执行后，Task中对split的任务具体处理的这一流程，其中还包括，数据的切片，mapTask完成后将中间结果上传等动作。下面我们来具体讨论<br>当jobClient向JobTracker提交了任务后，数据处理流程也随之开始</p>\n<ul>\n<li>在Client端将输入源的数据进行切片(split)，具体的切片机制参考后面</li>\n<li>JobTracker中的MRAppMaster将每个Split信息计算出需要的MapTask的实例数量，然后向集群申请机器启动相应数量的mapTask进程</li>\n<li>进程启动后，根据给定的数据切片范围进行数据处理，主要流程为<br>a)通过inputFormat来获取RecordReader读取数据，并形成输入的KV对<br>b)将输入KV对传递给用户定义的map（）方法，做逻辑处理，并map()方法的输出的kv对手机到缓存中(这里用到了缓存机制)<br>简而言之map中输入时要做的事是<br>1.反射构造InputFormat.<br>2.反射构造InputSplit.<br>3.创建RecordReader.<br>4.反射创建MapperRunner</li>\n</ul>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/3a461434b0604118aad45a4d38cf3649-image.png\" alt=\"3a461434b0604118aad45a4d38cf3649-image.png\"></p>\n<p>而map输出时相对复杂，主要涉及到的有Partitioner，shuffle，sort，combiner等概念，我们就来一一讨论。<br>在map（）方法执行后，map阶段是会有处理的数据输出，正常来说，就是每个split对应的每一行。如上图MapRunner的next为false时，对输入数据的map完成，这时对存内中这些map的数据，会对其进行sort,如果我们事先设置的有combiner那么，还会对sort完的数据执行combiner(<strong>一个类reduce操作，不过是对本地数据的reduce，使用它的原则是combiner的输入不会影响到reduce计算的最终输入，例如：如果计算只是求总数，最大值，最小值可以使用combiner，但是做平均值计算使用combiner的话，最终的reduce计算结果就会出错。</strong>)，然后开始map的内容spill到磁盘中，如果频繁的spill对磁盘会带来较大的损耗和效率影响。所以引入了一个写缓冲区的概念，即每一个Map Task都拥有一个“环形缓冲区”作为Mapper输出的写缓冲区。写缓冲区大小默认为100MB（通过属性io.sort.mb调整），当写缓冲区的数据量达到一定的容量限额时（默认为80%，通过属性io.sort.spill.percent调整），后台线程开始将写缓冲区的数据溢写到本地磁盘。在数据溢写的过程中，只要写缓冲区没有被写满，Mappper依然可以将数据写出到缓冲区中；否则Mapper的执行过程将被阻塞，直到溢写结束。<br>溢写以循环的方式进行（即写缓冲区的数据量大致限额时就会执行溢写），可以通过属性mapred.local.dir指定写出的目录。<br>spill结束前 溢写线程将数据最终写出到本地磁盘之前，首先根据Reducer的数目对这部分数据进行分区（即每一个分区中的数据会被传送至同一个Reducer进行处理，分区数目与Reducer数据保持一致）即 <strong>partition</strong>，partition是一个类inputSplit()的操作，即根据有多少个ReduceTask生成多少个partition,并通过jobTracker指定给相应的ReduceTask。<br>当spill完成后，本地磁盘中会有多个溢出文件存在。在MapTask结束前，这些文件会根据相应的分区进行合并，并排序，合并可能发生多次，具体由<strong>io.sort.factor控制一次最多合并多少个文件。</strong></p>\n<p>如果溢写文件个数超过3（通过属性min.num.spills.for.combine设置），会对合并且分区排序后的结果执行Combine过程（如果MapReduce有设置Combiner），而且combine过程在不影响最终结果的前提下可能会被执行多次；否则不会执行Combine过程（相对而言，Combine开销过大）。</p>\n<p>注意：Map Task执行过程中，Combine可能出现在两个地方：写缓冲区溢写过程中、溢写文件合并过程中。</p>\n<p>注意：Mapper的一条输出结果（由key、value表示）写出到写缓冲区之前，已经提前计算好相应的分区信息，即分区的过程在数据写入写缓冲区之前就已经完成，溢写过程实际是写缓冲区数据排序的过程（先按分区排序，如果分区相同时，再按键值排序）。</p>\n<p>这里涉及到MapReduce的两个组件：Comparator、Partitioner。<br>(由于篇幅的原因，这里暂不引入对这两个组件的源码分析，和自定义方式，后面有机会则单开文章讨论)</p>\n<p>在将map输出结果作为reducTask中的输入时，会涉及到磁盘写入，网络传输等资源的限制，所以对出于节省资源的考虑，可以在对map的输出结果进行压缩。默认情况下，压缩是不被开启的，可以通过属性<strong>mapred.compress.map.output</strong>、<strong>mapred.map.output.compression.codec</strong>进行相应设置。</p>\n<p>当MapTask任务结束后，被指定的分区ReduceTask会立即开始执行，即开始拷贝对应MapTask分区中的输出结果。<br>Reduce Task的这个阶段被称为“Copy Phase”。Reduce Task拥有少量的线程用于并行地获取Map Tasks的输出结果，默认线程数为5，可以通过属性mapred.reduce.parallel.copies进行设置。<br>同样：如果Map Task的输出结果足够小，它会被拷贝至Reduce Task的缓冲区中；否则拷贝至磁盘。当缓冲区中的数据达到一定量（由属性mapred.job.shuffle.merge.percent、mapred.inmem.merge.threshold），这些数据将被合并且溢写到磁盘。如果Combine过程被指定，它将在合并过程被执行，用来减少需要写出到磁盘的数据量。</p>\n<p>随着拷贝文件中磁盘上的不断积累，一个后台线程会将它们合并为更大地、有序的文件，用来节省后期的合并时间。如果Map Tasks的输出结合使用了压缩机制，则在合并的过程中需要对数据进行解压处理。</p>\n<p>当Reduce Task的所有Map Tasks输出结果均完成拷贝，Reduce Task进入“Sort Phase”（更为合适地应该被称为“Merge Phase”，排序在Map阶段已经被执行），该阶段在保持原有顺序的情况下进行合并。这种合并是以循环方式进行的，循环次数与合并因子（io.sort.factor）有关。<br>sort phase 通常不是合并成一个文件，而是略过磁盘操作，直接将数据合并输入至Reduce方法中 这次合并的数据可以结合内存、磁盘两部分进行操作），即“Reduce Phase”。</p>\n<p>通常这里还有一个“Group”的阶段，这个阶段决定着哪些键值对属于同一个键。如果没有特殊设置，只有在Map Task输出时那些键完全一样的数据属于同一个键，但这是可以被改变的。</p>\n<p>描述至这里终于能引用mapreduce中相当重要的一个概念，即shuffle。这个词在这里该怎么定义，我暂未找到个一个比较满意的答案，但我比较喜欢有人把这个比作是搓完牌一桌子人，在下一局开始前的整个过程。<br>即Shuffle操作，涉及到数据的partition、sort、[combine]、spill、[comress]、[merge]、copy、[combine]、merge、group，而这些操作不但决定着程序逻辑的正确性，也决定着MapReduce的运行效率。</p>\n<p>shuffle完后进入了ReduceTask的reduce()方法中<br>在Reduce Phase的过程中，它处理的是所有Map Tasks输出结果中某一个分区中的所有数据，这些数据整体表现为一个根据键有序的输入，对于每一个键都会相应地调用一次Reduce Function（同一个键对应的值可能有多个，这些值将作为Reduce Function的参数）</p>\n<p>至此MapReduce的逻辑过程基本描述完成，虽然洋洋洒洒可能会有数千字，但本文的出发点就不是简析，而更多是自我概念原理部份的总结，所以力求整个流程完整详细。后面我配上一些网络图片，方便大家快速理解，结合文字加深印象。<br><img src=\"http://img.wqkenqing.ren//file/2017/10/b26c54ff10ad4e2b9832a960ef4aab90-image.png\" alt=\"b26c54ff10ad4e2b9832a960ef4aab90-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/a179f15b88cc403b8bd84d7963823762-image.png\" alt=\"a179f15b88cc403b8bd84d7963823762-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/4a5b5498dd764087ade380db394e6f84-image.png\" alt=\"4a5b5498dd764087ade380db394e6f84-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/74ed215c305747a49348430782e5636a-image.png\" alt=\"74ed215c305747a49348430782e5636a-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/b55be177bd514ce79b7444c2dd3ddcfb-image.png\" alt=\"b55be177bd514ce79b7444c2dd3ddcfb-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/286bb24da172471793924b2b9b7c857c-image.png\" alt=\"286bb24da172471793924b2b9b7c857c-image.png\"><br><img src=\"http://img.wqkenqing.ren//file/2017/10/68bbc5f114f3496d886402cbb0da8fc1-image.png\" alt=\"68bbc5f114f3496d886402cbb0da8fc1-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/38d8bf9498274367b22856f62a8f7fcf-image.png\" alt=\"38d8bf9498274367b22856f62a8f7fcf-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/7b7e5da815064fdfa0d060910f8dfb9b-image.png\" alt=\"7b7e5da815064fdfa0d060910f8dfb9b-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/a4ac8a6f758b4bdfb5e1a752a02f0654-image.png\" alt=\"a4ac8a6f758b4bdfb5e1a752a02f0654-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/9e91f8a6b0ab488abbad0714487fe10f-image.png\" alt=\"9e91f8a6b0ab488abbad0714487fe10f-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/87a4d55fff15408c87f94029b8fb2aea-image.png\" alt=\"87a4d55fff15408c87f94029b8fb2aea-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/eb099e53d6fa44a5b76410040f41f3ae-image.png\" alt=\"eb099e53d6fa44a5b76410040f41f3ae-image.png\"></p>\n<p><img src=\"http://img.wqkenqing.ren//file/2017/10/98aac8b77be64d53b276f7a112aba12d-image.png\" alt=\"98aac8b77be64d53b276f7a112aba12d-image.png\"></p>"},{"title":"List学习","date":"2019-07-15T16:00:00.000Z","_content":"此处简介\n<!--more-->\n\n### List学习\n `ArrayList不是线程安全的，只能用在单线程环境下，多线程环境下可以考虑用Collections.synchronizedList(List l)函数返回一个线程安全的ArrayList类，也可以使用concurrent并发包下的CopyOnWriteArrayList类`\n\nArrayList实现了Serializable接口，因此它支持序列化，能够通过序列化传输，实现了RandomAccess接口，支持快速随机访问，实际上就是通过下标序号进行快速访问，实现了Cloneable接口，能被克隆。\n\n每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。\n每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。\n从上述代码中可以看出，数组进行扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量的增长大约是其原容量的1.5倍。这种操作的代价是很高的，因此在实际使用时，我们应该尽量避免数组容量的扩张。当我们可预知要保存的元素的多少时，要在构造ArrayList实例时，就指定其容量，以避免数组扩容的发生。或者根据实际需求，通过调用ensureCapacity方法来手动增加ArrayList实例的容量。\n\nObject oldData[] = elementData;//为什么要用到oldData[]\n乍一看来后面并没有用到关于oldData， 这句话显得多此一举！但是这是一个牵涉到内存管理的类， 所以要了解内部的问题。 而且为什么这一句还在if的内部，这跟elementData = Arrays.copyOf(elementData, newCapacity); 这句是有关系的，下面这句Arrays.copyOf的实现时新创建了newCapacity大小的内存，然后把老的elementData放入。好像也没有用到oldData，有什么问题呢。问题就在于旧的内存的引用是elementData， elementData指向了新的内存块，如果有一个局部变量oldData变量引用旧的内存块的话，在copy的过程中就会比较安全，因为这样证明这块老的内存依然有引用，分配内存的时候就不会被侵占掉，然后copy完成后这个局部变量的生命期也过去了，然后释放才是安全的。不然在copy的的时候万一新的内存或其他线程的分配内存侵占了这块老的内存，而copy还没有结束，这将是个严重的事情。 关于ArrayList和Vector区别如下：\n\n    ArrayList在内存不够时默认是扩展50% + 1个，Vector是默认扩展1倍。\n    Vector提供indexOf(obj, start)接口，ArrayList没有。\n    Vector属于线程安全级别的，但是大多数情况下不使用Vector，因为线程安全需要更大的系统开销。\n\n\n\n ArrayList还给我们提供了将底层数组的容量调整为当前列表保存的实际元素的大小的功能。它可以通过trimToSize方法来实现。代码如下：\n\n关于ArrayList的源码，给出几点比较重要的总结：\n\n    1、注意其三个不同的构造方法。无参构造方法构造的ArrayList的容量默认为10，带有Collection参数的构造方法，将Collection转化为数组赋给ArrayList的实现数组elementData。\n\n    2、注意扩充容量的方法ensureCapacity。ArrayList在每次增加元素（可能是1个，也可能是一组）时，都要调用该方法来确保足够的容量。当容量不足以容纳当前的元素个数时，就设置新的容量为旧的容量的1.5倍加1，如果设置后的新容量还不够，则直接新容量设置为传入的参数（也就是所需的容量），而后用Arrays.copyof()方法将元素拷贝到新的数组（详见下面的第3点）。从中可以看出，当容量不够时，每次增加元素，都要将原来的元素拷贝到一个新的数组中，非常之耗时，也因此建议在事先能确定元素数量的情况下，才使用ArrayList，否则建议使用LinkedList。\n\n    3、ArrayList的实现中大量地调用了Arrays.copyof()和System.arraycopy()方法。我们有必要对这两个方法的实现做下深入的了解。\n\nhttp://www.cnblogs.com/ITtangtang/p/3948555.html\n\n\n. LinkedList是用链表结构存储数据的，很适合数据的动态插入和删除，随机访问和遍历速度比较慢。另外，接口中没有定义的方法get，remove，insertList，专门用于操作表头和表尾元素，可以当作堆栈、队列和双向队列使用。LinkedList没有同步方法。如果多个线程同时访问一个List，则必须自己实现访问同步。一种解决方法是在创建 List时构造一个同步的List：\n             List list = Collections.synchronizedList(new LinkedList(...));\n\n查看Java源代码，发现当数组的大小不够的时候，\n需要重新建立数组，然后将元素拷贝到新的数组内，ArrayList和Vector的扩展数组的大小不同。\n\nhttp://www.tuicool.com/articles/iQZBFb\n\n\nhttp://www.cnblogs.com/azai/archive/2010/12/09/1901272.html\n\n---\n#### 简化\n+ Collections.synchronizedList(List l)\n+ Serializable 、RandomAccess 、Cloneable\n+ ensureCapacity\n\n  从上面的源码剖析可以看出这三种List实现的一些典型适用场景，如果经常对数组做随机插入操作，特别是插入的比较靠前，那么LinkedList的性能优势就非常明显，而如果都只是末尾插入，则ArrayList更占据优势，如果需要线程安全，则使用Vector或者创建线程安全的ArrayList。\n\n在使用基于数组实现的ArrayList 和Vector 时我们要指定初始容量，因为我们在源码中也看到了，在添加时首先要进行容量的判断，如果容量不够则要创建新数组，还要将原来数组中的数据复制到新数组中，这个过程会减低效率并且会浪费资源。","source":"_posts/技术/hexo/oldblog/blog4.md","raw":"---\n\ntitle: List学习\ndate: 2019-07-16\ntags:\n\n---\n此处简介\n<!--more-->\n\n### List学习\n `ArrayList不是线程安全的，只能用在单线程环境下，多线程环境下可以考虑用Collections.synchronizedList(List l)函数返回一个线程安全的ArrayList类，也可以使用concurrent并发包下的CopyOnWriteArrayList类`\n\nArrayList实现了Serializable接口，因此它支持序列化，能够通过序列化传输，实现了RandomAccess接口，支持快速随机访问，实际上就是通过下标序号进行快速访问，实现了Cloneable接口，能被克隆。\n\n每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。\n每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。\n从上述代码中可以看出，数组进行扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量的增长大约是其原容量的1.5倍。这种操作的代价是很高的，因此在实际使用时，我们应该尽量避免数组容量的扩张。当我们可预知要保存的元素的多少时，要在构造ArrayList实例时，就指定其容量，以避免数组扩容的发生。或者根据实际需求，通过调用ensureCapacity方法来手动增加ArrayList实例的容量。\n\nObject oldData[] = elementData;//为什么要用到oldData[]\n乍一看来后面并没有用到关于oldData， 这句话显得多此一举！但是这是一个牵涉到内存管理的类， 所以要了解内部的问题。 而且为什么这一句还在if的内部，这跟elementData = Arrays.copyOf(elementData, newCapacity); 这句是有关系的，下面这句Arrays.copyOf的实现时新创建了newCapacity大小的内存，然后把老的elementData放入。好像也没有用到oldData，有什么问题呢。问题就在于旧的内存的引用是elementData， elementData指向了新的内存块，如果有一个局部变量oldData变量引用旧的内存块的话，在copy的过程中就会比较安全，因为这样证明这块老的内存依然有引用，分配内存的时候就不会被侵占掉，然后copy完成后这个局部变量的生命期也过去了，然后释放才是安全的。不然在copy的的时候万一新的内存或其他线程的分配内存侵占了这块老的内存，而copy还没有结束，这将是个严重的事情。 关于ArrayList和Vector区别如下：\n\n    ArrayList在内存不够时默认是扩展50% + 1个，Vector是默认扩展1倍。\n    Vector提供indexOf(obj, start)接口，ArrayList没有。\n    Vector属于线程安全级别的，但是大多数情况下不使用Vector，因为线程安全需要更大的系统开销。\n\n\n\n ArrayList还给我们提供了将底层数组的容量调整为当前列表保存的实际元素的大小的功能。它可以通过trimToSize方法来实现。代码如下：\n\n关于ArrayList的源码，给出几点比较重要的总结：\n\n    1、注意其三个不同的构造方法。无参构造方法构造的ArrayList的容量默认为10，带有Collection参数的构造方法，将Collection转化为数组赋给ArrayList的实现数组elementData。\n\n    2、注意扩充容量的方法ensureCapacity。ArrayList在每次增加元素（可能是1个，也可能是一组）时，都要调用该方法来确保足够的容量。当容量不足以容纳当前的元素个数时，就设置新的容量为旧的容量的1.5倍加1，如果设置后的新容量还不够，则直接新容量设置为传入的参数（也就是所需的容量），而后用Arrays.copyof()方法将元素拷贝到新的数组（详见下面的第3点）。从中可以看出，当容量不够时，每次增加元素，都要将原来的元素拷贝到一个新的数组中，非常之耗时，也因此建议在事先能确定元素数量的情况下，才使用ArrayList，否则建议使用LinkedList。\n\n    3、ArrayList的实现中大量地调用了Arrays.copyof()和System.arraycopy()方法。我们有必要对这两个方法的实现做下深入的了解。\n\nhttp://www.cnblogs.com/ITtangtang/p/3948555.html\n\n\n. LinkedList是用链表结构存储数据的，很适合数据的动态插入和删除，随机访问和遍历速度比较慢。另外，接口中没有定义的方法get，remove，insertList，专门用于操作表头和表尾元素，可以当作堆栈、队列和双向队列使用。LinkedList没有同步方法。如果多个线程同时访问一个List，则必须自己实现访问同步。一种解决方法是在创建 List时构造一个同步的List：\n             List list = Collections.synchronizedList(new LinkedList(...));\n\n查看Java源代码，发现当数组的大小不够的时候，\n需要重新建立数组，然后将元素拷贝到新的数组内，ArrayList和Vector的扩展数组的大小不同。\n\nhttp://www.tuicool.com/articles/iQZBFb\n\n\nhttp://www.cnblogs.com/azai/archive/2010/12/09/1901272.html\n\n---\n#### 简化\n+ Collections.synchronizedList(List l)\n+ Serializable 、RandomAccess 、Cloneable\n+ ensureCapacity\n\n  从上面的源码剖析可以看出这三种List实现的一些典型适用场景，如果经常对数组做随机插入操作，特别是插入的比较靠前，那么LinkedList的性能优势就非常明显，而如果都只是末尾插入，则ArrayList更占据优势，如果需要线程安全，则使用Vector或者创建线程安全的ArrayList。\n\n在使用基于数组实现的ArrayList 和Vector 时我们要指定初始容量，因为我们在源码中也看到了，在添加时首先要进行容量的判断，如果容量不够则要创建新数组，还要将原来数组中的数据复制到新数组中，这个过程会减低效率并且会浪费资源。","slug":"技术/hexo/oldblog/blog4","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33i003638pw66ryf8jn","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h3 id=\"List学习\"><a href=\"#List学习\" class=\"headerlink\" title=\"List学习\"></a>List学习</h3><p> <code>ArrayList不是线程安全的，只能用在单线程环境下，多线程环境下可以考虑用Collections.synchronizedList(List l)函数返回一个线程安全的ArrayList类，也可以使用concurrent并发包下的CopyOnWriteArrayList类</code></p>\n<p>ArrayList实现了Serializable接口，因此它支持序列化，能够通过序列化传输，实现了RandomAccess接口，支持快速随机访问，实际上就是通过下标序号进行快速访问，实现了Cloneable接口，能被克隆。</p>\n<p>每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。<br>每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。<br>从上述代码中可以看出，数组进行扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量的增长大约是其原容量的1.5倍。这种操作的代价是很高的，因此在实际使用时，我们应该尽量避免数组容量的扩张。当我们可预知要保存的元素的多少时，要在构造ArrayList实例时，就指定其容量，以避免数组扩容的发生。或者根据实际需求，通过调用ensureCapacity方法来手动增加ArrayList实例的容量。</p>\n<p>Object oldData[] = elementData;//为什么要用到oldData[]<br>乍一看来后面并没有用到关于oldData， 这句话显得多此一举！但是这是一个牵涉到内存管理的类， 所以要了解内部的问题。 而且为什么这一句还在if的内部，这跟elementData = Arrays.copyOf(elementData, newCapacity); 这句是有关系的，下面这句Arrays.copyOf的实现时新创建了newCapacity大小的内存，然后把老的elementData放入。好像也没有用到oldData，有什么问题呢。问题就在于旧的内存的引用是elementData， elementData指向了新的内存块，如果有一个局部变量oldData变量引用旧的内存块的话，在copy的过程中就会比较安全，因为这样证明这块老的内存依然有引用，分配内存的时候就不会被侵占掉，然后copy完成后这个局部变量的生命期也过去了，然后释放才是安全的。不然在copy的的时候万一新的内存或其他线程的分配内存侵占了这块老的内存，而copy还没有结束，这将是个严重的事情。 关于ArrayList和Vector区别如下：</p>\n<pre><code>ArrayList在内存不够时默认是扩展50% + 1个，Vector是默认扩展1倍。\nVector提供indexOf(obj, start)接口，ArrayList没有。\nVector属于线程安全级别的，但是大多数情况下不使用Vector，因为线程安全需要更大的系统开销。</code></pre><p> ArrayList还给我们提供了将底层数组的容量调整为当前列表保存的实际元素的大小的功能。它可以通过trimToSize方法来实现。代码如下：</p>\n<p>关于ArrayList的源码，给出几点比较重要的总结：</p>\n<pre><code>1、注意其三个不同的构造方法。无参构造方法构造的ArrayList的容量默认为10，带有Collection参数的构造方法，将Collection转化为数组赋给ArrayList的实现数组elementData。\n\n2、注意扩充容量的方法ensureCapacity。ArrayList在每次增加元素（可能是1个，也可能是一组）时，都要调用该方法来确保足够的容量。当容量不足以容纳当前的元素个数时，就设置新的容量为旧的容量的1.5倍加1，如果设置后的新容量还不够，则直接新容量设置为传入的参数（也就是所需的容量），而后用Arrays.copyof()方法将元素拷贝到新的数组（详见下面的第3点）。从中可以看出，当容量不够时，每次增加元素，都要将原来的元素拷贝到一个新的数组中，非常之耗时，也因此建议在事先能确定元素数量的情况下，才使用ArrayList，否则建议使用LinkedList。\n\n3、ArrayList的实现中大量地调用了Arrays.copyof()和System.arraycopy()方法。我们有必要对这两个方法的实现做下深入的了解。</code></pre><p><a href=\"http://www.cnblogs.com/ITtangtang/p/3948555.html\" target=\"_blank\" rel=\"noopener\">http://www.cnblogs.com/ITtangtang/p/3948555.html</a></p>\n<p>. LinkedList是用链表结构存储数据的，很适合数据的动态插入和删除，随机访问和遍历速度比较慢。另外，接口中没有定义的方法get，remove，insertList，专门用于操作表头和表尾元素，可以当作堆栈、队列和双向队列使用。LinkedList没有同步方法。如果多个线程同时访问一个List，则必须自己实现访问同步。一种解决方法是在创建 List时构造一个同步的List：<br>             List list = Collections.synchronizedList(new LinkedList(…));</p>\n<p>查看Java源代码，发现当数组的大小不够的时候，<br>需要重新建立数组，然后将元素拷贝到新的数组内，ArrayList和Vector的扩展数组的大小不同。</p>\n<p><a href=\"http://www.tuicool.com/articles/iQZBFb\" target=\"_blank\" rel=\"noopener\">http://www.tuicool.com/articles/iQZBFb</a></p>\n<p><a href=\"http://www.cnblogs.com/azai/archive/2010/12/09/1901272.html\" target=\"_blank\" rel=\"noopener\">http://www.cnblogs.com/azai/archive/2010/12/09/1901272.html</a></p>\n<hr>\n<h4 id=\"简化\"><a href=\"#简化\" class=\"headerlink\" title=\"简化\"></a>简化</h4><ul>\n<li><p>Collections.synchronizedList(List l)</p>\n</li>\n<li><p>Serializable 、RandomAccess 、Cloneable</p>\n</li>\n<li><p>ensureCapacity</p>\n<p>从上面的源码剖析可以看出这三种List实现的一些典型适用场景，如果经常对数组做随机插入操作，特别是插入的比较靠前，那么LinkedList的性能优势就非常明显，而如果都只是末尾插入，则ArrayList更占据优势，如果需要线程安全，则使用Vector或者创建线程安全的ArrayList。</p>\n</li>\n</ul>\n<p>在使用基于数组实现的ArrayList 和Vector 时我们要指定初始容量，因为我们在源码中也看到了，在添加时首先要进行容量的判断，如果容量不够则要创建新数组，还要将原来数组中的数据复制到新数组中，这个过程会减低效率并且会浪费资源。</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h3 id=\"List学习\"><a href=\"#List学习\" class=\"headerlink\" title=\"List学习\"></a>List学习</h3><p> <code>ArrayList不是线程安全的，只能用在单线程环境下，多线程环境下可以考虑用Collections.synchronizedList(List l)函数返回一个线程安全的ArrayList类，也可以使用concurrent并发包下的CopyOnWriteArrayList类</code></p>\n<p>ArrayList实现了Serializable接口，因此它支持序列化，能够通过序列化传输，实现了RandomAccess接口，支持快速随机访问，实际上就是通过下标序号进行快速访问，实现了Cloneable接口，能被克隆。</p>\n<p>每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。<br>每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。它总是至少等于列表的大小。随着向ArrayList中不断添加元素，其容量也自动增长。自动增长会带来数据向新数组的重新拷贝，因此，如果可预知数据量的多少，可在构造ArrayList时指定其容量。在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。<br>从上述代码中可以看出，数组进行扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量的增长大约是其原容量的1.5倍。这种操作的代价是很高的，因此在实际使用时，我们应该尽量避免数组容量的扩张。当我们可预知要保存的元素的多少时，要在构造ArrayList实例时，就指定其容量，以避免数组扩容的发生。或者根据实际需求，通过调用ensureCapacity方法来手动增加ArrayList实例的容量。</p>\n<p>Object oldData[] = elementData;//为什么要用到oldData[]<br>乍一看来后面并没有用到关于oldData， 这句话显得多此一举！但是这是一个牵涉到内存管理的类， 所以要了解内部的问题。 而且为什么这一句还在if的内部，这跟elementData = Arrays.copyOf(elementData, newCapacity); 这句是有关系的，下面这句Arrays.copyOf的实现时新创建了newCapacity大小的内存，然后把老的elementData放入。好像也没有用到oldData，有什么问题呢。问题就在于旧的内存的引用是elementData， elementData指向了新的内存块，如果有一个局部变量oldData变量引用旧的内存块的话，在copy的过程中就会比较安全，因为这样证明这块老的内存依然有引用，分配内存的时候就不会被侵占掉，然后copy完成后这个局部变量的生命期也过去了，然后释放才是安全的。不然在copy的的时候万一新的内存或其他线程的分配内存侵占了这块老的内存，而copy还没有结束，这将是个严重的事情。 关于ArrayList和Vector区别如下：</p>\n<pre><code>ArrayList在内存不够时默认是扩展50% + 1个，Vector是默认扩展1倍。\nVector提供indexOf(obj, start)接口，ArrayList没有。\nVector属于线程安全级别的，但是大多数情况下不使用Vector，因为线程安全需要更大的系统开销。</code></pre><p> ArrayList还给我们提供了将底层数组的容量调整为当前列表保存的实际元素的大小的功能。它可以通过trimToSize方法来实现。代码如下：</p>\n<p>关于ArrayList的源码，给出几点比较重要的总结：</p>\n<pre><code>1、注意其三个不同的构造方法。无参构造方法构造的ArrayList的容量默认为10，带有Collection参数的构造方法，将Collection转化为数组赋给ArrayList的实现数组elementData。\n\n2、注意扩充容量的方法ensureCapacity。ArrayList在每次增加元素（可能是1个，也可能是一组）时，都要调用该方法来确保足够的容量。当容量不足以容纳当前的元素个数时，就设置新的容量为旧的容量的1.5倍加1，如果设置后的新容量还不够，则直接新容量设置为传入的参数（也就是所需的容量），而后用Arrays.copyof()方法将元素拷贝到新的数组（详见下面的第3点）。从中可以看出，当容量不够时，每次增加元素，都要将原来的元素拷贝到一个新的数组中，非常之耗时，也因此建议在事先能确定元素数量的情况下，才使用ArrayList，否则建议使用LinkedList。\n\n3、ArrayList的实现中大量地调用了Arrays.copyof()和System.arraycopy()方法。我们有必要对这两个方法的实现做下深入的了解。</code></pre><p><a href=\"http://www.cnblogs.com/ITtangtang/p/3948555.html\" target=\"_blank\" rel=\"noopener\">http://www.cnblogs.com/ITtangtang/p/3948555.html</a></p>\n<p>. LinkedList是用链表结构存储数据的，很适合数据的动态插入和删除，随机访问和遍历速度比较慢。另外，接口中没有定义的方法get，remove，insertList，专门用于操作表头和表尾元素，可以当作堆栈、队列和双向队列使用。LinkedList没有同步方法。如果多个线程同时访问一个List，则必须自己实现访问同步。一种解决方法是在创建 List时构造一个同步的List：<br>             List list = Collections.synchronizedList(new LinkedList(…));</p>\n<p>查看Java源代码，发现当数组的大小不够的时候，<br>需要重新建立数组，然后将元素拷贝到新的数组内，ArrayList和Vector的扩展数组的大小不同。</p>\n<p><a href=\"http://www.tuicool.com/articles/iQZBFb\" target=\"_blank\" rel=\"noopener\">http://www.tuicool.com/articles/iQZBFb</a></p>\n<p><a href=\"http://www.cnblogs.com/azai/archive/2010/12/09/1901272.html\" target=\"_blank\" rel=\"noopener\">http://www.cnblogs.com/azai/archive/2010/12/09/1901272.html</a></p>\n<hr>\n<h4 id=\"简化\"><a href=\"#简化\" class=\"headerlink\" title=\"简化\"></a>简化</h4><ul>\n<li><p>Collections.synchronizedList(List l)</p>\n</li>\n<li><p>Serializable 、RandomAccess 、Cloneable</p>\n</li>\n<li><p>ensureCapacity</p>\n<p>从上面的源码剖析可以看出这三种List实现的一些典型适用场景，如果经常对数组做随机插入操作，特别是插入的比较靠前，那么LinkedList的性能优势就非常明显，而如果都只是末尾插入，则ArrayList更占据优势，如果需要线程安全，则使用Vector或者创建线程安全的ArrayList。</p>\n</li>\n</ul>\n<p>在使用基于数组实现的ArrayList 和Vector 时我们要指定初始容量，因为我们在源码中也看到了，在添加时首先要进行容量的判断，如果容量不够则要创建新数组，还要将原来数组中的数据复制到新数组中，这个过程会减低效率并且会浪费资源。</p>"},{"title":"反射小结","date":"2019-07-15T16:00:00.000Z","_content":"\n此处简介\n\n<!--more-->\n\n## 反射小结\n### 什么是反射\n反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为java语言的反射机制。\n`对于反射我的理解是：正java的核心思想，万物对象，那么类的类也可被描述，如类也会有名，属性，方法的特性。皆是对类的类型的一种描述。在jvm中反射机制的存在让java也拥有了动态的特性，即我们可以在运行时才确定类的类型，即相应的方法与属性`\n\n### 反射的功能\n* 在运行时获取其类的相关特性，如名称，属性，方法\n* 可以构造一个类的对象\n* 动态代理\n\n### 反射的三种形式\n\n1. Class.forName(\" \");通过Class的静态方法加载相应的相对路径，获得指定的类的类型\n2. 类.class获取类的类型\n3. 实例.getClass();\n\n`要访问私有属性，要设置setAccessible`\nClass cls ~~\nField file =cls.getDeclaredFields();","source":"_posts/技术/hexo/oldblog/blog6.md","raw":"---\n\ntitle: 反射小结\ndate: 2019-07-16\ntags:\n\n---\n\n此处简介\n\n<!--more-->\n\n## 反射小结\n### 什么是反射\n反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为java语言的反射机制。\n`对于反射我的理解是：正java的核心思想，万物对象，那么类的类也可被描述，如类也会有名，属性，方法的特性。皆是对类的类型的一种描述。在jvm中反射机制的存在让java也拥有了动态的特性，即我们可以在运行时才确定类的类型，即相应的方法与属性`\n\n### 反射的功能\n* 在运行时获取其类的相关特性，如名称，属性，方法\n* 可以构造一个类的对象\n* 动态代理\n\n### 反射的三种形式\n\n1. Class.forName(\" \");通过Class的静态方法加载相应的相对路径，获得指定的类的类型\n2. 类.class获取类的类型\n3. 实例.getClass();\n\n`要访问私有属性，要设置setAccessible`\nClass cls ~~\nField file =cls.getDeclaredFields();","slug":"技术/hexo/oldblog/blog6","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33j003738pwbb725wrz","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h2 id=\"反射小结\"><a href=\"#反射小结\" class=\"headerlink\" title=\"反射小结\"></a>反射小结</h2><h3 id=\"什么是反射\"><a href=\"#什么是反射\" class=\"headerlink\" title=\"什么是反射\"></a>什么是反射</h3><p>反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为java语言的反射机制。<br><code>对于反射我的理解是：正java的核心思想，万物对象，那么类的类也可被描述，如类也会有名，属性，方法的特性。皆是对类的类型的一种描述。在jvm中反射机制的存在让java也拥有了动态的特性，即我们可以在运行时才确定类的类型，即相应的方法与属性</code></p>\n<h3 id=\"反射的功能\"><a href=\"#反射的功能\" class=\"headerlink\" title=\"反射的功能\"></a>反射的功能</h3><ul>\n<li>在运行时获取其类的相关特性，如名称，属性，方法</li>\n<li>可以构造一个类的对象</li>\n<li>动态代理</li>\n</ul>\n<h3 id=\"反射的三种形式\"><a href=\"#反射的三种形式\" class=\"headerlink\" title=\"反射的三种形式\"></a>反射的三种形式</h3><ol>\n<li>Class.forName(“ “);通过Class的静态方法加载相应的相对路径，获得指定的类的类型</li>\n<li>类.class获取类的类型</li>\n<li>实例.getClass();</li>\n</ol>\n<p><code>要访问私有属性，要设置setAccessible</code><br>Class cls ~~<br>Field file =cls.getDeclaredFields();</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h2 id=\"反射小结\"><a href=\"#反射小结\" class=\"headerlink\" title=\"反射小结\"></a>反射小结</h2><h3 id=\"什么是反射\"><a href=\"#什么是反射\" class=\"headerlink\" title=\"什么是反射\"></a>什么是反射</h3><p>反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为java语言的反射机制。<br><code>对于反射我的理解是：正java的核心思想，万物对象，那么类的类也可被描述，如类也会有名，属性，方法的特性。皆是对类的类型的一种描述。在jvm中反射机制的存在让java也拥有了动态的特性，即我们可以在运行时才确定类的类型，即相应的方法与属性</code></p>\n<h3 id=\"反射的功能\"><a href=\"#反射的功能\" class=\"headerlink\" title=\"反射的功能\"></a>反射的功能</h3><ul>\n<li>在运行时获取其类的相关特性，如名称，属性，方法</li>\n<li>可以构造一个类的对象</li>\n<li>动态代理</li>\n</ul>\n<h3 id=\"反射的三种形式\"><a href=\"#反射的三种形式\" class=\"headerlink\" title=\"反射的三种形式\"></a>反射的三种形式</h3><ol>\n<li>Class.forName(“ “);通过Class的静态方法加载相应的相对路径，获得指定的类的类型</li>\n<li>类.class获取类的类型</li>\n<li>实例.getClass();</li>\n</ol>\n<p><code>要访问私有属性，要设置setAccessible</code><br>Class cls ~~<br>Field file =cls.getDeclaredFields();</p>"},{"title":"爬虫之nutch","date":"2019-07-15T16:00:00.000Z","_content":"此处简介\n<!--more-->\n\n `这阵子主要研究的爬虫方向，主要以java为语言基础,nutch为自动框架，jsoup作为自主爬虫插件开发基础，进行了一些有针对性的实站，在这些过程中，也遇到了一些问题和心得，觉得有必要总结一下`\n\n## 爬虫之nutch\n### nutch使用体验与感悟\n在这里之所以以nutch开篇，因为在接触爬虫之前就一直有听过它的大名，知道它是我们java语言栈中的爬虫利器，而且因为它，诞生了hadoop这一重器，后者在如今天大数据技术圈的名头，应该是无人不晓吧。所以，多种因素的促使下，让我入了nutch的坑。而本文虽是不仅针对nutch的总结，但主要还是以nutch为主。\n\n经过一阵的实际体验,nutch给我的感觉，老实说确实是强,百科中说nutch的目的是“Nutch 致力于让每个人能很容易, 同时花费很少就可以配置世界一流的Web搜索引擎. 为了完成这一宏伟的目标, Nutch必须能够做到”,能有这么宏伟的目标，自然得有相应的实力来支撑。但就我体验来说，它确实是强，经过简单的配置后，几个简单的指令就能对页面进行抓取，但我觉得也有几个比较明显的让人体验不太好的地方\n\n\n1. 对动态生成的网页内容的抓取不理想，或要通过其它的插件，但插件的引入也不如想像般顺利\n2. 版本太多，且很多版本之间的差异明显。在后面会尝试说明一下。\n3. 资料太少，有很多场景国外都没有有效的解决策略（公网检索下），国内环境就更差了，为了搜集资料我多次去过国家图书馆，逛过书店等，都没太找到特别详尽且有针对性的资料，所以nutch的整个踩坑过程是很痛苦的，我后面会将我目前调使的nutch抓取服务配置和一些踩坑进行详细的总结，我觉得，就我之前对网上的搜集资料的掌握的情况来看，接下来我给出的总结，或者能带来一些更可靠的价值。\n综上，我对nutch的感悟是又爱又恨。爱得是它确实是能很便利的帮忙爬取一些东西，恨的是它又不完全是那么便利\n\n### nutch搭建与踩坑回顾\n\n前文中已经提到过nutch有很多版本，目前最新版本2.3.1，它的多个版本间是有很多差异的，比如1.X的版本间的持久层是没有抽象出来的，所以1.X版本的持久化形式比较单一。而2.X版本中持久层被抽象出来，通过一不同的配置即可实现多元的持久层存储。如mysql、hbase、avro等。但在2.3.X开始就不支持mysql，官网是明确公示过，或者说从2.2.X开始就不支持了，但就实际操作来看2.2.X还是能适配mysql的，而2.3.X在我的实际操作中是不行的。另在这里说一下，我的持久层主要以mysql为主，后面可能还会再试试hbase，而一开始不采用hbase不是我没尝试，而是尝试过，发现nutch目前适配的hbase版本太低，而我服务器上已经搭建的hbase集群环境的版本相对高了不少，无法适配。我想没有必要为此就针对hbase的版本进行更替，所以采用mysql作为持久层，**这是坑一**。\n在使用nutch之前，对其是一点都不了解的，或者说对爬虫需求也都是不熟悉的。所以，对其的功能是寄于厚望，一开始就配一个网站的首页url，就设置多线程，多层抓取。但经过多次实站后，发现有些网站中一部份或大部份都不能抓取。再随着深入观察发现，很多是动态生成的内容。更甚者，有些内容的加载是页面加载后，再发送ajax来加载页面的一些关键内容。而原本考虑nutch是支持插件扩展的，原想直接通过检索，求助于互联网，希望直接就能找到一款ajax扩展的插件，然而，几经周折，都未成行。**这是坑二**\n\n其它还有一些小坑，具体我在下面搭建环节具体结细节处再提，虽小，但影响也挺大，而且比较坑人。以上都是主要通过人力暂未能直接解决的坑。所以先列出来，下面，我回顾下我的nutch搭建环节，其中会给出一些小坑与相应的解决方案\n\n#### 搭建\nnutch的搭建分为分布式与单机版，我目前主要涉猎的是nutch单机版。具体的版本选择，我尝试过nutch1.2与nutch1.7以上的所有版本，1.X与2.X的主要差异在于2.X将持久层给抽象出来了。总得来说nuthc2.x的实用性相对高些。而我以我目前具体使用的版本2.2.1为例，进行介绍\nstep 1.获取nutch2.2.1 http://archive.apache.org/dist/\n这个url能定位到apache很多软件和历史版本。进入这里，然后找到nutch，下载相应的版本（该连接有时可能打不开，翻墙试试）。\nstep 2. 对ivy下的ivy.xml与ivysetting.xml进行修改。\n这里就有之前提到的坑一了，这里可能配置对持久层的依赖了。我这以mysql配置。\n\n修改${APACHE_NUTCH_HOME}/ivy/ivy.xml文件\n\n将以下行的注释取消\n\n```    org=”mysql”  name=”mysql-connector-java”  rev=”5.1.18″  conf=”*->default”/>```\n\n修改以下行。从默认的\n```   org=\"org.apache.gora\"  name=\"gora-core\"  rev=\"0.3\"  conf=\"*->default\"/>```\n改成\n```    org=\"org.apache.gora\"  name=\"gora-core\"  rev=\"0.2.1\"  conf=\"*->default\"/>```\n\n取消以下行的注释\n```  org=\"org.apache.gora\"  name=\"gora-sql\"  rev=\"0.1.1-incubating\"  conf=\"*->default\" />```\n\n如果按默认的不做修改，将会在抓取网页时遇到以下错误。\n\n![e8f001bf53174dd985d3ffd36c6fb32d-image.png](//img.wqkenqing.ren/file/2017/8/e8f001bf53174dd985d3ffd36c6fb32d-image.png)\n\n然后配置ivysetting.xml，这个文件类似maven的.setting.xml文件，主要修改相应的软件仓库源，默认地址可能会出现下载缓慢的情况，建议换成国内源，贴一个我配的阿里的。\n![ffc49faa6d304b2694313bd5c206ee94-image.png](//img.wqkenqing.ren/file/2017/8/ffc49faa6d304b2694313bd5c206ee94-image.png)\n\n\n速度杠杠的。\n这时就可以进行编译了，通过终端进nutch文件地址进执行ant\n\n然后配置持久层的配置文件，gora.properties\n![33e8c343b9854449897c28dd5177409f-image.png](//img.wqkenqing.ren/file/2017/8/33e8c343b9854449897c28dd5177409f-image.png)\n\n\n接下来配另mysql的映射文件gora-sql-mapping.xml，可以这么说，这个地方出现的坑是我遇到的坑最多的地方之一。因为我是事后总结，平日也有工任务，所以，在出现坑的时侯，我首要的考虑的是解决的它，所以解决这些问题后，可能能记得是大致是什么状况，但无法具体复现，我在这以总结，提练式的对这些小坑进行叙述，就不贴具体的错误日志了。\n\n首先，文件中的默认配置id的大小是512.这个对于以unicode，准确的说以utf8为编码格式的mysql来说是过长的，会在inject就报sql初始化错误，id too long\n**解决方式**\n将id的长度改小，我设置的是180，nutch默认一般以target url拼接成id，所以一般来讲，180的id是妥妥够用的。\n\n类似的问题还会出现一些如text ,content过长的问题，初次搭建我这先建议改小试试，后面还有根治的方式。\n\n修改了gora-sql-mapping.xml文件后，执行抓取指令，以bin/nutch crawl urlfile  -threads number -deepth num 这是nutch 普遍用的一个指行指令，这个指令执行时会默认将数据持久化到webpage的表中，这种方式缺点是不太灵活，复杂任务的时候不好操作。\n\n另一种bin/crawl urlfile -crawlId name (这个name会作为对应的表名组成部份) \"solrrl\" \"num\"用于指定解析程度\n这种方式相对而言更灵活，后面我主要采用这种方式。这里有个小细节，bin/crawl 这个脚本是可以尝试更改的，它默认执行任务时只创建了50，而这种方式无法像上面那样指定创建线程数，所以我更改了crawl文件的初始值，将参数调值2000.所以可以根据自己需求，酌情更改。\n\nbin/nutch parse 有些fetch 任务执行完成后，parse数据同步至数据库时可能会产生一些错误，那么可以通过这个指令尝试进行解析。\n示例：bin/nutch parse -crawlId name -all\n这里容易出现前面提到过的一个问题，就是在解析时，content 或text 中的内容格式不对，或text 内容中出现在了emoji等情况，都可能使解析任务中断。我经过一翻调整，算是有一个统一解决方案：在这里贴出来参考\n1、将数据库编码格式设置为utf8mb4；\n2、将text和content的gora-mapping.xml文件中添加jdbc-type=“text” 或jdbc-type=\"blob\"的设置，即指明其数据库中对应的类型，避免长度等问题\n3、上两者结合的一个情况，text存入文中的内容有编码格式，相对读取轻松，blob以二进制形式存放，取值需要转码，所以在不出错的情况下优先设为text，而这时，数据库设为utf8mb4时就不要在jdbc url 中设置utf8格式了，这样反而会出现问题。\n![ae5a036dcc9c45a29fdfa0ed2d345eca-image.png](//http://img.wqkenqing.ren//file/2017/9/ae5a036dcc9c45a29fdfa0ed2d345eca-image.png)\n\n至此 nutch日常的主要使用指令就这两个，还有些 如果bin/nutch fetch bin/nutch gernate\n这些相对出错较少，出错的影响和对策网上也相对较多，就不再缀述了。\n\n---\n\n至此爬虫服务的总结就要告一段落了，这篇文章体量相对较大，我是断断续续逐步完成的，所以，后面更多是凭回忆出的之前深刻影响且在网上少有明确解决方案的一些问题，给出我的解决方式，所以，如果这篇总结针对的算是nutch使用过程中，尝试更进一步的助力，而非特别基础的。如果看到此篇文章的你对nutch还有其它的一些问题，可以尝试留言，我们一起探讨。\n\n\n\n\n\n\n","source":"_posts/技术/hexo/oldblog/blog25.md","raw":"\n---\n\ntitle: 爬虫之nutch\ndate: 2019-07-16\ntags: \n\n---\n此处简介\n<!--more-->\n\n `这阵子主要研究的爬虫方向，主要以java为语言基础,nutch为自动框架，jsoup作为自主爬虫插件开发基础，进行了一些有针对性的实站，在这些过程中，也遇到了一些问题和心得，觉得有必要总结一下`\n\n## 爬虫之nutch\n### nutch使用体验与感悟\n在这里之所以以nutch开篇，因为在接触爬虫之前就一直有听过它的大名，知道它是我们java语言栈中的爬虫利器，而且因为它，诞生了hadoop这一重器，后者在如今天大数据技术圈的名头，应该是无人不晓吧。所以，多种因素的促使下，让我入了nutch的坑。而本文虽是不仅针对nutch的总结，但主要还是以nutch为主。\n\n经过一阵的实际体验,nutch给我的感觉，老实说确实是强,百科中说nutch的目的是“Nutch 致力于让每个人能很容易, 同时花费很少就可以配置世界一流的Web搜索引擎. 为了完成这一宏伟的目标, Nutch必须能够做到”,能有这么宏伟的目标，自然得有相应的实力来支撑。但就我体验来说，它确实是强，经过简单的配置后，几个简单的指令就能对页面进行抓取，但我觉得也有几个比较明显的让人体验不太好的地方\n\n\n1. 对动态生成的网页内容的抓取不理想，或要通过其它的插件，但插件的引入也不如想像般顺利\n2. 版本太多，且很多版本之间的差异明显。在后面会尝试说明一下。\n3. 资料太少，有很多场景国外都没有有效的解决策略（公网检索下），国内环境就更差了，为了搜集资料我多次去过国家图书馆，逛过书店等，都没太找到特别详尽且有针对性的资料，所以nutch的整个踩坑过程是很痛苦的，我后面会将我目前调使的nutch抓取服务配置和一些踩坑进行详细的总结，我觉得，就我之前对网上的搜集资料的掌握的情况来看，接下来我给出的总结，或者能带来一些更可靠的价值。\n综上，我对nutch的感悟是又爱又恨。爱得是它确实是能很便利的帮忙爬取一些东西，恨的是它又不完全是那么便利\n\n### nutch搭建与踩坑回顾\n\n前文中已经提到过nutch有很多版本，目前最新版本2.3.1，它的多个版本间是有很多差异的，比如1.X的版本间的持久层是没有抽象出来的，所以1.X版本的持久化形式比较单一。而2.X版本中持久层被抽象出来，通过一不同的配置即可实现多元的持久层存储。如mysql、hbase、avro等。但在2.3.X开始就不支持mysql，官网是明确公示过，或者说从2.2.X开始就不支持了，但就实际操作来看2.2.X还是能适配mysql的，而2.3.X在我的实际操作中是不行的。另在这里说一下，我的持久层主要以mysql为主，后面可能还会再试试hbase，而一开始不采用hbase不是我没尝试，而是尝试过，发现nutch目前适配的hbase版本太低，而我服务器上已经搭建的hbase集群环境的版本相对高了不少，无法适配。我想没有必要为此就针对hbase的版本进行更替，所以采用mysql作为持久层，**这是坑一**。\n在使用nutch之前，对其是一点都不了解的，或者说对爬虫需求也都是不熟悉的。所以，对其的功能是寄于厚望，一开始就配一个网站的首页url，就设置多线程，多层抓取。但经过多次实站后，发现有些网站中一部份或大部份都不能抓取。再随着深入观察发现，很多是动态生成的内容。更甚者，有些内容的加载是页面加载后，再发送ajax来加载页面的一些关键内容。而原本考虑nutch是支持插件扩展的，原想直接通过检索，求助于互联网，希望直接就能找到一款ajax扩展的插件，然而，几经周折，都未成行。**这是坑二**\n\n其它还有一些小坑，具体我在下面搭建环节具体结细节处再提，虽小，但影响也挺大，而且比较坑人。以上都是主要通过人力暂未能直接解决的坑。所以先列出来，下面，我回顾下我的nutch搭建环节，其中会给出一些小坑与相应的解决方案\n\n#### 搭建\nnutch的搭建分为分布式与单机版，我目前主要涉猎的是nutch单机版。具体的版本选择，我尝试过nutch1.2与nutch1.7以上的所有版本，1.X与2.X的主要差异在于2.X将持久层给抽象出来了。总得来说nuthc2.x的实用性相对高些。而我以我目前具体使用的版本2.2.1为例，进行介绍\nstep 1.获取nutch2.2.1 http://archive.apache.org/dist/\n这个url能定位到apache很多软件和历史版本。进入这里，然后找到nutch，下载相应的版本（该连接有时可能打不开，翻墙试试）。\nstep 2. 对ivy下的ivy.xml与ivysetting.xml进行修改。\n这里就有之前提到的坑一了，这里可能配置对持久层的依赖了。我这以mysql配置。\n\n修改${APACHE_NUTCH_HOME}/ivy/ivy.xml文件\n\n将以下行的注释取消\n\n```    org=”mysql”  name=”mysql-connector-java”  rev=”5.1.18″  conf=”*->default”/>```\n\n修改以下行。从默认的\n```   org=\"org.apache.gora\"  name=\"gora-core\"  rev=\"0.3\"  conf=\"*->default\"/>```\n改成\n```    org=\"org.apache.gora\"  name=\"gora-core\"  rev=\"0.2.1\"  conf=\"*->default\"/>```\n\n取消以下行的注释\n```  org=\"org.apache.gora\"  name=\"gora-sql\"  rev=\"0.1.1-incubating\"  conf=\"*->default\" />```\n\n如果按默认的不做修改，将会在抓取网页时遇到以下错误。\n\n![e8f001bf53174dd985d3ffd36c6fb32d-image.png](//img.wqkenqing.ren/file/2017/8/e8f001bf53174dd985d3ffd36c6fb32d-image.png)\n\n然后配置ivysetting.xml，这个文件类似maven的.setting.xml文件，主要修改相应的软件仓库源，默认地址可能会出现下载缓慢的情况，建议换成国内源，贴一个我配的阿里的。\n![ffc49faa6d304b2694313bd5c206ee94-image.png](//img.wqkenqing.ren/file/2017/8/ffc49faa6d304b2694313bd5c206ee94-image.png)\n\n\n速度杠杠的。\n这时就可以进行编译了，通过终端进nutch文件地址进执行ant\n\n然后配置持久层的配置文件，gora.properties\n![33e8c343b9854449897c28dd5177409f-image.png](//img.wqkenqing.ren/file/2017/8/33e8c343b9854449897c28dd5177409f-image.png)\n\n\n接下来配另mysql的映射文件gora-sql-mapping.xml，可以这么说，这个地方出现的坑是我遇到的坑最多的地方之一。因为我是事后总结，平日也有工任务，所以，在出现坑的时侯，我首要的考虑的是解决的它，所以解决这些问题后，可能能记得是大致是什么状况，但无法具体复现，我在这以总结，提练式的对这些小坑进行叙述，就不贴具体的错误日志了。\n\n首先，文件中的默认配置id的大小是512.这个对于以unicode，准确的说以utf8为编码格式的mysql来说是过长的，会在inject就报sql初始化错误，id too long\n**解决方式**\n将id的长度改小，我设置的是180，nutch默认一般以target url拼接成id，所以一般来讲，180的id是妥妥够用的。\n\n类似的问题还会出现一些如text ,content过长的问题，初次搭建我这先建议改小试试，后面还有根治的方式。\n\n修改了gora-sql-mapping.xml文件后，执行抓取指令，以bin/nutch crawl urlfile  -threads number -deepth num 这是nutch 普遍用的一个指行指令，这个指令执行时会默认将数据持久化到webpage的表中，这种方式缺点是不太灵活，复杂任务的时候不好操作。\n\n另一种bin/crawl urlfile -crawlId name (这个name会作为对应的表名组成部份) \"solrrl\" \"num\"用于指定解析程度\n这种方式相对而言更灵活，后面我主要采用这种方式。这里有个小细节，bin/crawl 这个脚本是可以尝试更改的，它默认执行任务时只创建了50，而这种方式无法像上面那样指定创建线程数，所以我更改了crawl文件的初始值，将参数调值2000.所以可以根据自己需求，酌情更改。\n\nbin/nutch parse 有些fetch 任务执行完成后，parse数据同步至数据库时可能会产生一些错误，那么可以通过这个指令尝试进行解析。\n示例：bin/nutch parse -crawlId name -all\n这里容易出现前面提到过的一个问题，就是在解析时，content 或text 中的内容格式不对，或text 内容中出现在了emoji等情况，都可能使解析任务中断。我经过一翻调整，算是有一个统一解决方案：在这里贴出来参考\n1、将数据库编码格式设置为utf8mb4；\n2、将text和content的gora-mapping.xml文件中添加jdbc-type=“text” 或jdbc-type=\"blob\"的设置，即指明其数据库中对应的类型，避免长度等问题\n3、上两者结合的一个情况，text存入文中的内容有编码格式，相对读取轻松，blob以二进制形式存放，取值需要转码，所以在不出错的情况下优先设为text，而这时，数据库设为utf8mb4时就不要在jdbc url 中设置utf8格式了，这样反而会出现问题。\n![ae5a036dcc9c45a29fdfa0ed2d345eca-image.png](//http://img.wqkenqing.ren//file/2017/9/ae5a036dcc9c45a29fdfa0ed2d345eca-image.png)\n\n至此 nutch日常的主要使用指令就这两个，还有些 如果bin/nutch fetch bin/nutch gernate\n这些相对出错较少，出错的影响和对策网上也相对较多，就不再缀述了。\n\n---\n\n至此爬虫服务的总结就要告一段落了，这篇文章体量相对较大，我是断断续续逐步完成的，所以，后面更多是凭回忆出的之前深刻影响且在网上少有明确解决方案的一些问题，给出我的解决方式，所以，如果这篇总结针对的算是nutch使用过程中，尝试更进一步的助力，而非特别基础的。如果看到此篇文章的你对nutch还有其它的一些问题，可以尝试留言，我们一起探讨。\n\n\n\n\n\n\n","slug":"技术/hexo/oldblog/blog25","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33j003838pwg7e81ggr","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<p> <code>这阵子主要研究的爬虫方向，主要以java为语言基础,nutch为自动框架，jsoup作为自主爬虫插件开发基础，进行了一些有针对性的实站，在这些过程中，也遇到了一些问题和心得，觉得有必要总结一下</code></p>\n<h2 id=\"爬虫之nutch\"><a href=\"#爬虫之nutch\" class=\"headerlink\" title=\"爬虫之nutch\"></a>爬虫之nutch</h2><h3 id=\"nutch使用体验与感悟\"><a href=\"#nutch使用体验与感悟\" class=\"headerlink\" title=\"nutch使用体验与感悟\"></a>nutch使用体验与感悟</h3><p>在这里之所以以nutch开篇，因为在接触爬虫之前就一直有听过它的大名，知道它是我们java语言栈中的爬虫利器，而且因为它，诞生了hadoop这一重器，后者在如今天大数据技术圈的名头，应该是无人不晓吧。所以，多种因素的促使下，让我入了nutch的坑。而本文虽是不仅针对nutch的总结，但主要还是以nutch为主。</p>\n<p>经过一阵的实际体验,nutch给我的感觉，老实说确实是强,百科中说nutch的目的是“Nutch 致力于让每个人能很容易, 同时花费很少就可以配置世界一流的Web搜索引擎. 为了完成这一宏伟的目标, Nutch必须能够做到”,能有这么宏伟的目标，自然得有相应的实力来支撑。但就我体验来说，它确实是强，经过简单的配置后，几个简单的指令就能对页面进行抓取，但我觉得也有几个比较明显的让人体验不太好的地方</p>\n<ol>\n<li>对动态生成的网页内容的抓取不理想，或要通过其它的插件，但插件的引入也不如想像般顺利</li>\n<li>版本太多，且很多版本之间的差异明显。在后面会尝试说明一下。</li>\n<li>资料太少，有很多场景国外都没有有效的解决策略（公网检索下），国内环境就更差了，为了搜集资料我多次去过国家图书馆，逛过书店等，都没太找到特别详尽且有针对性的资料，所以nutch的整个踩坑过程是很痛苦的，我后面会将我目前调使的nutch抓取服务配置和一些踩坑进行详细的总结，我觉得，就我之前对网上的搜集资料的掌握的情况来看，接下来我给出的总结，或者能带来一些更可靠的价值。<br>综上，我对nutch的感悟是又爱又恨。爱得是它确实是能很便利的帮忙爬取一些东西，恨的是它又不完全是那么便利</li>\n</ol>\n<h3 id=\"nutch搭建与踩坑回顾\"><a href=\"#nutch搭建与踩坑回顾\" class=\"headerlink\" title=\"nutch搭建与踩坑回顾\"></a>nutch搭建与踩坑回顾</h3><p>前文中已经提到过nutch有很多版本，目前最新版本2.3.1，它的多个版本间是有很多差异的，比如1.X的版本间的持久层是没有抽象出来的，所以1.X版本的持久化形式比较单一。而2.X版本中持久层被抽象出来，通过一不同的配置即可实现多元的持久层存储。如mysql、hbase、avro等。但在2.3.X开始就不支持mysql，官网是明确公示过，或者说从2.2.X开始就不支持了，但就实际操作来看2.2.X还是能适配mysql的，而2.3.X在我的实际操作中是不行的。另在这里说一下，我的持久层主要以mysql为主，后面可能还会再试试hbase，而一开始不采用hbase不是我没尝试，而是尝试过，发现nutch目前适配的hbase版本太低，而我服务器上已经搭建的hbase集群环境的版本相对高了不少，无法适配。我想没有必要为此就针对hbase的版本进行更替，所以采用mysql作为持久层，<strong>这是坑一</strong>。<br>在使用nutch之前，对其是一点都不了解的，或者说对爬虫需求也都是不熟悉的。所以，对其的功能是寄于厚望，一开始就配一个网站的首页url，就设置多线程，多层抓取。但经过多次实站后，发现有些网站中一部份或大部份都不能抓取。再随着深入观察发现，很多是动态生成的内容。更甚者，有些内容的加载是页面加载后，再发送ajax来加载页面的一些关键内容。而原本考虑nutch是支持插件扩展的，原想直接通过检索，求助于互联网，希望直接就能找到一款ajax扩展的插件，然而，几经周折，都未成行。<strong>这是坑二</strong></p>\n<p>其它还有一些小坑，具体我在下面搭建环节具体结细节处再提，虽小，但影响也挺大，而且比较坑人。以上都是主要通过人力暂未能直接解决的坑。所以先列出来，下面，我回顾下我的nutch搭建环节，其中会给出一些小坑与相应的解决方案</p>\n<h4 id=\"搭建\"><a href=\"#搭建\" class=\"headerlink\" title=\"搭建\"></a>搭建</h4><p>nutch的搭建分为分布式与单机版，我目前主要涉猎的是nutch单机版。具体的版本选择，我尝试过nutch1.2与nutch1.7以上的所有版本，1.X与2.X的主要差异在于2.X将持久层给抽象出来了。总得来说nuthc2.x的实用性相对高些。而我以我目前具体使用的版本2.2.1为例，进行介绍<br>step 1.获取nutch2.2.1 <a href=\"http://archive.apache.org/dist/\" target=\"_blank\" rel=\"noopener\">http://archive.apache.org/dist/</a><br>这个url能定位到apache很多软件和历史版本。进入这里，然后找到nutch，下载相应的版本（该连接有时可能打不开，翻墙试试）。<br>step 2. 对ivy下的ivy.xml与ivysetting.xml进行修改。<br>这里就有之前提到的坑一了，这里可能配置对持久层的依赖了。我这以mysql配置。</p>\n<p>修改${APACHE_NUTCH_HOME}/ivy/ivy.xml文件</p>\n<p>将以下行的注释取消</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">修改以下行。从默认的</span><br><span class=\"line\">&#96;&#96;&#96;   org&#x3D;&quot;org.apache.gora&quot;  name&#x3D;&quot;gora-core&quot;  rev&#x3D;&quot;0.3&quot;  conf&#x3D;&quot;*-&gt;default&quot;&#x2F;&gt;</span><br></pre></td></tr></table></figure>\n<p>改成</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">取消以下行的注释</span><br><span class=\"line\">&#96;&#96;&#96;  org&#x3D;&quot;org.apache.gora&quot;  name&#x3D;&quot;gora-sql&quot;  rev&#x3D;&quot;0.1.1-incubating&quot;  conf&#x3D;&quot;*-&gt;default&quot; &#x2F;&gt;</span><br></pre></td></tr></table></figure>\n\n<p>如果按默认的不做修改，将会在抓取网页时遇到以下错误。</p>\n<p><img src=\"//img.wqkenqing.ren/file/2017/8/e8f001bf53174dd985d3ffd36c6fb32d-image.png\" alt=\"e8f001bf53174dd985d3ffd36c6fb32d-image.png\"></p>\n<p>然后配置ivysetting.xml，这个文件类似maven的.setting.xml文件，主要修改相应的软件仓库源，默认地址可能会出现下载缓慢的情况，建议换成国内源，贴一个我配的阿里的。<br><img src=\"//img.wqkenqing.ren/file/2017/8/ffc49faa6d304b2694313bd5c206ee94-image.png\" alt=\"ffc49faa6d304b2694313bd5c206ee94-image.png\"></p>\n<p>速度杠杠的。<br>这时就可以进行编译了，通过终端进nutch文件地址进执行ant</p>\n<p>然后配置持久层的配置文件，gora.properties<br><img src=\"//img.wqkenqing.ren/file/2017/8/33e8c343b9854449897c28dd5177409f-image.png\" alt=\"33e8c343b9854449897c28dd5177409f-image.png\"></p>\n<p>接下来配另mysql的映射文件gora-sql-mapping.xml，可以这么说，这个地方出现的坑是我遇到的坑最多的地方之一。因为我是事后总结，平日也有工任务，所以，在出现坑的时侯，我首要的考虑的是解决的它，所以解决这些问题后，可能能记得是大致是什么状况，但无法具体复现，我在这以总结，提练式的对这些小坑进行叙述，就不贴具体的错误日志了。</p>\n<p>首先，文件中的默认配置id的大小是512.这个对于以unicode，准确的说以utf8为编码格式的mysql来说是过长的，会在inject就报sql初始化错误，id too long<br><strong>解决方式</strong><br>将id的长度改小，我设置的是180，nutch默认一般以target url拼接成id，所以一般来讲，180的id是妥妥够用的。</p>\n<p>类似的问题还会出现一些如text ,content过长的问题，初次搭建我这先建议改小试试，后面还有根治的方式。</p>\n<p>修改了gora-sql-mapping.xml文件后，执行抓取指令，以bin/nutch crawl urlfile  -threads number -deepth num 这是nutch 普遍用的一个指行指令，这个指令执行时会默认将数据持久化到webpage的表中，这种方式缺点是不太灵活，复杂任务的时候不好操作。</p>\n<p>另一种bin/crawl urlfile -crawlId name (这个name会作为对应的表名组成部份) “solrrl” “num”用于指定解析程度<br>这种方式相对而言更灵活，后面我主要采用这种方式。这里有个小细节，bin/crawl 这个脚本是可以尝试更改的，它默认执行任务时只创建了50，而这种方式无法像上面那样指定创建线程数，所以我更改了crawl文件的初始值，将参数调值2000.所以可以根据自己需求，酌情更改。</p>\n<p>bin/nutch parse 有些fetch 任务执行完成后，parse数据同步至数据库时可能会产生一些错误，那么可以通过这个指令尝试进行解析。<br>示例：bin/nutch parse -crawlId name -all<br>这里容易出现前面提到过的一个问题，就是在解析时，content 或text 中的内容格式不对，或text 内容中出现在了emoji等情况，都可能使解析任务中断。我经过一翻调整，算是有一个统一解决方案：在这里贴出来参考<br>1、将数据库编码格式设置为utf8mb4；<br>2、将text和content的gora-mapping.xml文件中添加jdbc-type=“text” 或jdbc-type=”blob”的设置，即指明其数据库中对应的类型，避免长度等问题<br>3、上两者结合的一个情况，text存入文中的内容有编码格式，相对读取轻松，blob以二进制形式存放，取值需要转码，所以在不出错的情况下优先设为text，而这时，数据库设为utf8mb4时就不要在jdbc url 中设置utf8格式了，这样反而会出现问题。<br><img src=\"//http://img.wqkenqing.ren//file/2017/9/ae5a036dcc9c45a29fdfa0ed2d345eca-image.png\" alt=\"ae5a036dcc9c45a29fdfa0ed2d345eca-image.png\"></p>\n<p>至此 nutch日常的主要使用指令就这两个，还有些 如果bin/nutch fetch bin/nutch gernate<br>这些相对出错较少，出错的影响和对策网上也相对较多，就不再缀述了。</p>\n<hr>\n<p>至此爬虫服务的总结就要告一段落了，这篇文章体量相对较大，我是断断续续逐步完成的，所以，后面更多是凭回忆出的之前深刻影响且在网上少有明确解决方案的一些问题，给出我的解决方式，所以，如果这篇总结针对的算是nutch使用过程中，尝试更进一步的助力，而非特别基础的。如果看到此篇文章的你对nutch还有其它的一些问题，可以尝试留言，我们一起探讨。</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<p> <code>这阵子主要研究的爬虫方向，主要以java为语言基础,nutch为自动框架，jsoup作为自主爬虫插件开发基础，进行了一些有针对性的实站，在这些过程中，也遇到了一些问题和心得，觉得有必要总结一下</code></p>\n<h2 id=\"爬虫之nutch\"><a href=\"#爬虫之nutch\" class=\"headerlink\" title=\"爬虫之nutch\"></a>爬虫之nutch</h2><h3 id=\"nutch使用体验与感悟\"><a href=\"#nutch使用体验与感悟\" class=\"headerlink\" title=\"nutch使用体验与感悟\"></a>nutch使用体验与感悟</h3><p>在这里之所以以nutch开篇，因为在接触爬虫之前就一直有听过它的大名，知道它是我们java语言栈中的爬虫利器，而且因为它，诞生了hadoop这一重器，后者在如今天大数据技术圈的名头，应该是无人不晓吧。所以，多种因素的促使下，让我入了nutch的坑。而本文虽是不仅针对nutch的总结，但主要还是以nutch为主。</p>\n<p>经过一阵的实际体验,nutch给我的感觉，老实说确实是强,百科中说nutch的目的是“Nutch 致力于让每个人能很容易, 同时花费很少就可以配置世界一流的Web搜索引擎. 为了完成这一宏伟的目标, Nutch必须能够做到”,能有这么宏伟的目标，自然得有相应的实力来支撑。但就我体验来说，它确实是强，经过简单的配置后，几个简单的指令就能对页面进行抓取，但我觉得也有几个比较明显的让人体验不太好的地方</p>\n<ol>\n<li>对动态生成的网页内容的抓取不理想，或要通过其它的插件，但插件的引入也不如想像般顺利</li>\n<li>版本太多，且很多版本之间的差异明显。在后面会尝试说明一下。</li>\n<li>资料太少，有很多场景国外都没有有效的解决策略（公网检索下），国内环境就更差了，为了搜集资料我多次去过国家图书馆，逛过书店等，都没太找到特别详尽且有针对性的资料，所以nutch的整个踩坑过程是很痛苦的，我后面会将我目前调使的nutch抓取服务配置和一些踩坑进行详细的总结，我觉得，就我之前对网上的搜集资料的掌握的情况来看，接下来我给出的总结，或者能带来一些更可靠的价值。<br>综上，我对nutch的感悟是又爱又恨。爱得是它确实是能很便利的帮忙爬取一些东西，恨的是它又不完全是那么便利</li>\n</ol>\n<h3 id=\"nutch搭建与踩坑回顾\"><a href=\"#nutch搭建与踩坑回顾\" class=\"headerlink\" title=\"nutch搭建与踩坑回顾\"></a>nutch搭建与踩坑回顾</h3><p>前文中已经提到过nutch有很多版本，目前最新版本2.3.1，它的多个版本间是有很多差异的，比如1.X的版本间的持久层是没有抽象出来的，所以1.X版本的持久化形式比较单一。而2.X版本中持久层被抽象出来，通过一不同的配置即可实现多元的持久层存储。如mysql、hbase、avro等。但在2.3.X开始就不支持mysql，官网是明确公示过，或者说从2.2.X开始就不支持了，但就实际操作来看2.2.X还是能适配mysql的，而2.3.X在我的实际操作中是不行的。另在这里说一下，我的持久层主要以mysql为主，后面可能还会再试试hbase，而一开始不采用hbase不是我没尝试，而是尝试过，发现nutch目前适配的hbase版本太低，而我服务器上已经搭建的hbase集群环境的版本相对高了不少，无法适配。我想没有必要为此就针对hbase的版本进行更替，所以采用mysql作为持久层，<strong>这是坑一</strong>。<br>在使用nutch之前，对其是一点都不了解的，或者说对爬虫需求也都是不熟悉的。所以，对其的功能是寄于厚望，一开始就配一个网站的首页url，就设置多线程，多层抓取。但经过多次实站后，发现有些网站中一部份或大部份都不能抓取。再随着深入观察发现，很多是动态生成的内容。更甚者，有些内容的加载是页面加载后，再发送ajax来加载页面的一些关键内容。而原本考虑nutch是支持插件扩展的，原想直接通过检索，求助于互联网，希望直接就能找到一款ajax扩展的插件，然而，几经周折，都未成行。<strong>这是坑二</strong></p>\n<p>其它还有一些小坑，具体我在下面搭建环节具体结细节处再提，虽小，但影响也挺大，而且比较坑人。以上都是主要通过人力暂未能直接解决的坑。所以先列出来，下面，我回顾下我的nutch搭建环节，其中会给出一些小坑与相应的解决方案</p>\n<h4 id=\"搭建\"><a href=\"#搭建\" class=\"headerlink\" title=\"搭建\"></a>搭建</h4><p>nutch的搭建分为分布式与单机版，我目前主要涉猎的是nutch单机版。具体的版本选择，我尝试过nutch1.2与nutch1.7以上的所有版本，1.X与2.X的主要差异在于2.X将持久层给抽象出来了。总得来说nuthc2.x的实用性相对高些。而我以我目前具体使用的版本2.2.1为例，进行介绍<br>step 1.获取nutch2.2.1 <a href=\"http://archive.apache.org/dist/\" target=\"_blank\" rel=\"noopener\">http://archive.apache.org/dist/</a><br>这个url能定位到apache很多软件和历史版本。进入这里，然后找到nutch，下载相应的版本（该连接有时可能打不开，翻墙试试）。<br>step 2. 对ivy下的ivy.xml与ivysetting.xml进行修改。<br>这里就有之前提到的坑一了，这里可能配置对持久层的依赖了。我这以mysql配置。</p>\n<p>修改${APACHE_NUTCH_HOME}/ivy/ivy.xml文件</p>\n<p>将以下行的注释取消</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">修改以下行。从默认的</span><br><span class=\"line\">&#96;&#96;&#96;   org&#x3D;&quot;org.apache.gora&quot;  name&#x3D;&quot;gora-core&quot;  rev&#x3D;&quot;0.3&quot;  conf&#x3D;&quot;*-&gt;default&quot;&#x2F;&gt;</span><br></pre></td></tr></table></figure>\n<p>改成</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">取消以下行的注释</span><br><span class=\"line\">&#96;&#96;&#96;  org&#x3D;&quot;org.apache.gora&quot;  name&#x3D;&quot;gora-sql&quot;  rev&#x3D;&quot;0.1.1-incubating&quot;  conf&#x3D;&quot;*-&gt;default&quot; &#x2F;&gt;</span><br></pre></td></tr></table></figure>\n\n<p>如果按默认的不做修改，将会在抓取网页时遇到以下错误。</p>\n<p><img src=\"//img.wqkenqing.ren/file/2017/8/e8f001bf53174dd985d3ffd36c6fb32d-image.png\" alt=\"e8f001bf53174dd985d3ffd36c6fb32d-image.png\"></p>\n<p>然后配置ivysetting.xml，这个文件类似maven的.setting.xml文件，主要修改相应的软件仓库源，默认地址可能会出现下载缓慢的情况，建议换成国内源，贴一个我配的阿里的。<br><img src=\"//img.wqkenqing.ren/file/2017/8/ffc49faa6d304b2694313bd5c206ee94-image.png\" alt=\"ffc49faa6d304b2694313bd5c206ee94-image.png\"></p>\n<p>速度杠杠的。<br>这时就可以进行编译了，通过终端进nutch文件地址进执行ant</p>\n<p>然后配置持久层的配置文件，gora.properties<br><img src=\"//img.wqkenqing.ren/file/2017/8/33e8c343b9854449897c28dd5177409f-image.png\" alt=\"33e8c343b9854449897c28dd5177409f-image.png\"></p>\n<p>接下来配另mysql的映射文件gora-sql-mapping.xml，可以这么说，这个地方出现的坑是我遇到的坑最多的地方之一。因为我是事后总结，平日也有工任务，所以，在出现坑的时侯，我首要的考虑的是解决的它，所以解决这些问题后，可能能记得是大致是什么状况，但无法具体复现，我在这以总结，提练式的对这些小坑进行叙述，就不贴具体的错误日志了。</p>\n<p>首先，文件中的默认配置id的大小是512.这个对于以unicode，准确的说以utf8为编码格式的mysql来说是过长的，会在inject就报sql初始化错误，id too long<br><strong>解决方式</strong><br>将id的长度改小，我设置的是180，nutch默认一般以target url拼接成id，所以一般来讲，180的id是妥妥够用的。</p>\n<p>类似的问题还会出现一些如text ,content过长的问题，初次搭建我这先建议改小试试，后面还有根治的方式。</p>\n<p>修改了gora-sql-mapping.xml文件后，执行抓取指令，以bin/nutch crawl urlfile  -threads number -deepth num 这是nutch 普遍用的一个指行指令，这个指令执行时会默认将数据持久化到webpage的表中，这种方式缺点是不太灵活，复杂任务的时候不好操作。</p>\n<p>另一种bin/crawl urlfile -crawlId name (这个name会作为对应的表名组成部份) “solrrl” “num”用于指定解析程度<br>这种方式相对而言更灵活，后面我主要采用这种方式。这里有个小细节，bin/crawl 这个脚本是可以尝试更改的，它默认执行任务时只创建了50，而这种方式无法像上面那样指定创建线程数，所以我更改了crawl文件的初始值，将参数调值2000.所以可以根据自己需求，酌情更改。</p>\n<p>bin/nutch parse 有些fetch 任务执行完成后，parse数据同步至数据库时可能会产生一些错误，那么可以通过这个指令尝试进行解析。<br>示例：bin/nutch parse -crawlId name -all<br>这里容易出现前面提到过的一个问题，就是在解析时，content 或text 中的内容格式不对，或text 内容中出现在了emoji等情况，都可能使解析任务中断。我经过一翻调整，算是有一个统一解决方案：在这里贴出来参考<br>1、将数据库编码格式设置为utf8mb4；<br>2、将text和content的gora-mapping.xml文件中添加jdbc-type=“text” 或jdbc-type=”blob”的设置，即指明其数据库中对应的类型，避免长度等问题<br>3、上两者结合的一个情况，text存入文中的内容有编码格式，相对读取轻松，blob以二进制形式存放，取值需要转码，所以在不出错的情况下优先设为text，而这时，数据库设为utf8mb4时就不要在jdbc url 中设置utf8格式了，这样反而会出现问题。<br><img src=\"//http://img.wqkenqing.ren//file/2017/9/ae5a036dcc9c45a29fdfa0ed2d345eca-image.png\" alt=\"ae5a036dcc9c45a29fdfa0ed2d345eca-image.png\"></p>\n<p>至此 nutch日常的主要使用指令就这两个，还有些 如果bin/nutch fetch bin/nutch gernate<br>这些相对出错较少，出错的影响和对策网上也相对较多，就不再缀述了。</p>\n<hr>\n<p>至此爬虫服务的总结就要告一段落了，这篇文章体量相对较大，我是断断续续逐步完成的，所以，后面更多是凭回忆出的之前深刻影响且在网上少有明确解决方案的一些问题，给出我的解决方式，所以，如果这篇总结针对的算是nutch使用过程中，尝试更进一步的助力，而非特别基础的。如果看到此篇文章的你对nutch还有其它的一些问题，可以尝试留言，我们一起探讨。</p>"},{"title":"java中IO小结","date":"2019-07-15T16:00:00.000Z","_content":"\n此处简介\n\n<!--more-->\n\n### java中IO小结\n#### File\nFile类,相关api\n* createNewFiles();\n* mkdir();\n* mkdirs()\n\n---\n### IO流\n按方向，分输入，输出流in、out\n流当中有缓冲区，即stream.read(),可以一次性读取stream.read(Byte[]byte);\n针对缓冲流还有两个方法\nmark与reset\nmarkSupported 判断该输入流能支持mark 和 reset 方法。\n\nmark用于标记当前位置；在读取一定数量的数据(小于readlimit的数据)后使用reset可以回到mark标记的位置。\n\nFileInputStream不支持mark/reset操作；BufferedInputStream支持此操作；\n\nmark(readlimit)的含义是在当前位置作一个标记，制定可以重新读取的最大字节数，也就是说你如果标记后读取的字节数大于readlimit，你就再也回不到回来的位置了。\n\n通常InputStream的read()返回-1后，说明到达文件尾，不能再读取。除非使用了mark/reset。\n\n#### 流的类型\n+  FileInputStream\n+  ObjectInputStream\n+  DataInputStream\n+  BufferedInputStream\n+  BufferedReader\n+  ByteArrayInputStream\n+  CharArrayReader\n+  Console\n+  FileReader\n+  PipedInputStream\n+  PipedReader\n+  PushbackInputStream\n+  StringReader\n\n| 流的名称  | 说明|\n| :-------- | --------: |\n| FileInputStream    | FileInputStream 从文件系统中的某个文件中获得输入字节。 |\n| ObjectInputStream  | ObjectInputStream 对以前使用 ObjectOutputStream 写入的基本数据和对象进行反序列化。 |\n| DataInputStream    | 数据输入流允许应用程序以与机器无关方式从底层输入流中读取基本 Java 数据类型。 |\n| BufferedInputStream    | BufferedInputStream 为另一个输入流添加一些功能，即缓冲输入以及支持 mark 和 reset 方法的能力 |\n| ByteArrayInputStream    | ByteArrayInputStream 包含一个内部缓冲区，该缓冲区包含从流中读取的字节。 |\n| CharArrayReader    | 此类实现一个可用作字符输入流的字符缓冲区 |\n| Console    | 此类包含多个方法，可访问与当前 Java 虚拟机关联的基于字符的控制台设备（如果有）。 |\n| FileReader | 用来读取字符文件的便捷类 |\n| PushbackInputStream | PushbackInputStream 为另一个输入流添加性能，即“推回 (push back)”或“取消读取 (unread)”一个字节的能力。 |\n>Java I/O默认是不缓冲流的，所谓“缓冲”就是先把从流中得到的一块字节序列暂存在一个被称为buffer的内部字节数组里，然后你可以一下子取到这一整块的字节数据，没有缓冲的流只能一个字节一个字节读，效率孰高孰低一目了然。有两个特殊的输入流实现了缓冲功能，一个是我们常用的BufferedInputStream\n\n---\n#### 一些小结\n**带array的流自带缓冲区。支持mark与reset方法**\n**其他的要套缓冲流**\n\n 存在readLine()方法的流\n\n+ `BufferReader`\n\nread(char[],0,len)的理解\n将len从0位开始装入char[]中，然后通过String.valueOf(char[])转换成字符串","source":"_posts/技术/hexo/oldblog/blog7.md","raw":"---\n\ntitle: java中IO小结\ndate: 2019-07-16\ntags:\n\n---\n\n此处简介\n\n<!--more-->\n\n### java中IO小结\n#### File\nFile类,相关api\n* createNewFiles();\n* mkdir();\n* mkdirs()\n\n---\n### IO流\n按方向，分输入，输出流in、out\n流当中有缓冲区，即stream.read(),可以一次性读取stream.read(Byte[]byte);\n针对缓冲流还有两个方法\nmark与reset\nmarkSupported 判断该输入流能支持mark 和 reset 方法。\n\nmark用于标记当前位置；在读取一定数量的数据(小于readlimit的数据)后使用reset可以回到mark标记的位置。\n\nFileInputStream不支持mark/reset操作；BufferedInputStream支持此操作；\n\nmark(readlimit)的含义是在当前位置作一个标记，制定可以重新读取的最大字节数，也就是说你如果标记后读取的字节数大于readlimit，你就再也回不到回来的位置了。\n\n通常InputStream的read()返回-1后，说明到达文件尾，不能再读取。除非使用了mark/reset。\n\n#### 流的类型\n+  FileInputStream\n+  ObjectInputStream\n+  DataInputStream\n+  BufferedInputStream\n+  BufferedReader\n+  ByteArrayInputStream\n+  CharArrayReader\n+  Console\n+  FileReader\n+  PipedInputStream\n+  PipedReader\n+  PushbackInputStream\n+  StringReader\n\n| 流的名称  | 说明|\n| :-------- | --------: |\n| FileInputStream    | FileInputStream 从文件系统中的某个文件中获得输入字节。 |\n| ObjectInputStream  | ObjectInputStream 对以前使用 ObjectOutputStream 写入的基本数据和对象进行反序列化。 |\n| DataInputStream    | 数据输入流允许应用程序以与机器无关方式从底层输入流中读取基本 Java 数据类型。 |\n| BufferedInputStream    | BufferedInputStream 为另一个输入流添加一些功能，即缓冲输入以及支持 mark 和 reset 方法的能力 |\n| ByteArrayInputStream    | ByteArrayInputStream 包含一个内部缓冲区，该缓冲区包含从流中读取的字节。 |\n| CharArrayReader    | 此类实现一个可用作字符输入流的字符缓冲区 |\n| Console    | 此类包含多个方法，可访问与当前 Java 虚拟机关联的基于字符的控制台设备（如果有）。 |\n| FileReader | 用来读取字符文件的便捷类 |\n| PushbackInputStream | PushbackInputStream 为另一个输入流添加性能，即“推回 (push back)”或“取消读取 (unread)”一个字节的能力。 |\n>Java I/O默认是不缓冲流的，所谓“缓冲”就是先把从流中得到的一块字节序列暂存在一个被称为buffer的内部字节数组里，然后你可以一下子取到这一整块的字节数据，没有缓冲的流只能一个字节一个字节读，效率孰高孰低一目了然。有两个特殊的输入流实现了缓冲功能，一个是我们常用的BufferedInputStream\n\n---\n#### 一些小结\n**带array的流自带缓冲区。支持mark与reset方法**\n**其他的要套缓冲流**\n\n 存在readLine()方法的流\n\n+ `BufferReader`\n\nread(char[],0,len)的理解\n将len从0位开始装入char[]中，然后通过String.valueOf(char[])转换成字符串","slug":"技术/hexo/oldblog/blog7","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33k003938pweg4q4tsi","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h3 id=\"java中IO小结\"><a href=\"#java中IO小结\" class=\"headerlink\" title=\"java中IO小结\"></a>java中IO小结</h3><h4 id=\"File\"><a href=\"#File\" class=\"headerlink\" title=\"File\"></a>File</h4><p>File类,相关api</p>\n<ul>\n<li>createNewFiles();</li>\n<li>mkdir();</li>\n<li>mkdirs()</li>\n</ul>\n<hr>\n<h3 id=\"IO流\"><a href=\"#IO流\" class=\"headerlink\" title=\"IO流\"></a>IO流</h3><p>按方向，分输入，输出流in、out<br>流当中有缓冲区，即stream.read(),可以一次性读取stream.read(Byte[]byte);<br>针对缓冲流还有两个方法<br>mark与reset<br>markSupported 判断该输入流能支持mark 和 reset 方法。</p>\n<p>mark用于标记当前位置；在读取一定数量的数据(小于readlimit的数据)后使用reset可以回到mark标记的位置。</p>\n<p>FileInputStream不支持mark/reset操作；BufferedInputStream支持此操作；</p>\n<p>mark(readlimit)的含义是在当前位置作一个标记，制定可以重新读取的最大字节数，也就是说你如果标记后读取的字节数大于readlimit，你就再也回不到回来的位置了。</p>\n<p>通常InputStream的read()返回-1后，说明到达文件尾，不能再读取。除非使用了mark/reset。</p>\n<h4 id=\"流的类型\"><a href=\"#流的类型\" class=\"headerlink\" title=\"流的类型\"></a>流的类型</h4><ul>\n<li>FileInputStream</li>\n<li>ObjectInputStream</li>\n<li>DataInputStream</li>\n<li>BufferedInputStream</li>\n<li>BufferedReader</li>\n<li>ByteArrayInputStream</li>\n<li>CharArrayReader</li>\n<li>Console</li>\n<li>FileReader</li>\n<li>PipedInputStream</li>\n<li>PipedReader</li>\n<li>PushbackInputStream</li>\n<li>StringReader</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">流的名称</th>\n<th align=\"right\">说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">FileInputStream</td>\n<td align=\"right\">FileInputStream 从文件系统中的某个文件中获得输入字节。</td>\n</tr>\n<tr>\n<td align=\"left\">ObjectInputStream</td>\n<td align=\"right\">ObjectInputStream 对以前使用 ObjectOutputStream 写入的基本数据和对象进行反序列化。</td>\n</tr>\n<tr>\n<td align=\"left\">DataInputStream</td>\n<td align=\"right\">数据输入流允许应用程序以与机器无关方式从底层输入流中读取基本 Java 数据类型。</td>\n</tr>\n<tr>\n<td align=\"left\">BufferedInputStream</td>\n<td align=\"right\">BufferedInputStream 为另一个输入流添加一些功能，即缓冲输入以及支持 mark 和 reset 方法的能力</td>\n</tr>\n<tr>\n<td align=\"left\">ByteArrayInputStream</td>\n<td align=\"right\">ByteArrayInputStream 包含一个内部缓冲区，该缓冲区包含从流中读取的字节。</td>\n</tr>\n<tr>\n<td align=\"left\">CharArrayReader</td>\n<td align=\"right\">此类实现一个可用作字符输入流的字符缓冲区</td>\n</tr>\n<tr>\n<td align=\"left\">Console</td>\n<td align=\"right\">此类包含多个方法，可访问与当前 Java 虚拟机关联的基于字符的控制台设备（如果有）。</td>\n</tr>\n<tr>\n<td align=\"left\">FileReader</td>\n<td align=\"right\">用来读取字符文件的便捷类</td>\n</tr>\n<tr>\n<td align=\"left\">PushbackInputStream</td>\n<td align=\"right\">PushbackInputStream 为另一个输入流添加性能，即“推回 (push back)”或“取消读取 (unread)”一个字节的能力。</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>Java I/O默认是不缓冲流的，所谓“缓冲”就是先把从流中得到的一块字节序列暂存在一个被称为buffer的内部字节数组里，然后你可以一下子取到这一整块的字节数据，没有缓冲的流只能一个字节一个字节读，效率孰高孰低一目了然。有两个特殊的输入流实现了缓冲功能，一个是我们常用的BufferedInputStream</p>\n</blockquote>\n<hr>\n<h4 id=\"一些小结\"><a href=\"#一些小结\" class=\"headerlink\" title=\"一些小结\"></a>一些小结</h4><p><strong>带array的流自带缓冲区。支持mark与reset方法</strong><br><strong>其他的要套缓冲流</strong></p>\n<p> 存在readLine()方法的流</p>\n<ul>\n<li><code>BufferReader</code></li>\n</ul>\n<p>read(char[],0,len)的理解<br>将len从0位开始装入char[]中，然后通过String.valueOf(char[])转换成字符串</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h3 id=\"java中IO小结\"><a href=\"#java中IO小结\" class=\"headerlink\" title=\"java中IO小结\"></a>java中IO小结</h3><h4 id=\"File\"><a href=\"#File\" class=\"headerlink\" title=\"File\"></a>File</h4><p>File类,相关api</p>\n<ul>\n<li>createNewFiles();</li>\n<li>mkdir();</li>\n<li>mkdirs()</li>\n</ul>\n<hr>\n<h3 id=\"IO流\"><a href=\"#IO流\" class=\"headerlink\" title=\"IO流\"></a>IO流</h3><p>按方向，分输入，输出流in、out<br>流当中有缓冲区，即stream.read(),可以一次性读取stream.read(Byte[]byte);<br>针对缓冲流还有两个方法<br>mark与reset<br>markSupported 判断该输入流能支持mark 和 reset 方法。</p>\n<p>mark用于标记当前位置；在读取一定数量的数据(小于readlimit的数据)后使用reset可以回到mark标记的位置。</p>\n<p>FileInputStream不支持mark/reset操作；BufferedInputStream支持此操作；</p>\n<p>mark(readlimit)的含义是在当前位置作一个标记，制定可以重新读取的最大字节数，也就是说你如果标记后读取的字节数大于readlimit，你就再也回不到回来的位置了。</p>\n<p>通常InputStream的read()返回-1后，说明到达文件尾，不能再读取。除非使用了mark/reset。</p>\n<h4 id=\"流的类型\"><a href=\"#流的类型\" class=\"headerlink\" title=\"流的类型\"></a>流的类型</h4><ul>\n<li>FileInputStream</li>\n<li>ObjectInputStream</li>\n<li>DataInputStream</li>\n<li>BufferedInputStream</li>\n<li>BufferedReader</li>\n<li>ByteArrayInputStream</li>\n<li>CharArrayReader</li>\n<li>Console</li>\n<li>FileReader</li>\n<li>PipedInputStream</li>\n<li>PipedReader</li>\n<li>PushbackInputStream</li>\n<li>StringReader</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">流的名称</th>\n<th align=\"right\">说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">FileInputStream</td>\n<td align=\"right\">FileInputStream 从文件系统中的某个文件中获得输入字节。</td>\n</tr>\n<tr>\n<td align=\"left\">ObjectInputStream</td>\n<td align=\"right\">ObjectInputStream 对以前使用 ObjectOutputStream 写入的基本数据和对象进行反序列化。</td>\n</tr>\n<tr>\n<td align=\"left\">DataInputStream</td>\n<td align=\"right\">数据输入流允许应用程序以与机器无关方式从底层输入流中读取基本 Java 数据类型。</td>\n</tr>\n<tr>\n<td align=\"left\">BufferedInputStream</td>\n<td align=\"right\">BufferedInputStream 为另一个输入流添加一些功能，即缓冲输入以及支持 mark 和 reset 方法的能力</td>\n</tr>\n<tr>\n<td align=\"left\">ByteArrayInputStream</td>\n<td align=\"right\">ByteArrayInputStream 包含一个内部缓冲区，该缓冲区包含从流中读取的字节。</td>\n</tr>\n<tr>\n<td align=\"left\">CharArrayReader</td>\n<td align=\"right\">此类实现一个可用作字符输入流的字符缓冲区</td>\n</tr>\n<tr>\n<td align=\"left\">Console</td>\n<td align=\"right\">此类包含多个方法，可访问与当前 Java 虚拟机关联的基于字符的控制台设备（如果有）。</td>\n</tr>\n<tr>\n<td align=\"left\">FileReader</td>\n<td align=\"right\">用来读取字符文件的便捷类</td>\n</tr>\n<tr>\n<td align=\"left\">PushbackInputStream</td>\n<td align=\"right\">PushbackInputStream 为另一个输入流添加性能，即“推回 (push back)”或“取消读取 (unread)”一个字节的能力。</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p>Java I/O默认是不缓冲流的，所谓“缓冲”就是先把从流中得到的一块字节序列暂存在一个被称为buffer的内部字节数组里，然后你可以一下子取到这一整块的字节数据，没有缓冲的流只能一个字节一个字节读，效率孰高孰低一目了然。有两个特殊的输入流实现了缓冲功能，一个是我们常用的BufferedInputStream</p>\n</blockquote>\n<hr>\n<h4 id=\"一些小结\"><a href=\"#一些小结\" class=\"headerlink\" title=\"一些小结\"></a>一些小结</h4><p><strong>带array的流自带缓冲区。支持mark与reset方法</strong><br><strong>其他的要套缓冲流</strong></p>\n<p> 存在readLine()方法的流</p>\n<ul>\n<li><code>BufferReader</code></li>\n</ul>\n<p>read(char[],0,len)的理解<br>将len从0位开始装入char[]中，然后通过String.valueOf(char[])转换成字符串</p>"},{"title":"map总结","date":"2019-07-15T16:00:00.000Z","_content":"\n此处简介\n\n<!--more-->\n\n### map总结\n\nHashtable\n├-HashMap\n└-WeakHashMap\n\n\n通用Map，用于在应用程序中管理映射，通常在 java.util 程序包中实现\n\n+ HashMap\n+ Hashtable\n+ Properties\n+ LinkedHashMap\n+ IdentityHashMap\n+ TreeMap\n+ WeakHashMap\n+ ConcurrentHashMap\n\n\n**HashMap**\n\n```\n最常用的Map,它根据键的HashCode 值存储数据,根据键可以直接获取它的值，具有很快的访问速度。HashMap最多只允许一条记录的键为Null(多条会覆盖);允许多条记录的值为 Null。非同步的。\n```\n\n**Hashtable**\n\n```\n与 HashMap类似,不同的是:key和value的值均不允许为null;它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了Hashtale在写入时会比较慢。\nLinkedHashMap\n\n保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的.在遍历的时候会比HashMap慢。key和value均允许为空，非同步的。\n```\n**TreeMap**\n\n![d98ba033b20b4f48a430419e96e98fa3-image.png](//img.wqkenqing.ren/file/2017/7/d98ba033b20b4f48a430419e96e98fa3-image.png)\n\n\n\n\n    + 增强for循环使用方便，但性能较差，不适合处理超大量级的数据。\n    + 迭代器的遍历速度要比增强for循环快很多，是增强for循环的2倍左右。\n    + 使用entrySet遍历的速度要比keySet快很多，是keySet的1.5倍左右。","source":"_posts/技术/hexo/oldblog/blog9.md","raw":"\n---\n\ntitle: map总结\ndate: 2019-07-16\ntags:\n\n---\n\n此处简介\n\n<!--more-->\n\n### map总结\n\nHashtable\n├-HashMap\n└-WeakHashMap\n\n\n通用Map，用于在应用程序中管理映射，通常在 java.util 程序包中实现\n\n+ HashMap\n+ Hashtable\n+ Properties\n+ LinkedHashMap\n+ IdentityHashMap\n+ TreeMap\n+ WeakHashMap\n+ ConcurrentHashMap\n\n\n**HashMap**\n\n```\n最常用的Map,它根据键的HashCode 值存储数据,根据键可以直接获取它的值，具有很快的访问速度。HashMap最多只允许一条记录的键为Null(多条会覆盖);允许多条记录的值为 Null。非同步的。\n```\n\n**Hashtable**\n\n```\n与 HashMap类似,不同的是:key和value的值均不允许为null;它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了Hashtale在写入时会比较慢。\nLinkedHashMap\n\n保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的.在遍历的时候会比HashMap慢。key和value均允许为空，非同步的。\n```\n**TreeMap**\n\n![d98ba033b20b4f48a430419e96e98fa3-image.png](//img.wqkenqing.ren/file/2017/7/d98ba033b20b4f48a430419e96e98fa3-image.png)\n\n\n\n\n    + 增强for循环使用方便，但性能较差，不适合处理超大量级的数据。\n    + 迭代器的遍历速度要比增强for循环快很多，是增强for循环的2倍左右。\n    + 使用entrySet遍历的速度要比keySet快很多，是keySet的1.5倍左右。","slug":"技术/hexo/oldblog/blog9","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33k003a38pw8m31fe1e","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h3 id=\"map总结\"><a href=\"#map总结\" class=\"headerlink\" title=\"map总结\"></a>map总结</h3><p>Hashtable<br>├-HashMap<br>└-WeakHashMap</p>\n<p>通用Map，用于在应用程序中管理映射，通常在 java.util 程序包中实现</p>\n<ul>\n<li>HashMap</li>\n<li>Hashtable</li>\n<li>Properties</li>\n<li>LinkedHashMap</li>\n<li>IdentityHashMap</li>\n<li>TreeMap</li>\n<li>WeakHashMap</li>\n<li>ConcurrentHashMap</li>\n</ul>\n<p><strong>HashMap</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">最常用的Map,它根据键的HashCode 值存储数据,根据键可以直接获取它的值，具有很快的访问速度。HashMap最多只允许一条记录的键为Null(多条会覆盖);允许多条记录的值为 Null。非同步的。</span><br></pre></td></tr></table></figure>\n\n<p><strong>Hashtable</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">与 HashMap类似,不同的是:key和value的值均不允许为null;它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了Hashtale在写入时会比较慢。</span><br><span class=\"line\">LinkedHashMap</span><br><span class=\"line\"></span><br><span class=\"line\">保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的.在遍历的时候会比HashMap慢。key和value均允许为空，非同步的。</span><br></pre></td></tr></table></figure>\n<p><strong>TreeMap</strong></p>\n<p><img src=\"//img.wqkenqing.ren/file/2017/7/d98ba033b20b4f48a430419e96e98fa3-image.png\" alt=\"d98ba033b20b4f48a430419e96e98fa3-image.png\"></p>\n<pre><code>+ 增强for循环使用方便，但性能较差，不适合处理超大量级的数据。\n+ 迭代器的遍历速度要比增强for循环快很多，是增强for循环的2倍左右。\n+ 使用entrySet遍历的速度要比keySet快很多，是keySet的1.5倍左右。</code></pre>","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h3 id=\"map总结\"><a href=\"#map总结\" class=\"headerlink\" title=\"map总结\"></a>map总结</h3><p>Hashtable<br>├-HashMap<br>└-WeakHashMap</p>\n<p>通用Map，用于在应用程序中管理映射，通常在 java.util 程序包中实现</p>\n<ul>\n<li>HashMap</li>\n<li>Hashtable</li>\n<li>Properties</li>\n<li>LinkedHashMap</li>\n<li>IdentityHashMap</li>\n<li>TreeMap</li>\n<li>WeakHashMap</li>\n<li>ConcurrentHashMap</li>\n</ul>\n<p><strong>HashMap</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">最常用的Map,它根据键的HashCode 值存储数据,根据键可以直接获取它的值，具有很快的访问速度。HashMap最多只允许一条记录的键为Null(多条会覆盖);允许多条记录的值为 Null。非同步的。</span><br></pre></td></tr></table></figure>\n\n<p><strong>Hashtable</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">与 HashMap类似,不同的是:key和value的值均不允许为null;它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了Hashtale在写入时会比较慢。</span><br><span class=\"line\">LinkedHashMap</span><br><span class=\"line\"></span><br><span class=\"line\">保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的.在遍历的时候会比HashMap慢。key和value均允许为空，非同步的。</span><br></pre></td></tr></table></figure>\n<p><strong>TreeMap</strong></p>\n<p><img src=\"//img.wqkenqing.ren/file/2017/7/d98ba033b20b4f48a430419e96e98fa3-image.png\" alt=\"d98ba033b20b4f48a430419e96e98fa3-image.png\"></p>\n<pre><code>+ 增强for循环使用方便，但性能较差，不适合处理超大量级的数据。\n+ 迭代器的遍历速度要比增强for循环快很多，是增强for循环的2倍左右。\n+ 使用entrySet遍历的速度要比keySet快很多，是keySet的1.5倍左右。</code></pre>"},{"title":"java中的形参与实参的理解","date":"2019-07-15T16:00:00.000Z","_content":"此处简介\n\n<!--more-->\n\n## java中的形参与实参的理解\n#### 值传递：\n方法调用时，实际参数把它的值传递给对应的形式参数，函数接收的是原始值的一个copy，此时内存中存在两个相等的基本类型，即实际参数和形式参数，后面方法中的操作都是对形参这个值的修改，不影响实际参数的值。\n#### 引用传递：\n也称为传地址。方法调用时，实际参数的引用(地址，而不是参数的值)被传递给方法中相对应的形式参数，函数接收的是原始值的内存地址；\n在方法执行中，形参和实参内容相同，指向同一块内存地址，方法执行中对引用的操作将会影响到实际对象。\n\n`*值传递：方法调用时，实际参数将它的值传递给对应的形式参数，函数接收到的是原始值的副本，此时内存中存在两个相等的基本类型，若方法中对形参执行处理操作，并不会影响实际参数的值。`\n\n`*引用传递：方法调用时，实际参数的引用（是指地址，而不是参数的值）被传递给方法中相应的形式参数，函数接收到的是原始值的内存地址，在方法中，形参与实参的内容相同，方法中对形参的处理会影响实参的值。`\n\n\n1）形参为基本类型时，对形参的处理不会影响实参。\n2）形参为引用类型时，对形参的处理会影响实参。\n3）String,Integer,Double等immutable类型的特殊处理，可以理解为值传递，形参操作不会影响实参对象。","source":"_posts/技术/hexo/oldblog/blog8.md","raw":"---\n\ntitle: java中的形参与实参的理解\ndate: 2019-07-16\ntags:\n\n---\n此处简介\n\n<!--more-->\n\n## java中的形参与实参的理解\n#### 值传递：\n方法调用时，实际参数把它的值传递给对应的形式参数，函数接收的是原始值的一个copy，此时内存中存在两个相等的基本类型，即实际参数和形式参数，后面方法中的操作都是对形参这个值的修改，不影响实际参数的值。\n#### 引用传递：\n也称为传地址。方法调用时，实际参数的引用(地址，而不是参数的值)被传递给方法中相对应的形式参数，函数接收的是原始值的内存地址；\n在方法执行中，形参和实参内容相同，指向同一块内存地址，方法执行中对引用的操作将会影响到实际对象。\n\n`*值传递：方法调用时，实际参数将它的值传递给对应的形式参数，函数接收到的是原始值的副本，此时内存中存在两个相等的基本类型，若方法中对形参执行处理操作，并不会影响实际参数的值。`\n\n`*引用传递：方法调用时，实际参数的引用（是指地址，而不是参数的值）被传递给方法中相应的形式参数，函数接收到的是原始值的内存地址，在方法中，形参与实参的内容相同，方法中对形参的处理会影响实参的值。`\n\n\n1）形参为基本类型时，对形参的处理不会影响实参。\n2）形参为引用类型时，对形参的处理会影响实参。\n3）String,Integer,Double等immutable类型的特殊处理，可以理解为值传递，形参操作不会影响实参对象。","slug":"技术/hexo/oldblog/blog8","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33m003b38pwaqlhbcpa","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h2 id=\"java中的形参与实参的理解\"><a href=\"#java中的形参与实参的理解\" class=\"headerlink\" title=\"java中的形参与实参的理解\"></a>java中的形参与实参的理解</h2><h4 id=\"值传递：\"><a href=\"#值传递：\" class=\"headerlink\" title=\"值传递：\"></a>值传递：</h4><p>方法调用时，实际参数把它的值传递给对应的形式参数，函数接收的是原始值的一个copy，此时内存中存在两个相等的基本类型，即实际参数和形式参数，后面方法中的操作都是对形参这个值的修改，不影响实际参数的值。</p>\n<h4 id=\"引用传递：\"><a href=\"#引用传递：\" class=\"headerlink\" title=\"引用传递：\"></a>引用传递：</h4><p>也称为传地址。方法调用时，实际参数的引用(地址，而不是参数的值)被传递给方法中相对应的形式参数，函数接收的是原始值的内存地址；<br>在方法执行中，形参和实参内容相同，指向同一块内存地址，方法执行中对引用的操作将会影响到实际对象。</p>\n<p><code>*值传递：方法调用时，实际参数将它的值传递给对应的形式参数，函数接收到的是原始值的副本，此时内存中存在两个相等的基本类型，若方法中对形参执行处理操作，并不会影响实际参数的值。</code></p>\n<p><code>*引用传递：方法调用时，实际参数的引用（是指地址，而不是参数的值）被传递给方法中相应的形式参数，函数接收到的是原始值的内存地址，在方法中，形参与实参的内容相同，方法中对形参的处理会影响实参的值。</code></p>\n<p>1）形参为基本类型时，对形参的处理不会影响实参。<br>2）形参为引用类型时，对形参的处理会影响实参。<br>3）String,Integer,Double等immutable类型的特殊处理，可以理解为值传递，形参操作不会影响实参对象。</p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h2 id=\"java中的形参与实参的理解\"><a href=\"#java中的形参与实参的理解\" class=\"headerlink\" title=\"java中的形参与实参的理解\"></a>java中的形参与实参的理解</h2><h4 id=\"值传递：\"><a href=\"#值传递：\" class=\"headerlink\" title=\"值传递：\"></a>值传递：</h4><p>方法调用时，实际参数把它的值传递给对应的形式参数，函数接收的是原始值的一个copy，此时内存中存在两个相等的基本类型，即实际参数和形式参数，后面方法中的操作都是对形参这个值的修改，不影响实际参数的值。</p>\n<h4 id=\"引用传递：\"><a href=\"#引用传递：\" class=\"headerlink\" title=\"引用传递：\"></a>引用传递：</h4><p>也称为传地址。方法调用时，实际参数的引用(地址，而不是参数的值)被传递给方法中相对应的形式参数，函数接收的是原始值的内存地址；<br>在方法执行中，形参和实参内容相同，指向同一块内存地址，方法执行中对引用的操作将会影响到实际对象。</p>\n<p><code>*值传递：方法调用时，实际参数将它的值传递给对应的形式参数，函数接收到的是原始值的副本，此时内存中存在两个相等的基本类型，若方法中对形参执行处理操作，并不会影响实际参数的值。</code></p>\n<p><code>*引用传递：方法调用时，实际参数的引用（是指地址，而不是参数的值）被传递给方法中相应的形式参数，函数接收到的是原始值的内存地址，在方法中，形参与实参的内容相同，方法中对形参的处理会影响实参的值。</code></p>\n<p>1）形参为基本类型时，对形参的处理不会影响实参。<br>2）形参为引用类型时，对形参的处理会影响实参。<br>3）String,Integer,Double等immutable类型的特殊处理，可以理解为值传递，形参操作不会影响实参对象。</p>"},{"title":"spark操作.md","date":"2019-05-16T16:00:00.000Z","_content":"spark编程积累\n\n<!--more-->\n\n# spark编程\n\n## Input\n### hdfs\n操作hdfs比较常规,直接通过\ncontext.textfile(path) //即可实现\n### hbase\nhbase 则要通过newAPIHadoopRDD来实现\n```java\n\nJavaPairRDD<ImmutableBytesWritable, Result> javaRDD = jsc.newAPIHadoopRDD(HbaseOperate.getConf(), TableInputFormat.class, ImmutableBytesWritable.class, Result.class);\n\n```\n这里要特别说明的是,这里的conf承担了更多的责任,如指定表名,指定scan传输字符串等.\n```java\n\n    Configuration hconf = HbaseOperate.getConf();\n        Scan scan = new Scan();\n        hconf.set(TableInputFormat.INPUT_TABLE, \"company\");\n        hconf.set(TableInputFormat.SCAN, convertScanToString(scan));\n\n```\n参考以上这段代码\n\n另\n```java\n   static String convertScanToString(Scan scan) throws IOException {\n        ClientProtos.Scan proto = ProtobufUtil.toScan(scan);\n        return Base64.encodeBytes(proto.toByteArray());\n    }\n```\n以上是为实现scan指令传输字符的封装.\n\n两者底层都是通过persist实现\n\n","source":"_posts/技术/hexo/spark/spark操作.md","raw":"---\n\ntitle: spark操作.md\ndate: 2019-05-17\ntags: \n\n---\nspark编程积累\n\n<!--more-->\n\n# spark编程\n\n## Input\n### hdfs\n操作hdfs比较常规,直接通过\ncontext.textfile(path) //即可实现\n### hbase\nhbase 则要通过newAPIHadoopRDD来实现\n```java\n\nJavaPairRDD<ImmutableBytesWritable, Result> javaRDD = jsc.newAPIHadoopRDD(HbaseOperate.getConf(), TableInputFormat.class, ImmutableBytesWritable.class, Result.class);\n\n```\n这里要特别说明的是,这里的conf承担了更多的责任,如指定表名,指定scan传输字符串等.\n```java\n\n    Configuration hconf = HbaseOperate.getConf();\n        Scan scan = new Scan();\n        hconf.set(TableInputFormat.INPUT_TABLE, \"company\");\n        hconf.set(TableInputFormat.SCAN, convertScanToString(scan));\n\n```\n参考以上这段代码\n\n另\n```java\n   static String convertScanToString(Scan scan) throws IOException {\n        ClientProtos.Scan proto = ProtobufUtil.toScan(scan);\n        return Base64.encodeBytes(proto.toByteArray());\n    }\n```\n以上是为实现scan指令传输字符的封装.\n\n两者底层都是通过persist实现\n\n","slug":"技术/hexo/spark/spark操作","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33m003c38pwgsus8vli","content":"<p>spark编程积累</p>\n<a id=\"more\"></a>\n\n<h1 id=\"spark编程\"><a href=\"#spark编程\" class=\"headerlink\" title=\"spark编程\"></a>spark编程</h1><h2 id=\"Input\"><a href=\"#Input\" class=\"headerlink\" title=\"Input\"></a>Input</h2><h3 id=\"hdfs\"><a href=\"#hdfs\" class=\"headerlink\" title=\"hdfs\"></a>hdfs</h3><p>操作hdfs比较常规,直接通过<br>context.textfile(path) //即可实现</p>\n<h3 id=\"hbase\"><a href=\"#hbase\" class=\"headerlink\" title=\"hbase\"></a>hbase</h3><p>hbase 则要通过newAPIHadoopRDD来实现</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">JavaPairRDD&lt;ImmutableBytesWritable, Result&gt; javaRDD = jsc.newAPIHadoopRDD(HbaseOperate.getConf(), TableInputFormat<span class=\"class\">.<span class=\"keyword\">class</span>, <span class=\"title\">ImmutableBytesWritable</span>.<span class=\"title\">class</span>, <span class=\"title\">Result</span>.<span class=\"title\">class</span>)</span>;</span><br></pre></td></tr></table></figure>\n<p>这里要特别说明的是,这里的conf承担了更多的责任,如指定表名,指定scan传输字符串等.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">Configuration hconf = HbaseOperate.getConf();</span><br><span class=\"line\">    Scan scan = <span class=\"keyword\">new</span> Scan();</span><br><span class=\"line\">    hconf.set(TableInputFormat.INPUT_TABLE, <span class=\"string\">\"company\"</span>);</span><br><span class=\"line\">    hconf.set(TableInputFormat.SCAN, convertScanToString(scan));</span><br></pre></td></tr></table></figure>\n<p>参考以上这段代码</p>\n<p>另</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> String <span class=\"title\">convertScanToString</span><span class=\"params\">(Scan scan)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">     ClientProtos.Scan proto = ProtobufUtil.toScan(scan);</span><br><span class=\"line\">     <span class=\"keyword\">return</span> Base64.encodeBytes(proto.toByteArray());</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>以上是为实现scan指令传输字符的封装.</p>\n<p>两者底层都是通过persist实现</p>\n","site":{"data":{}},"excerpt":"<p>spark编程积累</p>","more":"<h1 id=\"spark编程\"><a href=\"#spark编程\" class=\"headerlink\" title=\"spark编程\"></a>spark编程</h1><h2 id=\"Input\"><a href=\"#Input\" class=\"headerlink\" title=\"Input\"></a>Input</h2><h3 id=\"hdfs\"><a href=\"#hdfs\" class=\"headerlink\" title=\"hdfs\"></a>hdfs</h3><p>操作hdfs比较常规,直接通过<br>context.textfile(path) //即可实现</p>\n<h3 id=\"hbase\"><a href=\"#hbase\" class=\"headerlink\" title=\"hbase\"></a>hbase</h3><p>hbase 则要通过newAPIHadoopRDD来实现</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">JavaPairRDD&lt;ImmutableBytesWritable, Result&gt; javaRDD = jsc.newAPIHadoopRDD(HbaseOperate.getConf(), TableInputFormat<span class=\"class\">.<span class=\"keyword\">class</span>, <span class=\"title\">ImmutableBytesWritable</span>.<span class=\"title\">class</span>, <span class=\"title\">Result</span>.<span class=\"title\">class</span>)</span>;</span><br></pre></td></tr></table></figure>\n<p>这里要特别说明的是,这里的conf承担了更多的责任,如指定表名,指定scan传输字符串等.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">Configuration hconf = HbaseOperate.getConf();</span><br><span class=\"line\">    Scan scan = <span class=\"keyword\">new</span> Scan();</span><br><span class=\"line\">    hconf.set(TableInputFormat.INPUT_TABLE, <span class=\"string\">\"company\"</span>);</span><br><span class=\"line\">    hconf.set(TableInputFormat.SCAN, convertScanToString(scan));</span><br></pre></td></tr></table></figure>\n<p>参考以上这段代码</p>\n<p>另</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">static</span> String <span class=\"title\">convertScanToString</span><span class=\"params\">(Scan scan)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</span><br><span class=\"line\">     ClientProtos.Scan proto = ProtobufUtil.toScan(scan);</span><br><span class=\"line\">     <span class=\"keyword\">return</span> Base64.encodeBytes(proto.toByteArray());</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>以上是为实现scan指令传输字符的封装.</p>\n<p>两者底层都是通过persist实现</p>"},{"title":"sparkstreaming 窗口操作","date":"2019-07-15T16:00:00.000Z","_content":"\nsparkstreaming时间窗口设置\n\n<!--more-->\n\n## 说明\n\n通过sparkstreaming设置窗口函数,可达到如,每10秒计算前30秒内数据的效果\n\n如上 主要有两个参数\n\n1. 窗口大小\n2. 滑动距离\n\nval windowedWordCounts = pairs.reduceByKeyAndWindow(_ + _, Seconds(30), Seconds(10))\n\n如上\n\n## 常用api\n\n| Transformation | Meaning |\n| :--- | :----: |\n| window(windowLength, slideInterval)\t | Return a new DStream which is computed based on windowed batches of the source DStream.|\n| countByWindow(windowLength,slideInterval)\t    | Return a sliding window count of elements in the stream.|\n|reduceByWindow(func, windowLength,slideInterval)| |\n|reduceByKeyAndWindow(func,windowLength, slideInterval, [numTasks])| |\n|reduceByKeyAndWindow(func, invFunc,windowLength, slideInterval, [numTasks])\t| |\n|countByValueAndWindow(windowLength,slideInterval, [numTasks])\t| |\n\n","source":"_posts/技术/hexo/spark/stream2.md","raw":"---\n\ntitle: sparkstreaming 窗口操作\ndate: 2019-07-16\ntags: sparkstreaming\n\n---\n\nsparkstreaming时间窗口设置\n\n<!--more-->\n\n## 说明\n\n通过sparkstreaming设置窗口函数,可达到如,每10秒计算前30秒内数据的效果\n\n如上 主要有两个参数\n\n1. 窗口大小\n2. 滑动距离\n\nval windowedWordCounts = pairs.reduceByKeyAndWindow(_ + _, Seconds(30), Seconds(10))\n\n如上\n\n## 常用api\n\n| Transformation | Meaning |\n| :--- | :----: |\n| window(windowLength, slideInterval)\t | Return a new DStream which is computed based on windowed batches of the source DStream.|\n| countByWindow(windowLength,slideInterval)\t    | Return a sliding window count of elements in the stream.|\n|reduceByWindow(func, windowLength,slideInterval)| |\n|reduceByKeyAndWindow(func,windowLength, slideInterval, [numTasks])| |\n|reduceByKeyAndWindow(func, invFunc,windowLength, slideInterval, [numTasks])\t| |\n|countByValueAndWindow(windowLength,slideInterval, [numTasks])\t| |\n\n","slug":"技术/hexo/spark/stream2","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33n003d38pwf4dqdqsb","content":"<p>sparkstreaming时间窗口设置</p>\n<a id=\"more\"></a>\n\n<h2 id=\"说明\"><a href=\"#说明\" class=\"headerlink\" title=\"说明\"></a>说明</h2><p>通过sparkstreaming设置窗口函数,可达到如,每10秒计算前30秒内数据的效果</p>\n<p>如上 主要有两个参数</p>\n<ol>\n<li>窗口大小</li>\n<li>滑动距离</li>\n</ol>\n<p>val windowedWordCounts = pairs.reduceByKeyAndWindow(_ + _, Seconds(30), Seconds(10))</p>\n<p>如上</p>\n<h2 id=\"常用api\"><a href=\"#常用api\" class=\"headerlink\" title=\"常用api\"></a>常用api</h2><table>\n<thead>\n<tr>\n<th align=\"left\">Transformation</th>\n<th align=\"center\">Meaning</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">window(windowLength, slideInterval)</td>\n<td align=\"center\">Return a new DStream which is computed based on windowed batches of the source DStream.</td>\n</tr>\n<tr>\n<td align=\"left\">countByWindow(windowLength,slideInterval)</td>\n<td align=\"center\">Return a sliding window count of elements in the stream.</td>\n</tr>\n<tr>\n<td align=\"left\">reduceByWindow(func, windowLength,slideInterval)</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"left\">reduceByKeyAndWindow(func,windowLength, slideInterval, [numTasks])</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"left\">reduceByKeyAndWindow(func, invFunc,windowLength, slideInterval, [numTasks])</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"left\">countByValueAndWindow(windowLength,slideInterval, [numTasks])</td>\n<td align=\"center\"></td>\n</tr>\n</tbody></table>\n","site":{"data":{}},"excerpt":"<p>sparkstreaming时间窗口设置</p>","more":"<h2 id=\"说明\"><a href=\"#说明\" class=\"headerlink\" title=\"说明\"></a>说明</h2><p>通过sparkstreaming设置窗口函数,可达到如,每10秒计算前30秒内数据的效果</p>\n<p>如上 主要有两个参数</p>\n<ol>\n<li>窗口大小</li>\n<li>滑动距离</li>\n</ol>\n<p>val windowedWordCounts = pairs.reduceByKeyAndWindow(_ + _, Seconds(30), Seconds(10))</p>\n<p>如上</p>\n<h2 id=\"常用api\"><a href=\"#常用api\" class=\"headerlink\" title=\"常用api\"></a>常用api</h2><table>\n<thead>\n<tr>\n<th align=\"left\">Transformation</th>\n<th align=\"center\">Meaning</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">window(windowLength, slideInterval)</td>\n<td align=\"center\">Return a new DStream which is computed based on windowed batches of the source DStream.</td>\n</tr>\n<tr>\n<td align=\"left\">countByWindow(windowLength,slideInterval)</td>\n<td align=\"center\">Return a sliding window count of elements in the stream.</td>\n</tr>\n<tr>\n<td align=\"left\">reduceByWindow(func, windowLength,slideInterval)</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"left\">reduceByKeyAndWindow(func,windowLength, slideInterval, [numTasks])</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"left\">reduceByKeyAndWindow(func, invFunc,windowLength, slideInterval, [numTasks])</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"left\">countByValueAndWindow(windowLength,slideInterval, [numTasks])</td>\n<td align=\"center\"></td>\n</tr>\n</tbody></table>"},{"title":"updateStateByKey&mapStateWithKey","date":"2019-07-14T16:00:00.000Z","_content":"\nspark中如何实现全局count\n\n<!--more-->\n\n## 说明\n\n两种方式都可以实现对同一key的累计统计\n\n区别\nupdateStateByKey会返回无增量数据的状态,所以会相对较大的数据资源开销\nmapStateWithKey 相当于增量统计\n\n## 使用\n\nupdateStateByKey :\n\n```java\n\n public static Function2<List<Integer>, Optional<Integer>, Optional<Integer>> updateFunctionByUpdate() {\n        Function2<List<Integer>, Optional<Integer>, Optional<Integer>> updateFunction = (values, s1) -> {\n            Integer newSum = 0;\n            if (s1.isPresent()) {\n                newSum = s1.get();\n            }\n\n            Iterator<Integer> i = values.iterator();\n            while (i.hasNext()) {\n                newSum += i.next();\n            }\n            return Optional.of(newSum);\n        };\n        return updateFunction;\n    }\n\n```\nmapStateWithKey :\n\n```java\n public static Function3<String, Optional<Integer>, State<Integer>, Tuple2<String, Integer>> updateFunctionByMap() {\n        Function3<String, Optional<Integer>, State<Integer>, Tuple2<String, Integer>> updateFunction2 = (word, one,\n                                                                                                         state) -> {\n            int sum = one.or(0) + (state.exists() ? state.get() : 0);\n            Tuple2<String, Integer> output = new Tuple2<String, Integer>(word, sum);\n            state.update(sum);\n            return output;\n        };\n        return updateFunction2;\n    }\n\n```\n","source":"_posts/技术/hexo/spark/stream.md","raw":"---\n\ntitle: updateStateByKey&mapStateWithKey\ndate: 2019-07-15\ntags: sparkstream\n\n---\n\nspark中如何实现全局count\n\n<!--more-->\n\n## 说明\n\n两种方式都可以实现对同一key的累计统计\n\n区别\nupdateStateByKey会返回无增量数据的状态,所以会相对较大的数据资源开销\nmapStateWithKey 相当于增量统计\n\n## 使用\n\nupdateStateByKey :\n\n```java\n\n public static Function2<List<Integer>, Optional<Integer>, Optional<Integer>> updateFunctionByUpdate() {\n        Function2<List<Integer>, Optional<Integer>, Optional<Integer>> updateFunction = (values, s1) -> {\n            Integer newSum = 0;\n            if (s1.isPresent()) {\n                newSum = s1.get();\n            }\n\n            Iterator<Integer> i = values.iterator();\n            while (i.hasNext()) {\n                newSum += i.next();\n            }\n            return Optional.of(newSum);\n        };\n        return updateFunction;\n    }\n\n```\nmapStateWithKey :\n\n```java\n public static Function3<String, Optional<Integer>, State<Integer>, Tuple2<String, Integer>> updateFunctionByMap() {\n        Function3<String, Optional<Integer>, State<Integer>, Tuple2<String, Integer>> updateFunction2 = (word, one,\n                                                                                                         state) -> {\n            int sum = one.or(0) + (state.exists() ? state.get() : 0);\n            Tuple2<String, Integer> output = new Tuple2<String, Integer>(word, sum);\n            state.update(sum);\n            return output;\n        };\n        return updateFunction2;\n    }\n\n```\n","slug":"技术/hexo/spark/stream","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33n003e38pwgyx80c2w","content":"<p>spark中如何实现全局count</p>\n<a id=\"more\"></a>\n\n<h2 id=\"说明\"><a href=\"#说明\" class=\"headerlink\" title=\"说明\"></a>说明</h2><p>两种方式都可以实现对同一key的累计统计</p>\n<p>区别<br>updateStateByKey会返回无增量数据的状态,所以会相对较大的数据资源开销<br>mapStateWithKey 相当于增量统计</p>\n<h2 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h2><p>updateStateByKey :</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt; updateFunctionByUpdate() &#123;</span><br><span class=\"line\">       Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt; updateFunction = (values, s1) -&gt; &#123;</span><br><span class=\"line\">           Integer newSum = <span class=\"number\">0</span>;</span><br><span class=\"line\">           <span class=\"keyword\">if</span> (s1.isPresent()) &#123;</span><br><span class=\"line\">               newSum = s1.get();</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">           Iterator&lt;Integer&gt; i = values.iterator();</span><br><span class=\"line\">           <span class=\"keyword\">while</span> (i.hasNext()) &#123;</span><br><span class=\"line\">               newSum += i.next();</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">           <span class=\"keyword\">return</span> Optional.of(newSum);</span><br><span class=\"line\">       &#125;;</span><br><span class=\"line\">       <span class=\"keyword\">return</span> updateFunction;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>mapStateWithKey :</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> Function3&lt;String, Optional&lt;Integer&gt;, State&lt;Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; updateFunctionByMap() &#123;</span><br><span class=\"line\">       Function3&lt;String, Optional&lt;Integer&gt;, State&lt;Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; updateFunction2 = (word, one,</span><br><span class=\"line\">                                                                                                        state) -&gt; &#123;</span><br><span class=\"line\">           <span class=\"keyword\">int</span> sum = one.or(<span class=\"number\">0</span>) + (state.exists() ? state.get() : <span class=\"number\">0</span>);</span><br><span class=\"line\">           Tuple2&lt;String, Integer&gt; output = <span class=\"keyword\">new</span> Tuple2&lt;String, Integer&gt;(word, sum);</span><br><span class=\"line\">           state.update(sum);</span><br><span class=\"line\">           <span class=\"keyword\">return</span> output;</span><br><span class=\"line\">       &#125;;</span><br><span class=\"line\">       <span class=\"keyword\">return</span> updateFunction2;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<p>spark中如何实现全局count</p>","more":"<h2 id=\"说明\"><a href=\"#说明\" class=\"headerlink\" title=\"说明\"></a>说明</h2><p>两种方式都可以实现对同一key的累计统计</p>\n<p>区别<br>updateStateByKey会返回无增量数据的状态,所以会相对较大的数据资源开销<br>mapStateWithKey 相当于增量统计</p>\n<h2 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h2><p>updateStateByKey :</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt; updateFunctionByUpdate() &#123;</span><br><span class=\"line\">       Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt; updateFunction = (values, s1) -&gt; &#123;</span><br><span class=\"line\">           Integer newSum = <span class=\"number\">0</span>;</span><br><span class=\"line\">           <span class=\"keyword\">if</span> (s1.isPresent()) &#123;</span><br><span class=\"line\">               newSum = s1.get();</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">           Iterator&lt;Integer&gt; i = values.iterator();</span><br><span class=\"line\">           <span class=\"keyword\">while</span> (i.hasNext()) &#123;</span><br><span class=\"line\">               newSum += i.next();</span><br><span class=\"line\">           &#125;</span><br><span class=\"line\">           <span class=\"keyword\">return</span> Optional.of(newSum);</span><br><span class=\"line\">       &#125;;</span><br><span class=\"line\">       <span class=\"keyword\">return</span> updateFunction;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>mapStateWithKey :</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> Function3&lt;String, Optional&lt;Integer&gt;, State&lt;Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; updateFunctionByMap() &#123;</span><br><span class=\"line\">       Function3&lt;String, Optional&lt;Integer&gt;, State&lt;Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; updateFunction2 = (word, one,</span><br><span class=\"line\">                                                                                                        state) -&gt; &#123;</span><br><span class=\"line\">           <span class=\"keyword\">int</span> sum = one.or(<span class=\"number\">0</span>) + (state.exists() ? state.get() : <span class=\"number\">0</span>);</span><br><span class=\"line\">           Tuple2&lt;String, Integer&gt; output = <span class=\"keyword\">new</span> Tuple2&lt;String, Integer&gt;(word, sum);</span><br><span class=\"line\">           state.update(sum);</span><br><span class=\"line\">           <span class=\"keyword\">return</span> output;</span><br><span class=\"line\">       &#125;;</span><br><span class=\"line\">       <span class=\"keyword\">return</span> updateFunction2;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>"},{"title":"宽窄依赖","date":"2019-07-15T16:00:00.000Z","_content":"\nspark依赖说明\n\n<!--more-->\n\n## 种类\n\nspark的依赖关系大致有两类\n\n* narrow dependency\n* wide dependency\n\n## 说明\n\n### narrow dependency\n\n父Partition ===> 子partition 多对一或一对一   flatMap ,mapToPair ,map ,filter等算子\n父partition ===> 子partition 一对多          reduce ,group by 等.\n\n### stage\n\n当一个dag串联遇到宽依赖时形成stage.一个stage对应一个task.这个task的并行度由最后一个依赖决定.应该就是说由wide dependency 的具体并行度决度.如\nreduce ,partition=3.就3的并行度.这里的参数可以设置.\n\nwide dependency 必定对应的有shuffle.但shuffle不一定是wide dependency  如sort orderby\n\njoin 即可能发生shuffle也可能不,具体看情况.\n\n### pipeline\n\n一个stage划分好后.一条数据的具体运算逻辑是会一直走完所有计算逻辑后才会落地.这是与mapreduce的区别\nmapreduce是计算逻辑走完落地,再启动,计算又落地.\n\n所以说spark的效率比mapreduce高也是有这个原因.dag串联后,运算优先.\n\n","source":"_posts/技术/hexo/spark/宽窄依赖.md","raw":"---\n\ntitle: 宽窄依赖\ndate: 2019-07-16\ntags: spark dependency\n\n---\n\nspark依赖说明\n\n<!--more-->\n\n## 种类\n\nspark的依赖关系大致有两类\n\n* narrow dependency\n* wide dependency\n\n## 说明\n\n### narrow dependency\n\n父Partition ===> 子partition 多对一或一对一   flatMap ,mapToPair ,map ,filter等算子\n父partition ===> 子partition 一对多          reduce ,group by 等.\n\n### stage\n\n当一个dag串联遇到宽依赖时形成stage.一个stage对应一个task.这个task的并行度由最后一个依赖决定.应该就是说由wide dependency 的具体并行度决度.如\nreduce ,partition=3.就3的并行度.这里的参数可以设置.\n\nwide dependency 必定对应的有shuffle.但shuffle不一定是wide dependency  如sort orderby\n\njoin 即可能发生shuffle也可能不,具体看情况.\n\n### pipeline\n\n一个stage划分好后.一条数据的具体运算逻辑是会一直走完所有计算逻辑后才会落地.这是与mapreduce的区别\nmapreduce是计算逻辑走完落地,再启动,计算又落地.\n\n所以说spark的效率比mapreduce高也是有这个原因.dag串联后,运算优先.\n\n","slug":"技术/hexo/spark/宽窄依赖","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33o003g38pw1n6ac3km","content":"<p>spark依赖说明</p>\n<a id=\"more\"></a>\n\n<h2 id=\"种类\"><a href=\"#种类\" class=\"headerlink\" title=\"种类\"></a>种类</h2><p>spark的依赖关系大致有两类</p>\n<ul>\n<li>narrow dependency</li>\n<li>wide dependency</li>\n</ul>\n<h2 id=\"说明\"><a href=\"#说明\" class=\"headerlink\" title=\"说明\"></a>说明</h2><h3 id=\"narrow-dependency\"><a href=\"#narrow-dependency\" class=\"headerlink\" title=\"narrow dependency\"></a>narrow dependency</h3><p>父Partition ===&gt; 子partition 多对一或一对一   flatMap ,mapToPair ,map ,filter等算子<br>父partition ===&gt; 子partition 一对多          reduce ,group by 等.</p>\n<h3 id=\"stage\"><a href=\"#stage\" class=\"headerlink\" title=\"stage\"></a>stage</h3><p>当一个dag串联遇到宽依赖时形成stage.一个stage对应一个task.这个task的并行度由最后一个依赖决定.应该就是说由wide dependency 的具体并行度决度.如<br>reduce ,partition=3.就3的并行度.这里的参数可以设置.</p>\n<p>wide dependency 必定对应的有shuffle.但shuffle不一定是wide dependency  如sort orderby</p>\n<p>join 即可能发生shuffle也可能不,具体看情况.</p>\n<h3 id=\"pipeline\"><a href=\"#pipeline\" class=\"headerlink\" title=\"pipeline\"></a>pipeline</h3><p>一个stage划分好后.一条数据的具体运算逻辑是会一直走完所有计算逻辑后才会落地.这是与mapreduce的区别<br>mapreduce是计算逻辑走完落地,再启动,计算又落地.</p>\n<p>所以说spark的效率比mapreduce高也是有这个原因.dag串联后,运算优先.</p>\n","site":{"data":{}},"excerpt":"<p>spark依赖说明</p>","more":"<h2 id=\"种类\"><a href=\"#种类\" class=\"headerlink\" title=\"种类\"></a>种类</h2><p>spark的依赖关系大致有两类</p>\n<ul>\n<li>narrow dependency</li>\n<li>wide dependency</li>\n</ul>\n<h2 id=\"说明\"><a href=\"#说明\" class=\"headerlink\" title=\"说明\"></a>说明</h2><h3 id=\"narrow-dependency\"><a href=\"#narrow-dependency\" class=\"headerlink\" title=\"narrow dependency\"></a>narrow dependency</h3><p>父Partition ===&gt; 子partition 多对一或一对一   flatMap ,mapToPair ,map ,filter等算子<br>父partition ===&gt; 子partition 一对多          reduce ,group by 等.</p>\n<h3 id=\"stage\"><a href=\"#stage\" class=\"headerlink\" title=\"stage\"></a>stage</h3><p>当一个dag串联遇到宽依赖时形成stage.一个stage对应一个task.这个task的并行度由最后一个依赖决定.应该就是说由wide dependency 的具体并行度决度.如<br>reduce ,partition=3.就3的并行度.这里的参数可以设置.</p>\n<p>wide dependency 必定对应的有shuffle.但shuffle不一定是wide dependency  如sort orderby</p>\n<p>join 即可能发生shuffle也可能不,具体看情况.</p>\n<h3 id=\"pipeline\"><a href=\"#pipeline\" class=\"headerlink\" title=\"pipeline\"></a>pipeline</h3><p>一个stage划分好后.一条数据的具体运算逻辑是会一直走完所有计算逻辑后才会落地.这是与mapreduce的区别<br>mapreduce是计算逻辑走完落地,再启动,计算又落地.</p>\n<p>所以说spark的效率比mapreduce高也是有这个原因.dag串联后,运算优先.</p>"},{"_content":"# python summary \n\n```\nthe summary of recently knowledge which was about python !\n```\n\n​\t\n\n## class and method\n\n\n\n\n\n\n\n\n\n","source":"_posts/技术/python/summary/2020.3.17.md","raw":"# python summary \n\n```\nthe summary of recently knowledge which was about python !\n```\n\n​\t\n\n## class and method\n\n\n\n\n\n\n\n\n\n","slug":"技术/python/summary/2020.3.17","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/python/summary/2020.3.17","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33o003h38pw6me2cmhz","content":"<h1 id=\"python-summary\"><a href=\"#python-summary\" class=\"headerlink\" title=\"python summary\"></a>python summary</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">the summary of recently knowledge which was about python !</span><br></pre></td></tr></table></figure>\n\n<p>​    </p>\n<h2 id=\"class-and-method\"><a href=\"#class-and-method\" class=\"headerlink\" title=\"class and method\"></a>class and method</h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"python-summary\"><a href=\"#python-summary\" class=\"headerlink\" title=\"python summary\"></a>python summary</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">the summary of recently knowledge which was about python !</span><br></pre></td></tr></table></figure>\n\n<p>​    </p>\n<h2 id=\"class-and-method\"><a href=\"#class-and-method\" class=\"headerlink\" title=\"class and method\"></a>class and method</h2>"},{"title":"python小结3","date":"2020-05-28T16:00:00.000Z","_content":"\n <!--more-->\n\n # python小结3\n\n## 模块\n\n\n\n\n\n## 面向对象\n### 类与实例\n\npython类的声()明会在类名上加\nos.rmdir()\n\neg\n\n```python\n\nclass Student(object):\n    pass\n```\n\n构造函数\n```python\ndef __init__(self,field1,field2):\n    pass\n```\n\n### 访问控制\n\npython中 私有属性是通过 __来标识,用__标识后则表示该属性是这个类的私有属性,则意味着直接通过一般的*.__是难以访问到的.\n\n如果是_的形式,则也是声明将这个认作是私有变量的属性,但却是可以直接通过*._*的形式进行访问的.\n\n\n`\n注意:__的属性并不是完全不能用实例直接访问,如Student类为例通过student._Stuent__name的形式也能直接访问\n\n从这里来看,这一点是不太符合面向对象编程的精神的,也就是说python解释器不会从根本上防止你乱搞,取用哪种编程规范主要还是要靠自觉\n\n`\n\n### 继承与多态\n\n总得来讲与java类似.但因为python是动态语言,所以它符合\"鸭子类型\"的特点,即在多态使用时,只要具有同样的方法,即可接收调用.\n\n### 获取对象信息\n\n通过types模块下的type()方法来获取对象类型\n\n通过isinstance()来判断是否是这个类,可用于测试继承,也可通过逗号分隔,测试是不是这些类其中之一\n通过dir()获取一个对象的所有属性和方法\n\n### 实例属性与类属性\n\npython 是动态语言,所以可以在创建实例后再赋值\n\neg:\n```python\n\nstudent=Stuent()\nstudent.score=90\n\n```\n即这score是后续才赋值上去的.\n\n这就是实例属性\n\n而本身类需要绑定一些属性呢,这就可以直接定义在class中,即类属性.\n\n```python\nclass Student(object):\n    name='ken'\n    age=18\n\n```\n## 面向对象进阶\n\n### 使用__slots__\n\n因为动态语言的特性,python实例在创建后还能再绑定方法与属性. 这里如果我们想要约束只绑定某些属性时可以通过\n\n__slots__(a,b)的形式来操作.\n\n\n#\n\n\n注意:该类若被子类继承,在子类未定义__slots__时,子类实例是不会生效的.\n\n\n### @property\n\n类似java getter\n\n@feild.setter\n\n### 多重继承\n因为python是动态语言,所以能够继承多个类,但需要考虑下设计理念,即扩展继承时,扩展继承的类可以命名为**Runnable,与 XXMixIn\n\n### 定制类的方法\n__init__\n__iter__\n__str__\n\n等方法,这些方法统一后面来整理\n\n### 枚举类\n\n### 元类有些类似反射\n\n后面再定向研究\n\n\n## 错误与调试\n\ntry...except...finally..\n\n## 文件读写\n\n### 读文件\n要以读文件的模式打开一个文件对象，使用Python内置的open()函数，传入文件名和标示符：\n\nfile open 最后需要file.close \n\n可以通过with open(\"path\",'') as f : 的写法来简写,这样写不用写f.close()\n\nopen('path','rb') //这样写才是读文件\n\n读时,f.read()会将文件全量写到内存里去,所以小文件这么简写还能接受,但若文件较大,内存会有较大压力.\n\nf.read(size),则是一次读取内容的大小.\n\nf.readlines()则是读成list的形式.\n\nopen('path','wb')//这样获取的文件操作实例是写对象\n\nf.write(\"content\")\n\n同样也可以用with的写法来获取对象\n\n`前者写对象的获取的实例,若文件存在,则会覆盖该文件`\n\nopen('path','wba')这样的写法是续写文件\n\n\n### 内存数据操作(StringIO ByteIO)\n\n\n### 操作文件和目录\nos.path.join\nos.path\nos.path.exists()\n\nos.mkdir()\nos.rmdir()\n\nos.path.abspath()\n## 常用包\n\ndatetime.datetime\n\n即datetime包下的datetime类\n\n``` python\nimport datetime.datetime\nor \nfrom datetime import datetime\n```\n\n### 获取当前时间\n\ndatetime.now()\n\n### 获取指定的时间\n\ndt=datetime(2020,05,23,12,32)\n\n`转成timestamp`\ndt.timestamp()\n\n`timestamp to datetime`\n\ndatetime.fromtimestamp(t)\n\n`转成格林制式时间`\ndatetime.utcfromtimestamp(t)\n\n`str转成datetime`\n\n\n\n## collections\n### namedtuple\n\n### deque (\"双向列表\")\n\n### defaultdict\n\n使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict：\n\n\n### OrderedDict\n\n注意，OrderedDict的Key会按照插入的顺序排列，不是Key本身排序：\n\n\n### ChainMap(再研究)\n\n### Counter\n\nCounter实际上也是dict的一个子类，上面的结果可以看出每个字符出现的次数。\n\n","source":"_posts/技术/python/summary/python小结3.md","raw":"title:  python小结3\ndate: 2020-05-29\ntags: [python]\n\n---\n\n <!--more-->\n\n # python小结3\n\n## 模块\n\n\n\n\n\n## 面向对象\n### 类与实例\n\npython类的声()明会在类名上加\nos.rmdir()\n\neg\n\n```python\n\nclass Student(object):\n    pass\n```\n\n构造函数\n```python\ndef __init__(self,field1,field2):\n    pass\n```\n\n### 访问控制\n\npython中 私有属性是通过 __来标识,用__标识后则表示该属性是这个类的私有属性,则意味着直接通过一般的*.__是难以访问到的.\n\n如果是_的形式,则也是声明将这个认作是私有变量的属性,但却是可以直接通过*._*的形式进行访问的.\n\n\n`\n注意:__的属性并不是完全不能用实例直接访问,如Student类为例通过student._Stuent__name的形式也能直接访问\n\n从这里来看,这一点是不太符合面向对象编程的精神的,也就是说python解释器不会从根本上防止你乱搞,取用哪种编程规范主要还是要靠自觉\n\n`\n\n### 继承与多态\n\n总得来讲与java类似.但因为python是动态语言,所以它符合\"鸭子类型\"的特点,即在多态使用时,只要具有同样的方法,即可接收调用.\n\n### 获取对象信息\n\n通过types模块下的type()方法来获取对象类型\n\n通过isinstance()来判断是否是这个类,可用于测试继承,也可通过逗号分隔,测试是不是这些类其中之一\n通过dir()获取一个对象的所有属性和方法\n\n### 实例属性与类属性\n\npython 是动态语言,所以可以在创建实例后再赋值\n\neg:\n```python\n\nstudent=Stuent()\nstudent.score=90\n\n```\n即这score是后续才赋值上去的.\n\n这就是实例属性\n\n而本身类需要绑定一些属性呢,这就可以直接定义在class中,即类属性.\n\n```python\nclass Student(object):\n    name='ken'\n    age=18\n\n```\n## 面向对象进阶\n\n### 使用__slots__\n\n因为动态语言的特性,python实例在创建后还能再绑定方法与属性. 这里如果我们想要约束只绑定某些属性时可以通过\n\n__slots__(a,b)的形式来操作.\n\n\n#\n\n\n注意:该类若被子类继承,在子类未定义__slots__时,子类实例是不会生效的.\n\n\n### @property\n\n类似java getter\n\n@feild.setter\n\n### 多重继承\n因为python是动态语言,所以能够继承多个类,但需要考虑下设计理念,即扩展继承时,扩展继承的类可以命名为**Runnable,与 XXMixIn\n\n### 定制类的方法\n__init__\n__iter__\n__str__\n\n等方法,这些方法统一后面来整理\n\n### 枚举类\n\n### 元类有些类似反射\n\n后面再定向研究\n\n\n## 错误与调试\n\ntry...except...finally..\n\n## 文件读写\n\n### 读文件\n要以读文件的模式打开一个文件对象，使用Python内置的open()函数，传入文件名和标示符：\n\nfile open 最后需要file.close \n\n可以通过with open(\"path\",'') as f : 的写法来简写,这样写不用写f.close()\n\nopen('path','rb') //这样写才是读文件\n\n读时,f.read()会将文件全量写到内存里去,所以小文件这么简写还能接受,但若文件较大,内存会有较大压力.\n\nf.read(size),则是一次读取内容的大小.\n\nf.readlines()则是读成list的形式.\n\nopen('path','wb')//这样获取的文件操作实例是写对象\n\nf.write(\"content\")\n\n同样也可以用with的写法来获取对象\n\n`前者写对象的获取的实例,若文件存在,则会覆盖该文件`\n\nopen('path','wba')这样的写法是续写文件\n\n\n### 内存数据操作(StringIO ByteIO)\n\n\n### 操作文件和目录\nos.path.join\nos.path\nos.path.exists()\n\nos.mkdir()\nos.rmdir()\n\nos.path.abspath()\n## 常用包\n\ndatetime.datetime\n\n即datetime包下的datetime类\n\n``` python\nimport datetime.datetime\nor \nfrom datetime import datetime\n```\n\n### 获取当前时间\n\ndatetime.now()\n\n### 获取指定的时间\n\ndt=datetime(2020,05,23,12,32)\n\n`转成timestamp`\ndt.timestamp()\n\n`timestamp to datetime`\n\ndatetime.fromtimestamp(t)\n\n`转成格林制式时间`\ndatetime.utcfromtimestamp(t)\n\n`str转成datetime`\n\n\n\n## collections\n### namedtuple\n\n### deque (\"双向列表\")\n\n### defaultdict\n\n使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict：\n\n\n### OrderedDict\n\n注意，OrderedDict的Key会按照插入的顺序排列，不是Key本身排序：\n\n\n### ChainMap(再研究)\n\n### Counter\n\nCounter实际上也是dict的一个子类，上面的结果可以看出每个字符出现的次数。\n\n","slug":"技术/python/summary/python小结3","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33p003i38pw1y6qdfog","content":" <a id=\"more\"></a>\n\n<h1 id=\"python小结3\"><a href=\"#python小结3\" class=\"headerlink\" title=\"python小结3\"></a>python小结3</h1><h2 id=\"模块\"><a href=\"#模块\" class=\"headerlink\" title=\"模块\"></a>模块</h2><h2 id=\"面向对象\"><a href=\"#面向对象\" class=\"headerlink\" title=\"面向对象\"></a>面向对象</h2><h3 id=\"类与实例\"><a href=\"#类与实例\" class=\"headerlink\" title=\"类与实例\"></a>类与实例</h3><p>python类的声()明会在类名上加<br>os.rmdir()</p>\n<p>eg</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Student</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n\n<p>构造函数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self,field1,field2)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"访问控制\"><a href=\"#访问控制\" class=\"headerlink\" title=\"访问控制\"></a>访问控制</h3><p>python中 私有属性是通过 <strong>来标识,用</strong>标识后则表示该属性是这个类的私有属性,则意味着直接通过一般的*.__是难以访问到的.</p>\n<p>如果是<em>的形式,则也是声明将这个认作是私有变量的属性,但却是可以直接通过*.</em>*的形式进行访问的.</p>\n<p>`<br>注意:<strong>的属性并不是完全不能用实例直接访问,如Student类为例通过student._Stuent</strong>name的形式也能直接访问</p>\n<p>从这里来看,这一点是不太符合面向对象编程的精神的,也就是说python解释器不会从根本上防止你乱搞,取用哪种编程规范主要还是要靠自觉</p>\n<p>`</p>\n<h3 id=\"继承与多态\"><a href=\"#继承与多态\" class=\"headerlink\" title=\"继承与多态\"></a>继承与多态</h3><p>总得来讲与java类似.但因为python是动态语言,所以它符合”鸭子类型”的特点,即在多态使用时,只要具有同样的方法,即可接收调用.</p>\n<h3 id=\"获取对象信息\"><a href=\"#获取对象信息\" class=\"headerlink\" title=\"获取对象信息\"></a>获取对象信息</h3><p>通过types模块下的type()方法来获取对象类型</p>\n<p>通过isinstance()来判断是否是这个类,可用于测试继承,也可通过逗号分隔,测试是不是这些类其中之一<br>通过dir()获取一个对象的所有属性和方法</p>\n<h3 id=\"实例属性与类属性\"><a href=\"#实例属性与类属性\" class=\"headerlink\" title=\"实例属性与类属性\"></a>实例属性与类属性</h3><p>python 是动态语言,所以可以在创建实例后再赋值</p>\n<p>eg:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">student=Stuent()</span><br><span class=\"line\">student.score=<span class=\"number\">90</span></span><br></pre></td></tr></table></figure>\n<p>即这score是后续才赋值上去的.</p>\n<p>这就是实例属性</p>\n<p>而本身类需要绑定一些属性呢,这就可以直接定义在class中,即类属性.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Student</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    name=<span class=\"string\">'ken'</span></span><br><span class=\"line\">    age=<span class=\"number\">18</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"面向对象进阶\"><a href=\"#面向对象进阶\" class=\"headerlink\" title=\"面向对象进阶\"></a>面向对象进阶</h2><h3 id=\"使用slots\"><a href=\"#使用slots\" class=\"headerlink\" title=\"使用slots\"></a>使用<strong>slots</strong></h3><p>因为动态语言的特性,python实例在创建后还能再绑定方法与属性. 这里如果我们想要约束只绑定某些属性时可以通过</p>\n<p><strong>slots</strong>(a,b)的形式来操作.</p>\n<p>#</p>\n<p>注意:该类若被子类继承,在子类未定义<strong>slots</strong>时,子类实例是不会生效的.</p>\n<h3 id=\"property\"><a href=\"#property\" class=\"headerlink\" title=\"@property\"></a>@property</h3><p>类似java getter</p>\n<p>@feild.setter</p>\n<h3 id=\"多重继承\"><a href=\"#多重继承\" class=\"headerlink\" title=\"多重继承\"></a>多重继承</h3><p>因为python是动态语言,所以能够继承多个类,但需要考虑下设计理念,即扩展继承时,扩展继承的类可以命名为**Runnable,与 XXMixIn</p>\n<h3 id=\"定制类的方法\"><a href=\"#定制类的方法\" class=\"headerlink\" title=\"定制类的方法\"></a>定制类的方法</h3><p><strong>init</strong><br><strong>iter</strong><br><strong>str</strong></p>\n<p>等方法,这些方法统一后面来整理</p>\n<h3 id=\"枚举类\"><a href=\"#枚举类\" class=\"headerlink\" title=\"枚举类\"></a>枚举类</h3><h3 id=\"元类有些类似反射\"><a href=\"#元类有些类似反射\" class=\"headerlink\" title=\"元类有些类似反射\"></a>元类有些类似反射</h3><p>后面再定向研究</p>\n<h2 id=\"错误与调试\"><a href=\"#错误与调试\" class=\"headerlink\" title=\"错误与调试\"></a>错误与调试</h2><p>try…except…finally..</p>\n<h2 id=\"文件读写\"><a href=\"#文件读写\" class=\"headerlink\" title=\"文件读写\"></a>文件读写</h2><h3 id=\"读文件\"><a href=\"#读文件\" class=\"headerlink\" title=\"读文件\"></a>读文件</h3><p>要以读文件的模式打开一个文件对象，使用Python内置的open()函数，传入文件名和标示符：</p>\n<p>file open 最后需要file.close </p>\n<p>可以通过with open(“path”,’’) as f : 的写法来简写,这样写不用写f.close()</p>\n<p>open(‘path’,’rb’) //这样写才是读文件</p>\n<p>读时,f.read()会将文件全量写到内存里去,所以小文件这么简写还能接受,但若文件较大,内存会有较大压力.</p>\n<p>f.read(size),则是一次读取内容的大小.</p>\n<p>f.readlines()则是读成list的形式.</p>\n<p>open(‘path’,’wb’)//这样获取的文件操作实例是写对象</p>\n<p>f.write(“content”)</p>\n<p>同样也可以用with的写法来获取对象</p>\n<p><code>前者写对象的获取的实例,若文件存在,则会覆盖该文件</code></p>\n<p>open(‘path’,’wba’)这样的写法是续写文件</p>\n<h3 id=\"内存数据操作-StringIO-ByteIO\"><a href=\"#内存数据操作-StringIO-ByteIO\" class=\"headerlink\" title=\"内存数据操作(StringIO ByteIO)\"></a>内存数据操作(StringIO ByteIO)</h3><h3 id=\"操作文件和目录\"><a href=\"#操作文件和目录\" class=\"headerlink\" title=\"操作文件和目录\"></a>操作文件和目录</h3><p>os.path.join<br>os.path<br>os.path.exists()</p>\n<p>os.mkdir()<br>os.rmdir()</p>\n<p>os.path.abspath()</p>\n<h2 id=\"常用包\"><a href=\"#常用包\" class=\"headerlink\" title=\"常用包\"></a>常用包</h2><p>datetime.datetime</p>\n<p>即datetime包下的datetime类</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> datetime.datetime</span><br><span class=\"line\"><span class=\"keyword\">or</span> </span><br><span class=\"line\"><span class=\"keyword\">from</span> datetime <span class=\"keyword\">import</span> datetime</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"获取当前时间\"><a href=\"#获取当前时间\" class=\"headerlink\" title=\"获取当前时间\"></a>获取当前时间</h3><p>datetime.now()</p>\n<h3 id=\"获取指定的时间\"><a href=\"#获取指定的时间\" class=\"headerlink\" title=\"获取指定的时间\"></a>获取指定的时间</h3><p>dt=datetime(2020,05,23,12,32)</p>\n<p><code>转成timestamp</code><br>dt.timestamp()</p>\n<p><code>timestamp to datetime</code></p>\n<p>datetime.fromtimestamp(t)</p>\n<p><code>转成格林制式时间</code><br>datetime.utcfromtimestamp(t)</p>\n<p><code>str转成datetime</code></p>\n<h2 id=\"collections\"><a href=\"#collections\" class=\"headerlink\" title=\"collections\"></a>collections</h2><h3 id=\"namedtuple\"><a href=\"#namedtuple\" class=\"headerlink\" title=\"namedtuple\"></a>namedtuple</h3><h3 id=\"deque-“双向列表”\"><a href=\"#deque-“双向列表”\" class=\"headerlink\" title=\"deque (“双向列表”)\"></a>deque (“双向列表”)</h3><h3 id=\"defaultdict\"><a href=\"#defaultdict\" class=\"headerlink\" title=\"defaultdict\"></a>defaultdict</h3><p>使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict：</p>\n<h3 id=\"OrderedDict\"><a href=\"#OrderedDict\" class=\"headerlink\" title=\"OrderedDict\"></a>OrderedDict</h3><p>注意，OrderedDict的Key会按照插入的顺序排列，不是Key本身排序：</p>\n<h3 id=\"ChainMap-再研究\"><a href=\"#ChainMap-再研究\" class=\"headerlink\" title=\"ChainMap(再研究)\"></a>ChainMap(再研究)</h3><h3 id=\"Counter\"><a href=\"#Counter\" class=\"headerlink\" title=\"Counter\"></a>Counter</h3><p>Counter实际上也是dict的一个子类，上面的结果可以看出每个字符出现的次数。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"python小结3\"><a href=\"#python小结3\" class=\"headerlink\" title=\"python小结3\"></a>python小结3</h1><h2 id=\"模块\"><a href=\"#模块\" class=\"headerlink\" title=\"模块\"></a>模块</h2><h2 id=\"面向对象\"><a href=\"#面向对象\" class=\"headerlink\" title=\"面向对象\"></a>面向对象</h2><h3 id=\"类与实例\"><a href=\"#类与实例\" class=\"headerlink\" title=\"类与实例\"></a>类与实例</h3><p>python类的声()明会在类名上加<br>os.rmdir()</p>\n<p>eg</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Student</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n\n<p>构造函数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self,field1,field2)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"访问控制\"><a href=\"#访问控制\" class=\"headerlink\" title=\"访问控制\"></a>访问控制</h3><p>python中 私有属性是通过 <strong>来标识,用</strong>标识后则表示该属性是这个类的私有属性,则意味着直接通过一般的*.__是难以访问到的.</p>\n<p>如果是<em>的形式,则也是声明将这个认作是私有变量的属性,但却是可以直接通过*.</em>*的形式进行访问的.</p>\n<p>`<br>注意:<strong>的属性并不是完全不能用实例直接访问,如Student类为例通过student._Stuent</strong>name的形式也能直接访问</p>\n<p>从这里来看,这一点是不太符合面向对象编程的精神的,也就是说python解释器不会从根本上防止你乱搞,取用哪种编程规范主要还是要靠自觉</p>\n<p>`</p>\n<h3 id=\"继承与多态\"><a href=\"#继承与多态\" class=\"headerlink\" title=\"继承与多态\"></a>继承与多态</h3><p>总得来讲与java类似.但因为python是动态语言,所以它符合”鸭子类型”的特点,即在多态使用时,只要具有同样的方法,即可接收调用.</p>\n<h3 id=\"获取对象信息\"><a href=\"#获取对象信息\" class=\"headerlink\" title=\"获取对象信息\"></a>获取对象信息</h3><p>通过types模块下的type()方法来获取对象类型</p>\n<p>通过isinstance()来判断是否是这个类,可用于测试继承,也可通过逗号分隔,测试是不是这些类其中之一<br>通过dir()获取一个对象的所有属性和方法</p>\n<h3 id=\"实例属性与类属性\"><a href=\"#实例属性与类属性\" class=\"headerlink\" title=\"实例属性与类属性\"></a>实例属性与类属性</h3><p>python 是动态语言,所以可以在创建实例后再赋值</p>\n<p>eg:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">student=Stuent()</span><br><span class=\"line\">student.score=<span class=\"number\">90</span></span><br></pre></td></tr></table></figure>\n<p>即这score是后续才赋值上去的.</p>\n<p>这就是实例属性</p>\n<p>而本身类需要绑定一些属性呢,这就可以直接定义在class中,即类属性.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Student</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    name=<span class=\"string\">'ken'</span></span><br><span class=\"line\">    age=<span class=\"number\">18</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"面向对象进阶\"><a href=\"#面向对象进阶\" class=\"headerlink\" title=\"面向对象进阶\"></a>面向对象进阶</h2><h3 id=\"使用slots\"><a href=\"#使用slots\" class=\"headerlink\" title=\"使用slots\"></a>使用<strong>slots</strong></h3><p>因为动态语言的特性,python实例在创建后还能再绑定方法与属性. 这里如果我们想要约束只绑定某些属性时可以通过</p>\n<p><strong>slots</strong>(a,b)的形式来操作.</p>\n<p>#</p>\n<p>注意:该类若被子类继承,在子类未定义<strong>slots</strong>时,子类实例是不会生效的.</p>\n<h3 id=\"property\"><a href=\"#property\" class=\"headerlink\" title=\"@property\"></a>@property</h3><p>类似java getter</p>\n<p>@feild.setter</p>\n<h3 id=\"多重继承\"><a href=\"#多重继承\" class=\"headerlink\" title=\"多重继承\"></a>多重继承</h3><p>因为python是动态语言,所以能够继承多个类,但需要考虑下设计理念,即扩展继承时,扩展继承的类可以命名为**Runnable,与 XXMixIn</p>\n<h3 id=\"定制类的方法\"><a href=\"#定制类的方法\" class=\"headerlink\" title=\"定制类的方法\"></a>定制类的方法</h3><p><strong>init</strong><br><strong>iter</strong><br><strong>str</strong></p>\n<p>等方法,这些方法统一后面来整理</p>\n<h3 id=\"枚举类\"><a href=\"#枚举类\" class=\"headerlink\" title=\"枚举类\"></a>枚举类</h3><h3 id=\"元类有些类似反射\"><a href=\"#元类有些类似反射\" class=\"headerlink\" title=\"元类有些类似反射\"></a>元类有些类似反射</h3><p>后面再定向研究</p>\n<h2 id=\"错误与调试\"><a href=\"#错误与调试\" class=\"headerlink\" title=\"错误与调试\"></a>错误与调试</h2><p>try…except…finally..</p>\n<h2 id=\"文件读写\"><a href=\"#文件读写\" class=\"headerlink\" title=\"文件读写\"></a>文件读写</h2><h3 id=\"读文件\"><a href=\"#读文件\" class=\"headerlink\" title=\"读文件\"></a>读文件</h3><p>要以读文件的模式打开一个文件对象，使用Python内置的open()函数，传入文件名和标示符：</p>\n<p>file open 最后需要file.close </p>\n<p>可以通过with open(“path”,’’) as f : 的写法来简写,这样写不用写f.close()</p>\n<p>open(‘path’,’rb’) //这样写才是读文件</p>\n<p>读时,f.read()会将文件全量写到内存里去,所以小文件这么简写还能接受,但若文件较大,内存会有较大压力.</p>\n<p>f.read(size),则是一次读取内容的大小.</p>\n<p>f.readlines()则是读成list的形式.</p>\n<p>open(‘path’,’wb’)//这样获取的文件操作实例是写对象</p>\n<p>f.write(“content”)</p>\n<p>同样也可以用with的写法来获取对象</p>\n<p><code>前者写对象的获取的实例,若文件存在,则会覆盖该文件</code></p>\n<p>open(‘path’,’wba’)这样的写法是续写文件</p>\n<h3 id=\"内存数据操作-StringIO-ByteIO\"><a href=\"#内存数据操作-StringIO-ByteIO\" class=\"headerlink\" title=\"内存数据操作(StringIO ByteIO)\"></a>内存数据操作(StringIO ByteIO)</h3><h3 id=\"操作文件和目录\"><a href=\"#操作文件和目录\" class=\"headerlink\" title=\"操作文件和目录\"></a>操作文件和目录</h3><p>os.path.join<br>os.path<br>os.path.exists()</p>\n<p>os.mkdir()<br>os.rmdir()</p>\n<p>os.path.abspath()</p>\n<h2 id=\"常用包\"><a href=\"#常用包\" class=\"headerlink\" title=\"常用包\"></a>常用包</h2><p>datetime.datetime</p>\n<p>即datetime包下的datetime类</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> datetime.datetime</span><br><span class=\"line\"><span class=\"keyword\">or</span> </span><br><span class=\"line\"><span class=\"keyword\">from</span> datetime <span class=\"keyword\">import</span> datetime</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"获取当前时间\"><a href=\"#获取当前时间\" class=\"headerlink\" title=\"获取当前时间\"></a>获取当前时间</h3><p>datetime.now()</p>\n<h3 id=\"获取指定的时间\"><a href=\"#获取指定的时间\" class=\"headerlink\" title=\"获取指定的时间\"></a>获取指定的时间</h3><p>dt=datetime(2020,05,23,12,32)</p>\n<p><code>转成timestamp</code><br>dt.timestamp()</p>\n<p><code>timestamp to datetime</code></p>\n<p>datetime.fromtimestamp(t)</p>\n<p><code>转成格林制式时间</code><br>datetime.utcfromtimestamp(t)</p>\n<p><code>str转成datetime</code></p>\n<h2 id=\"collections\"><a href=\"#collections\" class=\"headerlink\" title=\"collections\"></a>collections</h2><h3 id=\"namedtuple\"><a href=\"#namedtuple\" class=\"headerlink\" title=\"namedtuple\"></a>namedtuple</h3><h3 id=\"deque-“双向列表”\"><a href=\"#deque-“双向列表”\" class=\"headerlink\" title=\"deque (“双向列表”)\"></a>deque (“双向列表”)</h3><h3 id=\"defaultdict\"><a href=\"#defaultdict\" class=\"headerlink\" title=\"defaultdict\"></a>defaultdict</h3><p>使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict：</p>\n<h3 id=\"OrderedDict\"><a href=\"#OrderedDict\" class=\"headerlink\" title=\"OrderedDict\"></a>OrderedDict</h3><p>注意，OrderedDict的Key会按照插入的顺序排列，不是Key本身排序：</p>\n<h3 id=\"ChainMap-再研究\"><a href=\"#ChainMap-再研究\" class=\"headerlink\" title=\"ChainMap(再研究)\"></a>ChainMap(再研究)</h3><h3 id=\"Counter\"><a href=\"#Counter\" class=\"headerlink\" title=\"Counter\"></a>Counter</h3><p>Counter实际上也是dict的一个子类，上面的结果可以看出每个字符出现的次数。</p>"},{"_content":" ---\n title:  scala学习\n date:  2020-07-23\n tags:\n ---\n <!--more-->\n","source":"_posts/技术/编程/scala/scala学习.md","raw":" ---\n title:  scala学习\n date:  2020-07-23\n tags:\n ---\n <!--more-->\n","slug":"技术/编程/scala/scala学习","published":1,"date":"2021-01-05T02:35:38.000Z","updated":"2021-01-05T02:35:38.000Z","title":"技术/编程/scala/scala学习","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33q003l38pwhldxe1cp","content":"<hr>\n<p> title:  scala学习<br> date:  2020-07-23<br> tags:</p>\n<hr>\n <a id=\"more\"></a>\n","site":{"data":{}},"excerpt":"<hr>\n<p> title:  scala学习<br> date:  2020-07-23<br> tags:</p>\n<hr>","more":""},{"_content":"version: '2.4'\nservices:\n  sjksh-store-info:\n    image: registry.lisong.pub:28500/hy/hy-sjksh-info-v1.5\n    container_name: hy-sjksh-info\n    cpus: 4\n    mem_limit: 2g\n    restart: unless-stopped\n    extra_hosts:\n      - \"data1:10.103.22.40\"\n      - \"data2:10.103.22.64\"\n      - \"data3:10.103.22.65\"\n      - \"app1:10.103.22.66\"\n      - \"app2:10.103.22.67\"\n      - \"app3:10.103.22.68\"\n    env_file:\n      - common.env\n    environment:\n      - JAVA_OPTIONS=-XX:CompressedClassSpaceSize=32m\n","source":"_posts/技术/yd_server_config/sjksh_store_info/docker-compose.yaml","raw":"version: '2.4'\nservices:\n  sjksh-store-info:\n    image: registry.lisong.pub:28500/hy/hy-sjksh-info-v1.5\n    container_name: hy-sjksh-info\n    cpus: 4\n    mem_limit: 2g\n    restart: unless-stopped\n    extra_hosts:\n      - \"data1:10.103.22.40\"\n      - \"data2:10.103.22.64\"\n      - \"data3:10.103.22.65\"\n      - \"app1:10.103.22.66\"\n      - \"app2:10.103.22.67\"\n      - \"app3:10.103.22.68\"\n    env_file:\n      - common.env\n    environment:\n      - JAVA_OPTIONS=-XX:CompressedClassSpaceSize=32m\n","slug":"技术/yd_server_config/sjksh_store_info/docker-compose","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/yd_server_config/sjksh_store_info/docker-compose","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33q003m38pw0ui58zkw","content":"{\"version\":\"2.4\",\"services\":{\"sjksh-store-info\":{\"image\":\"registry.lisong.pub:28500/hy/hy-sjksh-info-v1.5\",\"container_name\":\"hy-sjksh-info\",\"cpus\":4,\"mem_limit\":\"2g\",\"restart\":\"unless-stopped\",\"extra_hosts\":[\"data1:10.103.22.40\",\"data2:10.103.22.64\",\"data3:10.103.22.65\",\"app1:10.103.22.66\",\"app2:10.103.22.67\",\"app3:10.103.22.68\"],\"env_file\":[\"common.env\"],\"environment\":[\"JAVA_OPTIONS=-XX:CompressedClassSpaceSize=32m\"]}}}","site":{"data":{}},"excerpt":"","more":"{\"version\":\"2.4\",\"services\":{\"sjksh-store-info\":{\"image\":\"registry.lisong.pub:28500/hy/hy-sjksh-info-v1.5\",\"container_name\":\"hy-sjksh-info\",\"cpus\":4,\"mem_limit\":\"2g\",\"restart\":\"unless-stopped\",\"extra_hosts\":[\"data1:10.103.22.40\",\"data2:10.103.22.64\",\"data3:10.103.22.65\",\"app1:10.103.22.66\",\"app2:10.103.22.67\",\"app3:10.103.22.68\"],\"env_file\":[\"common.env\"],\"environment\":[\"JAVA_OPTIONS=-XX:CompressedClassSpaceSize=32m\"]}}}"},{"_content":"version: \"2.3\"\nservices:\n  sjksh-store-info:\n    image: registry.lisong.pub:28500/hy/hy-sjksh-info-dev-v6\n    container_name: hy-sjksh-info-dev\n    cpus: 2\n    mem_limit: 2g\n    restart: unless-stopped\n    extra_hosts:\n        - \"namenode:192.168.10.100\"\n        - \"datanode1:192.168.10.101\"\n        - \"datanode2:192.168.10.102\"\n        - \"datanode3:192.168.10.103\"\n        - \"namenode2:192.168.10.104\"\n    env_file:\n      - common.env\n    environment:\n      - JAVA_OPTIONS=-XX:CompressedClassSpaceSize=32m\n","source":"_posts/技术/yg_config/hy-sjksh-info-dev/docker-compose.yaml","raw":"version: \"2.3\"\nservices:\n  sjksh-store-info:\n    image: registry.lisong.pub:28500/hy/hy-sjksh-info-dev-v6\n    container_name: hy-sjksh-info-dev\n    cpus: 2\n    mem_limit: 2g\n    restart: unless-stopped\n    extra_hosts:\n        - \"namenode:192.168.10.100\"\n        - \"datanode1:192.168.10.101\"\n        - \"datanode2:192.168.10.102\"\n        - \"datanode3:192.168.10.103\"\n        - \"namenode2:192.168.10.104\"\n    env_file:\n      - common.env\n    environment:\n      - JAVA_OPTIONS=-XX:CompressedClassSpaceSize=32m\n","slug":"技术/yg_config/hy-sjksh-info-dev/docker-compose","published":1,"date":"2021-01-05T02:35:38.000Z","updated":"2021-01-05T02:35:38.000Z","title":"技术/yg_config/hy-sjksh-info-dev/docker-compose","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33r003o38pw6lm87buv","content":"{\"version\":\"2.3\",\"services\":{\"sjksh-store-info\":{\"image\":\"registry.lisong.pub:28500/hy/hy-sjksh-info-dev-v6\",\"container_name\":\"hy-sjksh-info-dev\",\"cpus\":2,\"mem_limit\":\"2g\",\"restart\":\"unless-stopped\",\"extra_hosts\":[\"namenode:192.168.10.100\",\"datanode1:192.168.10.101\",\"datanode2:192.168.10.102\",\"datanode3:192.168.10.103\",\"namenode2:192.168.10.104\"],\"env_file\":[\"common.env\"],\"environment\":[\"JAVA_OPTIONS=-XX:CompressedClassSpaceSize=32m\"]}}}","site":{"data":{}},"excerpt":"","more":"{\"version\":\"2.3\",\"services\":{\"sjksh-store-info\":{\"image\":\"registry.lisong.pub:28500/hy/hy-sjksh-info-dev-v6\",\"container_name\":\"hy-sjksh-info-dev\",\"cpus\":2,\"mem_limit\":\"2g\",\"restart\":\"unless-stopped\",\"extra_hosts\":[\"namenode:192.168.10.100\",\"datanode1:192.168.10.101\",\"datanode2:192.168.10.102\",\"datanode3:192.168.10.103\",\"namenode2:192.168.10.104\"],\"env_file\":[\"common.env\"],\"environment\":[\"JAVA_OPTIONS=-XX:CompressedClassSpaceSize=32m\"]}}}"},{"title":"sed & awk小结","date":"2019-07-15T16:00:00.000Z","_content":"此处简介\n\n<!--more-->\n\n# sed & awk小结\n\n`一直想认真掌握sed与awk已经很久了，但一直未找个以特别详细的资料和时间来做这件事，正好这两天受到启发，转而翻墙搜索国外资源，有了很大的收获，趁次机会攻克下来`\n\n## 前言\n\nsed与awk总得来说是两样东西，本身无直接关联，做在日常使用时两者经常使用到，并且常常混合使用，所以此次小结放在一起，分总式结构进行小结\n\n## sed\n\nsed相较awk更偏于工具一点，全称应该是strem editor (即流式编辑器)。面向的是一行一行内容\n\n### 使用形式\n\nsed [-nefr] [动作]\n选项与参数：\n-n ：使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到终端上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。\n-e ：直接在命令列模式上进行 sed 的动作编辑；\n-f ：直接将 sed 的动作写在一个文件内， -f filename 则可以运行 filename 内的 sed 动作；\n-r ：sed 的动作支持的是延伸型正规表示法的语法。(默认是基础正规表示法语法)\n-i ：直接修改读取的文件内容，而不是输出到终端。\n\n动作说明： [n1[,n2]]function\nn1, n2 ：不见得会存在，一般代表『选择进行动作的行数』，举例来说，如果我的动作是需要在 10 到 20 行之间进行的，则『 10,20[动作行为] 』\n\n**function：**\na ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～\nc ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！\nd ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚；\ni ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)；\np ：列印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～\ns ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！\n\n* * *\n\n## awk\n\n`尝试看awk已有些时日，整体效率不太好，但大体也有思路，现对awk进行一些简单总结，后面则进行实例操作`\nawk的细节小点比较多，一次或无法完全总结全，但总体感觉下来发现熟悉大体模式，具体细节可以再通过即时检索解决\n\n### awk命令的基本格式是:\n\n> `awk '/search_pattern/ { action_to_t[](http://)ake_on_matches; another_action; }' file_to_parse`\n\n其中searach 与action都可省略其中之一，若action省略，那么action默认为print操作\n如何search省略，那么默认action针对的是每一行如\n![b193032f8daa40faaffc84b673b46885-image.png](//img.wqkenqing.ren/file/2017/7/b193032f8daa40faaffc84b673b46885-image.png)\n![06ddb1ad717c434e939c3d0461b2d937-image.png](//img.wqkenqing.ren/file/2017/7/06ddb1ad717c434e939c3d0461b2d937-image.png)\n\n---\n附几个操作实例如\n![1b777aa53595428e9d56e0b66052f48c-image.png](//img.wqkenqing.ren/file/2017/7/1b777aa53595428e9d56e0b66052f48c-image.png)\n\n![a1e8b526c4a042b48e0e161e6eae35fe-image.png](//img.wqkenqing.ren/file/2017/7/a1e8b526c4a042b48e0e161e6eae35fe-image.png)\n\n![de2e1925530d410ba52f25ff0781f0b5-image.png](//img.wqkenqing.ren/file/2017/7/de2e1925530d410ba52f25ff0781f0b5-image.png)\n\n**这里是以空白区分了列，通过$后加不同的数字，表示不同的列，$0表示这一行，$1表示第一列**，类推。\n\n\n### awk的内置变量\n + **FILENAME**:当前输入文件的名称\n + **FNR**:当前输入文件数\n + **FS**:当前环境中的分隔符，默认是空白\n + **NF**:输入文件的每行对应的列数\n + **NR**:当前是第几个记录\n + **OFS**:列输出时的分隔符，默认是空白\n + **ORS**:记录输出时的分隔符，默认是新起一行\n + **RS**输入记录中的分隔符，默认是newline character\n\n 正如以上内置变量的存在，所以，awk在正式使用进可能面临更多复杂情况，而之前那种简单模式可以无法应对。于是，常用的awk的使用形态扩充为\n\n> awk 'BEGIN { action; }\n/search/ { action; }\nEND { action; }' input_file\n\n这里引入了BEGIN与END两个部份，用于做一些初始化或善后处理。\n![8a8d39f410944dcc9584c7052e2e46a4-image.png](//img.wqkenqing.ren/file/2017/7/8a8d39f410944dcc9584c7052e2e46a4-image.png)\n\n![3fb8e3723fbd439990cd8e000c673c9b-image.png](//img.wqkenqing.ren/file/2017/7/3fb8e3723fbd439990cd8e000c673c9b-image.png)\n\n\n### awk的一些常见的匹配操作\n+ awk '/sa/' file\n+ awk '$2 ~ /^sa/' file\n$number表示只匹配该列\n~ 表示“是”\n!~表示“不是”\n\n---\n\n至此一这就总结了一些较为基础的awk使用，就一般工作而言，已经能应对很多场景了。\n后续我会再进行更为详细的总结\n待续~~~","source":"_posts/技术/hexo/oldblog/blog23.md","raw":"---\n\ntitle: sed & awk小结\ndate: 2019-07-16\ntags:\n\n---\n此处简介\n\n<!--more-->\n\n# sed & awk小结\n\n`一直想认真掌握sed与awk已经很久了，但一直未找个以特别详细的资料和时间来做这件事，正好这两天受到启发，转而翻墙搜索国外资源，有了很大的收获，趁次机会攻克下来`\n\n## 前言\n\nsed与awk总得来说是两样东西，本身无直接关联，做在日常使用时两者经常使用到，并且常常混合使用，所以此次小结放在一起，分总式结构进行小结\n\n## sed\n\nsed相较awk更偏于工具一点，全称应该是strem editor (即流式编辑器)。面向的是一行一行内容\n\n### 使用形式\n\nsed [-nefr] [动作]\n选项与参数：\n-n ：使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到终端上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。\n-e ：直接在命令列模式上进行 sed 的动作编辑；\n-f ：直接将 sed 的动作写在一个文件内， -f filename 则可以运行 filename 内的 sed 动作；\n-r ：sed 的动作支持的是延伸型正规表示法的语法。(默认是基础正规表示法语法)\n-i ：直接修改读取的文件内容，而不是输出到终端。\n\n动作说明： [n1[,n2]]function\nn1, n2 ：不见得会存在，一般代表『选择进行动作的行数』，举例来说，如果我的动作是需要在 10 到 20 行之间进行的，则『 10,20[动作行为] 』\n\n**function：**\na ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～\nc ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！\nd ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚；\ni ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)；\np ：列印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～\ns ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！\n\n* * *\n\n## awk\n\n`尝试看awk已有些时日，整体效率不太好，但大体也有思路，现对awk进行一些简单总结，后面则进行实例操作`\nawk的细节小点比较多，一次或无法完全总结全，但总体感觉下来发现熟悉大体模式，具体细节可以再通过即时检索解决\n\n### awk命令的基本格式是:\n\n> `awk '/search_pattern/ { action_to_t[](http://)ake_on_matches; another_action; }' file_to_parse`\n\n其中searach 与action都可省略其中之一，若action省略，那么action默认为print操作\n如何search省略，那么默认action针对的是每一行如\n![b193032f8daa40faaffc84b673b46885-image.png](//img.wqkenqing.ren/file/2017/7/b193032f8daa40faaffc84b673b46885-image.png)\n![06ddb1ad717c434e939c3d0461b2d937-image.png](//img.wqkenqing.ren/file/2017/7/06ddb1ad717c434e939c3d0461b2d937-image.png)\n\n---\n附几个操作实例如\n![1b777aa53595428e9d56e0b66052f48c-image.png](//img.wqkenqing.ren/file/2017/7/1b777aa53595428e9d56e0b66052f48c-image.png)\n\n![a1e8b526c4a042b48e0e161e6eae35fe-image.png](//img.wqkenqing.ren/file/2017/7/a1e8b526c4a042b48e0e161e6eae35fe-image.png)\n\n![de2e1925530d410ba52f25ff0781f0b5-image.png](//img.wqkenqing.ren/file/2017/7/de2e1925530d410ba52f25ff0781f0b5-image.png)\n\n**这里是以空白区分了列，通过$后加不同的数字，表示不同的列，$0表示这一行，$1表示第一列**，类推。\n\n\n### awk的内置变量\n + **FILENAME**:当前输入文件的名称\n + **FNR**:当前输入文件数\n + **FS**:当前环境中的分隔符，默认是空白\n + **NF**:输入文件的每行对应的列数\n + **NR**:当前是第几个记录\n + **OFS**:列输出时的分隔符，默认是空白\n + **ORS**:记录输出时的分隔符，默认是新起一行\n + **RS**输入记录中的分隔符，默认是newline character\n\n 正如以上内置变量的存在，所以，awk在正式使用进可能面临更多复杂情况，而之前那种简单模式可以无法应对。于是，常用的awk的使用形态扩充为\n\n> awk 'BEGIN { action; }\n/search/ { action; }\nEND { action; }' input_file\n\n这里引入了BEGIN与END两个部份，用于做一些初始化或善后处理。\n![8a8d39f410944dcc9584c7052e2e46a4-image.png](//img.wqkenqing.ren/file/2017/7/8a8d39f410944dcc9584c7052e2e46a4-image.png)\n\n![3fb8e3723fbd439990cd8e000c673c9b-image.png](//img.wqkenqing.ren/file/2017/7/3fb8e3723fbd439990cd8e000c673c9b-image.png)\n\n\n### awk的一些常见的匹配操作\n+ awk '/sa/' file\n+ awk '$2 ~ /^sa/' file\n$number表示只匹配该列\n~ 表示“是”\n!~表示“不是”\n\n---\n\n至此一这就总结了一些较为基础的awk使用，就一般工作而言，已经能应对很多场景了。\n后续我会再进行更为详细的总结\n待续~~~","slug":"技术/hexo/oldblog/blog23","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33t003t38pwe2d52oon","content":"<p>此处简介</p>\n<a id=\"more\"></a>\n\n<h1 id=\"sed-amp-awk小结\"><a href=\"#sed-amp-awk小结\" class=\"headerlink\" title=\"sed &amp; awk小结\"></a>sed &amp; awk小结</h1><p><code>一直想认真掌握sed与awk已经很久了，但一直未找个以特别详细的资料和时间来做这件事，正好这两天受到启发，转而翻墙搜索国外资源，有了很大的收获，趁次机会攻克下来</code></p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>sed与awk总得来说是两样东西，本身无直接关联，做在日常使用时两者经常使用到，并且常常混合使用，所以此次小结放在一起，分总式结构进行小结</p>\n<h2 id=\"sed\"><a href=\"#sed\" class=\"headerlink\" title=\"sed\"></a>sed</h2><p>sed相较awk更偏于工具一点，全称应该是strem editor (即流式编辑器)。面向的是一行一行内容</p>\n<h3 id=\"使用形式\"><a href=\"#使用形式\" class=\"headerlink\" title=\"使用形式\"></a>使用形式</h3><p>sed [-nefr] [动作]<br>选项与参数：<br>-n ：使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到终端上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。<br>-e ：直接在命令列模式上进行 sed 的动作编辑；<br>-f ：直接将 sed 的动作写在一个文件内， -f filename 则可以运行 filename 内的 sed 动作；<br>-r ：sed 的动作支持的是延伸型正规表示法的语法。(默认是基础正规表示法语法)<br>-i ：直接修改读取的文件内容，而不是输出到终端。</p>\n<p>动作说明： [n1[,n2]]function<br>n1, n2 ：不见得会存在，一般代表『选择进行动作的行数』，举例来说，如果我的动作是需要在 10 到 20 行之间进行的，则『 10,20[动作行为] 』</p>\n<p><strong>function：</strong><br>a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～<br>c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！<br>d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚；<br>i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)；<br>p ：列印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～<br>s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！</p>\n<hr>\n<h2 id=\"awk\"><a href=\"#awk\" class=\"headerlink\" title=\"awk\"></a>awk</h2><p><code>尝试看awk已有些时日，整体效率不太好，但大体也有思路，现对awk进行一些简单总结，后面则进行实例操作</code><br>awk的细节小点比较多，一次或无法完全总结全，但总体感觉下来发现熟悉大体模式，具体细节可以再通过即时检索解决</p>\n<h3 id=\"awk命令的基本格式是\"><a href=\"#awk命令的基本格式是\" class=\"headerlink\" title=\"awk命令的基本格式是:\"></a>awk命令的基本格式是:</h3><blockquote>\n<p><code>awk &#39;/search_pattern/ { action_to_t[](http://)ake_on_matches; another_action; }&#39; file_to_parse</code></p>\n</blockquote>\n<p>其中searach 与action都可省略其中之一，若action省略，那么action默认为print操作<br>如何search省略，那么默认action针对的是每一行如<br><img src=\"//img.wqkenqing.ren/file/2017/7/b193032f8daa40faaffc84b673b46885-image.png\" alt=\"b193032f8daa40faaffc84b673b46885-image.png\"><br><img src=\"//img.wqkenqing.ren/file/2017/7/06ddb1ad717c434e939c3d0461b2d937-image.png\" alt=\"06ddb1ad717c434e939c3d0461b2d937-image.png\"></p>\n<hr>\n<p>附几个操作实例如<br><img src=\"//img.wqkenqing.ren/file/2017/7/1b777aa53595428e9d56e0b66052f48c-image.png\" alt=\"1b777aa53595428e9d56e0b66052f48c-image.png\"></p>\n<p><img src=\"//img.wqkenqing.ren/file/2017/7/a1e8b526c4a042b48e0e161e6eae35fe-image.png\" alt=\"a1e8b526c4a042b48e0e161e6eae35fe-image.png\"></p>\n<p><img src=\"//img.wqkenqing.ren/file/2017/7/de2e1925530d410ba52f25ff0781f0b5-image.png\" alt=\"de2e1925530d410ba52f25ff0781f0b5-image.png\"></p>\n<p><strong>这里是以空白区分了列，通过$后加不同的数字，表示不同的列，$0表示这一行，$1表示第一列</strong>，类推。</p>\n<h3 id=\"awk的内置变量\"><a href=\"#awk的内置变量\" class=\"headerlink\" title=\"awk的内置变量\"></a>awk的内置变量</h3><ul>\n<li><p><strong>FILENAME</strong>:当前输入文件的名称</p>\n</li>\n<li><p><strong>FNR</strong>:当前输入文件数</p>\n</li>\n<li><p><strong>FS</strong>:当前环境中的分隔符，默认是空白</p>\n</li>\n<li><p><strong>NF</strong>:输入文件的每行对应的列数</p>\n</li>\n<li><p><strong>NR</strong>:当前是第几个记录</p>\n</li>\n<li><p><strong>OFS</strong>:列输出时的分隔符，默认是空白</p>\n</li>\n<li><p><strong>ORS</strong>:记录输出时的分隔符，默认是新起一行</p>\n</li>\n<li><p><strong>RS</strong>输入记录中的分隔符，默认是newline character</p>\n<p>正如以上内置变量的存在，所以，awk在正式使用进可能面临更多复杂情况，而之前那种简单模式可以无法应对。于是，常用的awk的使用形态扩充为</p>\n</li>\n</ul>\n<blockquote>\n<p>awk ‘BEGIN { action; }<br>/search/ { action; }<br>END { action; }’ input_file</p>\n</blockquote>\n<p>这里引入了BEGIN与END两个部份，用于做一些初始化或善后处理。<br><img src=\"//img.wqkenqing.ren/file/2017/7/8a8d39f410944dcc9584c7052e2e46a4-image.png\" alt=\"8a8d39f410944dcc9584c7052e2e46a4-image.png\"></p>\n<p><img src=\"//img.wqkenqing.ren/file/2017/7/3fb8e3723fbd439990cd8e000c673c9b-image.png\" alt=\"3fb8e3723fbd439990cd8e000c673c9b-image.png\"></p>\n<h3 id=\"awk的一些常见的匹配操作\"><a href=\"#awk的一些常见的匹配操作\" class=\"headerlink\" title=\"awk的一些常见的匹配操作\"></a>awk的一些常见的匹配操作</h3><ul>\n<li>awk ‘/sa/‘ file</li>\n<li>awk ‘$2 ~ /^sa/‘ file<br>$number表示只匹配该列<br>~ 表示“是”<br>!~表示“不是”</li>\n</ul>\n<hr>\n<p>至此一这就总结了一些较为基础的awk使用，就一般工作而言，已经能应对很多场景了。<br>后续我会再进行更为详细的总结<br>待续<del>~</del></p>\n","site":{"data":{}},"excerpt":"<p>此处简介</p>","more":"<h1 id=\"sed-amp-awk小结\"><a href=\"#sed-amp-awk小结\" class=\"headerlink\" title=\"sed &amp; awk小结\"></a>sed &amp; awk小结</h1><p><code>一直想认真掌握sed与awk已经很久了，但一直未找个以特别详细的资料和时间来做这件事，正好这两天受到启发，转而翻墙搜索国外资源，有了很大的收获，趁次机会攻克下来</code></p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>sed与awk总得来说是两样东西，本身无直接关联，做在日常使用时两者经常使用到，并且常常混合使用，所以此次小结放在一起，分总式结构进行小结</p>\n<h2 id=\"sed\"><a href=\"#sed\" class=\"headerlink\" title=\"sed\"></a>sed</h2><p>sed相较awk更偏于工具一点，全称应该是strem editor (即流式编辑器)。面向的是一行一行内容</p>\n<h3 id=\"使用形式\"><a href=\"#使用形式\" class=\"headerlink\" title=\"使用形式\"></a>使用形式</h3><p>sed [-nefr] [动作]<br>选项与参数：<br>-n ：使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到终端上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。<br>-e ：直接在命令列模式上进行 sed 的动作编辑；<br>-f ：直接将 sed 的动作写在一个文件内， -f filename 则可以运行 filename 内的 sed 动作；<br>-r ：sed 的动作支持的是延伸型正规表示法的语法。(默认是基础正规表示法语法)<br>-i ：直接修改读取的文件内容，而不是输出到终端。</p>\n<p>动作说明： [n1[,n2]]function<br>n1, n2 ：不见得会存在，一般代表『选择进行动作的行数』，举例来说，如果我的动作是需要在 10 到 20 行之间进行的，则『 10,20[动作行为] 』</p>\n<p><strong>function：</strong><br>a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～<br>c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！<br>d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚；<br>i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)；<br>p ：列印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～<br>s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！</p>\n<hr>\n<h2 id=\"awk\"><a href=\"#awk\" class=\"headerlink\" title=\"awk\"></a>awk</h2><p><code>尝试看awk已有些时日，整体效率不太好，但大体也有思路，现对awk进行一些简单总结，后面则进行实例操作</code><br>awk的细节小点比较多，一次或无法完全总结全，但总体感觉下来发现熟悉大体模式，具体细节可以再通过即时检索解决</p>\n<h3 id=\"awk命令的基本格式是\"><a href=\"#awk命令的基本格式是\" class=\"headerlink\" title=\"awk命令的基本格式是:\"></a>awk命令的基本格式是:</h3><blockquote>\n<p><code>awk &#39;/search_pattern/ { action_to_t[](http://)ake_on_matches; another_action; }&#39; file_to_parse</code></p>\n</blockquote>\n<p>其中searach 与action都可省略其中之一，若action省略，那么action默认为print操作<br>如何search省略，那么默认action针对的是每一行如<br><img src=\"//img.wqkenqing.ren/file/2017/7/b193032f8daa40faaffc84b673b46885-image.png\" alt=\"b193032f8daa40faaffc84b673b46885-image.png\"><br><img src=\"//img.wqkenqing.ren/file/2017/7/06ddb1ad717c434e939c3d0461b2d937-image.png\" alt=\"06ddb1ad717c434e939c3d0461b2d937-image.png\"></p>\n<hr>\n<p>附几个操作实例如<br><img src=\"//img.wqkenqing.ren/file/2017/7/1b777aa53595428e9d56e0b66052f48c-image.png\" alt=\"1b777aa53595428e9d56e0b66052f48c-image.png\"></p>\n<p><img src=\"//img.wqkenqing.ren/file/2017/7/a1e8b526c4a042b48e0e161e6eae35fe-image.png\" alt=\"a1e8b526c4a042b48e0e161e6eae35fe-image.png\"></p>\n<p><img src=\"//img.wqkenqing.ren/file/2017/7/de2e1925530d410ba52f25ff0781f0b5-image.png\" alt=\"de2e1925530d410ba52f25ff0781f0b5-image.png\"></p>\n<p><strong>这里是以空白区分了列，通过$后加不同的数字，表示不同的列，$0表示这一行，$1表示第一列</strong>，类推。</p>\n<h3 id=\"awk的内置变量\"><a href=\"#awk的内置变量\" class=\"headerlink\" title=\"awk的内置变量\"></a>awk的内置变量</h3><ul>\n<li><p><strong>FILENAME</strong>:当前输入文件的名称</p>\n</li>\n<li><p><strong>FNR</strong>:当前输入文件数</p>\n</li>\n<li><p><strong>FS</strong>:当前环境中的分隔符，默认是空白</p>\n</li>\n<li><p><strong>NF</strong>:输入文件的每行对应的列数</p>\n</li>\n<li><p><strong>NR</strong>:当前是第几个记录</p>\n</li>\n<li><p><strong>OFS</strong>:列输出时的分隔符，默认是空白</p>\n</li>\n<li><p><strong>ORS</strong>:记录输出时的分隔符，默认是新起一行</p>\n</li>\n<li><p><strong>RS</strong>输入记录中的分隔符，默认是newline character</p>\n<p>正如以上内置变量的存在，所以，awk在正式使用进可能面临更多复杂情况，而之前那种简单模式可以无法应对。于是，常用的awk的使用形态扩充为</p>\n</li>\n</ul>\n<blockquote>\n<p>awk ‘BEGIN { action; }<br>/search/ { action; }<br>END { action; }’ input_file</p>\n</blockquote>\n<p>这里引入了BEGIN与END两个部份，用于做一些初始化或善后处理。<br><img src=\"//img.wqkenqing.ren/file/2017/7/8a8d39f410944dcc9584c7052e2e46a4-image.png\" alt=\"8a8d39f410944dcc9584c7052e2e46a4-image.png\"></p>\n<p><img src=\"//img.wqkenqing.ren/file/2017/7/3fb8e3723fbd439990cd8e000c673c9b-image.png\" alt=\"3fb8e3723fbd439990cd8e000c673c9b-image.png\"></p>\n<h3 id=\"awk的一些常见的匹配操作\"><a href=\"#awk的一些常见的匹配操作\" class=\"headerlink\" title=\"awk的一些常见的匹配操作\"></a>awk的一些常见的匹配操作</h3><ul>\n<li>awk ‘/sa/‘ file</li>\n<li>awk ‘$2 ~ /^sa/‘ file<br>$number表示只匹配该列<br>~ 表示“是”<br>!~表示“不是”</li>\n</ul>\n<hr>\n<p>至此一这就总结了一些较为基础的awk使用，就一般工作而言，已经能应对很多场景了。<br>后续我会再进行更为详细的总结<br>待续<del>~</del></p>"},{"title":"hdfs操作细节","date":"2019-05-15T16:00:00.000Z","_content":"\n针对hdfs一些较细节的api封装\n\n<!--more-->\n\n# hdfs操作\n\n## 常规操作\n1. 创建文件 \n2. 写数据 \n3. 删除文件 \n4. 上传文件 \n5. 下载文件 \n6. 断点续写\n\n``` error\n错误：\n\n    java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try\n```\n\n原因：\n    无法写入；我的环境中有3个datanode，备份数量设置的是3。在写操作时，它会在pipeline中写3个机器。默认replace-datanode-on-failure.policy是DEFAULT,如果系统中的datanode大于等于3，它会找另外一个datanode来拷贝。目前机器只有3台，因此只要一台datanode出问题，就一直无法写入成功。\n    \n```xml\n<property>\n\n        <name>dfs.client.block.write.replace-datanode-on-failure.enable</name>         <value>true</value>\n\n    </property>\n\n   \n\n    <property>\n    <name>dfs.client.block.write.replace-datanode-on-failure.policy</name>\n\n        <value>NEVER</value>\n\n    </property>\n```\n\n对于dfs.client.block.write.replace-datanode-on-failure.enable，客户端在写失败的时候，是否使用更换策略，默认是true没有问题\n对于，dfs.client.block.write.replace-datanode-on-failure.policy，default在3个或以上备份的时候，是会尝试更换结点尝试写入datanode。而在两个备份的时候，不更换datanode，直接开始写。对于3个datanode的集群，只要一个节点没响应写入就会出问题，所以可以关掉。","source":"_posts/技术/hexo/hadoop/hdfs/hdfs操作.md","raw":"---\n\ntitle: hdfs操作细节\ndate: 2019-05-16\ntags: \n\n---\n\n针对hdfs一些较细节的api封装\n\n<!--more-->\n\n# hdfs操作\n\n## 常规操作\n1. 创建文件 \n2. 写数据 \n3. 删除文件 \n4. 上传文件 \n5. 下载文件 \n6. 断点续写\n\n``` error\n错误：\n\n    java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try\n```\n\n原因：\n    无法写入；我的环境中有3个datanode，备份数量设置的是3。在写操作时，它会在pipeline中写3个机器。默认replace-datanode-on-failure.policy是DEFAULT,如果系统中的datanode大于等于3，它会找另外一个datanode来拷贝。目前机器只有3台，因此只要一台datanode出问题，就一直无法写入成功。\n    \n```xml\n<property>\n\n        <name>dfs.client.block.write.replace-datanode-on-failure.enable</name>         <value>true</value>\n\n    </property>\n\n   \n\n    <property>\n    <name>dfs.client.block.write.replace-datanode-on-failure.policy</name>\n\n        <value>NEVER</value>\n\n    </property>\n```\n\n对于dfs.client.block.write.replace-datanode-on-failure.enable，客户端在写失败的时候，是否使用更换策略，默认是true没有问题\n对于，dfs.client.block.write.replace-datanode-on-failure.policy，default在3个或以上备份的时候，是会尝试更换结点尝试写入datanode。而在两个备份的时候，不更换datanode，直接开始写。对于3个datanode的集群，只要一个节点没响应写入就会出问题，所以可以关掉。","slug":"技术/hexo/hadoop/hdfs/hdfs操作","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33w003u38pw1ehae1yj","content":"<p>针对hdfs一些较细节的api封装</p>\n<a id=\"more\"></a>\n\n<h1 id=\"hdfs操作\"><a href=\"#hdfs操作\" class=\"headerlink\" title=\"hdfs操作\"></a>hdfs操作</h1><h2 id=\"常规操作\"><a href=\"#常规操作\" class=\"headerlink\" title=\"常规操作\"></a>常规操作</h2><ol>\n<li>创建文件 </li>\n<li>写数据 </li>\n<li>删除文件 </li>\n<li>上传文件 </li>\n<li>下载文件 </li>\n<li>断点续写</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">错误：</span><br><span class=\"line\"></span><br><span class=\"line\">    java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try</span><br></pre></td></tr></table></figure>\n\n<p>原因：<br>    无法写入；我的环境中有3个datanode，备份数量设置的是3。在写操作时，它会在pipeline中写3个机器。默认replace-datanode-on-failure.policy是DEFAULT,如果系统中的datanode大于等于3，它会找另外一个datanode来拷贝。目前机器只有3台，因此只要一台datanode出问题，就一直无法写入成功。</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.client.block.write.replace-datanode-on-failure.enable<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span>         <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">   </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.client.block.write.replace-datanode-on-failure.policy<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>NEVER<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<p>对于dfs.client.block.write.replace-datanode-on-failure.enable，客户端在写失败的时候，是否使用更换策略，默认是true没有问题<br>对于，dfs.client.block.write.replace-datanode-on-failure.policy，default在3个或以上备份的时候，是会尝试更换结点尝试写入datanode。而在两个备份的时候，不更换datanode，直接开始写。对于3个datanode的集群，只要一个节点没响应写入就会出问题，所以可以关掉。</p>\n","site":{"data":{}},"excerpt":"<p>针对hdfs一些较细节的api封装</p>","more":"<h1 id=\"hdfs操作\"><a href=\"#hdfs操作\" class=\"headerlink\" title=\"hdfs操作\"></a>hdfs操作</h1><h2 id=\"常规操作\"><a href=\"#常规操作\" class=\"headerlink\" title=\"常规操作\"></a>常规操作</h2><ol>\n<li>创建文件 </li>\n<li>写数据 </li>\n<li>删除文件 </li>\n<li>上传文件 </li>\n<li>下载文件 </li>\n<li>断点续写</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">错误：</span><br><span class=\"line\"></span><br><span class=\"line\">    java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try</span><br></pre></td></tr></table></figure>\n\n<p>原因：<br>    无法写入；我的环境中有3个datanode，备份数量设置的是3。在写操作时，它会在pipeline中写3个机器。默认replace-datanode-on-failure.policy是DEFAULT,如果系统中的datanode大于等于3，它会找另外一个datanode来拷贝。目前机器只有3台，因此只要一台datanode出问题，就一直无法写入成功。</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.client.block.write.replace-datanode-on-failure.enable<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span>         <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">   </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.client.block.write.replace-datanode-on-failure.policy<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>NEVER<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n\n<p>对于dfs.client.block.write.replace-datanode-on-failure.enable，客户端在写失败的时候，是否使用更换策略，默认是true没有问题<br>对于，dfs.client.block.write.replace-datanode-on-failure.policy，default在3个或以上备份的时候，是会尝试更换结点尝试写入datanode。而在两个备份的时候，不更换datanode，直接开始写。对于3个datanode的集群，只要一个节点没响应写入就会出问题，所以可以关掉。</p>"},{"title":"hbaes操作","date":"2019-05-16T16:00:00.000Z","_content":"\n对hbase常规api进行封装\n\n<!--more-->\n\n# hbase日常api类封装\n\n","source":"_posts/技术/hexo/hadoop/hbase/hbase操作.md","raw":"---\n\ntitle: hbaes操作\ndate: 2019-05-17\ntags: 常规api封装\n\n---\n\n对hbase常规api进行封装\n\n<!--more-->\n\n# hbase日常api类封装\n\n","slug":"技术/hexo/hadoop/hbase/hbase操作","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33x003v38pw0dg39nde","content":"<p>对hbase常规api进行封装</p>\n<a id=\"more\"></a>\n\n<h1 id=\"hbase日常api类封装\"><a href=\"#hbase日常api类封装\" class=\"headerlink\" title=\"hbase日常api类封装\"></a>hbase日常api类封装</h1>","site":{"data":{}},"excerpt":"<p>对hbase常规api进行封装</p>","more":"<h1 id=\"hbase日常api类封装\"><a href=\"#hbase日常api类封装\" class=\"headerlink\" title=\"hbase日常api类封装\"></a>hbase日常api类封装</h1>"},{"title":"hdfs命令","date":"2019-07-16T16:00:00.000Z","_content":"\nhdfs常用命令\n\n<!--more-->\n\n\ncount\n\n\n```\n该命令选项显示指定路径下的文件夹数量、文件数量、文件总大小信息，如图4-6所示。\n```\n![](http://img.wqkenqing.ren/QFvpXA.png)\n\n\ndu\n\n```\n统计目录下各文件大小\n```\ntouchz\n```\n创建空白文件\n\n```\n\n-stat\n\n```\n“%b %n %o %r %Y”依次表示文件大小、文件名称、块大小、副本数、访问时间。\n\n```\n\n","source":"_posts/技术/hexo/hadoop/hdfs/hdfs命令.md","raw":"---\n\ntitle: hdfs命令\ndate: 2019-07-17\ntags:  hdfs\n\n---\n\nhdfs常用命令\n\n<!--more-->\n\n\ncount\n\n\n```\n该命令选项显示指定路径下的文件夹数量、文件数量、文件总大小信息，如图4-6所示。\n```\n![](http://img.wqkenqing.ren/QFvpXA.png)\n\n\ndu\n\n```\n统计目录下各文件大小\n```\ntouchz\n```\n创建空白文件\n\n```\n\n-stat\n\n```\n“%b %n %o %r %Y”依次表示文件大小、文件名称、块大小、副本数、访问时间。\n\n```\n\n","slug":"技术/hexo/hadoop/hdfs/hdfs命令","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd33y003w38pw0h2rek9v","content":"<p>hdfs常用命令</p>\n<a id=\"more\"></a>\n\n\n<p>count</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">该命令选项显示指定路径下的文件夹数量、文件数量、文件总大小信息，如图4-6所示。</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.wqkenqing.ren/QFvpXA.png\" alt=\"\"></p>\n<p>du</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">统计目录下各文件大小</span><br></pre></td></tr></table></figure>\n<p>touchz</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">创建空白文件</span><br></pre></td></tr></table></figure>\n\n<p>-stat</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">“%b %n %o %r %Y”依次表示文件大小、文件名称、块大小、副本数、访问时间。</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{}},"excerpt":"<p>hdfs常用命令</p>","more":"<p>count</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">该命令选项显示指定路径下的文件夹数量、文件数量、文件总大小信息，如图4-6所示。</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://img.wqkenqing.ren/QFvpXA.png\" alt=\"\"></p>\n<p>du</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">统计目录下各文件大小</span><br></pre></td></tr></table></figure>\n<p>touchz</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">创建空白文件</span><br></pre></td></tr></table></figure>\n\n<p>-stat</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">“%b %n %o %r %Y”依次表示文件大小、文件名称、块大小、副本数、访问时间。</span><br></pre></td></tr></table></figure>"},{"_content":" ---\n title:  spark-windows函数\n date: 2020-7-20\n tags: [spark,window]\n ---\n spark windows函数\n\n <!--more-->\n\n# spark-windows函数\n","source":"_posts/技术/hexo/spark/算子/Spark-Windos函数.md","raw":" ---\n title:  spark-windows函数\n date: 2020-7-20\n tags: [spark,window]\n ---\n spark windows函数\n\n <!--more-->\n\n# spark-windows函数\n","slug":"技术/hexo/spark/算子/Spark-Windos函数","published":1,"date":"2021-01-05T02:35:37.000Z","updated":"2021-01-05T02:35:37.000Z","title":"技术/hexo/spark/算子/Spark-Windos函数","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd349003y38pw8qifhfjf","content":"<hr>\n<p> title:  spark-windows函数<br> date: 2020-7-20<br> tags: [spark,window]</p>\n<hr>\n<p> spark windows函数</p>\n <a id=\"more\"></a>\n\n<h1 id=\"spark-windows函数\"><a href=\"#spark-windows函数\" class=\"headerlink\" title=\"spark-windows函数\"></a>spark-windows函数</h1>","site":{"data":{}},"excerpt":"<hr>\n<p> title:  spark-windows函数<br> date: 2020-7-20<br> tags: [spark,window]</p>\n<hr>\n<p> spark windows函数</p>","more":"<h1 id=\"spark-windows函数\"><a href=\"#spark-windows函数\" class=\"headerlink\" title=\"spark-windows函数\"></a>spark-windows函数</h1>"},{"title":"SparkSql","date":"2019-06-12T16:00:00.000Z","_content":"\n```\nspark sql 相关内容\n```\n<!--more-->\n\n# sparksql\n","source":"_posts/技术/hexo/spark/sql/SparkSql.md","raw":"---\n\ntitle: SparkSql\ndate: 2019-06-13\ntags:\n\n---\n\n```\nspark sql 相关内容\n```\n<!--more-->\n\n# sparksql\n","slug":"技术/hexo/spark/sql/SparkSql","published":1,"updated":"2021-01-05T02:35:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckntqd34k003z38pwhihr18mt","content":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">spark sql 相关内容</span><br></pre></td></tr></table></figure>\n<a id=\"more\"></a>\n\n<h1 id=\"sparksql\"><a href=\"#sparksql\" class=\"headerlink\" title=\"sparksql\"></a>sparksql</h1>","site":{"data":{}},"excerpt":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">spark sql 相关内容</span><br></pre></td></tr></table></figure>","more":"<h1 id=\"sparksql\"><a href=\"#sparksql\" class=\"headerlink\" title=\"sparksql\"></a>sparksql</h1>"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"ckntqd31v001c38pw3i7vg1xr","tag_id":"ckntqd31w001d38pwfd006p4x","_id":"ckntqd322001e38pwh25836l9"},{"post_id":"ckntqd32r001j38pwaggr3hh8","tag_id":"ckntqd32s001l38pw05hg969p","_id":"ckntqd32u001q38pw8v98cjmy"},{"post_id":"ckntqd32v001s38pwfyefe7d4","tag_id":"ckntqd32u001p38pw3bsnbzsf","_id":"ckntqd32w001v38pwe32tfa6s"},{"post_id":"ckntqd32t001n38pwdw0586at","tag_id":"ckntqd32u001p38pw3bsnbzsf","_id":"ckntqd32x001x38pw1q4kg2xf"},{"post_id":"ckntqd32t001o38pwe6yy17em","tag_id":"ckntqd32w001u38pwaehoc7fz","_id":"ckntqd32z002138pw9j8b43bp"},{"post_id":"ckntqd32v001t38pw8xlycxn8","tag_id":"ckntqd32w001u38pwaehoc7fz","_id":"ckntqd332002538pwbu5v6m3c"},{"post_id":"ckntqd332002438pw137a9psp","tag_id":"ckntqd330002338pw6fxhc7cg","_id":"ckntqd334002838pwam644cal"},{"post_id":"ckntqd32w001w38pw4qxgh2hj","tag_id":"ckntqd330002338pw6fxhc7cg","_id":"ckntqd334002a38pw8yq39z37"},{"post_id":"ckntqd334002938pw47ozeuzl","tag_id":"ckntqd330002338pw6fxhc7cg","_id":"ckntqd335002c38pwf6t38wi3"},{"post_id":"ckntqd32z002038pw0aub85xl","tag_id":"ckntqd333002738pw5qmtecgt","_id":"ckntqd336002f38pw81k28if2"},{"post_id":"ckntqd334002b38pw39rgbw0b","tag_id":"ckntqd32w001u38pwaehoc7fz","_id":"ckntqd337002h38pwa7ieg6s4"},{"post_id":"ckntqd330002238pwfljd6vsz","tag_id":"ckntqd335002d38pw706a8vkh","_id":"ckntqd338002k38pw1qel0pac"},{"post_id":"ckntqd338002l38pw0e7m8hw5","tag_id":"ckntqd32w001u38pwaehoc7fz","_id":"ckntqd339002o38pw0m8g5s6f"},{"post_id":"ckntqd333002638pw417xfj8o","tag_id":"ckntqd337002j38pwgnbm6pzd","_id":"ckntqd33a002q38pwf4rodam5"},{"post_id":"ckntqd336002e38pwctxo7cch","tag_id":"ckntqd339002n38pw1adhai5p","_id":"ckntqd33b002t38pw6i0kec88"},{"post_id":"ckntqd33n003d38pwf4dqdqsb","tag_id":"ckntqd33o003f38pwht764mu3","_id":"ckntqd33q003k38pwaepe31xh"},{"post_id":"ckntqd33n003e38pwgyx80c2w","tag_id":"ckntqd33p003j38pw9k8m2gzy","_id":"ckntqd33r003p38pw4vrt33gw"},{"post_id":"ckntqd33o003g38pw1n6ac3km","tag_id":"ckntqd33r003n38pw8mja5yke","_id":"ckntqd33s003r38pwc247dbj3"},{"post_id":"ckntqd33p003i38pw1y6qdfog","tag_id":"ckntqd33s003q38pw0er7fwd7","_id":"ckntqd33s003s38pw49xqaj6y"},{"post_id":"ckntqd33x003v38pw0dg39nde","tag_id":"ckntqd348003x38pwb02y8m8j","_id":"ckntqd34k004138pw7y399wyh"},{"post_id":"ckntqd33y003w38pw0h2rek9v","tag_id":"ckntqd34k004038pw1vddgfee","_id":"ckntqd34l004238pwd5j155nb"}],"Tag":[{"name":"mac python2 python3","_id":"ckntqd31w001d38pwfd006p4x"},{"name":"flink","_id":"ckntqd32s001l38pw05hg969p"},{"name":"kafka","_id":"ckntqd32u001p38pw3bsnbzsf"},{"name":"日常总结","_id":"ckntqd32w001u38pwaehoc7fz"},{"name":"bigdata","_id":"ckntqd330002338pw6fxhc7cg"},{"name":"学习spark2","_id":"ckntqd333002738pw5qmtecgt"},{"name":"学习spark","_id":"ckntqd335002d38pw706a8vkh"},{"name":"spark学习","_id":"ckntqd337002j38pwgnbm6pzd"},{"name":"小结","_id":"ckntqd339002n38pw1adhai5p"},{"name":"sparkstreaming","_id":"ckntqd33o003f38pwht764mu3"},{"name":"sparkstream","_id":"ckntqd33p003j38pw9k8m2gzy"},{"name":"spark dependency","_id":"ckntqd33r003n38pw8mja5yke"},{"name":"python","_id":"ckntqd33s003q38pw0er7fwd7"},{"name":"常规api封装","_id":"ckntqd348003x38pwb02y8m8j"},{"name":"hdfs","_id":"ckntqd34k004038pw1vddgfee"}]}}
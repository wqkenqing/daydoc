<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>python 小结2</title>
      <link href="/daydoc/2020/05/25/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E7%BC%96%E7%A8%8B/python/"/>
      <url>/daydoc/2020/05/25/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E7%BC%96%E7%A8%8B/python/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><!-- TOC --><ul><li><a href="#python中的变量创建">python中的变量创建</a><ul><li><a href="#python-变量命名">python 变量命名</a></li><li><a href="#多个变量赋值">多个变量赋值</a></li></ul></li><li><a href="#列表与元组">列表与元组</a><ul><li><a href="#列表">列表</a><ul><li><a href="#如何访问列表中的值">如何访问列表中的值</a></li><li><a href="#列表运算符">列表运算符</a></li></ul></li><li><a href="#元组tuple">元组(tuple)</a><ul><li><a href="#创建元组">创建元组</a></li><li><a href="#访问元组">访问元组</a></li></ul></li></ul></li><li><a href="#dict字典">dict(字典)</a><ul><li><a href="#什么是字典">什么是字典</a></li><li><a href="#字典的创建">字典的创建</a></li></ul></li><li><a href="#set">set()</a><ul><li><a href="#什么是set">什么是set</a></li><li><a href="#set的创建">set的创建</a></li></ul></li><li><a href="#函数">函数</a><ul><li><a href="#什么是函数">什么是函数</a></li><li><a href="#自定义函数的创建">自定义函数的创建</a></li><li><a href="#函数的返回值">函数的返回值</a></li><li><a href="#函数的参数">函数的参数</a><ul><li><a href="#位置参数">位置参数</a></li><li><a href="#不定长参数">不定长参数</a></li><li><a href="#只接受关键字参数">只接受关键字参数</a></li></ul></li></ul></li><li><a href="#迭代器与生成器">迭代器与生成器</a></li><li><a href="#面向对象">面向对象</a><ul><li><a href="#类的定义与调用">类的定义与调用</a></li></ul></li></ul><!-- /TOC --><h2 id="python中的变量创建"><a href="#python中的变量创建" class="headerlink" title="python中的变量创建"></a>python中的变量创建</h2><h3 id="python-变量命名"><a href="#python-变量命名" class="headerlink" title="python 变量命名"></a>python 变量命名</h3><p>在 Python 程序中，变量是用一个变量名表示，可以是任意数据类型，变量名必须是大小写英文、数字和下划线（_）的组合，且不能用数字开头</p><h3 id="多个变量赋值"><a href="#多个变量赋值" class="headerlink" title="多个变量赋值"></a>多个变量赋值</h3><p>eg:<br>a=b=c=1</p><p>eg:<br>a,b,c=1,2,”the is varible”<br><img src="http://img.wqkenqing.ren/2020-05-25-14-48-23.png" alt="2020-05-25-14-48-23"></p><h2 id="列表与元组"><a href="#列表与元组" class="headerlink" title="列表与元组"></a>列表与元组</h2><p><img src="http://img.wqkenqing.ren/2020-05-25-14-54-41.png" alt="2020-05-25-14-54-41"></p><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><p>eg:</p><p>list1=[“1”,2,”three”]</p><p>即列表名,’[]’ 列表符组成</p><p>列表中的元素类型可以不一致.</p><h4 id="如何访问列表中的值"><a href="#如何访问列表中的值" class="headerlink" title="如何访问列表中的值"></a>如何访问列表中的值</h4><ol><li>通过索引下标来访问</li></ol><p>即已知索引长度为3</p><p>我们可以通过<br>list1[i]</p><p>i为0~2之间的整数</p><p>这与java中的数组类似</p><p>2.通过下标区间来访问</p><p>如 list1[0:2]<br>可以取到list[0],list[1]<br>这里是因为[0:2]是左闭右开的</p><p><img src="http://img.wqkenqing.ren/2020-05-25-15-09-50.png" alt="2020-05-25-15-09-50"><br>`</p><h4 id="列表运算符"><a href="#列表运算符" class="headerlink" title="列表运算符"></a>列表运算符</h4><table><thead><tr><th>Python 表达式</th><th>结果</th><th>描述</th></tr></thead><tbody><tr><td>len(list1)</td><td>3</td><td>list的长度</td></tr><tr><td>[1, 2, 3] + [4, 5, 6]</td><td>[1, 2, 3, 4, 5, 6]</td><td>组合</td></tr><tr><td>[“yes”]*2</td><td>[“yes”,”yes”]</td><td>复制</td></tr><tr><td>2 in [1,2,3]</td><td>True</td><td>元素是否存在于列表中</td></tr><tr><td>for a in list : function</td><td></td><td>迭代</td></tr></tbody></table><p>一些常见的方法<br><img src="http://img.wqkenqing.ren/2020-05-25-15-26-54.png" alt="2020-05-25-15-26-54"></p><h3 id="元组-tuple"><a href="#元组-tuple" class="headerlink" title="元组(tuple)"></a>元组(tuple)</h3><p>uple 一旦初始化就不能修改。 也就是说元组（tuple）是不可变的，那么不可变是指什么意思呢？<br>即声明元组后,只可获取相应的内容,但不能再通过insert ,append ,pop,remove等方法再操作其内容</p><h4 id="创建元组"><a href="#创建元组" class="headerlink" title="创建元组"></a>创建元组</h4><p>tuple1=(1,”2”,”three”)</p><p><code>空元组</code><br>tuple2=()</p><h4 id="访问元组"><a href="#访问元组" class="headerlink" title="访问元组"></a>访问元组</h4><p><img src="http://img.wqkenqing.ren/2020-05-25-15-45-07.png" alt="2020-05-25-15-45-07"><br><img src="http://img.wqkenqing.ren/2020-05-25-15-44-48.png" alt="2020-05-25-15-44-48"></p><h2 id="dict-字典"><a href="#dict-字典" class="headerlink" title="dict(字典)"></a>dict(字典)</h2><h3 id="什么是字典"><a href="#什么是字典" class="headerlink" title="什么是字典"></a>什么是字典</h3><p>类似java中的map key-val形式</p><h3 id="字典的创建"><a href="#字典的创建" class="headerlink" title="字典的创建"></a>字典的创建</h3><p>dict={“key1”:”val1”,”key2”:”val2”}</p><h2 id="set"><a href="#set" class="headerlink" title="set()"></a>set()</h2><h3 id="什么是set"><a href="#什么是set" class="headerlink" title="什么是set"></a>什么是set</h3><p>set与java中的set异同,这里是一个无序不重复的元素集,基本功能包括数据存放与消除重复元素</p><h3 id="set的创建"><a href="#set的创建" class="headerlink" title="set的创建"></a>set的创建</h3><p>创建一个set,需要提供一个list</p><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><h3 id="什么是函数"><a href="#什么是函数" class="headerlink" title="什么是函数"></a>什么是函数</h3><p>类比java里的方法</p><h3 id="自定义函数的创建"><a href="#自定义函数的创建" class="headerlink" title="自定义函数的创建"></a>自定义函数的创建</h3><p>def functionName():<br>    methodBody<br>    return 没有return 则自动返回None</p><h3 id="函数的返回值"><a href="#函数的返回值" class="headerlink" title="函数的返回值"></a>函数的返回值</h3><p>不带参数值的 return 语句返回 None。</p><p>通过return即可返回</p><p>同时还可以返回多个返回值,通过逗 号分隔,即以元组的形式返回</p><p>Python 一次接受多个返回值的数据类型就是元组。</p><h3 id="函数的参数"><a href="#函数的参数" class="headerlink" title="函数的参数"></a>函数的参数</h3><ol><li>默认参数</li><li>关键字参数(位置参数)</li><li>不定长参数</li></ol><p>当你设置了默认参数的时候，在调用函数的时候，不传该参数，就会使用默认值</p><p><code>只有在形参表末尾的那些参数可以有默认参数值，也就是说你不能在声明函数形参的时候，先声明有默认值的形参而后声明没有默认值的形参。</code><br>这是因为赋给形参的值是根据位置而赋值的。例如，def func(a, b=1) 是有效的，但是 def func(a=1, b) 是 无效 的。</p><h4 id="位置参数"><a href="#位置参数" class="headerlink" title="位置参数"></a>位置参数</h4><h4 id="不定长参数"><a href="#不定长参数" class="headerlink" title="不定长参数"></a>不定长参数</h4><p>*arg 一个元组的形式</p><p>**arg 支持关键字参数,没有被定义的参数会被放到一个字典里.</p><h4 id="只接受关键字参数"><a href="#只接受关键字参数" class="headerlink" title="只接受关键字参数"></a>只接受关键字参数</h4><p>将强制关键字参数放到某个<em>参数或者单个</em>后面就能达到这种效果</p><h2 id="迭代器与生成器"><a href="#迭代器与生成器" class="headerlink" title="迭代器与生成器"></a>迭代器与生成器</h2><h2 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h2><h3 id="类的定义与调用"><a href="#类的定义与调用" class="headerlink" title="类的定义与调用"></a>类的定义与调用</h3>]]></content>
      
      
      
        <tags>
            
            <tag> python learn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>es添加字段并赋值</title>
      <link href="/daydoc/2020/05/22/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/es%E6%B7%BB%E5%8A%A0%E5%AD%97%E6%AE%B5%E5%92%8C%E9%BB%98%E8%AE%A4%E5%80%BC/"/>
      <url>/daydoc/2020/05/22/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/es%E6%B7%BB%E5%8A%A0%E5%AD%97%E6%AE%B5%E5%92%8C%E9%BB%98%E8%AE%A4%E5%80%BC/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>添加字段即</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PUT /index/_mapping</span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">给es添加字段</span><br></pre></td></tr></table></figure><p>这里主要通过script脚本.<br>并且在elasticsearch1.3后这个默认配置是被关掉的.</p><p>即在elasticsearch.yml中添加<br>script.inline=true<br>script.indexed=true<br>一开始我在true后面加了空格,配置不被识别,所以这里要注意</p><p>开启了script.line后通过update_by_query进行操作,这里注意以下写法已经过时</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">POST /index/_update_by_query?conflicts=proceed</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"script"</span>:&#123;</span><br><span class="line">    <span class="attr">"lang"</span>:<span class="string">"painless"</span>,</span><br><span class="line">    <span class="attr">"inline"</span>: <span class="string">"ctx._source.name='trhee'"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>即inline这种写法已经过时. 现在主要用的是source</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">POST /index/_update_by_query?conflicts=proceed</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"script"</span>:&#123;</span><br><span class="line">    <span class="attr">"lang"</span>:<span class="string">"painless"</span>,</span><br><span class="line">    <span class="attr">"source"</span>: <span class="string">"ctx._source.name='trhee'"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>还能加入条件判断</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">POST /index/_update_by_query?conflicts=proceed</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"script"</span>:&#123;</span><br><span class="line">    <span class="attr">"lang"</span>:<span class="string">"painless"</span>,</span><br><span class="line">    <span class="attr">"source"</span>: <span class="string">"if(ctx._source.name=='')&#123;ctx._source.name='trhee'&#125;"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至此即可以实现对_source.name字段的数据进行默认赋值</p>]]></content>
      
      
      
        <tags>
            
            <tag> es feild script </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>es常用操作工具</title>
      <link href="/daydoc/2020/05/21/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/es/es%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%B7%A5%E5%85%B7/"/>
      <url>/daydoc/2020/05/21/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/es/es%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%B7%A5%E5%85%B7/</url>
      
        <content type="html"><![CDATA[<p>es操作工具</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">es常用的操作工具介绍</span><br></pre></td></tr></table></figure><h2 id="elasticsearch-head"><a href="#elasticsearch-head" class="headerlink" title="elasticsearch-head"></a>elasticsearch-head</h2><p>elascticsearch-head是一款操作工具,这里不过多介绍,我日常使用较少,但这个工具有一个好处就是有对应的chrome插件,在非常用电脑上有相关需求时,可以通过登陆chrome账号就使用head插件应急</p><h2 id="kibana"><a href="#kibana" class="headerlink" title="kibana"></a>kibana</h2><p>kibana是elastic stack集成产中.一开始主要就是作为elasticsearch可视化需求的一个承载体,后续才单独拆开,承担了更多复杂的view功能</p><p>结合对应版本的elasticsearch部署对应的kibana版本后,即可开箱使用,这里不过多介绍具体使用细节.主要是说明下功能内容</p><h4 id="Dev-Tools模块"><a href="#Dev-Tools模块" class="headerlink" title="Dev Tools模块"></a>Dev Tools模块</h4><p>用来写DSL语句,管理elasticsearch</p><h4 id="Management-模块"><a href="#Management-模块" class="headerlink" title="Management 模块"></a>Management 模块</h4><p>kibana集成的理管模块,能对index进行一些管理操作.</p><p><img src="http://img.wqkenqing.ren/image-20200521134325351.png" alt="image-20200521134325351"></p><h2 id="vscode"><a href="#vscode" class="headerlink" title="vscode"></a>vscode</h2><p>前两者都是主流的elasticsearch的管理和操作工具,但实集使用过程中,特别是大量的DSL语句操作的时候,前两者虽然功能较全, 但操作是不方便的.因为如果把DSL比作是sql的话,在网页中书写DSL,量一大,一是语句的备份不好管理,相关内容太多,不能分类存放,容易造成误操作.</p><p>而vscode的插件库里有与elasticsearch结合的插件,这里elasticsearch for vscode </p><p><img src="http://img.wqkenqing.ren/image-20200521135215143.png" alt="image-20200521135215143"></p><p>安装好后,即可开箱使用.只需要将文件创建为*.es的后缀.然后设置需要启用的elasticsearch的hostp:port</p><p>设置好后,即可在vscode 下的es文件中开始写对应的dsl语句.</p><p>极为方便,而且能在某文件内容过多后,轻易的再另起一个es文件.方便维护管理</p><p><img src="http://img.wqkenqing.ren/image-20200521135527747.png" alt="image-20200521135527747"></p>]]></content>
      
      
      
        <tags>
            
            <tag> elasticsearch kibana head vscode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vscode插件</title>
      <link href="/daydoc/2020/05/21/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/vscode%E6%8F%92%E4%BB%B6/"/>
      <url>/daydoc/2020/05/21/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/vscode%E6%8F%92%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h1 id="vscode插件"><a href="#vscode插件" class="headerlink" title="vscode插件"></a>vscode插件</h1><h2 id="draw-io-思维导图"><a href="#draw-io-思维导图" class="headerlink" title="draw.io(思维导图)"></a>draw.io(思维导图)</h2><p>在应用商店搜索<br>Draw.io Integration</p><p>并install<br>即可安装draw.io</p><p>然后创建drawio后缀的文件即可使用对应的导图功能</p><p>这个好处是免费并与vscode集成.即实现all in one 的功能</p><p><img src="http://img.wqkenqing.ren/2020-05-21-16-35-19.png" alt="2020-05-21-16-35-19"></p>]]></content>
      
      
      
        <tags>
            
            <tag> elasticsearch drawio </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阿里云DDNS[转载]</title>
      <link href="/daydoc/2020/05/20/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/%E9%98%BF%E9%87%8C%E4%BA%91DDNS/"/>
      <url>/daydoc/2020/05/20/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/%E9%98%BF%E9%87%8C%E4%BA%91DDNS/</url>
      
        <content type="html"><![CDATA[<p>阿里云DDNS</p><a id="more"></a><h1 id="阿里云DDNS"><a href="#阿里云DDNS" class="headerlink" title="阿里云DDNS"></a>阿里云DDNS</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">因为电信带宽的动态ip的问题,域名解析时常失效,所以用到这里的DDNS</span><br></pre></td></tr></table></figure><p>自己带宽的设置略过.</p><p>前提准备</p><ol><li>阿里云accesskyes配置</li><li>能运行docker镜像的服务器</li><li>docker镜像</li></ol><h2 id="阿里云-accesskeys申请"><a href="#阿里云-accesskeys申请" class="headerlink" title="阿里云 accesskeys申请"></a>阿里云 accesskeys申请</h2><p>登陆,点击右侧头像选择accesskeys即可.</p><p>获得ID与scret内容</p><h2 id="docker容器下载"><a href="#docker容器下载" class="headerlink" title="docker容器下载"></a>docker容器下载</h2><p><a href="https://hub.docker.com/r/chenhw2/aliyun-ddns-cli/" target="_blank" rel="noopener">https://hub.docker.com/r/chenhw2/aliyun-ddns-cli/</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull chenhw2&#x2F;aliyun-ddns-cli</span><br></pre></td></tr></table></figure><p>启动容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run -d \</span><br><span class="line">--restart&#x3D;always \</span><br><span class="line">--name ddns-aliyun \</span><br><span class="line">    -e &quot;AKID&#x3D;131323131231212&quot; \</span><br><span class="line">    -e &quot;AKSCT&#x3D;dsfasfwerwefdfsfsdfsfs&quot; \</span><br><span class="line">    -e &quot;DOMAIN&#x3D;dev.foxwho.com&quot; \</span><br><span class="line">    -e &quot;REDO&#x3D;600&quot; \</span><br><span class="line">    chenhw2&#x2F;aliyun-ddns-cli</span><br></pre></td></tr></table></figure><p>容器启动成功后，你可以看看 域名解析是否已经自动更新解析IP</p><p><a href="https://blog.csdn.net/fenglailea/article/details/102511502" target="_blank" rel="noopener">原文地址</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> DDNS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>杂记(操作系统)</title>
      <link href="/daydoc/2020/05/20/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/%E6%9D%82%E8%AE%B0(%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%BD%BF%E7%94%A8%E4%BD%93%E9%AA%8C)/"/>
      <url>/daydoc/2020/05/20/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/%E6%9D%82%E8%AE%B0(%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%BD%BF%E7%94%A8%E4%BD%93%E9%AA%8C)/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>从小学二年级接触windowsNT到大学毕业拥有自己第一台mac之前，一直是windows用户。windows版本基本也用NT-&gt;xp-&gt;win7-&gt;win8-&gt;w10等主要系统版本。使用场景也从文本编辑，软件应用，游戏，编程等多个场景。</p><p>因为参加工作，需要接触linux等原因，了解到了macos ,也在参加工作不久后入手了一台mbp。当时考虑的主要应用场景还是工作，编码和一些视频需求。</p><p>简单下来，对mac os 与windows有一些使用层面上的简单感受，所以开篇记录一番</p><p>主要诱因是因为最近一番调研下来发现还是windows阵营中的电脑较为好配，简单来讲就是一笔相对容易接受的价格就能在windows阵营配置下一台更方面硬件都相当不错的电脑。在这种情况影响用户体验的主要就是系统了。而综上，windows系统主要的差异点于mac比较来看的话是于系统操作的便捷性，与linux和unix内核下文件系统的差异性，并且在这之下的编程体验的不同。</p><p>而这里的编程体验，与系统的干净性，通过一些操作，如今已经能找到处理方案，所以现在我想尝试去再熟悉windows的操作。方便以后的换机时的选择。</p><p>to be continue!</p>]]></content>
      
      
      
        <tags>
            
            <tag> mac os &amp; windows </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tmux上手[转载]</title>
      <link href="/daydoc/2020/05/19/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/tmux%E4%B8%8A%E6%89%8B/"/>
      <url>/daydoc/2020/05/19/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/tmux%E4%B8%8A%E6%89%8B/</url>
      
        <content type="html"><![CDATA[<p>此处是简介</p><a id="more"></a><p>mac os 通过 brew install tmux</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux 接收快捷键的指令是 ^ + B 即mac中的control + B</span><br></pre></td></tr></table></figure><p>窗格的操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这些操作都是通过 ^+b 来接收</span><br></pre></td></tr></table></figure><table><thead><tr><th align="left">操作符</th><th align="left">作用</th></tr></thead><tbody><tr><td align="left">%</td><td align="left">左右创建两个窗格</td></tr><tr><td align="left">‘’</td><td align="left">左右创建两个窗格</td></tr><tr><td align="left">x</td><td align="left">关闭当前窗格</td></tr><tr><td align="left">{</td><td align="left">前移当前窗格</td></tr><tr><td align="left">}</td><td align="left">后移当前窗格</td></tr><tr><td align="left">;</td><td align="left">选择上次用的窗格</td></tr><tr><td align="left">o</td><td align="left">选择下一个窗格</td></tr><tr><td align="left">space</td><td align="left">切换窗格布局</td></tr><tr><td align="left">z</td><td align="left">放大窗格</td></tr><tr><td align="left">q</td><td align="left">显示序号</td></tr></tbody></table><h3 id="窗口操作"><a href="#窗口操作" class="headerlink" title="窗口操作"></a>窗口操作</h3><p>tmux 除了窗格以外，还有窗口（window） 的概念。依次使用以下快捷键来熟悉 tmux 的窗口操作：</p><ul><li><code>c</code> 新建窗口，此时当前窗口会切换至新窗口，不影响原有窗口的状态</li><li><code>p</code> 切换至上一窗口</li><li><code>n</code> 切换至下一窗口</li><li><code>w</code> 窗口列表选择，注意 macOS 下使用 <code>⌃p</code> 和 <code>⌃n</code> 进行上下选择</li><li><code>&amp;</code> 关闭当前窗口</li><li><code>,</code> 重命名窗口，可以使用中文，重命名后能在 tmux 状态栏更快速的识别窗口 id</li><li><code>0</code> 切换至 0 号窗口，使用其他数字 id 切换至对应窗口</li><li><code>f</code> 根据窗口名搜索选择窗口，可模糊匹配</li></ul><h3 id="会话操作"><a href="#会话操作" class="headerlink" title="会话操作"></a>会话操作</h3><p>如果运行了多次 <code>tmux</code> 命令则会开启多个 tmux 会话（session）。在 tmux 会话中，使用前缀快捷键 <code>⌃b</code> 配合以下快捷键可操作会话：</p><ul><li><code>$</code> 重命名当前会话</li><li><code>s</code> 选择会话列表</li><li><code>d</code> detach 当前会话，运行后将会退出 tmux 进程，返回至 shell 主进程</li></ul><p>在 shell 主进程下运行以下命令可以操作 tmux 会话： </p><p>在 shell 主进程下运行以下命令可以操作 tmux 会话：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tmux new -s foo # 新建名称为 foo 的会话</span><br><span class="line">tmux ls # 列出所有 tmux 会话</span><br><span class="line">tmux a # 恢复至上一次的会话</span><br><span class="line">tmux a -t foo # 恢复名称为 foo 的会话，会话默认名称为数字</span><br><span class="line">tmux kill-session -t foo # 删除名称为 foo 的会话</span><br><span class="line">tmux kill-server # 删除所有的会话</span><br></pre></td></tr></table></figure><p><a href="https://www.cnblogs.com/kaiye/p/6275207.html" target="_blank" rel="noopener">原文连接</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> tmux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cvim f失灵问题修复</title>
      <link href="/daydoc/2020/05/19/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/cvim%20f%E5%A4%B1%E7%81%B5%E9%97%AE%E9%A2%98%E4%BF%AE%E5%A4%8D/"/>
      <url>/daydoc/2020/05/19/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/cvim%20f%E5%A4%B1%E7%81%B5%E9%97%AE%E9%A2%98%E4%BF%AE%E5%A4%8D/</url>
      
        <content type="html"><![CDATA[<p>此处是简介</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cvim突然失灵,调试发现是js版本问题.cvim插件发行版暂未解决该问题.但有对应的修复分支</span><br></pre></td></tr></table></figure><p>问题修复</p><p>one : 克隆库: <a href="https://github.com/antonioyon/chromium-vim/tree/issue-716-fix-broken-hints" target="_blank" rel="noopener">https://github.com/antonioyon/chromium-vim/tree/issue-716-fix-broken-hints</a></p><p>two: 切换分支到 <code>issue-716-fix-broken-hints</code></p><p>three: 若无npm 则先安装npm</p><p>four: npm install , make</p><p>five: chrome 地址栏输入 chrome://extensions .选择加载已解压文件</p><p>six: 刷新页面,检查是否已经生效</p>]]></content>
      
      
      
        <tags>
            
            <tag> cvim </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka学习</title>
      <link href="/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/kafka/kafka%E5%AE%9E%E7%8E%B0/"/>
      <url>/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/kafka/kafka%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<p>[ x ]  Consumer Group里只会被某一个Consumer消费 ,Kafka还允许不同Consumer Group同时消费同一条消息，这一特性可以为消息的多元化处理提供支持。</p><a id="more"></a><h2 id="kafka-发送模式"><a href="#kafka-发送模式" class="headerlink" title="kafka 发送模式"></a>kafka 发送模式</h2><p>通过producer.type设置,可以设置producer的发送模式,具体参数据有<br>producer.type=false即同步(默认就是同步),设置为true为异步,即以batch形式像broker发送信息.(这里的batch可以设置)<br>还有一种oneway.即通过对ack的设置即可实现,ack=0时,即为oneway,只管发,不管是否接收成功.-1则是全部副本接收成功才算成功.</p><h2 id="kakfa消费模式"><a href="#kakfa消费模式" class="headerlink" title="kakfa消费模式"></a>kakfa消费模式</h2><ol><li>at last one</li><li>at most one</li><li>exactly one</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lambda&amp;Stream.md</title>
      <link href="/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/Lamda%E7%A7%AF%E7%B4%AF/"/>
      <url>/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/Lamda%E7%A7%AF%E7%B4%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="Lambda-amp-Stream积累"><a href="#Lambda-amp-Stream积累" class="headerlink" title="Lambda&amp;Stream积累"></a>Lambda&amp;Stream积累</h1><a id="more"></a><h2 id="Lambda"><a href="#Lambda" class="headerlink" title="Lambda"></a>Lambda</h2><p>Lambda主要是一个类语法长糖,尽量为java引入函数编程等实现,细节后续再补充</p><h2 id="Stream"><a href="#Stream" class="headerlink" title="Stream"></a>Stream</h2><p>java8提拱的新特性之一就有stream.stream主要是针对集合的处理类.提供了一系列集合处理方式.<br>配合使用lambda写出简介优美的代码</p><h3 id="Stream的使用"><a href="#Stream的使用" class="headerlink" title="Stream的使用"></a>Stream的使用</h3><p>通过如</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">List&lt;Integer&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">list.stream();<span class="comment">//即可以开启串行流;</span></span><br><span class="line">list.parallelStream().filter(a -&gt; &#123;</span><br><span class="line">            <span class="keyword">return</span> a &gt; <span class="number">20</span>;</span><br><span class="line">        &#125;);<span class="comment">//开启并行流</span></span><br></pre></td></tr></table></figure><p>串行流即内部单线程顺序执行,并行则是启用多线程执行.<br>后者并不一定效率就比前者高.因为并行执行启用分配线程资源时同样要消耗时间和资源,在一定量级下,前者的执行效率一度要高过后者.        </p><p>我这里对三种对集合的处理形式的比较,可以简单参考一下</p><ol><li>stream 串行流</li><li>parallelStream 并行流</li><li>常规循环式</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">List&lt;Integer&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100000</span>; i++) &#123;</span><br><span class="line">          list.add(getRandomNum());</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      DateUtil.setBegin();</span><br><span class="line">      list.stream().filter(a -&gt; &#123;</span><br><span class="line">          <span class="keyword">return</span> a &gt; <span class="number">20</span>;</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">      DateUtil.setStop();</span><br><span class="line">      System.out.println(<span class="string">"串行耗时"</span>+DateUtil.calCostTime());</span><br><span class="line"></span><br><span class="line">      DateUtil.setBegin();</span><br><span class="line">      list.parallelStream().filter(a -&gt; &#123;</span><br><span class="line">          <span class="keyword">return</span> a &gt; <span class="number">20</span>;</span><br><span class="line">      &#125;);</span><br><span class="line">      DateUtil.setStop();</span><br><span class="line">      System.out.println(<span class="string">"并行耗时"</span>+DateUtil.calCostTime());</span><br><span class="line">      <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      DateUtil.setBegin();</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> l : list) &#123;</span><br><span class="line">          <span class="keyword">if</span> (l &gt; <span class="number">20</span>) &#123;</span><br><span class="line">              count++;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      DateUtil.setStop();</span><br><span class="line">      System.out.println(<span class="string">"循环耗时"</span>+DateUtil.calCostTime());</span><br></pre></td></tr></table></figure><p>经由相当量次的测试后,我觉得如果要对集合中的数据进行遍历操作,根据量级的不同,<br>建议低量级还是采用普通循环,量级特别大,可考虑用并行流.书写方便,又不是大批量数据处理操作<br>可以直接采用串行流</p><h3 id="Stream的操作分类"><a href="#Stream的操作分类" class="headerlink" title="Stream的操作分类"></a>Stream的操作分类</h3><ol><li>Intermediate</li><li>Terminal</li><li>Short-circuiting</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 日常总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka</title>
      <link href="/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/kafka/"/>
      <url>/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/kafka/</url>
      
        <content type="html"><![CDATA[<p>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic sparkstreaming<br>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic flumetest</p><a id="more"></a><p>kafka-console-producer.sh –broker-list localhost:9092 –topic flumetest :创建生产者</p><p>kafka-console-consumer.sh –bootstrap-server namenode:9092  –topic  flume-ng</p><h1 id="Kafka相关小结"><a href="#Kafka相关小结" class="headerlink" title="Kafka相关小结"></a>Kafka相关小结</h1><h2 id="kafka-相关指令"><a href="#kafka-相关指令" class="headerlink" title="kafka 相关指令"></a>kafka 相关指令</h2><p>kafka-server-start.sh config/server.properties &amp; 启动<br>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic topic_name  :创建topic<br>kafka-console-producer.sh –broker-list localhost:9092 –topic topic_name :创建生产者</p><p>kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic topic_name :创建消费者</p><p>kafka-console-producer.sh –broker-list namenode:9092 –topic sparkstreaming</p><p>删除group</p><p>kafka-consumer-groups –bootstrap-server 192.168.10.100:9092,192.168.10.101:9092,192.168.10.102:9092  —group traffic_history —delete</p><h2 id="kafka-java-api"><a href="#kafka-java-api" class="headerlink" title="kafka java api"></a>kafka java api</h2><p>kafka 虽然搭建较为简单,但想要对针它编程体验还是有些问题.初步使用下来明显感觉对版本的强约束性.以我线上版本</p><p><img src="http://img.wqkenqing.ren/2019-03-12-11-02-43.png" alt="2019-03-12-11-02-43">为例,我java项目对应的版本则是</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;0.8.1&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;0.8.2.1&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>以上版本搭配经由我亲测通过</p>]]></content>
      
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mapreduce组件总结</title>
      <link href="/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/mapreduce%E7%BB%84%E4%BB%B6%E6%80%BB%E7%BB%93/"/>
      <url>/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/mapreduce%E7%BB%84%E4%BB%B6%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="mapreduce组件总结"><a href="#mapreduce组件总结" class="headerlink" title="mapreduce组件总结"></a>mapreduce组件总结</h1><a id="more"></a><p>相关组件大致有</p><ol><li>Inputformat</li><li>Inputsplit</li><li>ReadRecorder</li><li>mapper</li><li>Combiner</li><li>Partioner</li><li>Reduce</li><li>GroupComparator</li><li>Reduce</li></ol><h1 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h1><p><img src="http://img.wqkenqing.ren/2019-03-19-15-39-59.png" alt="2019-03-19-15-39-59"><br><img src="http://img.wqkenqing.ren/2019-03-19-16-46-06.png" alt="2019-03-19-16-46-06"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shuffle 被称为mapreduce的核心,一个真正让奇迹发生的地方.但它到底是什么呢?简练的讲,它就是 map out 到 reduce in 这段过程中对数据的处理过程.</span><br></pre></td></tr></table></figure><p>shuffle过程中主要发生的操作有,Partion,Sort,spill,merge,copy,sort,merge.(还有可能有combine操作)</p><p>具体流程是<br>map out后,Collector 对out后的数据进行处理. 数据将会写入到内存缓冲区,该内存缓冲区的数据达到80%后,会开启一个溢写线程,在磁盘本地创建一个文件.如果reduce设置了多个分区,写入buffer区的数据,会被打上一个分区标记.通过sortAndSpill()方法进行指对数据按分区号,key排序.最后溢出的文件是分区的,按key有序的文件.若buffer区中的20%一直未被填满,buffer写入进程不会断.但若达到100%,Buffer写入进程则会阻塞.并在buffer区中的数据全部spill完后才会再开启. (buffer区的内存默认是100M),spill过程中,若设置过combiner.则会对数据先进行combiner逻辑处理,再将处理后的数据写出</p><p>spill完成后则会对本地的spill后的文件进行Merge.即把多个spill后的文件进行合并,并排序.最后会行成一个有序文件</p><p>当1个Map Task 完成后,reduce 就会开启copy进程(默认是5个线程).这个过程中会通过http请求去各taskTracker(nodemanager),拉取相应的spill&amp;merge后的文件.<br>当copy完成后,则又会对数据进行merge.这个过程中同样有个类似map shuffle 中的buffer 溢写的阶段. 这个过程同样会触发combiner组件.这里的merge数据源有三种</p><ol><li>memory to memory</li><li>memory to disk</li><li>disk   to disk<br>默认1是不开启的.</li></ol><p>copy phase 完成后,是reduceTask 中的 sort phase<br>即对merge 中的文件继续进行sort and group .</p><p>当sort phase 完成.则开启reduce phase .到此shuffle正式完成.</p><p>##二次排序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>mapreduce 常见的辅助排序</p><ol><li>partitioner</li><li>key的比较Comparator</li><li>分组函数Grouping Comparator</li></ol><h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><p>map join ,semi join ,reduce join</p><h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
      
      
      
        <tags>
            
            <tag> bigdata </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark go on</title>
      <link href="/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/spark%E5%AD%A6%E4%B9%A03/"/>
      <url>/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/spark%E5%AD%A6%E4%B9%A03/</url>
      
        <content type="html"><![CDATA[<p>spark-core,spark-streaming再深造</p><a id="more"></a><h1 id="spark-go-on"><a href="#spark-go-on" class="headerlink" title="spark go on"></a>spark go on</h1><p>初始规划</p><p>spark-core<br>spark-streaming</p>]]></content>
      
      
      
        <tags>
            
            <tag> bigdata </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据分享</title>
      <link href="/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E5%88%86%E4%BA%AB/"/>
      <url>/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E5%88%86%E4%BA%AB/</url>
      
        <content type="html"><![CDATA[<h2 id="开头语"><a href="#开头语" class="headerlink" title="开头语"></a>开头语</h2><p><code>工欲善其事，必先利其器</code></p><a id="more"></a><p>本次分享,是我在公司的第一次分享,我考虑后,将本次分享的主要内容分为了三大部块.先是针对相关基础组件分类介绍.再介绍下通过对这些组件进行组织配搭的大数据基础环境架构.再结合我的一些经历,为大家介绍下相关的应用与产品落地.</p><h2 id="技术栈简介"><a href="#技术栈简介" class="headerlink" title="技术栈简介"></a>技术栈简介</h2><ul><li>数据采集</li><li>数据存储</li><li>数据治理(清洗&amp;处理)</li><li>数据应用</li><li>产品落地</li></ul><p>我又根据不同组件的特性将他们分</p><ul><li>采集类</li><li>存储类</li><li>计算处理类</li><li>传输类</li><li>管理类 </li><li>其它类</li></ul><p>下面开始具体介绍</p><h2 id="采集类"><a href="#采集类" class="headerlink" title="采集类"></a>采集类</h2><p>数据源:</p><ul><li>日志</li><li>业务数据</li><li>公网数据(爬虫)</li><li>文本数据</li><li>出行数据(gps,手机定位等)</li></ul><p><img src="http://img.wqkenqing.ren/2019-04-15-10-36-53.png" alt="2019-04-15-10-36-53"></p><ul><li>sqoop flume crawler datax kettle  elk</li></ul><ol><li>Flume(水槽) 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单,可拓展</li><li>Sqoop是一个在结构化数据和Hadoop之间进行批量数据迁移的工具，结构化数据可以是Mysql、Oracle等RDBMS。Sqoop底层用MapReduce程序实现抽取、转换、加载，MapReduce天生的特性保证了并行化和高容错率，而且相比Kettle等传统ETL工具，任务跑在Hadoop集群上，减少了ETL服务器资源的使用情况。在特定场景下，抽取过程会有很大的性能提升。</li><li>crawler , jsoup ,httpclient, nutch 等.</li><li>elk  集中式日志系统 ELK 协议栈详解<br><img src="http://img.wqkenqing.ren/2019-04-15-10-11-16.png" alt="2019-04-15-10-11-16"></li></ol><hr><h2 id="存储类"><a href="#存储类" class="headerlink" title="存储类"></a>存储类</h2><ul><li>hdfs</li><li>hbase</li><li>hive</li><li>mongdb</li><li>redis</li><li>RDBMS</li></ul><h3 id="hdfs"><a href="#hdfs" class="headerlink" title="hdfs"></a>hdfs</h3><pre><code>* 分布式文件存储系统* 提供了高可靠性、高扩展性和高吞吐率的数据存储服务* hdfs典型结构：物理结构+逻辑结构* 文件线性切割成Block：偏移量（offset）* Block分散存储在集群节点中* 单一文件Block大小一致，文件与文件可以不一致* Block可以设置副本数，副本分散在不同的节点中* 副本数不要超过节点数量* 文件上传可以设置Block大小和副本数* 已上传的文件Block副本数可以调整，大小不变* 只支持一次写入多次读取，同一时刻只有一个写入者* 只能追加，不能修改</code></pre><p><img src="http://img.wqkenqing.ren/2019-04-15-11-12-40.png" alt="2019-04-15-11-12-40"></p><h3 id="hbase"><a href="#hbase" class="headerlink" title="hbase"></a>hbase</h3><p>Base是一个构建在HDFS上的分布式列存储系统；<br>Base是基于Google BigTable模型开发的，典型的key/value系统；<br>Base是Apache Hadoop生态系统中的重要一员，主要用于海量结构化数据存储；</p><p>大：一个表可以有数十亿行，上百万列；无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有截然不同的列；面向列：面向列（族）的存储和权限控制，列（族）独立检索；稀疏：空（null）列并不占用存储空间，表可以设计的非常稀疏；数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳；数据类型单一：Hbase中的数据都是字符串，没有类型</p><h4 id="openTSDB"><a href="#openTSDB" class="headerlink" title="openTSDB"></a>openTSDB</h4><p>基于Hbase的分布式的，可伸缩的时间序列数据库。<br>主要用途，就是做监控系统；譬如收集大规模集群（包括网络设备、操作系统、应用程序）的监控数据并进行存储，查询。<br><img src="http://img.wqkenqing.ren/2019-04-15-11-27-46.png" alt="2019-04-15-11-27-46"></p><h4 id="solr-amp-Phoenix"><a href="#solr-amp-Phoenix" class="headerlink" title="solr &amp; Phoenix"></a>solr &amp; Phoenix</h4><p>二级索引</p><h3 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h3><p>ive 是一个基于 Hadoop 文件系统之上的数据仓库架构。它可以将结构化的数据文件映射为一张数据库表，并提供简单的 sql 查询功能。还可以将 sql 语句转换为 MapReduce 任务运行。<br>底部计算引擎还可以用用Tez, spark等.<br><img src="http://img.wqkenqing.ren/2019-04-15-11-36-43.png" alt="2019-04-15-11-36-43"></p><h5 id="Impala"><a href="#Impala" class="headerlink" title="Impala"></a>Impala</h5><p>Impala是Cloudera公司推出，提供对HDFS、Hbase数据的高性能、低延迟的交互式SQL查询功能。</p><ul><li>基于Hive使用内存计算，兼顾数据仓库、具有实时、批处理、多并发等优点</li><li>对内存依赖大,稳定性不如hive</li></ul><p>相比hive数据仓库,impala针对的量级相关少些,但会有效率的提升.但一般来讲,数据仓库一类需求对时间上的要要求一般不会太高,所以常规方式一般就符合大多数需求.</p><h2 id="计算处理类"><a href="#计算处理类" class="headerlink" title="计算处理类"></a>计算处理类</h2><ul><li>mapreduce</li><li>mapreduce on oozie ,on tez </li><li>spark </li><li>flink</li></ul><h3 id="mapreduce"><a href="#mapreduce" class="headerlink" title="mapreduce"></a>mapreduce</h3><p>Mapreduce是一个计算框架，既然是做计算的框架，那么表现形式就是有个输入（input），mapreduce操作这个输（input），通过本身定义好的计算模型，得到一个输出（output），这个输出就是我们所需要的结果。我们要学习的就是这个计算模型的运行规则。在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，每个阶段都是用键值对（key/value）作为输入（input）和输出（output）。而程序员要做的就是定义好这两个阶段的函数：map函数和reduce函数。</p><p>分布式计算；<br>移动计算而不移动数据。<br><img src="http://img.wqkenqing.ren/2019-04-15-11-59-56.png" alt="2019-04-15-11-59-56"></p><h3 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h3><p>相比一二代计算引擎,在兼并了一二代的特色之外,还引放了流计算这一能力,还丰富了计算函数.<br>其中比较有代表性的主要就是spark&amp;storm.<br>也就是说这代计算引擎兼具无边界数据与有边界数据同样的处理能力.同时还具有DAG特性.<br>这里主要介绍spark</p><p>spark主要组成有以下</p><ul><li>spark-core</li><li>spark-streaming</li><li>spark-sql</li><li>spark-mlib</li><li>spark-graphX。</li></ul><p>spark-core是一个提供内存计算的框架,其他的四大框架都是基于spark core上进行计算的,所以没有spark core,其他的框架是浮云.<br>spark-core的主要内容就是对RDD的操作<br>RDD的创建 -&gt;RDD的转换 -&gt;RDD的缓存 -&gt;RDD的行动 -&gt;RDD的输出</p><p>spark-streaming中使用离散化流（discretized stream）作为抽象的表示，叫做DStream。它是随时间推移而收集数据的序列，每个时间段收集到的数据在DStream内部以一个RDD的形式存在。DStream支持从kafka，flume,hdfs,s3等获取输入。DStream也支持两种操作，即转化操作和输出操作</p><p>spark-sql<br>Spark SQL 提供了查询结构化数据及计算结果等信息的接口.<br>查询结果以 Datasets and DataFrames 形式返回</p><p>…</p><h3 id="flink-blink"><a href="#flink-blink" class="headerlink" title="flink/blink"></a>flink/blink</h3><p>略</p><h2 id="传输类"><a href="#传输类" class="headerlink" title="传输类"></a>传输类</h2><h3 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h3><p>Kafka是分布式发布-订阅消息系统,一个分布式的，可划分的，冗余备份的持久性的日志服务。它主要用于处理活跃的流式数据。日常中常与spark-streaming结合实用,为其提供无边界数据</p><p><img src="http://img.wqkenqing.ren/2019-04-16-15-06-25.png" alt="2019-04-16-15-06-25"></p><h2 id="管理类-Hue-cloudera-manager"><a href="#管理类-Hue-cloudera-manager" class="headerlink" title="管理类 Hue cloudera-manager"></a>管理类 Hue cloudera-manager</h2><p>ue与cm 都是由cloudera提供,后面cloudera将hue开源给了apache.如果基础集群环境是采用的是开源自主搭建,可考虑引入hue.另一些大数据服务公司,有集成打包自己的一些大数据产品,如cdh等.但这些服务收费,涉及到成本问题.所以如何选用,需要相关斟酌.</p><h2 id="其它类-zookeeper-yarn等"><a href="#其它类-zookeeper-yarn等" class="headerlink" title="其它类 zookeeper ,yarn等"></a>其它类 zookeeper ,yarn等</h2><p>zookeeper在集中基础环境中主要作为配置分享中心,与kafka,hbase等组件集成.yarn则作为资源管理组件,可以与mapreduce ,spark等集成</p><h2 id="各类组件架构"><a href="#各类组件架构" class="headerlink" title="各类组件架构"></a>各类组件架构</h2><p>以上,已经大致介绍了各类工具,基本了解了相应的特性和使用场景,而根据它们的特性,进行合理的配备,架构,从而实现一个功能全面,稳定的大数据环境.</p><p>于我个人经历与平时了解来讲,一般的架构主要如下<br><img src="http://img.wqkenqing.ren/2019-04-17-09-23-07.png" alt="2019-04-17-09-23-07"><br>另:<br><img src="http://img.wqkenqing.ren/2019-04-16-10-42-08.png" alt="2019-04-16-10-42-08"></p><p>总得来说,各类组件供选型一般来讲都不是单一的.所以,我们的大数据环境各部份组件都是插销式可插拔的.所以不同公司可能不一而同,具体看自身需求和实际情况.比如上图中的storm流式计算模块,就可以替换成spark-streaming等.</p><p>通过对上图的架构的拆解,再组合,可能还会有以下组织架构.</p><p>数据仓库<br>可以理解为上图中间部份.作为一个数据集市的存在,算作数据中心的一部份.</p><ul><li>ODS：是数据仓库第一层数据，直接从原始数据过来的，经过简单地处理，比喻：字段体重的数据为175cm等数据。</li><li>DW*：这个是数据仓库的第二层数据，DWD和DWS很多情况下是并列存在的，这一层储存经过处理后的标准数据，比喻订单、用户、页面点击流量等数据。</li><li>ADS：这个是数据仓库的最后一层数据，为应用层数据，直接可以给业务人员使用。<br><img src="http://img.wqkenqing.ren/2019-04-16-11-01-33.png" alt="2019-04-16-11-01-33"></li></ul><p>星型模型</p><p>星型模型中有两个重要的概念：事实表和维度表。<br>事实表：一些主键ID的集合，没有存放任何实际的内容<br>维度表：存放详细的数据信息，有唯一的主键ID。如上面的关键词表、用户表等等。<br><img src="http://img.wqkenqing.ren/2019-04-16-11-04-52.png" alt="2019-04-16-11-04-52"></p><p> 数据中心:<br>概念相对更大一些,可能即作为具体平台产品集合,也可能是一个团队行政划分.总得来说,是如</p><ul><li>大数据基础平台</li><li>数据仓库</li><li>DMP平台</li><li>相关应用平台如推荐系统,报表系统,可视化平台等.</li></ul><p>数据中台:<br>这个是由阿里于15年率先提出.主导思想是大中台,小前台.这块暂无特别明确的解释说法,但现在也有不少公司效仿.我个人从它的主导思想”大中台,小前台”的理解是,这个可能是体量更大,壁垒更少的一个数据集成体.比如阿里系的旗下公司,数据流都会归集到中台,同时阿里系下的公司也能获得不仅自己公司数据中心归集的数据反馈,还能获得阿里中台整合后流出的反馈数据.</p><h2 id="应用落地"><a href="#应用落地" class="headerlink" title="应用落地"></a>应用落地</h2><h3 id="公共服务"><a href="#公共服务" class="headerlink" title="公共服务"></a>公共服务</h3><ul><li>交通出行<br><img src="http://img.wqkenqing.ren/2019-04-16-16-15-42.png" alt="2019-04-16-16-15-42"></li><li>智慧城市</li><li>…</li></ul><h3 id="产品应用"><a href="#产品应用" class="headerlink" title="产品应用"></a>产品应用</h3><ul><li>用户画像</li><li>征信模型</li><li>推荐系统</li><li>精确营销</li><li>前沿科学(无人驾驶,人工智能,AR等)</li></ul><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>以上,就是我今天分享的主要内容.今天的主题是”器”,但对这些工具的讲解浅尝辄止,在实际的开发实战中涉及的情况是更为复杂,需要掌握的内容更多,深度也更深.我这里主要是想抛砖引玉,为大家提供一点自己的理解,若能有所帮助,不胜荣幸.</p><p>另外,工具始终是工具,菜刀再利也要厨子好,才能做好菜.所以如何利用这些工具,与我们的业务结合,实现我们想要的价值,这是我一直在探索的,也愿与各位同仁一同前行.</p><p>附上图中涉及到的技术栈</p><p><img src="http://img.wqkenqing.ren/2019-04-17-09-09-00.png" alt="2019-04-17-09-09-00"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 日常总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark常见的输入源</title>
      <link href="/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/spark/spark%E5%B8%B8%E8%A7%81%E7%9A%84%E8%BE%93%E5%85%A5%E6%BA%90/"/>
      <url>/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/spark/spark%E5%B8%B8%E8%A7%81%E7%9A%84%E8%BE%93%E5%85%A5%E6%BA%90/</url>
      
        <content type="html"><![CDATA[<p>spark 输入源整理</p><a id="more"></a><h1 id="spark常见的输入源"><a href="#spark常见的输入源" class="headerlink" title="spark常见的输入源"></a>spark常见的输入源</h1><h2 id="text"><a href="#text" class="headerlink" title="text"></a>text</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"text_count"</span>);</span><br><span class="line"></span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;String&gt; tRDD = sc.textFile(<span class="string">"/Users/kuiq.wang/Desktop/upload/yd_conver.txt"</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> res = tRDD.count();</span><br><span class="line"></span><br><span class="line">        log.info(<span class="string">"text_count's result is [&#123;&#125;]"</span>, res);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">    SparkConf conf &#x3D; new SparkConf().setAppName(&quot;collectRDD&quot;);</span><br><span class="line">    JavaSparkContext jsc &#x3D; new JavaSparkContext(conf);</span><br><span class="line"></span><br><span class="line">    JavaRDD collectRDD &#x3D; jsc.parallelize(Arrays.asList(new String[]&#123;&quot;one&quot;, &quot;two&quot;, &quot;three&quot;&#125;));</span><br><span class="line">     long res &#x3D; collectRDD.count();</span><br><span class="line"></span><br><span class="line">    log.info(&quot;collect rdd res is [&#123;&#125;]&quot;, res);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="elasticsearch"><a href="#elasticsearch" class="headerlink" title="elasticsearch"></a>elasticsearch</h2><p>准备</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--spark-es--&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.elasticsearch&lt;&#x2F;groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;elasticsearch-spark-20_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">      &lt;version&gt;6.7.2&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">readES</span><span class="params">(String url, String index)</span> </span>&#123;</span><br><span class="line">    SparkConf conf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"es_count"</span>).set(<span class="string">"es.nodes"</span>, <span class="string">"data1:9200"</span>);</span><br><span class="line">    JavaSparkContext jsc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">    JavaPairRDD&lt;String, Map&lt;String, Object&gt;&gt; esRDD = JavaEsSpark.esRDD(jsc, <span class="string">"funnylog_test"</span>);</span><br><span class="line">    <span class="keyword">long</span> es = esRDD.count();</span><br><span class="line">    System.out.println(<span class="string">"res is :"</span> + es);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="hbase"><a href="#hbase" class="headerlink" title="hbase"></a>hbase</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>日常运维</title>
      <link href="/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/%E6%97%A5%E5%B8%B8%E8%BF%90%E7%BB%B4/"/>
      <url>/daydoc/2020/05/11/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E8%BF%90%E7%BB%B4/%E6%97%A5%E5%B8%B8%E8%BF%90%E7%BB%B4/</url>
      
        <content type="html"><![CDATA[<p>此处是简介</p><a id="more"></a><h2 id="hexo-加速"><a href="#hexo-加速" class="headerlink" title="hexo 加速"></a>hexo 加速</h2><p>通过更换公共CDN的js css资源.实现了较大改变</p><h2 id="hexo-LocalSearch一直转圈问题修复"><a href="#hexo-LocalSearch一直转圈问题修复" class="headerlink" title="hexo LocalSearch一直转圈问题修复"></a>hexo LocalSearch一直转圈问题修复</h2><p>主要成因有两,一是search.xml没有成功加载,二是search.xml中的数据有问题.</p><p>前者通过配置_config.yml文档和对应主题下的_config.yml配置</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">search:</span></span><br><span class="line">  <span class="attr">path:</span> <span class="string">search.xml</span></span><br><span class="line">  <span class="attr">field:</span> <span class="string">post</span></span><br><span class="line">  <span class="attr">format:</span> <span class="string">html</span></span><br><span class="line">  <span class="attr">limit:</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">local_search:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># if auto, trigger search by changing input</span></span><br><span class="line">  <span class="comment"># if manual, trigger search by pressing enter key or search button</span></span><br><span class="line">  <span class="attr">trigger:</span> <span class="string">auto</span></span><br><span class="line">  <span class="comment"># show top n results per article, show all results by setting to -1</span></span><br><span class="line">  <span class="attr">top_n_per_article:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>同时记得关闭Algolia_Search</strong></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Algolia Search</span></span><br><span class="line"><span class="attr">algolia_search:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 运维 cdn 加速 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>准备小结</title>
      <link href="/daydoc/2020/05/10/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/%E5%87%86%E5%A4%87%E5%B0%8F%E7%BB%93/"/>
      <url>/daydoc/2020/05/10/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/%E5%87%86%E5%A4%87%E5%B0%8F%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="准备小结"><a href="#准备小结" class="headerlink" title="准备小结"></a>准备小结</h1><a id="more"></a><h2 id="hdfs存储机制是怎样的"><a href="#hdfs存储机制是怎样的" class="headerlink" title="hdfs存储机制是怎样的?"></a>hdfs存储机制是怎样的?</h2><p>client端发送写文件请求，namenode检查文件是否存在，如果已存在，直接返回错误信息，否则，发送给client一些可用namenode节点<br>client将文件分块，并行存储到不同节点上datanode上，发送完成后，client同时发送信息给namenode和datanode<br>namenode收到的client信息后，发送确信信息给datanode<br>datanode同时收到namenode和datanode的确认信息后，提交写操作。</p><h2 id="hadoop中combiner的作用是什么"><a href="#hadoop中combiner的作用是什么" class="headerlink" title="hadoop中combiner的作用是什么?"></a>hadoop中combiner的作用是什么?</h2><p>当map生成的数据过大时，带宽就成了瓶颈，怎样精简压缩传给Reduce的数据，又不影响最终的结果呢。有一种方法就是使用Combiner，Combiner号称本地的Reduce，Reduce最终的输入，是Combiner的输出。</p><h2 id="你们数据库怎么导入hive-的-有没有出现问题"><a href="#你们数据库怎么导入hive-的-有没有出现问题" class="headerlink" title="你们数据库怎么导入hive 的,有没有出现问题"></a>你们数据库怎么导入hive 的,有没有出现问题</h2><p>在导入hive的时候，如果数据库中有blob或者text字段，会报错，解决方案在sqoop笔记中。在将数据由Oracle数据库导入到Hive时，发现带有clob字段的表的数据会错乱，出现一些字段全为NULL的空行。由于在项目中CLOB字段没有实际的分析用途，因此考虑将CLOB字段去掉。</p><h2 id="hdfs-site-xml的3个主要属性"><a href="#hdfs-site-xml的3个主要属性" class="headerlink" title="hdfs-site.xml的3个主要属性?"></a>hdfs-site.xml的3个主要属性?</h2><p>dfs.name.dir决定的是元数据存储的路径以及DFS的存储方式(磁盘或是远端)<br>dfs.data.dir决定的是数据存储的路径<br>fs.checkpoint.dir用于第二Namenode</p><h2 id="下列哪项通常是集群的最主要瓶颈"><a href="#下列哪项通常是集群的最主要瓶颈" class="headerlink" title="下列哪项通常是集群的最主要瓶颈"></a>下列哪项通常是集群的最主要瓶颈</h2><p>磁盘 IO<br>答案：C 磁盘<br>首先集群的目的是为了节省成本，用廉价的 pc 机，取代小型机及大型机。小型机和大型机有什么特点？<br>1.cpu 处理能力强<br>2.内存够大，所以集群的瓶颈不可能是 a 和 d<br>3.如果是互联网有瓶颈，可以让集群搭建内网。每次写入数据都要通过网络（集群是内网），然后还要写入 3 份数据，所以 IO 就会打折扣。</p><h2 id="关于-SecondaryNameNode-哪项是正确的？"><a href="#关于-SecondaryNameNode-哪项是正确的？" class="headerlink" title="关于 SecondaryNameNode 哪项是正确的？"></a>关于 SecondaryNameNode 哪项是正确的？</h2><p>它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间 </p><h2 id="mapreduce的原理"><a href="#mapreduce的原理" class="headerlink" title="mapreduce的原理?"></a>mapreduce的原理?</h2><p>MapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果，<br>得到最终结果。简单地说，MapReduce就是”任务的分解与结果的汇总”。<br>在Hadoop中，用于执行MapReduce任务的机器角色有两个：一个是JobTracker；另一个是TaskTracker，JobTracker是用于调度工作的，TaskTracker<br>是用于执行工作的。一个Hadoop集群中只有一台JobTracker。<br>在分布式计算中，MapReduce框架负责处理了并行编程中分布式存储、工作调度、负载均衡、容错均衡、容错处理以及网络通信等复杂问题，把处理<br>过程高度抽象为两个函数：map和reduce，map负责把任务分解成多个任务，reduce负责把分解后多任务处理的结果汇总起来。<br>需要注意的是，用MapReduce来处理的数据集（或任务）必须具备这样的特点：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都<br>可以完全并行地进行处理。</p><h2 id="HDFS存储的机制"><a href="#HDFS存储的机制" class="headerlink" title="HDFS存储的机制?"></a>HDFS存储的机制?</h2><h3 id="写流程："><a href="#写流程：" class="headerlink" title="写流程："></a>写流程：</h3><p>client链接namenode存数据<br>namenode记录一条数据位置信息（元数据），告诉client存哪。<br>client用hdfs的api将数据块（默认是64M）存储到datanode上。<br>datanode将数据水平备份。并且备份完将反馈client。<br>client通知namenode存储块完毕。<br>namenode将元数据同步到内存中。<br>另一块循环上面的过程。</p><h3 id="读流程"><a href="#读流程" class="headerlink" title="读流程"></a>读流程</h3><h2 id="举一个简单的例子说明mapreduce是怎么来运行的"><a href="#举一个简单的例子说明mapreduce是怎么来运行的" class="headerlink" title="举一个简单的例子说明mapreduce是怎么来运行的 ?"></a>举一个简单的例子说明mapreduce是怎么来运行的 ?</h2><p>MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。<br>　　Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。<br>Mapper任务的执行过程详解<br>　　每个Mapper任务是一个Java进程，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后，<br>转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下六个阶段：<br>　　第一阶段是把输入文件按照一定的标准分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit)<br>的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值128MB，输入文件有两个，一个是32MB，一个是　172MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由　一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。</p><p>　　第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一　行的起始位置(单位是字节)，“值”是本行的文本内容。<br>　　<br>　　第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会　调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。</p><p>　　第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、　山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer　任务运行的数量。默认只有一个Reducer任务。<br>第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值　对&lt;2,2&gt;、&lt;1,3&gt;、&lt;2,1&gt;，键和值分别是整数。那么排序后的结果是&lt;1,3&gt;、&lt;2,1&gt;、&lt;2,2&gt;。如果有第六阶段，那么进入</p><p>第六阶段　如果没有，直接输出到本地的Linux文件中。　第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。　归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。　Reducer任务的执行过程详解<br>每个Reducer任务是一个java进程。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为三个阶段：<br>第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。<br>第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。<br>第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。<br>最后把这些输出的键值对写入到HDFS文件中。<br>在整个MapReduce程序的开发过程中，我们最大的工作量是覆盖map函数和覆盖reduce函数。</p><h2 id="了解hashMap-和hashTable吗介绍下，他们有什么区别。"><a href="#了解hashMap-和hashTable吗介绍下，他们有什么区别。" class="headerlink" title="了解hashMap 和hashTable吗介绍下，他们有什么区别。"></a>了解hashMap 和hashTable吗介绍下，他们有什么区别。</h2><h2 id="为什么重写equals还要重写hashcode"><a href="#为什么重写equals还要重写hashcode" class="headerlink" title="为什么重写equals还要重写hashcode"></a>为什么重写equals还要重写hashcode</h2><p>因为equals比较的是内容是一致.但hashcode</p><h2 id="说一下map的分类和常见的情况"><a href="#说一下map的分类和常见的情况" class="headerlink" title="说一下map的分类和常见的情况"></a>说一下map的分类和常见的情况</h2><p> hashmap,hashtable,treemap,LinkedHashMap</p><ul><li>根据键得到值，因此不允许键重复(重复了覆盖了),但允许值重复<h3 id="Hashmap"><a href="#Hashmap" class="headerlink" title="Hashmap"></a>Hashmap</h3>是一个最常用的Map</li><li>它根据键的HashCode值存储数据,根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的</li><li>最多只允许一条记录的键为Null;允许多条记录的值为 Null;</li><li>HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。</li><li>如果需要同步，可以用 Collections的synchronizedMap方法使HashMap具有同步的能力，或者使用ConcurrentHashMap<h3 id="Hashtable"><a href="#Hashtable" class="headerlink" title="Hashtable"></a>Hashtable</h3>Hashtable与 HashMap类似,它继承自Dictionary类,不同的是:它不允许记录的键或者值为空;</li><li>它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了 Hashtable在写入时会比较慢<h3 id="LinkedHashMap"><a href="#LinkedHashMap" class="headerlink" title="LinkedHashMap"></a>LinkedHashMap</h3>是 HashMap 的一个子类，保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的.<br>也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比 HashMap 慢，不过有种情况例外，当 HashMap 容量很大，实际数据较少时，遍历起来可能会比 LinkedHashMap 慢，因为 LinkedHashMap 的遍历速度只和实际数据有关，和容量无关，而 HashMap 的遍历速度和他的容量有关<h3 id="TreeMap"><a href="#TreeMap" class="headerlink" title="TreeMap"></a>TreeMap</h3>实现 SortMap 接口,能够把它保存的记录根据键排序, 默认是按键值的升序排序，也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的</li></ul><p>HashMap，链表法存储，entry[]数组，线程不安全，可能死锁 concurrentHashMap，segment数组，每个segent下维护一组entry[]数组，每个segment是一把锁，线程安全 LinkedHashMap</p><hr><h2 id="Object若不重写hashCode-的话，hashCode-如何计算出来的？"><a href="#Object若不重写hashCode-的话，hashCode-如何计算出来的？" class="headerlink" title="Object若不重写hashCode()的话，hashCode()如何计算出来的？"></a>Object若不重写hashCode()的话，hashCode()如何计算出来的？</h2><p>hashcode采用的是</p><h2 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h2><h3 id="1-spark的有几种部署模式，每种模式特点？"><a href="#1-spark的有几种部署模式，每种模式特点？" class="headerlink" title="1. spark的有几种部署模式，每种模式特点？"></a>1. spark的有几种部署模式，每种模式特点？</h3><h4 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h4><p>本地模式分三类</p><ul><li>local：只启动一个executor</li><li>local[k]: 启动k个executor</li><li>local[*]：启动跟cpu数目相同的 executor</li></ul><h3 id="cluster模式"><a href="#cluster模式" class="headerlink" title="cluster模式"></a>cluster模式</h3><p>cluster模式肯定就是运行很多机器上了，但是它又分为以下三种模式，区别在于谁去管理资源调度。（说白了，就好像后勤管家，哪里需要资源，后勤管家要负责调度这些资源）</p><h4 id="standalone模式"><a href="#standalone模式" class="headerlink" title="standalone模式"></a>standalone模式</h4><p>分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础</p><h4 id="Spark-on-yarn模式"><a href="#Spark-on-yarn模式" class="headerlink" title="Spark on yarn模式"></a>Spark on yarn模式</h4><p>分布式部署集群，资源和任务监控交给yarn管理<br>粗粒度资源分配方式，包含cluster和client运行模式<br>cluster 适合生产，driver运行在集群子节点，具有容错功能<br>client 适合调试，dirver运行在客户端</p><h3 id="2-Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？"><a href="#2-Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？" class="headerlink" title="2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？"></a>2. Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？</h3><h4 id="Spark-core"><a href="#Spark-core" class="headerlink" title="Spark core"></a>Spark core</h4><p>是其它组件的基础，spark的内核<br>主要包含：有向循环图、RDD、Lingage、Cache、broadcast等</p><h4 id="SparkStreaming"><a href="#SparkStreaming" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h4><p>是一个对实时数据流进行高通量、容错处理的流式处理系统<br>将流式计算分解成一系列短小的批处理作业</p><h4 id="Spark-sql："><a href="#Spark-sql：" class="headerlink" title="Spark sql："></a>Spark sql：</h4><p>能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询</p><h4 id="MLBase"><a href="#MLBase" class="headerlink" title="MLBase"></a>MLBase</h4><p>是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低<br>MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。</p><h4 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h4><p>是Spark中用于图和图并行计算</p><h4 id="spark有哪些组件"><a href="#spark有哪些组件" class="headerlink" title="spark有哪些组件"></a>spark有哪些组件</h4><p>master：管理集群和节点，不参与计算。<br>worker：计算节点，进程本身不参与计算，和master汇报。<br>Driver：运行程序的main方法，创建spark context对象。<br>spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。<br>client：用户提交程序的入口。</p><ul><li><a href="https://blog.csdn.net/yirenboy/article/details/47441465" target="_blank" rel="noopener">https://blog.csdn.net/yirenboy/article/details/47441465</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 小结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5.7</title>
      <link href="/daydoc/2020/05/07/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E6%9C%80%E8%BF%91/5.7/"/>
      <url>/daydoc/2020/05/07/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/%E6%9C%80%E8%BF%91/5.7/</url>
      
        <content type="html"><![CDATA[<p>Q: git 本地与远程分支建立联系</p>]]></content>
      
      
      
        <tags>
            
            <tag> spark es kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>es常用命令</title>
      <link href="/daydoc/2020/02/25/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/es/es%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/daydoc/2020/02/25/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/es/es%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<p>此处是简介</p><a id="more"></a><p>查看集群是否健康</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;_cluster&#x2F;healt</span><br></pre></td></tr></table></figure><p>查看节点列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;_cat&#x2F;nodes?v</span><br><span class="line"></span><br><span class="line">加v将表头显示出来</span><br></pre></td></tr></table></figure><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><h3 id="查询所有索引"><a href="#查询所有索引" class="headerlink" title="查询所有索引"></a>查询所有索引</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;_cat&#x2F;indices?v</span><br></pre></td></tr></table></figure><h3 id="查看某个索引的映射"><a href="#查看某个索引的映射" class="headerlink" title="查看某个索引的映射"></a>查看某个索引的映射</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;indeName&#x2F;mapping</span><br></pre></td></tr></table></figure><h3 id="查看某个索引的设置"><a href="#查看某个索引的设置" class="headerlink" title="查看某个索引的设置"></a>查看某个索引的设置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;indeName&#x2F;mapping</span><br></pre></td></tr></table></figure><h3 id="添加一个索引"><a href="#添加一个索引" class="headerlink" title="添加一个索引"></a>添加一个索引</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">PUT &#x2F;indexName</span><br><span class="line">&#123;</span><br><span class="line">  &quot;settings&quot;: &#123;</span><br><span class="line">     &quot;number_of_shards&quot;: 3,</span><br><span class="line">     &quot;number_of_replicas&quot;: 1</span><br><span class="line">   &#125;,</span><br><span class="line">   &quot;mappings&quot;: &#123;</span><br><span class="line">     &quot;man&quot;: &#123;</span><br><span class="line">       &quot;dynamic&quot;: &quot;strict&quot;,</span><br><span class="line">       &quot;properties&quot;: &#123;</span><br><span class="line">         &quot;name&quot;: &#123;</span><br><span class="line">           &quot;type&quot;: &quot;text&quot;</span><br><span class="line">         &#125;,</span><br><span class="line">         &quot;age&quot;: &#123;</span><br><span class="line">           &quot;type&quot;: &quot;integer&quot;</span><br><span class="line">         &#125;,</span><br><span class="line">         &quot;birthday&quot;: &#123;</span><br><span class="line">           &quot;type&quot;: &quot;date&quot;,</span><br><span class="line">           &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot;</span><br><span class="line">         &#125;,</span><br><span class="line">         &quot;address&quot;:&#123;</span><br><span class="line">           &quot;dynamic&quot;: &quot;true&quot;,</span><br><span class="line">           &quot;type&quot;: &quot;object&quot;</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>dynamic关键词说明:<br>“dynamic”:”strict”  表示如果遇到陌生field会报错<br>“dynamic”: true  表示如果遇到陌生字段，就进行dynamic mapping<br>“dynamic”: “false”   表示如果遇到陌生字段，就忽略</p><hr><h3 id="删除索引"><a href="#删除索引" class="headerlink" title="删除索引"></a>删除索引</h3><p>删除单个索引</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">DELETE &#x2F;indexName</span><br></pre></td></tr></table></figure><p>删除多个</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">DELETE &#x2F;indexName1,indexName2</span><br></pre></td></tr></table></figure><h3 id="添加字段映射"><a href="#添加字段映射" class="headerlink" title="添加字段映射"></a>添加字段映射</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">PUT &#x2F;indexName&#x2F;_mapping&#x2F;Field</span><br><span class="line">&#123;</span><br><span class="line">  &quot;properties&quot;:&#123;</span><br><span class="line">    &quot;Field&quot;:&#123;</span><br><span class="line">      &quot;type&quot;:&quot;text&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="索引的别名"><a href="#索引的别名" class="headerlink" title="索引的别名"></a>索引的别名</h3><h4 id="创建索引别名"><a href="#创建索引别名" class="headerlink" title="创建索引别名"></a>创建索引别名</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PUT &#x2F;indeName&#x2F;_alias&#x2F;aliasName</span><br></pre></td></tr></table></figure><p>获取索引别名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;indexName&#x2F;_alias&#x2F;*</span><br></pre></td></tr></table></figure><p>查询别名对应的索引</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;*&#x2F;_alias&#x2F;aliasName</span><br></pre></td></tr></table></figure><h2 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h2><h3 id="向索引中添加文档"><a href="#向索引中添加文档" class="headerlink" title="向索引中添加文档"></a>向索引中添加文档</h3><ol><li>自定义ID</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PUT &#x2F;indexName&#x2F;type&#x2F;id</span><br><span class="line">&#123;</span><br><span class="line">  &quot;Field1&quot;:&quot;message&quot;,</span><br><span class="line">  &quot;Field2&quot;:&quot;message&quot;,</span><br><span class="line">  &quot;Field3&quot;:&quot;message&quot;,</span><br><span class="line">  &quot;Field4&quot;:&quot;message&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>随机生成id</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">POST &#x2F;indexName&#x2F;type</span><br><span class="line">&#123;</span><br><span class="line">  &quot;Field1&quot;:&quot;message&quot;,</span><br><span class="line">  &quot;Field2&quot;:&quot;message&quot;,</span><br><span class="line">  &quot;Field3&quot;:&quot;message&quot;,</span><br><span class="line">  &quot;Field4&quot;:&quot;message&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>后者则会自动生成id字符串</p><ol start="3"><li>修改文档</li></ol><p>全文修改,即所有字段信息都要修改</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PUT &#x2F;indexName&#x2F;type&#x2F;id</span><br><span class="line">&#123;</span><br><span class="line">  &quot;Field1&quot;:&quot;update message&quot;,</span><br><span class="line">  &quot;Field2&quot;:&quot;update message&quot;,</span><br><span class="line">  &quot;Field3&quot;:&quot;message&quot;,</span><br><span class="line">  &quot;Field4&quot;:&quot;message&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>部份修改</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">POST &#x2F;indexName&#x2F;type&#x2F;id&#x2F;_update</span><br><span class="line">&#123;</span><br><span class="line">  &quot;doc&quot;: &#123;</span><br><span class="line">    &quot;FIELD&quot;: &quot;message&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>脚本(再深入)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">POST &#x2F;indexName&#x2F;type&#x2F;_id&#x2F;_update</span><br><span class="line">&#123;</span><br><span class="line">  &quot;script&quot;: &#123;</span><br><span class="line">    &quot;lang&quot;: &quot;painless&quot;,</span><br><span class="line">    &quot;source&quot;: &quot;ctx._source.age +&#x3D; 10&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在修改document的时候，如果该文档不存在，则使用upsert操作进行初始化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">POST people&#x2F;man&#x2F;1&#x2F;_update</span><br><span class="line">&#123;</span><br><span class="line">  &quot;script&quot;: &quot;ctx._source.age +&#x3D; 10&quot;,</span><br><span class="line">  &quot;upsert&quot;: &#123;</span><br><span class="line">    &quot;age&quot;: 20</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="删除文档"><a href="#删除文档" class="headerlink" title="删除文档"></a>删除文档</h3><p>删除单个文档</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DELETE &#x2F;indexName&#x2F;type&#x2F;id</span><br></pre></td></tr></table></figure><p>删除type下所有的文档</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">POST &#x2F;indexName&#x2F;type&#x2F;_delete_by_query?conflicts&#x3D;proceed</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match_all&quot;:&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="查询文档"><a href="#查询文档" class="headerlink" title="查询文档"></a>查询文档</h3><p>查询单个文档</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;indexName&#x2F;type&#x2F;id</span><br></pre></td></tr></table></figure><p>批量查询文档(待验证)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;_mget</span><br><span class="line">&#123;</span><br><span class="line">  &quot;docs&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;_index&quot;: &quot;index1&quot;,</span><br><span class="line">        &quot;_type&quot;: &quot;type&quot;,</span><br><span class="line">        &quot;_id&quot;: 1</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;_index&quot;: &quot;index2&quot;,</span><br><span class="line">        &quot;_type&quot;: &quot;type&quot;,</span><br><span class="line">        &quot;_id&quot;: 2</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">GET &#x2F;indexName&#x2F;type&#x2F;_mget</span><br><span class="line">&#123;</span><br><span class="line">&quot;docs&quot;:[</span><br><span class="line">&#123;</span><br><span class="line">  &quot;FEILD&quot;:&quot;value&quot;</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">  &quot;FEILD2&quot;:&quot;value&quot;</span><br><span class="line">&#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查询所有文档</p><p>简单查询</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;indexName&#x2F;_serach</span><br></pre></td></tr></table></figure><p>法二</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">POST &#x2F;people&#x2F;_serach</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match_all&quot;:&#123;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查询某些字段内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">后面跟了 ?_source&#x3D;field1,field2</span><br><span class="line"></span><br><span class="line">POST &#x2F;people&#x2F;_serach?_source&#x3D;field1,field2</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match_all&quot;:&#123;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查询多个索引下的多个type</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;index1,index2&#x2F;type1,type2&#x2F;_search</span><br></pre></td></tr></table></figure><p>查询所有索引下的部分type</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;_all&#x2F;type1,type2&#x2F;_search</span><br></pre></td></tr></table></figure><h3 id="模糊查询"><a href="#模糊查询" class="headerlink" title="模糊查询"></a>模糊查询</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">POST &#x2F;indexName&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match&quot;:&#123;</span><br><span class="line">      &quot;field&quot;:&quot;message&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ,</span><br><span class="line">  &quot;sort&quot;:[</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;filed&quot;:&#123;&quot;order&quot;:&quot;desc&quot;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>注意</strong><br>message将会被拆分进行匹配,若message是中文,则会按切分后的每个字来匹配,若<br>message是英语,则会是按每个单词来匹配</p><p><img src="http://img.wqkenqing.ren/51ccc5cad8b8fbbbb6ef55d3106bfe43.png" alt=""><br><img src="http://img.wqkenqing.ren/47a3ffe716f0026c004b155836a56641.png" alt="示列图"></p><p>全文搜索(按准度)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">GET indexName&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match&quot;:&#123;</span><br><span class="line">      &quot;Field&quot;:&#123;</span><br><span class="line">        &quot;query&quot;:&quot;val1 val2&quot;,</span><br><span class="line">        &quot;operator&quot;:&quot;and&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>即Fileld 中必须有val1,val2</p><p>按匹配度查询</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;indexName&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">&quot;query&quot;:&#123;</span><br><span class="line">  &quot;match&quot;:&#123;</span><br><span class="line">    &quot;Field&quot;:&#123;</span><br><span class="line">      &quot;query&quot;:&quot;val1 val2 val3&quot;</span><br><span class="line">      &quot;minimum_should_match&quot;:&quot;val&quot; eg:20%</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>即indexName,按Field中 val1 val2 val3 匹配度达到val即返回查询</p><h3 id="高级查询"><a href="#高级查询" class="headerlink" title="高级查询"></a>高级查询</h3><p>简单精准查询</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;indexName&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match_phrase&quot;:&#123;</span><br><span class="line">      &quot;Field&quot;:&quot;val&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>即查询要完全匹配val,但若val只有一个中文,则会Field只要含有val,就会被查出</p><p>slop结合</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;people&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;match_phrase&quot;: &#123;</span><br><span class="line">      &quot;name&quot;: &#123;</span><br><span class="line">        &quot;query&quot;: &quot;张三&quot;,</span><br><span class="line">        &quot;slop&quot;: 3</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>解读：slop是移动次数，上面案例表示“张”、“三”两个字可以经过最多挪动3次查询到！</p><p>rescore (重打分）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;forum&#x2F;article&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;match&quot;: &#123;</span><br><span class="line">      &quot;content&quot;: &quot;java spark&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;rescore&quot;:&#123;</span><br><span class="line">    &quot;window_size&quot;: 50,</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">      &quot;rescore_query&quot;: &#123;</span><br><span class="line">        &quot;match_phrase&quot;: &#123;</span><br><span class="line">          &quot;content&quot;: &#123;</span><br><span class="line">            &quot;query&quot;: &quot;java spark&quot;,</span><br><span class="line">            &quot;slop&quot;: 50</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>多字段匹配查询</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;indexName&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;multi_match&quot;:&#123;</span><br><span class="line">      &quot;query&quot;:&quot;val&quot;</span><br><span class="line">      &quot;fields&quot;:[&quot;val1&quot;,&quot;val2&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在多个字段中,也是模糊查询val</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;people&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;query_string&quot;: &#123;</span><br><span class="line">      &quot;query&quot;: &quot;(叶良辰 AND 火) OR (赵日天 AND 风)&quot;,</span><br><span class="line">      &quot;fields&quot;: [&quot;name&quot;,&quot;desc&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="字段查询"><a href="#字段查询" class="headerlink" title="字段查询"></a>字段查询</h3><p>精准查询</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;indexName&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">&quot;query&quot;:&#123;</span><br><span class="line">  &quot;term&quot;:&#123;</span><br><span class="line">    &quot;field&quot;:&quot;val&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>分页查询</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;indexName&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;match_all&quot;:&#123;&#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;from&quot;:num,</span><br><span class="line">  &quot;size&quot;:num</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="范围查询"><a href="#范围查询" class="headerlink" title="范围查询"></a>范围查询</h3><p>数据值型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">GET &#x2F;people&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;range&quot;: &#123;</span><br><span class="line">      &quot;age&quot;: &#123;</span><br><span class="line">        &quot;gt&quot;: 16,</span><br><span class="line">        &quot;lte&quot;: 30</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>日期类型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;people&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;range&quot;: &#123;</span><br><span class="line">      &quot;birthday&quot;: &#123;</span><br><span class="line">        &quot;gte&quot;: &quot;2013-01-01&quot;,</span><br><span class="line">        &quot;lte&quot;: &quot;now&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">GET book&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;constant_score&quot;: &#123;</span><br><span class="line">      &quot;filter&quot;: &#123;</span><br><span class="line">        &quot;range&quot;: &#123;</span><br><span class="line">          &quot;date&quot;: &#123;</span><br><span class="line">            &quot;gt&quot;: &quot;now-1M&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;boost&quot;: 1.2</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>“gt”: “now-1M”表示从今天开始，往前推一个月！</p><h3 id="过滤查询"><a href="#过滤查询" class="headerlink" title="过滤查询"></a>过滤查询</h3><p>法一</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">POST &#x2F;people&#x2F;man&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;constant_score&quot;: &#123;</span><br><span class="line">      &quot;filter&quot;: &#123;</span><br><span class="line">        &quot;range&quot;: &#123;</span><br><span class="line">          &quot;age&quot;: &#123;</span><br><span class="line">            &quot;gte&quot;: 20,</span><br><span class="line">            &quot;lte&quot;: 30</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;boost&quot;: 1.2</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>法二</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">POST &#x2F;people&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;bool&quot;: &#123;</span><br><span class="line">      &quot;filter&quot;: &#123;</span><br><span class="line">        &quot;term&quot;: &#123;</span><br><span class="line">          &quot;age&quot;: 18</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="布尔查询"><a href="#布尔查询" class="headerlink" title="布尔查询"></a>布尔查询</h3><p>should查询<br>注意：should相当于 或 ，里面的match也是模糊匹配</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">POST &#x2F;people&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;bool&quot;: &#123;</span><br><span class="line">      &quot;should&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;match&quot;: &#123;</span><br><span class="line">            &quot;name&quot;: &quot;叶良辰&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;match&quot;: &#123;</span><br><span class="line">            &quot;desc&quot;: &quot;赵日天&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>must查询<br>注意：两个条件都要满足，并且这里也会把must里面的“叶良辰”拆分成“叶”、“良”和“辰”进行查询；“赵日天”拆分成“赵”、“日”、和“天”！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">POST &#x2F;people&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;bool&quot;: &#123;</span><br><span class="line">      &quot;must&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;match&quot;: &#123;</span><br><span class="line">            &quot;name&quot;: &quot;叶良辰&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;match&quot;: &#123;</span><br><span class="line">            &quot;desc&quot;: &quot;赵日天&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>must与filter相结合<br>这里也会把must里面的“叶良辰”拆分成“叶”、“良”和“辰”进行查询</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">POST &#x2F;people&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;bool&quot;: &#123;</span><br><span class="line">      &quot;must&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;match&quot;: &#123;</span><br><span class="line">            &quot;name&quot;: &quot;叶良辰&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;match&quot;: &#123;</span><br><span class="line">            &quot;desc&quot;: &quot;赵日天&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ],</span><br><span class="line">      &quot;filter&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;term&quot;: &#123;</span><br><span class="line">            &quot;age&quot;: 18</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>must_not<br>注意：下面语句是精准匹配</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">POST &#x2F;people&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;bool&quot;: &#123;</span><br><span class="line">      &quot;must_not&quot;: &#123;</span><br><span class="line">        &quot;term&quot;: &#123;</span><br><span class="line">          &quot;name&quot;: &quot;叶良辰&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="聚合查询"><a href="#聚合查询" class="headerlink" title="聚合查询"></a>聚合查询</h3><p>根据字段类型查询</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">GET &#x2F;people&#x2F;man&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 0,</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;group_by_age&quot;: &#123;</span><br><span class="line">      &quot;terms&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;age&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查询总体值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">POST &#x2F;people&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;grads_age&quot;: &#123;</span><br><span class="line">      &quot;stats&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;age&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查询最小值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">POST &#x2F;people&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;grads_age&quot;: &#123;</span><br><span class="line">      &quot;min&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;age&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>根据国家分组，然后计算年龄平均值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">GET &#x2F;people&#x2F;man&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 0,</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;group_by_age&quot;: &#123;</span><br><span class="line">      &quot;terms&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;country&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;avg_age&quot;: &#123;</span><br><span class="line">          &quot;avg&quot;: &#123;</span><br><span class="line">            &quot;field&quot;: &quot;age&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>解决：上面的reason里面说的很清楚，将fielddata设置为true就行了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">POST &#x2F;people&#x2F;_mapping&#x2F;man</span><br><span class="line">&#123;</span><br><span class="line">  &quot;properties&quot;: &#123;</span><br><span class="line">    &quot;country&quot;: &#123;</span><br><span class="line">      &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">      &quot;fielddata&quot;: true</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="排序查询"><a href="#排序查询" class="headerlink" title="排序查询"></a>排序查询</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">排序查询通常没有排到我们想要的结果，因为字段分词后，有很多单词，再排序跟我们想要的结果又出入</span><br><span class="line"></span><br><span class="line">解决办法：把需要排序的字段建立两次索引，一个排序，另一个不排序。</span><br><span class="line"></span><br><span class="line">如下面的案例：把title.raw的fielddata设置为true，是排序的；而title的fielddata默认是false，可以用来搜索</span><br><span class="line"></span><br><span class="line">index: true 是在title.raw建立索引可以被搜索到，</span><br><span class="line"></span><br><span class="line">fielddata: true是让其可以排序</span><br><span class="line"></span><br><span class="line">PUT &#x2F;blog</span><br><span class="line">&#123;</span><br><span class="line">  &quot;mappings&quot;: &#123;</span><br><span class="line">    &quot;article&quot;: &#123;</span><br><span class="line">      &quot;properties&quot;: &#123;</span><br><span class="line">        &quot;auther&quot;: &#123;</span><br><span class="line">          &quot;type&quot;: &quot;text&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;title&quot;: &#123;</span><br><span class="line">          &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">          &quot;fields&quot;: &#123;</span><br><span class="line">            &quot;raw&quot;:&#123;</span><br><span class="line">              &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">              &quot;index&quot;: true,</span><br><span class="line">              &quot;fielddata&quot;: true</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;content&quot;:&#123;</span><br><span class="line">          &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">          &quot;analyzer&quot;: &quot;english&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;publishdate&quot;: &#123;</span><br><span class="line">          &quot;type&quot;: &quot;date&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;blog&#x2F;article&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;match_all&quot;: &#123;&#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;sort&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;title.raw&quot;: &#123;</span><br><span class="line">        &quot;order&quot;: &quot;desc&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="scroll查询"><a href="#scroll查询" class="headerlink" title="scroll查询"></a>scroll查询</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">当搜索量比较大的时候，我们在短时间内不可能一次性搜索完然后展示出来</span><br><span class="line"></span><br><span class="line">这个时候，可以使用scroll进行搜索</span><br><span class="line"></span><br><span class="line">比如下面的案例，可以先搜索3条数据，然后结果中会有一个_scroll_id，下次搜索就可以直接用这个_scroll_id进行搜索了</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">GET test_index&#x2F;test_type&#x2F;_search?scroll&#x3D;1m</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;match_all&quot;: &#123;&#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;sort&quot;: &quot;_doc&quot;,</span><br><span class="line">  &quot;size&quot;: 3</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>step3 把scroll_id粘贴到下面的命令中再次搜索</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">GET _search&#x2F;scroll</span><br><span class="line">&#123;</span><br><span class="line">  &quot;scroll&quot;: &quot;1m&quot;,</span><br><span class="line">  &quot;scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAA6FnZPSl9sbVR4UVVDU1NLb2wxVXJlbWcAAAAAAAAAPhZ2T0pfbG1UeFFVQ1NTS29sMVVyZW1nAAAAAAAAADsWdk9KX2xtVHhRVUNTU0tvbDFVcmVtZwAAAAAAAAA8FnZPSl9sbVR4UVVDU1NLb2wxVXJlbWcAAAAAAAAAPRZ2T0pfbG1UeFFVQ1NTS29sMVVyZW1n&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="运维"><a href="#运维" class="headerlink" title="运维"></a>运维</h2><h3 id="reindex"><a href="#reindex" class="headerlink" title="reindex"></a>reindex</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;activity_mini2&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;activity_mini3&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="别名"><a href="#别名" class="headerlink" title="别名"></a>别名</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">POST _aliases</span><br><span class="line">&#123;</span><br><span class="line">  &quot;actions&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;remove&quot;: &#123;</span><br><span class="line">        &quot;index&quot;: &quot;activity_mini2&quot;,</span><br><span class="line">        &quot;alias&quot;: &quot;activity_mini2_alias&quot;</span><br><span class="line">      &#125;&#125;, </span><br><span class="line">      &#123;</span><br><span class="line">      &quot;add&quot;: &#123;</span><br><span class="line">        &quot;index&quot;: &quot;activity_mini3&quot;,</span><br><span class="line">        &quot;alias&quot;: &quot;activity_mini2_alias&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="快照"><a href="#快照" class="headerlink" title="快照"></a>快照</h3><h3 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h3><p>这里先简单介绍以elasticsearch-dump为基础的同步</p><h4 id="全量同步"><a href="#全量同步" class="headerlink" title="全量同步"></a>全量同步</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">elastcisearchdump --input&#x3D;path \</span><br><span class="line">--output&#x3D;path \</span><br><span class="line">--type&#x3D;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">上面常见的type有settings,mapping,data,alias等主要内容</span><br></pre></td></tr></table></figure><h4 id="DSL-语句同步"><a href="#DSL-语句同步" class="headerlink" title="DSL 语句同步"></a>DSL 语句同步</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">elasticdump \</span><br><span class="line">  --input&#x3D;http:&#x2F;&#x2F;path1:9200&#x2F;$1 \</span><br><span class="line">  --output&#x3D;http:&#x2F;&#x2F;path2:9200&#x2F;$1 \</span><br><span class="line">  --type&#x3D;data \</span><br><span class="line">   --searchBody&#x3D;&quot;$body&quot;</span><br></pre></td></tr></table></figure><p>因为走的是网络IO所以效率上会有较大问题.适用于小批量数据同步,后续再补充较大规模数据的迁移</p><h2 id="拓展QA"><a href="#拓展QA" class="headerlink" title="拓展QA"></a>拓展QA</h2><h3 id="Q-同一个字段如何通过不同的域-feild-实现多种类型的功能"><a href="#Q-同一个字段如何通过不同的域-feild-实现多种类型的功能" class="headerlink" title="Q: 同一个字段如何通过不同的域(feild)实现多种类型的功能"></a>Q: 同一个字段如何通过不同的域(feild)实现多种类型的功能</h3><p>A: 如一个字段被定义成了text,但同时这个字段的内容也需要keyword的不分词的特性.那些通过配置fields 可实现该功能.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&quot;title&quot;: &#123;</span><br><span class="line">          &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">          &quot;fields&quot;: &#123;</span><br><span class="line">            &quot;keyword&quot;: &#123;</span><br><span class="line">              &quot;type&quot;: &quot;keyword&quot;,</span><br><span class="line">              &quot;ignore_above&quot;: 256</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br></pre></td></tr></table></figure><p>如上查询DSL配成 title.keyword时,则不可分词,查询出来的内容是唯一结果.</p><p>如果查询DSL配成titile时.则会分词</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ELK使用</title>
      <link href="/daydoc/2020/02/25/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/es/ELK/ELK/"/>
      <url>/daydoc/2020/02/25/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/es/ELK/ELK/</url>
      
        <content type="html"><![CDATA[<p>针对ELK使用</p><a id="more"></a><h1 id="ELK-使用"><a href="#ELK-使用" class="headerlink" title="ELK 使用"></a>ELK 使用</h1><h2 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h2><p>elasticsearch之前已经另开篇章记录,这里略过</p><h2 id="Logstash"><a href="#Logstash" class="headerlink" title="Logstash"></a>Logstash</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一款轻量级的日志搜索处理框架,可以把分散,多样化的日志搜集起来,并通过配置发送到指定的位置</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Beats</title>
      <link href="/daydoc/2020/02/25/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/es/ELK/Beats/"/>
      <url>/daydoc/2020/02/25/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/es/ELK/Beats/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">本文拟用的beats版本是6.7</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="Beats"><a href="#Beats" class="headerlink" title="Beats"></a>Beats</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beats 是一款开源的轻量级采集组件,用于采集流转的数据.</span><br></pre></td></tr></table></figure><p>常见的beats类型有</p><ul><li><a href="https://www.elastic.co/products/beats/auditbeat" target="_blank" rel="noopener"> Auditbeat</a></li><li><a href="https://www.elastic.co/products/beats/filebeat" target="_blank" rel="noopener"> Filebeat</a></li><li><a href="https://www.elastic.co/products/beats/functionbeat" target="_blank" rel="noopener">Functionbeat</a></li><li><a href="https://www.elastic.co/products/beats/heartbeat" target="_blank" rel="noopener"> Heartbeat</a></li><li><a href="https://www.elastic.co/downloads/beats/journalbeat" target="_blank" rel="noopener"> Journalbeat</a></li><li><a href="https://www.elastic.co/products/beats/metricbeat" target="_blank" rel="noopener">Metricbeat</a></li><li><a href="https://www.elastic.co/products/beats/packetbeat" target="_blank" rel="noopener"> Packetbeat</a></li><li><a href="https://www.elastic.co/products/beats/winlogbeat" target="_blank" rel="noopener"> Winlogbeat</a></li></ul><hr><p>beats 能直接将数据采集发送到Logstash 或elasticsearch</p><h2 id="Auditbeat"><a href="#Auditbeat" class="headerlink" title="Auditbeat"></a>Auditbeat</h2><h2 id="Filebeat"><a href="#Filebeat" class="headerlink" title="Filebeat"></a>Filebeat</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>hive2总结</title>
      <link href="/daydoc/2019/12/24/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/hive2%E6%80%BB%E7%BB%93/"/>
      <url>/daydoc/2019/12/24/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/hive2%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="Hive相关点小结"><a href="#Hive相关点小结" class="headerlink" title="Hive相关点小结"></a>Hive相关点小结</h1><a id="more"></a><h2 id="启动指令"><a href="#启动指令" class="headerlink" title="启动指令"></a>启动指令</h2><ol><li>hive ==  hive –service cli<br>不需要启动server，使用本地的metastore，可以直接做一些简单的数据操作和测试。</li><li>启动hiveserver2<br>hive –service hiveserver2</li><li>beeline工具测试使用jdbc方式连接<br>beeline -u jdbc:hive2://localhost:10000</li></ol><p>1.managed table<br>管理表。<br>删除表时，数据也删除了</p><p>2.external table<br>外部表。<br>删除表时，数据不删</p><h2 id="建表"><a href="#建表" class="headerlink" title="建表:"></a>建表:</h2><p>CREATE TABLE IF NOT EXISTS t2(id int,name string,age int)<br>COMMENT ‘xx’                                     //注释<br>ROW FORMAT DELIMITED                             //行分隔符<br>FIELDS TERMINATED BY ‘,’                         //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改<br>STORED AS TEXTFILE ;</p><h3 id="外部表"><a href="#外部表" class="headerlink" title="外部表:"></a>外部表:</h3><p> CREATE  TABLE IF NOT EXISTS t2(id int,name string,age int)<br> COMMENT ‘xx’<br> ROW FORMAT DELIMITED<br> FIELDS TERMINATED BY ‘,’<br> STORED AS TEXTFILE ; </p><h3 id="分区表，桶表"><a href="#分区表，桶表" class="headerlink" title="分区表，桶表"></a>分区表，桶表</h3><h4 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h4><p>Hive中有分区表的概念。我们可以看到分区表具有重要的性能，而且分区表还可以将数据以一种符合逻辑的方式进行组织，比如分层存储。Hive的分区表，是把数据放在满足条件的分区目录下<br>CREATE TABLE t3(id int,name string,age int) </p><p>PARTITIONED BY (Year INT, Month INT)   //按照年月进行分区</p><p> ROW FORMAT DELIMITED                      //行分隔符</p><p>FIELDS TERMINATED BY ‘,’ ;                    //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改<br>load data local inpath ‘/home/zpx/customers.txt’ into table t3 partition</p><h4 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h4><p>这样做，在查找数据的时候就可以跨越多个桶，直接查找复合条件的数据了。速度快，时间成本低。Hive中的桶表默认使用的机制也是hash。<br>CREATE TABLE t4(id int,name string,age int) </p><pre><code>CLUSTERED BY (id) INTO 3 BUCKETS      //创建3个通桶表，按照字段id进行分桶ROW FORMAT DELIMITED                     //行分隔符FIELDS TERMINATED BY &apos;,&apos; ; </code></pre><p>load data local inpath ‘/home/centos/customers.txt’ into table t4 ;</p><h2 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h2><p>load data local inpath ‘/home/zpx/customers.txt’ into table t2 ; //local上传文件<br>load data inpath ‘/user/zpx/customers.txt’ [overwrite] into table t2 //分布式文件系统上移动文件</p><h2 id="建视图"><a href="#建视图" class="headerlink" title="建视图"></a>建视图</h2><p>Hive也可以建立视图，是一张虚表，方便我们进行操作.</p><p>create view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ;</p><h2 id="Hive的严格模式"><a href="#Hive的严格模式" class="headerlink" title="Hive的严格模式"></a>Hive的严格模式</h2><p>Hive提供了一个严格模式，可以防止用户执行那些产生意想不到的不好的影响的查询。<br>使用了严格模式之后主要对以下3种不良操作进行控制：</p><p>1.分区表必须指定分区进行查询。<br>2.order by时必须使用limit子句。<br>3.不允许笛卡尔积。<br><img src="http://img.wqkenqing.ren/2019-03-18-17-13-36.png" alt="2019-03-18-17-13-36"></p><h2 id="Hive的动态分区"><a href="#Hive的动态分区" class="headerlink" title="Hive的动态分区"></a>Hive的动态分区</h2><p>像分区表里面存储了数据。我们在进行存储数据的时候，都是明确的指定了分区。在这个过程中Hive也提供了一种比较任性化的操作，就是动态分区，不需要我们指定分区目录，Hive能够把数据进行动态的分发,<strong>我们需要将当前的严格模式设置成非严格模式，否则不允许使用动态分区</strong><br>set hive.exec.dynamic.partition.mode=nonstrict//设置非严格模式</p><h2 id="Hive的排序"><a href="#Hive的排序" class="headerlink" title="Hive的排序"></a>Hive的排序</h2><p>Hive也提供了一些排序的语法，包括order by,sort by。</p><p>order by=MapReduce的全排序<br>sort by=MapReduce的部分排序<br>distribute by=MapReduce的分区</p><p>selece …….from …… order by 字段；//按照这个字段全排序</p><p>selece …….from …… sort by 字段； //按照这个字段局部有序</p><p>selece 字段…..from …… distribute by 字段；//按照这个字段分区<br>特别注意的是：</p><ol><li>在上面的最后一个distribute by使用过程中，按照排序的字段要出现在最左侧也就是select中有这个字段，因为我们要告诉MapReduce你要按照哪一个字段分区，当然获取的数据中要出现这个字段了。类似于我们使用group by的用法，字段也必须出现在最左侧，因为数据要包含这个字段，才能按照这个字段分组，至于Hive什么时候会自行的开启MapReduce，那就是在使用聚合的情况下开启，使用select …from ….以及使用分区表的selece ….from……where …..不会开启</li><li>distribute by与sort by可以组合使用，但是distribute by要放在前边，因为MapReduce要先分区，后排序，再归并</li></ol><p>select 字段a,……..from …….distribute by字段a，sort by字段<br>如果distribute by与sort by使用的字段一样，则可以使用cluster by 字段替代：<br>select 字段a,……..from …….cluster by 字段</p><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><ol><li>show functions; 展示相关函数</li><li>desc function split;</li><li>desc function  extended split;  //查看函数的扩展信息</li></ol><h3 id="用户自定义函数（UDF）"><a href="#用户自定义函数（UDF）" class="headerlink" title="用户自定义函数（UDF）"></a>用户自定义函数（UDF）</h3><p>具体步骤如下：</p><p>（1）.自定义类（继承UDF，或是GenericUDF。GenericUDF是更为复杂的抽象概念，但是其支持更好的null值处理同时还可以处理一些标准的UDF无法支持的编程操作）。<br>（2）.导出jar包，通过命令添加到hive的类路径。<br>$hive&gt;add jar xxx.jar<br>（3）.注册函数<br>$hive&gt;CREATE TEMPORARY FUNCTION 函数名 AS ‘具体类路径：包.类’;<br>（4）.使用<br> $hive&gt;select 函数名(参数);<br>自定义实现类如下(继承UDF)：</p>]]></content>
      
      
      
        <tags>
            
            <tag> bigdata </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink学习</title>
      <link href="/daydoc/2019/07/31/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/flink/Flink/"/>
      <url>/daydoc/2019/07/31/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/flink/Flink/</url>
      
        <content type="html"><![CDATA[<p>flink内容记录</p><a id="more"></a><h2 id="搭建"><a href="#搭建" class="headerlink" title="搭建"></a>搭建</h2><h2 id="创建maven项目"><a href="#创建maven项目" class="headerlink" title="创建maven项目"></a>创建maven项目</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mvn archetype:generate \</span><br><span class="line">    -DarchetypeGroupId&#x3D;org.apache.flink \</span><br><span class="line">    -DarchetypeArtifactId&#x3D;flink-quickstart-java \</span><br><span class="line">    -DarchetypeVersion&#x3D;1.6.1 \</span><br><span class="line">    -DgroupId&#x3D;my-flink-project \</span><br><span class="line">    -DartifactId&#x3D;my-flink-project \</span><br><span class="line">    -Dversion&#x3D;0.1 \</span><br><span class="line">    -Dpackage&#x3D;myflink \</span><br><span class="line">    -DinteractiveMode&#x3D;false</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mvn clean package -Dmaven.test.skip&#x3D;true</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink run -c myflink.demo.SocketTextStreamWordCount my-flink-project-0.1.jar 127.0.0.1 9000</span><br></pre></td></tr></table></figure><h2 id="DataStream-API"><a href="#DataStream-API" class="headerlink" title="DataStream API"></a>DataStream API</h2><p>flink程序工作解剖图<br><img src="http://img.wqkenqing.ren/FIyXNI.png" alt=""></p><h3 id="执行环境"><a href="#执行环境" class="headerlink" title="执行环境"></a>执行环境</h3><p>flink支持</p><ul><li>获取已经存在的flink环境</li><li>创建一个本地环境</li><li>创建一个远程环境</li></ul><h3 id="DataSource"><a href="#DataSource" class="headerlink" title="DataSource"></a>DataSource</h3><h4 id="预置source"><a href="#预置source" class="headerlink" title="预置source"></a>预置source</h4><p>Socket-based</p><ul><li>socketTextStream();</li></ul><p>File-based</p><h3 id="Transfomations"><a href="#Transfomations" class="headerlink" title="Transfomations"></a>Transfomations</h3><ul><li>map</li><li>flatMap</li><li>filter</li><li>keyBy</li><li>reduce</li><li>fold</li></ul><p>合计</p><ul><li>min</li><li>max</li><li>sum</li></ul><p>窗口</p>]]></content>
      
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hdfs命令</title>
      <link href="/daydoc/2019/07/17/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/hadoop/hdfs/hdfs%E5%91%BD%E4%BB%A4/"/>
      <url>/daydoc/2019/07/17/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/hadoop/hdfs/hdfs%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<p>hdfs常用命令</p><a id="more"></a><p>count</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">该命令选项显示指定路径下的文件夹数量、文件数量、文件总大小信息，如图4-6所示。</span><br></pre></td></tr></table></figure><p><img src="http://img.wqkenqing.ren/QFvpXA.png" alt=""></p><p>du</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">统计目录下各文件大小</span><br></pre></td></tr></table></figure><p>touchz</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建空白文件</span><br></pre></td></tr></table></figure><p>-stat</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">“%b %n %o %r %Y”依次表示文件大小、文件名称、块大小、副本数、访问时间。</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> hdfs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka小结</title>
      <link href="/daydoc/2019/07/16/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/oldblog/blog17/"/>
      <url>/daydoc/2019/07/16/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/oldblog/blog17/</url>
      
        <content type="html"><![CDATA[<p>此处简介</p><a id="more"></a><h2 id="kafka小结"><a href="#kafka小结" class="headerlink" title="kafka小结"></a>kafka小结</h2><p>消息系统术语<br>kafka特性</p><ul><li>分布式的</li><li>可分区的</li><li>可复制的</li></ul><p>在普通的消息系统的功上，还有自己独特的设计</p><p>Kafka将消息以topic为单位进行归纳。<br>将向Kafka topic发布消息的程序成为producers.<br>将预订topics并消费消息的程序成为consumer.<br>Kafka以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个broker.<br>producers通过网络将消息发送到Kafka集群，集群向消费者提供消息，</p><p><img src="//img.wqkenqing.ren/file/2017/7/827fdc820cae4619859042761c3b40a9-image.png" alt="827fdc820cae4619859042761c3b40a9-image.png"></p><p>客户端和服务端通过TCP协议通信。Kafka提供了Java客户端，并且对多种语言都提供了支持。</p><hr><p>Topics 和Logs</p><p>先来看一下Kafka提供的一个抽象概念:topic.<br>一个topic是对一组消息的归纳。对每个topic，Kafka 对它的日志进行了分区<br><img src="//img.wqkenqing.ren/file/2017/7/bf0d2ddee1d14cb29fd54483a622d67c-image.png" alt="bf0d2ddee1d14cb29fd54483a622d67c-image.png"></p><p>一个topic是对一组消息的归纳。<br>对每个topic，Kafka 对它的日志进行了分区，</p><p>每个分区都由一系列有序的、不可变的消息组成，这些消息被连续的追加到分区中。分区中的每个消息都有一个连续的序列号叫做offset,用来在分区中唯一的标识这个消息。</p><hr><h3 id="kafka常用指令收集"><a href="#kafka常用指令收集" class="headerlink" title="kafka常用指令收集"></a>kafka常用指令收集</h3><p><strong>查看topic的详细信息</strong><br>kafka-topics.sh -zookeeper 127.0.0.1:2181 -describe -topic topic name</p><p><strong>为topic增加副本</strong><br>kafka-reassign-partitions.sh -zookeeper 127.0.0.1:2181 -reassignment-json-file json/partitions-to-move.json -execute<br><strong>创建topic</strong><br>kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic name<br><strong>为topic增加partition</strong><br>kafka-topics.sh –zookeeper 127.0.0.1:2181 –alter –partitions 20 –topic name<br><strong>kafka生产者客户端命令</strong><br>kafka-console-producer.sh –broker-list localhost:9092 –topic name<br><strong>kafka消费者客户端命令</strong><br>kafka-console-consumer.sh -zookeeper localhost:2181 –from-beginning –topic name<br><strong>kafka服务启动</strong><br>kafka-server-start.sh -daemon ../config/server.properties<br><strong>删除topic</strong><br>kafka-run-class.sh kafka.admin.DeleteTopicCommand –topic testKJ1 –zookeeper 127.0.0.1:2181<br>kafka-topics.sh –zookeeper localhost:2181 –delete –topic testKJ1<br><strong>查看consumer组内消费的offset</strong><br>kafka-run-class.sh kafka.tools.ConsumerOffsetChecker –zookeeper localhost:2181 –group test –topic name</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>sparkstreaming 窗口操作</title>
      <link href="/daydoc/2019/07/16/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/spark/stream2/"/>
      <url>/daydoc/2019/07/16/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/spark/stream2/</url>
      
        <content type="html"><![CDATA[<p>sparkstreaming时间窗口设置</p><a id="more"></a><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>通过sparkstreaming设置窗口函数,可达到如,每10秒计算前30秒内数据的效果</p><p>如上 主要有两个参数</p><ol><li>窗口大小</li><li>滑动距离</li></ol><p>val windowedWordCounts = pairs.reduceByKeyAndWindow(_ + _, Seconds(30), Seconds(10))</p><p>如上</p><h2 id="常用api"><a href="#常用api" class="headerlink" title="常用api"></a>常用api</h2><table><thead><tr><th align="left">Transformation</th><th align="center">Meaning</th></tr></thead><tbody><tr><td align="left">window(windowLength, slideInterval)</td><td align="center">Return a new DStream which is computed based on windowed batches of the source DStream.</td></tr><tr><td align="left">countByWindow(windowLength,slideInterval)</td><td align="center">Return a sliding window count of elements in the stream.</td></tr><tr><td align="left">reduceByWindow(func, windowLength,slideInterval)</td><td align="center"></td></tr><tr><td align="left">reduceByKeyAndWindow(func,windowLength, slideInterval, [numTasks])</td><td align="center"></td></tr><tr><td align="left">reduceByKeyAndWindow(func, invFunc,windowLength, slideInterval, [numTasks])</td><td align="center"></td></tr><tr><td align="left">countByValueAndWindow(windowLength,slideInterval, [numTasks])</td><td align="center"></td></tr></tbody></table>]]></content>
      
      
      
        <tags>
            
            <tag> sparkstreaming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>宽窄依赖</title>
      <link href="/daydoc/2019/07/16/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/spark/%E5%AE%BD%E7%AA%84%E4%BE%9D%E8%B5%96/"/>
      <url>/daydoc/2019/07/16/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/spark/%E5%AE%BD%E7%AA%84%E4%BE%9D%E8%B5%96/</url>
      
        <content type="html"><![CDATA[<p>spark依赖说明</p><a id="more"></a><h2 id="种类"><a href="#种类" class="headerlink" title="种类"></a>种类</h2><p>spark的依赖关系大致有两类</p><ul><li>narrow dependency</li><li>wide dependency</li></ul><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><h3 id="narrow-dependency"><a href="#narrow-dependency" class="headerlink" title="narrow dependency"></a>narrow dependency</h3><p>父Partition ===&gt; 子partition 多对一或一对一   flatMap ,mapToPair ,map ,filter等算子<br>父partition ===&gt; 子partition 一对多          reduce ,group by 等.</p><h3 id="stage"><a href="#stage" class="headerlink" title="stage"></a>stage</h3><p>当一个dag串联遇到宽依赖时形成stage.一个stage对应一个task.这个task的并行度由最后一个依赖决定.应该就是说由wide dependency 的具体并行度决度.如<br>reduce ,partition=3.就3的并行度.这里的参数可以设置.</p><p>wide dependency 必定对应的有shuffle.但shuffle不一定是wide dependency  如sort orderby</p><p>join 即可能发生shuffle也可能不,具体看情况.</p><h3 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h3><p>一个stage划分好后.一条数据的具体运算逻辑是会一直走完所有计算逻辑后才会落地.这是与mapreduce的区别<br>mapreduce是计算逻辑走完落地,再启动,计算又落地.</p><p>所以说spark的效率比mapreduce高也是有这个原因.dag串联后,运算优先.</p>]]></content>
      
      
      
        <tags>
            
            <tag> spark dependency </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>updateStateByKey&amp;mapStateWithKey</title>
      <link href="/daydoc/2019/07/15/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/spark/stream/"/>
      <url>/daydoc/2019/07/15/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/spark/stream/</url>
      
        <content type="html"><![CDATA[<p>spark中如何实现全局count</p><a id="more"></a><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>两种方式都可以实现对同一key的累计统计</p><p>区别<br>updateStateByKey会返回无增量数据的状态,所以会相对较大的数据资源开销<br>mapStateWithKey 相当于增量统计</p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>updateStateByKey :</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt; updateFunctionByUpdate() &#123;</span><br><span class="line">       Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt; updateFunction = (values, s1) -&gt; &#123;</span><br><span class="line">           Integer newSum = <span class="number">0</span>;</span><br><span class="line">           <span class="keyword">if</span> (s1.isPresent()) &#123;</span><br><span class="line">               newSum = s1.get();</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           Iterator&lt;Integer&gt; i = values.iterator();</span><br><span class="line">           <span class="keyword">while</span> (i.hasNext()) &#123;</span><br><span class="line">               newSum += i.next();</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">return</span> Optional.of(newSum);</span><br><span class="line">       &#125;;</span><br><span class="line">       <span class="keyword">return</span> updateFunction;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>mapStateWithKey :</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Function3&lt;String, Optional&lt;Integer&gt;, State&lt;Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; updateFunctionByMap() &#123;</span><br><span class="line">       Function3&lt;String, Optional&lt;Integer&gt;, State&lt;Integer&gt;, Tuple2&lt;String, Integer&gt;&gt; updateFunction2 = (word, one,</span><br><span class="line">                                                                                                        state) -&gt; &#123;</span><br><span class="line">           <span class="keyword">int</span> sum = one.or(<span class="number">0</span>) + (state.exists() ? state.get() : <span class="number">0</span>);</span><br><span class="line">           Tuple2&lt;String, Integer&gt; output = <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word, sum);</span><br><span class="line">           state.update(sum);</span><br><span class="line">           <span class="keyword">return</span> output;</span><br><span class="line">       &#125;;</span><br><span class="line">       <span class="keyword">return</span> updateFunction2;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> sparkstream </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Yarn配置细节</title>
      <link href="/daydoc/2019/06/13/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/Yarn%E9%85%8D%E7%BD%AE/"/>
      <url>/daydoc/2019/06/13/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/Yarn%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<p>此处简介</p><a id="more"></a><h1 id="Yarn配置细节"><a href="#Yarn配置细节" class="headerlink" title="Yarn配置细节"></a>Yarn配置细节</h1><p>##内存,核数设置</p><!-- more --><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">      &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;4096&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;1024&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;3072&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;!--该配置用于配置任务请求时的资源. --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;2048&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;-Xmx3276m&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;2&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>flume记录</title>
      <link href="/daydoc/2019/06/13/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/flume%E8%AE%B0%E5%BD%95/"/>
      <url>/daydoc/2019/06/13/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/flume%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<p>此处简介</p><a id="more"></a><h1 id="flume记录"><a href="#flume记录" class="headerlink" title="flume记录"></a>flume记录</h1><h2 id="from-kafka"><a href="#from-kafka" class="headerlink" title="from kafka"></a>from kafka</h2><!-- more --><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">a1.sources &#x3D; source1</span><br><span class="line"></span><br><span class="line">a1.sources.source1.type &#x3D; org.apache.flume.source.kafka.KafkaSource</span><br><span class="line"></span><br><span class="line">a1.sources.source1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line">a1.sources.source1.batchSize &#x3D; 5000</span><br><span class="line"></span><br><span class="line">a1.sources.source1.batchDurationMillis &#x3D; 2000</span><br><span class="line">a1.sources.source1.zookeeperConnect &#x3D; localhost:2181</span><br><span class="line"></span><br><span class="line">#a1.sources.source1.kafka.brokerList &#x3D; localhost:9092</span><br><span class="line">a1.sources.source1.kafka.bootstrap.servers &#x3D; localhost:9092</span><br><span class="line">a1.sources.source1.topic &#x3D; flumetest</span><br><span class="line">a1.sources.source1.kafka.consumer.group.id &#x3D; custom.g.id</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line"></span><br><span class="line">a1.channels.c1.capacity &#x3D; 10000</span><br><span class="line"></span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 10000</span><br><span class="line"></span><br><span class="line">a1.channels.c1.byteCapacityBufferPercentage &#x3D; 20</span><br><span class="line"></span><br><span class="line">a1.channels.c1.byteCapacity &#x3D; 800000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type &#x3D; file_roll</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.sink.directory &#x3D; &#x2F;home&#x2F;hadoop&#x2F;testfile&#x2F;flume</span><br></pre></td></tr></table></figure><p>这里也有版本匹配的问题.经过多番尝试,这里的组合版本是flume1.6+kafka_2.11-2.2.0.tgz<br>其它版本可能会有request header 问题.<br>另外还遇到了指定topic 和 zookeeper的问题.</p><p>执行语句:flume-ng agent -n a1 -c conf -f kafka.properties -Dflume.root.logger=INFO,console</p><h2 id="flume-采集到kafka"><a href="#flume-采集到kafka" class="headerlink" title="flume 采集到kafka"></a>flume 采集到kafka</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">agent.sources&#x3D;r1</span><br><span class="line">agent.sinks&#x3D;k1</span><br><span class="line">agent.channels&#x3D;c1</span><br><span class="line"></span><br><span class="line">agent.sources.r1.type&#x3D;exec</span><br><span class="line">agent.sources.r1.command&#x3D;tail &#x2F;root&#x2F;tomcat&#x2F;logs&#x2F;catalina.out</span><br><span class="line">agent.sources.r1.restart&#x3D;true</span><br><span class="line">agent.sources.r1.batchSize&#x3D;1000</span><br><span class="line">agent.sources.r1.batchTimeout&#x3D;3000</span><br><span class="line">agent.sources.r1.channels&#x3D;c1</span><br><span class="line"></span><br><span class="line">agent.channels.c1.type&#x3D;memory</span><br><span class="line">agent.channels.c1.capacity&#x3D;102400</span><br><span class="line">agent.channels.c1.transactionCapacity&#x3D;1000</span><br><span class="line"></span><br><span class="line">agent.channels.c1.byteCapacity&#x3D;134217728</span><br><span class="line">agent.channels.c1.byteCapacityBufferPercentage&#x3D;80</span><br><span class="line"></span><br><span class="line">agent.sinks.k1.channel&#x3D;c1</span><br><span class="line">agent.sinks.k1.type&#x3D;org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">agent.sinks.k1.kafka.topic&#x3D;sparkstreaming</span><br><span class="line">agent.sinks.k1.kafka.zookeeperConnect&#x3D;47.102.199.215:2181</span><br><span class="line">#agent.sinks.k1.kafka.bootstrap.servers&#x3D;47.102.199.215:9092</span><br><span class="line">agent.sinks.k1.kafka.brokerList &#x3D;47.102.199.215:9092</span><br><span class="line">agent.sinks.k1.serializer.class&#x3D;kafka.serializer.StringEncoder</span><br><span class="line">agent.sinks.k1.flumeBatchSize&#x3D;1000</span><br><span class="line">agent.sinks.k1.useFlumeEventFormat&#x3D;true</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>SparkSql</title>
      <link href="/daydoc/2019/06/13/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/spark/sql/SparkSql/"/>
      <url>/daydoc/2019/06/13/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/spark/sql/SparkSql/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark sql 相关内容</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="sparksql"><a href="#sparksql" class="headerlink" title="sparksql"></a>sparksql</h1>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CDH搭建细节</title>
      <link href="/daydoc/2019/05/27/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/CDH/%E6%90%AD%E5%BB%BA/"/>
      <url>/daydoc/2019/05/27/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/CDH/%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<p>…..</p><a id="more"></a><h2 id="CDH安装准备"><a href="#CDH安装准备" class="headerlink" title="CDH安装准备"></a>CDH安装准备</h2><p><img src="http://img.wqkenqing.ren/Ibp691.png" alt=""></p><p>ubuntu ulimit</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>spark操作.md</title>
      <link href="/daydoc/2019/05/17/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/spark/spark%E6%93%8D%E4%BD%9C/"/>
      <url>/daydoc/2019/05/17/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/spark/spark%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>spark编程积累</p><a id="more"></a><h1 id="spark编程"><a href="#spark编程" class="headerlink" title="spark编程"></a>spark编程</h1><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><h3 id="hdfs"><a href="#hdfs" class="headerlink" title="hdfs"></a>hdfs</h3><p>操作hdfs比较常规,直接通过<br>context.textfile(path) //即可实现</p><h3 id="hbase"><a href="#hbase" class="headerlink" title="hbase"></a>hbase</h3><p>hbase 则要通过newAPIHadoopRDD来实现</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">JavaPairRDD&lt;ImmutableBytesWritable, Result&gt; javaRDD = jsc.newAPIHadoopRDD(HbaseOperate.getConf(), TableInputFormat<span class="class">.<span class="keyword">class</span>, <span class="title">ImmutableBytesWritable</span>.<span class="title">class</span>, <span class="title">Result</span>.<span class="title">class</span>)</span>;</span><br></pre></td></tr></table></figure><p>这里要特别说明的是,这里的conf承担了更多的责任,如指定表名,指定scan传输字符串等.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Configuration hconf = HbaseOperate.getConf();</span><br><span class="line">    Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">    hconf.set(TableInputFormat.INPUT_TABLE, <span class="string">"company"</span>);</span><br><span class="line">    hconf.set(TableInputFormat.SCAN, convertScanToString(scan));</span><br></pre></td></tr></table></figure><p>参考以上这段代码</p><p>另</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> String <span class="title">convertScanToString</span><span class="params">(Scan scan)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">     ClientProtos.Scan proto = ProtobufUtil.toScan(scan);</span><br><span class="line">     <span class="keyword">return</span> Base64.encodeBytes(proto.toByteArray());</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>以上是为实现scan指令传输字符的封装.</p><p>两者底层都是通过persist实现</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>hbaes操作</title>
      <link href="/daydoc/2019/05/17/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/hadoop/hbase/hbase%E6%93%8D%E4%BD%9C/"/>
      <url>/daydoc/2019/05/17/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/hadoop/hbase/hbase%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>对hbase常规api进行封装</p><a id="more"></a><h1 id="hbase日常api类封装"><a href="#hbase日常api类封装" class="headerlink" title="hbase日常api类封装"></a>hbase日常api类封装</h1>]]></content>
      
      
      
        <tags>
            
            <tag> 常规api封装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop高可用模式搭建</title>
      <link href="/daydoc/2019/05/16/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/hadoop/hadoopHA%E6%90%AD%E5%BB%BA/"/>
      <url>/daydoc/2019/05/16/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/hadoop/hadoopHA%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<p>发现对hadoop的相关版本的组件,进程还有些模糊,借着针对hadoopHA模式搭建的过程,对hadoop<br>进行一次细统的回顾.</p><a id="more"></a><h1 id="hadoop-HA搭建与总结"><a href="#hadoop-HA搭建与总结" class="headerlink" title="hadoop HA搭建与总结"></a>hadoop HA搭建与总结</h1><h2 id="什么是HA"><a href="#什么是HA" class="headerlink" title="什么是HA"></a>什么是HA</h2><p>HA即高可用</p><h2 id="HA相关配置"><a href="#HA相关配置" class="headerlink" title="HA相关配置"></a>HA相关配置</h2><h3 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h3><p>基本一致</p><h3 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h3><p>这里有明显差别<br>hadoop2.X与hadoop1.X的高可能中的明显差异就是从这里开始的.<br>2.x 引入了nameservice. 该nameservice可支持最大两个namenode.<br>1.x img 和edits统一放置在namenode上.<br>2.x 则通过journalnodes来共享edits日志.</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">            Licensed under the Apache License, Version 2.0 (the "License");</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 为namenode集群定义一个services name --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">&lt;!-- nameservice 包含哪些namenode，为各个namenode起名 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>namenode,datanode1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">   <span class="comment">&lt;!-- 名为master188的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.namenode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>namenode:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">&lt;!-- 名为master189的namenode的rpc地址和端口号，rpc用来和datanode通讯 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.datanode1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>datanode1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!--名为master188的namenode的http地址和端口号，用来和web客户端通讯 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.namenode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>namenode:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 名为master189的namenode的http地址和端口号，用来和web客户端通讯 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.datanode1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>datanode1:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">&lt;!-- namenode间用于共享编辑日志的journal节点列表 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://namenode:8485;datanode1:8485;datanode2:8485/ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">&lt;!-- 指定该集群出现故障时，是否自动切换到另一台namenode --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">      <span class="comment">&lt;!-- journalnode 上用于存放edits日志的目录 --&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop_store/dfs/data/dfs/journalnode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- 客户端连接可用状态的NameNode所用的代理类 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">&lt;!-- 一旦需要NameNode切换，使用ssh方式进行操作 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">&lt;!-- 如果使用ssh进行故障切换，使用ssh通信时用的密钥存储的位置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">&lt;!-- connect-timeout超时时间 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop_store/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop_store/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="mapreduce-site-xml"><a href="#mapreduce-site-xml" class="headerlink" title="mapreduce-site.xml"></a>mapreduce-site.xml</h3><p>变动不大</p><h3 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">            Licensed under the Apache License, Version 2.0 (the "License");</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">       <span class="comment">&lt;!-- 启用HA高可用性 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">      </span><br><span class="line">        <span class="comment">&lt;!-- 指定resourcemanager的名字 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">value</span>&gt;</span>yrc<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">      </span><br><span class="line">        <span class="comment">&lt;!-- 使用了2个resourcemanager,分别指定Resourcemanager的地址 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">&lt;!-- 指定rm1的地址 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">value</span>&gt;</span>namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">&lt;!-- 指定rm2的地址  --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">value</span>&gt;</span>datanode1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">&lt;!-- 指定当前机器master188作为rm1 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">&lt;!-- 指定zookeeper集群机器 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">value</span>&gt;</span>namenode:2181,datanode1:2181,datanode2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">&lt;!-- NodeManager上运行的附属服务，默认是mapreduce_shuffle --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">      </span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>kuiqwang<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://namenode:19888/tmp/logs/hadoop/logs/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.local-dirs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop_store/logs/yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.log-dirs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop_store/logs/userlogs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--内存,核数大小配置 --&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>3072<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.resource.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>3072<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.command-opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx3276m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="HA过程中主要用到的操作命令"><a href="#HA过程中主要用到的操作命令" class="headerlink" title="HA过程中主要用到的操作命令"></a>HA过程中主要用到的操作命令</h3><p>当配置文件完成后,先启动journalnode,以助namenode 和standby node 共享edits文件<br>hadoop-daemon.sh<br><img src="http://img.wqkenqing.ren/urHEX6.png" alt=""></p><p>然后再进行namdnode格式化,hadoop namenode -format<br>进行namenode格式化<br>当namenode格式化完成后可以先启动该节点的namenode<br>hadoop-daemon.sh start namenode<br>然后再在另一namdnode节点执行<br>hdfs namenode -bootstrapStandby<br>到这可以将之前的journalnode停用,然后start-dfs.sh</p><p>因为要用到zookeeper协助同步配置文件与操作日志,所以这里可以先对zookeeper进行hdfs内容的格式化<br>hdfs zkfc –formatZK<br>然后启动FailOver进程<br>hadoop-daemon.sh start zkfc<br><img src="http://img.wqkenqing.ren/lvuo9N.png" alt=""><br>至此则是这些进程<br>然后启用yarn.<br>即<br>start-yarn.sh<br>到这里HA过程中用到的一些常用指令大致总结完成</p><hr><p>至此 hadoop HA的常规总结完成.后续再补充一些细节,如standy 节点切的,与切换机制.HA背后的运作机制,与效果</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>hdfs操作细节</title>
      <link href="/daydoc/2019/05/16/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/hadoop/hdfs/hdfs%E6%93%8D%E4%BD%9C/"/>
      <url>/daydoc/2019/05/16/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/hadoop/hdfs/hdfs%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>针对hdfs一些较细节的api封装</p><a id="more"></a><h1 id="hdfs操作"><a href="#hdfs操作" class="headerlink" title="hdfs操作"></a>hdfs操作</h1><h2 id="常规操作"><a href="#常规操作" class="headerlink" title="常规操作"></a>常规操作</h2><ol><li>创建文件 </li><li>写数据 </li><li>删除文件 </li><li>上传文件 </li><li>下载文件 </li><li>断点续写</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">错误：</span><br><span class="line"></span><br><span class="line">    java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try</span><br></pre></td></tr></table></figure><p>原因：<br>    无法写入；我的环境中有3个datanode，备份数量设置的是3。在写操作时，它会在pipeline中写3个机器。默认replace-datanode-on-failure.policy是DEFAULT,如果系统中的datanode大于等于3，它会找另外一个datanode来拷贝。目前机器只有3台，因此只要一台datanode出问题，就一直无法写入成功。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.block.write.replace-datanode-on-failure.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span>         <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.block.write.replace-datanode-on-failure.policy<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>NEVER<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>对于dfs.client.block.write.replace-datanode-on-failure.enable，客户端在写失败的时候，是否使用更换策略，默认是true没有问题<br>对于，dfs.client.block.write.replace-datanode-on-failure.policy，default在3个或以上备份的时候，是会尝试更换结点尝试写入datanode。而在两个备份的时候，不更换datanode，直接开始写。对于3个datanode的集群，只要一个节点没响应写入就会出问题，所以可以关掉。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>hive总结</title>
      <link href="/daydoc/2018/12/24/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/hive%E6%80%BB%E7%BB%93/"/>
      <url>/daydoc/2018/12/24/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/hive%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="Hive相关点小结"><a href="#Hive相关点小结" class="headerlink" title="Hive相关点小结"></a>Hive相关点小结</h1><a id="more"></a><h2 id="启动指令"><a href="#启动指令" class="headerlink" title="启动指令"></a>启动指令</h2><ol><li>hive ==  hive –service cli<br>不需要启动server，使用本地的metastore，可以直接做一些简单的数据操作和测试。</li><li>启动hiveserver2<br>hive –service hiveserver2</li><li>beeline工具测试使用jdbc方式连接<br>beeline -u jdbc:hive2://localhost:10000</li></ol><p>1.managed table<br>管理表。<br>删除表时，数据也删除了</p><p>2.external table<br>外部表。<br>删除表时，数据不删</p><h2 id="建表"><a href="#建表" class="headerlink" title="建表:"></a>建表:</h2><p>CREATE TABLE IF NOT EXISTS t2(id int,name string,age int)<br>COMMENT ‘xx’                                     //注释<br>ROW FORMAT DELIMITED                             //行分隔符<br>FIELDS TERMINATED BY ‘,’                         //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改<br>STORED AS TEXTFILE ;</p><h3 id="外部表"><a href="#外部表" class="headerlink" title="外部表:"></a>外部表:</h3><p> CREATE  TABLE IF NOT EXISTS t2(id int,name string,age int)<br> COMMENT ‘xx’<br> ROW FORMAT DELIMITED<br> FIELDS TERMINATED BY ‘,’<br> STORED AS TEXTFILE ; </p><h3 id="分区表，桶表"><a href="#分区表，桶表" class="headerlink" title="分区表，桶表"></a>分区表，桶表</h3><h4 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h4><p>Hive中有分区表的概念。我们可以看到分区表具有重要的性能，而且分区表还可以将数据以一种符合逻辑的方式进行组织，比如分层存储。Hive的分区表，是把数据放在满足条件的分区目录下<br>CREATE TABLE t3(id int,name string,age int) </p><p>PARTITIONED BY (Year INT, Month INT)   //按照年月进行分区</p><p> ROW FORMAT DELIMITED                      //行分隔符</p><p>FIELDS TERMINATED BY ‘,’ ;                    //字段分隔符，这里使用的是逗号可以根据自己的需要自行进行修改<br>load data local inpath ‘/home/zpx/customers.txt’ into table t3 partition</p><h4 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h4><p>这样做，在查找数据的时候就可以跨越多个桶，直接查找复合条件的数据了。速度快，时间成本低。Hive中的桶表默认使用的机制也是hash。<br>CREATE TABLE t4(id int,name string,age int) </p><pre><code>CLUSTERED BY (id) INTO 3 BUCKETS      //创建3个通桶表，按照字段id进行分桶ROW FORMAT DELIMITED                     //行分隔符FIELDS TERMINATED BY &apos;,&apos; ; </code></pre><p>load data local inpath ‘/home/centos/customers.txt’ into table t4 ;</p><h2 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h2><p>load data local inpath ‘/home/zpx/customers.txt’ into table t2 ; //local上传文件<br>load data inpath ‘/user/zpx/customers.txt’ [overwrite] into table t2 //分布式文件系统上移动文件</p><h2 id="建视图"><a href="#建视图" class="headerlink" title="建视图"></a>建视图</h2><p>Hive也可以建立视图，是一张虚表，方便我们进行操作.</p><p>create view v1 as select a.id aid,a.name ,b.id bid , b.order from customers a left outer join default.tt b on a.id = b.cid ;</p><h2 id="Hive的严格模式"><a href="#Hive的严格模式" class="headerlink" title="Hive的严格模式"></a>Hive的严格模式</h2><p>Hive提供了一个严格模式，可以防止用户执行那些产生意想不到的不好的影响的查询。<br>使用了严格模式之后主要对以下3种不良操作进行控制：</p><p>1.分区表必须指定分区进行查询。<br>2.order by时必须使用limit子句。<br>3.不允许笛卡尔积。<br><img src="http://img.wqkenqing.ren/2019-03-18-17-13-36.png" alt="2019-03-18-17-13-36"></p><h2 id="Hive的动态分区"><a href="#Hive的动态分区" class="headerlink" title="Hive的动态分区"></a>Hive的动态分区</h2><p>像分区表里面存储了数据。我们在进行存储数据的时候，都是明确的指定了分区。在这个过程中Hive也提供了一种比较任性化的操作，就是动态分区，不需要我们指定分区目录，Hive能够把数据进行动态的分发,<strong>我们需要将当前的严格模式设置成非严格模式，否则不允许使用动态分区</strong><br>set hive.exec.dynamic.partition.mode=nonstrict//设置非严格模式</p><h2 id="Hive的排序"><a href="#Hive的排序" class="headerlink" title="Hive的排序"></a>Hive的排序</h2><p>Hive也提供了一些排序的语法，包括order by,sort by。</p><p>order by=MapReduce的全排序<br>sort by=MapReduce的部分排序<br>distribute by=MapReduce的分区</p><p>selece …….from …… order by 字段；//按照这个字段全排序</p><p>selece …….from …… sort by 字段； //按照这个字段局部有序</p><p>selece 字段…..from …… distribute by 字段；//按照这个字段分区<br>特别注意的是：</p><ol><li>在上面的最后一个distribute by使用过程中，按照排序的字段要出现在最左侧也就是select中有这个字段，因为我们要告诉MapReduce你要按照哪一个字段分区，当然获取的数据中要出现这个字段了。类似于我们使用group by的用法，字段也必须出现在最左侧，因为数据要包含这个字段，才能按照这个字段分组，至于Hive什么时候会自行的开启MapReduce，那就是在使用聚合的情况下开启，使用select …from ….以及使用分区表的selece ….from……where …..不会开启</li><li>distribute by与sort by可以组合使用，但是distribute by要放在前边，因为MapReduce要先分区，后排序，再归并</li></ol><p>select 字段a,……..from …….distribute by字段a，sort by字段<br>如果distribute by与sort by使用的字段一样，则可以使用cluster by 字段替代：<br>select 字段a,……..from …….cluster by 字段</p><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><ol><li>show functions; 展示相关函数</li><li>desc function split;</li><li>desc function  extended split;  //查看函数的扩展信息</li></ol><h3 id="用户自定义函数（UDF）"><a href="#用户自定义函数（UDF）" class="headerlink" title="用户自定义函数（UDF）"></a>用户自定义函数（UDF）</h3><p>具体步骤如下：</p><p>（1）.自定义类（继承UDF，或是GenericUDF。GenericUDF是更为复杂的抽象概念，但是其支持更好的null值处理同时还可以处理一些标准的UDF无法支持的编程操作）。<br>（2）.导出jar包，通过命令添加到hive的类路径。<br>$hive&gt;add jar xxx.jar<br>（3）.注册函数<br>$hive&gt;CREATE TEMPORARY FUNCTION 函数名 AS ‘具体类路径：包.类’;<br>（4）.使用<br> $hive&gt;select 函数名(参数);<br>自定义实现类如下(继承UDF)：</p>]]></content>
      
      
      
        <tags>
            
            <tag> bigdata </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hbase积累.md</title>
      <link href="/daydoc/2018/06/04/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/hbase%E7%A7%AF%E7%B4%AF/"/>
      <url>/daydoc/2018/06/04/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/hbase%E7%A7%AF%E7%B4%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="hbase积累"><a href="#hbase积累" class="headerlink" title="hbase积累"></a>hbase积累</h1><a id="more"></a><h2 id="细节点"><a href="#细节点" class="headerlink" title="细节点"></a>细节点</h2><h3 id="1-Rowkey设计原则"><a href="#1-Rowkey设计原则" class="headerlink" title="1.Rowkey设计原则"></a>1.Rowkey设计原则</h3><!-- more --><p>1.1 <strong>长度原则</strong> rowkey 在hbase以二进制码流,可以是任意字符串,</p><ul><li><p><strong>最大长度是64kb</strong>,实际应用主要是100~100bytes</p></li><li><p>长度尽量为8的整数倍,因为现在的系统主要是64位,内存8字节对齐.控制在16字节,符合操作系统特性</p></li></ul><p>1.2 <strong>散列原则</strong>:因为hbase是分布式存储,rowkey的高位尽量是散列字段,散列性弱的尽量放在低位段.如Time AND Device_id的组合,相对而言device_id 应该量级较小,散列性高.而TIME散列性低,如果TIME放在高位,可能造成数据在某个RegeionServer上堆积的情况.所以较合理的rowkey组合应是<br>device_id+time.</p><p>1.3 <strong>RowKey唯一原则</strong>：必须在设计上保证其唯一性.<br>hbase 中以KeyValue形式存储,key若重复,行内容则会被覆盖.</p><hr><h3 id="2-Hbase的Regeion热点问题解决"><a href="#2-Hbase的Regeion热点问题解决" class="headerlink" title="2.Hbase的Regeion热点问题解决"></a>2.Hbase的Regeion热点问题解决</h3><p><code>因为在创建表是没有提前预分区,创建的表默认就只会有一个region,这个region的rowkey是没有边界的,即没有startkey与stopkey.数据在写入时,都会写入到这个region.随着数据的不断增加,达到某个阈值时,才会split成2个region.在这个过程中就会产生所有数据囤积在一个regionServer上,出现热点问题.另在split时,会占用集群的I/O资源.通过预分区可以解决该问题</code></p><h4 id="2-1-预分区"><a href="#2-1-预分区" class="headerlink" title="2.1 预分区"></a>2.1 预分区</h4><p>预分区,”预”字是核心.我们在建表时,预先对表中要存放的数据形式和可能的量级,心中必然会有所估量,即这里应<strong>预</strong>估数据量.若数据量较大,则在建表时又应该预分区.即根据数据形式,量级,事先预设好一定量的region,后面数据写入时,则会写入到相应的分区.从而避免热点,减少split.</p><p>2.1.2 salting(加盐)<br>hbase rowkey设计,避免热点,常会用到该操作,这里的加盐本身不是加密操作,而是在原数据前加入一些随机数据,从而起到分散不同region的作用.</p><p>2.1.3 预习区具体方案</p><p>hbase预分区的相关操作,如shell形式,可直接在hbase shell<br>操作.如</p><p>[<a href="https://blog.csdn.net/xiao_jun_0820/article/details/24419793]" target="_blank" rel="noopener">https://blog.csdn.net/xiao_jun_0820/article/details/24419793]</a>(Hbase shell 预分区操作.)</p><p>java形式<br>[<a href="https://blog.csdn.net/qq_20641565/article/details/56482407]" target="_blank" rel="noopener">https://blog.csdn.net/qq_20641565/article/details/56482407]</a>(Hbase 预分区 java API形式)</p><p>以上操作形式有个问题就是rowkey是随机生成的,虽起到了散列存储,避免了热点堆积,但因为加盐的缘故,想要直接的获取某行数据较为困难.若针对的是高频使用的数据,则会出现问题.</p><p>2.1.4 hash分区</p><p>在原先预分区的基础上,通过相关规则将原数据hash,从而获得这个原数据对应在哪个分区,使当拿到相关原数据,就能推演出相关rowkey.从而能准确的get数据.</p><h3 id="hbase优化"><a href="#hbase优化" class="headerlink" title="hbase优化"></a>hbase优化</h3><h4 id="确定优化目标"><a href="#确定优化目标" class="headerlink" title="确定优化目标"></a>确定优化目标</h4><p>沟通交流后，业务方更看重降低成本。数据量梳理后略有降低，保证吞吐，无长期请求堆积前提下可以放宽延时要求。为了更快的进行优化，放宽稳定性可以要求接受短期波动。<br>另外，该分组的RegionServer之前存在不稳定的问题，这次优化也一并解决。</p><hr>]]></content>
      
      
      
        <tags>
            
            <tag> 日常总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark学习</title>
      <link href="/daydoc/2018/03/04/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/spark%E5%AD%A6%E4%B9%A0/"/>
      <url>/daydoc/2018/03/04/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/spark%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="spark-学习"><a href="#spark-学习" class="headerlink" title="spark 学习"></a>spark 学习</h1><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark 作为主流的实时计算引擎,需要高度掌握</span><br></pre></td></tr></table></figure><!-- more --><h2 id="spark介绍"><a href="#spark介绍" class="headerlink" title="spark介绍"></a>spark介绍</h2><p>Apache Spark是一用于实时处理的开源集群计算框架.持多种语言编程,Spark Streaming有高吞吐量和容错能力强等特点.<br>数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算,而结果也能保存在很多地方，如HDFS，数据库等。另外Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。</p><p>优点</p><ul><li>易用</li><li>容错</li><li>spark体系整合</li></ul><p><img src="http://img.wqkenqing.ren/2019-03-04-15-45-38.png" alt="spark&amp;storm对比"></p><h2 id="RDD详解"><a href="#RDD详解" class="headerlink" title="RDD详解"></a>RDD详解</h2><h3 id="RDD是什么"><a href="#RDD是什么" class="headerlink" title="RDD是什么"></a>RDD是什么</h3><p>RDD：Spark的核心概念是RDD (resilientdistributed dataset)，指的是一个只读的，可分区的分布式数据集，这个数据集的全部或部分可以缓存在内存中，在多次计算间重用。</p><p>另:RDD即弹性分布式数据集，有容错机制并可以被并行操作的元素集合，具有只读、分区、容错、高效、无需物化、可以缓存、RDD依赖等特征。RDD只是数据集的抽象，分区内部并不会存储具体的数据。</p><p>RDD的五个特性</p><ol><li>有一个分片列表。就是能被切分，和hadoop一样的，能够切分的数据才能并行计算。 </li><li>有一个函数计算每一个分片，这里指的是下面会提到的compute函数.</li><li>对其他的RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖.</li><li>可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。</li><li>可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。(存储的是一个表，可以将处理的分区“本地化”).</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//只计算一次  </span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]  </span><br><span class="line">  <span class="comment">//对一个分片进行计算，得出一个可遍历的结果</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br><span class="line">  <span class="comment">//只计算一次，计算RDD对父RDD的依赖</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps</span><br><span class="line">  <span class="comment">//可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce</span></span><br><span class="line">  <span class="meta">@transient</span> <span class="keyword">val</span> partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span></span><br><span class="line">  <span class="comment">//可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span></span><br></pre></td></tr></table></figure><h3 id="为什么会产生RDD"><a href="#为什么会产生RDD" class="headerlink" title="为什么会产生RDD"></a>为什么会产生RDD</h3><h3 id="RDD数据集"><a href="#RDD数据集" class="headerlink" title="RDD数据集"></a>RDD数据集</h3><ol><li>并行集合</li></ol><p>接收一个已经存在的集合,然后进行各种并行计算.并行化集合是通过调用SparkContext的parallelize方法，在一个已经存在的Scala集合上创建（一个Seq对象）。集合的对象将会被拷贝，创建出一个可以被并行操作的分布式数据集。</p><ol start="2"><li>Hadoop数据集</li></ol><p>Spark可以将任何Hadoop所支持的存储资源转化成RDD，只要文件系统是HDFS，或者Hadoop支持的任意存储系统即可，如本地文件（需要网络文件系统，所有的节点都必须能访问到）、HDFS、Cassandra、HBase、Amazon S3等，Spark支持文本文件、SequenceFiles和任何Hadoop InputFormat格式。</p><p>此两种类型的RDD都可以通过相同的方式进行操作，从而获得子RDD等一系列拓展，形成lineage血统关系图。</p><h3 id="Spark-RDD算子"><a href="#Spark-RDD算子" class="headerlink" title="Spark RDD算子"></a>Spark RDD算子</h3><ol><li>Transformation<br>不触发提交作业，完成作业中间处理过程。</li></ol><h2 id="DStream"><a href="#DStream" class="headerlink" title="DStream"></a>DStream</h2><h3 id="什么是DStream"><a href="#什么是DStream" class="headerlink" title="什么是DStream"></a>什么是DStream</h3><p>Discretized Stream :代表持续性的数据流和经过各种Spark原语操作后的结果数据流,在内部实现上是一系列连续的RDD来表示.每个RDD含有一段时间间隔内的数据,如下图<br><img src="http://img.wqkenqing.ren/2019-03-04-15-50-41.png" alt="DStream"></p><p>计算则由spark engine来完成<br><img src="http://img.wqkenqing.ren/2019-03-04-15-51-58.png" alt="spark engine流程"></p><h2 id="spark-java"><a href="#spark-java" class="headerlink" title="spark java"></a>spark java</h2><p>因为我是主要掌握的语言是java,从效率上来考虑,这里</p><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><p><a href="https://blog.csdn.net/wangxiaotongfan/article/details/51395769" target="_blank" rel="noopener">https://blog.csdn.net/wangxiaotongfan/article/details/51395769</a> RDD详解<br><a href="https://blog.csdn.net/zuochang_liu/article/details/81459185" target="_blank" rel="noopener">https://blog.csdn.net/zuochang_liu/article/details/81459185</a>  spark streaming学习<br><a href="https://blog.csdn.net/hellozhxy/article/details/81672845" target="_blank" rel="noopener">https://blog.csdn.net/hellozhxy/article/details/81672845</a> spark java 使用指南<br><a href="https://blog.csdn.net/t1dmzks/article/details/70198430" target="_blank" rel="noopener">https://blog.csdn.net/t1dmzks/article/details/70198430</a> sparkRDD算子介绍<br><a href="https://blog.csdn.net/wxycx11111/article/details/79123482" target="_blank" rel="noopener">https://blog.csdn.net/wxycx11111/article/details/79123482</a> <strong>sparkRDD入门介绍</strong><br><a href="https://github.com/zhaikaishun/spark_tutorial" target="_blank" rel="noopener">https://github.com/zhaikaishun/spark_tutorial</a> <strong>RDD算子介绍</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> 学习spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark学习2</title>
      <link href="/daydoc/2018/03/04/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/spark%E5%AD%A6%E4%B9%A02/"/>
      <url>/daydoc/2018/03/04/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/spark%E5%AD%A6%E4%B9%A02/</url>
      
        <content type="html"><![CDATA[<h1 id="spark学习2"><a href="#spark学习2" class="headerlink" title="spark学习2"></a>spark学习2</h1><h2 id="spark-运行的四种模式"><a href="#spark-运行的四种模式" class="headerlink" title="spark 运行的四种模式"></a>spark 运行的四种模式</h2><a id="more"></a><h3 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h3><p>如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;spark-submit --class org.apache.spark.examples.SparkPi --master local[1] .&#x2F;lib&#x2F;spark-examples-1.3.1-hadoop2.4.0.jar 100</span><br></pre></td></tr></table></figure><h3 id="standlone模式"><a href="#standlone模式" class="headerlink" title="standlone模式"></a>standlone模式</h3><h4 id="client"><a href="#client" class="headerlink" title="client"></a>client</h4><p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master spark://spark001:7077 –executor-memory 1G –total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100</p><h4 id="cluster"><a href="#cluster" class="headerlink" title="cluster"></a>cluster</h4><p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master spark://spark001:7077 –deploy-mode cluster –supervise –executor-memory 1G –total-executor-cores 1 ./lib/spark-examples-1.3.1-hadoop2.7.0.jar 100</p><h3 id="Yarn模式"><a href="#Yarn模式" class="headerlink" title="Yarn模式"></a>Yarn模式</h3><h4 id="client模式"><a href="#client模式" class="headerlink" title="client模式"></a>client模式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">client模式：</span><br><span class="line">结果xshell可见：</span><br><span class="line">.&#x2F;bin&#x2F;spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --executor-memory 1G --num-executors 1 .&#x2F;lib&#x2F;spark-examples-1.3.1-hadoop2.7.0.jar 100</span><br></pre></td></tr></table></figure><h4 id="cluster模式"><a href="#cluster模式" class="headerlink" title="cluster模式"></a>cluster模式</h4><p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master yarn-cluster –executor-memory 1G –num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100</p><h2 id="spark-sql"><a href="#spark-sql" class="headerlink" title="spark sql"></a>spark sql</h2>]]></content>
      
      
      
        <tags>
            
            <tag> 学习spark2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark算子</title>
      <link href="/daydoc/2018/03/04/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/spark%E7%AE%97%E5%AD%90/"/>
      <url>/daydoc/2018/03/04/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/spark%E7%AE%97%E5%AD%90/</url>
      
        <content type="html"><![CDATA[<h1 id="spark-算子"><a href="#spark-算子" class="headerlink" title="spark 算子"></a>spark 算子</h1><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sparkRDD封装的函数方法又称算子,通过这些算子可以对RDD进行相关处理,从而获我们想要的结果,因为可能涉及的算子较多.因此单独开篇进行粒度更细,更集中的总结.</span><br><span class="line"></span><br><span class="line">总得来讲spark的算子,本就是scala集合的一些高阶用法.</span><br></pre></td></tr></table></figure><h2 id="Transformation-转换"><a href="#Transformation-转换" class="headerlink" title="Transformation(转换)"></a>Transformation(转换)</h2><p>不触发提交作业，完成作业中间处理过程。</p><h3 id="parallelize-并行化"><a href="#parallelize-并行化" class="headerlink" title="parallelize (并行化)"></a>parallelize (并行化)</h3><p>将一个存在的集合，变成一个RDD ,返回的是一个JavaRDD[T]<br>** in scala **</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(<span class="type">List</span>(<span class="string">"shenzhen"</span>, <span class="string">"is a beautiful city"</span>))</span><br></pre></td></tr></table></figure><p> ** in java **<br> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; javaStringRDD = sc.parallelize(Arrays.asList(<span class="string">"shenzhen"</span>, <span class="string">"is a beautiful city"</span>));</span><br></pre></td></tr></table></figure></p><h3 id="makeRDD"><a href="#makeRDD" class="headerlink" title="makeRDD"></a>makeRDD</h3><p>只有scala版本的才有makeRDD ,与parallelize类似.</p><h3 id="textFile"><a href="#textFile" class="headerlink" title="textFile"></a>textFile</h3><p>调用SparkContext.textFile()方法，从外部存储中读取数据来创建 RDD<br>** in scala **<br> <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> lines = sc.textFile(inpath)</span><br></pre></td></tr></table></figure></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// java</span></span><br><span class="line"> JavaRDD&lt;String&gt; lines = sc.textFile(inpath);</span><br></pre></td></tr></table></figure><h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><p>对RDD数据进行过滤</p><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p>接收一个函数,并将这个函数作用于RDD中的每个元素.RDD 中对应*<em>元素的值 map是一对一的关系 *</em></p><h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><p>有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap() ,faltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器</p><h3 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h3><p>去重,我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销很大 </p><h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><p>两个RDD进行合并 </p><h3 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h3><p>RDD1.intersection(RDD2) 返回两个RDD的交集，** 并且去重 **<br>intersection 需要混洗数据，比较浪费性能</p><h3 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h3><p>RDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重 </p><h3 id="cartesian"><a href="#cartesian" class="headerlink" title="cartesian"></a>cartesian</h3><p>cartesian(RDD2) 返回RDD1和RDD2的笛卡儿积，这个开销非常大</p><h3 id="mapToPair"><a href="#mapToPair" class="headerlink" title="mapToPair"></a>mapToPair</h3><p>将元素该成key-value形式</p><h3 id="flatMapToPair"><a href="#flatMapToPair" class="headerlink" title="flatMapToPair"></a>flatMapToPair</h3><p>差异同mapToPair</p><h3 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h3><p>该方法主要针对不同分区的同一key进行元素合并函数操作.<br>需要对pairRDD进行</p><ol><li>createCombiner  会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过,要么就<br>和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建<br>那个键对应的累加器的初始值</li><li>mergeValue 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并</li><li>mergeCombiners 于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更<br>多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各<br>个分区的结果进行合并。<h3 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h3>接收一个函数，按照相同的key进行reduce操<h3 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h3>该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V ,与reduce不同的是 foldByKey开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素 <h3 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h3>SortByKey用于对pairRDD按照key进行排序，第一个参数可以设置true或者false，默认是true <h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3>groupByKey会将RDD[key,value] 按照相同的key进行分组，形成RDD[key,Iterable[value]]的形式， 有点类似于sql中的groupby，例如类似于mysql中的group_concat <h3 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h3>groupByKey是对单个 RDD 的数据进行分组，还可以使用一个叫作 cogroup() 的函数对多个共享同一个键的 RDD 进行分组<br>RDD1.cogroup(RDD2) 会将RDD1和RDD2按照相同的key进行分组，得到(key,RDD[key,Iterable[value1],Iterable[value2]])的形式 <h3 id="subtractByKey"><a href="#subtractByKey" class="headerlink" title="subtractByKey"></a>subtractByKey</h3>类似于subtrac，删掉 RDD 中键与 other RDD 中的键相同的元素<h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3>可以把RDD1,RDD2中的相同的key给连接起来，类似于sql中的join操作<br>RDD1.join(RDD2) <h3 id="fullOuterJoin"><a href="#fullOuterJoin" class="headerlink" title="fullOuterJoin"></a>fullOuterJoin</h3>全连接<h3 id="leftOuterJoin"><a href="#leftOuterJoin" class="headerlink" title="leftOuterJoin"></a>leftOuterJoin</h3><h3 id="rightOuterJoin"><a href="#rightOuterJoin" class="headerlink" title="rightOuterJoin"></a>rightOuterJoin</h3></li></ol><h2 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h2><h3 id="first"><a href="#first" class="headerlink" title="first"></a>first</h3><p>返回第一个元素 </p><h3 id="take"><a href="#take" class="headerlink" title="take"></a>take</h3><p>rdd.take(n)返回第n个元素 </p><h3 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h3><p>rdd.collect() 返回 RDD 中的所有元素 </p><h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><p>rdd.count() 返回 RDD 中的元素个数 </p><h3 id="countByValue"><a href="#countByValue" class="headerlink" title="countByValue"></a>countByValue</h3><p>各元素在 RDD 中出现的次数 返回{(key1,次数),(key2,次数),…(keyn,次数)} </p><h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><p>并行整合RDD中所有数据</p><h3 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h3><p>和 reduce() 一 样， 但是提供了初始值num,每个元素计算时，先要合这个初始值进行折叠, 注意，这里会按照每个分区进行fold，然后分区之间还会再次进行fold </p><h3 id="top"><a href="#top" class="headerlink" title="top"></a>top</h3><p>rdd.top(n)<br>按照降序的或者指定的排序规则，返回前n个元素 </p><h3 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h3><p>rdd.take(n)<br>对RDD元素进行升序排序,取出前n个元素并返回，也可以自定义比较器（这里不介绍），类似于top的相反的方法 </p><h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><p>对 RDD 中的每个元素使用给<br>定的函数</p><h3 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h3><p>以RDD{(1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)}为例 rdd.countByKey会返回{(1,1),(2,2),(3,3)} </p><h3 id="collectAsMap"><a href="#collectAsMap" class="headerlink" title="collectAsMap"></a>collectAsMap</h3><p>将pair类型(键值对类型)的RDD转换成map, 还是上面的例子</p><h3 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h3><p>saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。</p><h3 id="saveAsSequenceFile"><a href="#saveAsSequenceFile" class="headerlink" title="saveAsSequenceFile"></a>saveAsSequenceFile</h3><p>saveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。</p><h3 id="saveAsObjectFile"><a href="#saveAsObjectFile" class="headerlink" title="saveAsObjectFile"></a>saveAsObjectFile</h3><p>saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。</p><h3 id="saveAsHadoopFile"><a href="#saveAsHadoopFile" class="headerlink" title="saveAsHadoopFile"></a>saveAsHadoopFile</h3><h3 id="saveAsNewAPIHadoopFile"><a href="#saveAsNewAPIHadoopFile" class="headerlink" title="saveAsNewAPIHadoopFile"></a>saveAsNewAPIHadoopFile</h3><h3 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h3><h3 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h3><h3 id="HashPartitioner"><a href="#HashPartitioner" class="headerlink" title="HashPartitioner"></a>HashPartitioner</h3><h3 id="RangePartitioner"><a href="#RangePartitioner" class="headerlink" title="RangePartitioner"></a>RangePartitioner</h3><h3 id="自定义分区"><a href="#自定义分区" class="headerlink" title="自定义分区"></a>自定义分区</h3>]]></content>
      
      
      
        <tags>
            
            <tag> spark学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sqoop记录</title>
      <link href="/daydoc/2018/03/04/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/sqoop%E8%AE%B0%E5%BD%95/"/>
      <url>/daydoc/2018/03/04/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/old/sqoop%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h2 id="将Mysql数据导入Hive中"><a href="#将Mysql数据导入Hive中" class="headerlink" title="将Mysql数据导入Hive中"></a>将Mysql数据导入Hive中</h2><a id="more"></a><p>命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop import  </span><br><span class="line">-Dorg.apache.sqoop.splitter.allow_text_splitter&#x3D;true       </span><br><span class="line">--connect jdbc:mysql:&#x2F;&#x2F;211.159.172.76:3306&#x2F;solo</span><br><span class="line">--username root </span><br><span class="line">--password 125323Wkq </span><br><span class="line">--table  tablename </span><br><span class="line">--hive-import </span><br><span class="line">--hive-table tablename</span><br></pre></td></tr></table></figure><h3 id="整库导入"><a href="#整库导入" class="headerlink" title="整库导入"></a>整库导入</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sqoop import-all-tables --connect jdbc:mysql:&#x2F;&#x2F;211.159.172.76:3306&#x2F; --username root --password 125323Wkq --hive-database solo  -m 10  </span><br><span class="line">--create-hive-table  </span><br><span class="line">--fields-terminated-by &quot;\t&quot;</span><br><span class="line">--hive-import --hive-database qianyang --hive-overwrite</span><br></pre></td></tr></table></figure><p>sqoop  import-all-tables -Dorg.apache.sqoop.splitter.allow_text_splitter=true –connect jdbc:mysql://211.159.172.76:3306/solo –username root –password 125323Wkq –hive-database blog  –create-hive-table  –hive-import –hive-overwrite -m 10 </p><h3 id="单表导入"><a href="#单表导入" class="headerlink" title="单表导入"></a>单表导入</h3><p>sqoop import  –connect   jdbc:mysql://211.159.172.76:3306/solo –username root     –password 125323Wkq    –table b3_solo_article –target-dir /blog/article   –hive-import  –hive-database blog<br>–fields-terminated-by “\t” –hive-table article  –hive-overwrite<br>–m 10  </p><p>sqoop  import  –connect jdbc:mysql://211.159.172.76:3306/solo –username root –password 125323Wkq –table b3_solo_article –target-dir /blog/article –hive-import –hive-database blog  –create-hive-table  –hive-table article –hive-overwrite -m 1 </p>]]></content>
      
      
      
        <tags>
            
            <tag> 日常总结 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

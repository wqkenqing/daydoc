title:  问题记录
date:  2023年10月14日
tags: []

---
 <!--more-->


### 1、Hive数据倾斜

1. 空值引发的数据倾斜
2. 不同数据类型引发的数据倾斜
3. 不可拆分大文件引发的数据倾斜
4. 数据膨胀引发的数据倾斜
5. 表连接时引发的数据倾斜
6. 确实无法减少数据量引发的数据倾斜

---

### 2、hbase 数据热点与数据倾斜问题的处理

#### 2.1 数据热点

a. 什么是数据热点？

client访问的数据集中在某几个region上。局部region server的访问压力很大，与其它regionserver相比失衡严重

b. 怎么发生的呢?

这个问题的发生可能跟rowkey的设计有关。因为rowkey的变化较少，按字典排序时，rowkey过度集中到某几个region中了

c. 如何处理？

1. 反转
2. 加盐（Salting）
3. hash

hbase的数据倾斜问题与处理方式基本类似

### 3、hive小文件过多的情况处理

#### 3.1 小文件产生的原因

a.  直接向表中插入数据

```
insert into table demo values (round(),'lisi'),88)
```

b. 通过load方式加载数据

c. 通过查询方式加载数据

```
insert overwrite table demo select c_id,c_name from raw_user;
```

>
> 这种方式是生产环境中常用的，也是最容易产生小文件的方式
>
> insert 导入数据时会启动 MR 任务，MR中 reduce 有多少个就输出多少个文件
>
> 所以， 文件数量=ReduceTask数量*分区数
>
> 也有很多简单任务没有reduce，只有map阶段，则
>
> 文件数量=MapTask数量*分区数
>
> 每执行一次 insert 时hive中至少产生一个文件，因为 insert 导入时至少会有一个MapTask。
> 像有的业务需要每10分钟就要把数据同步到 hive 中，这样产生的文件就会很多。

#### 3.2 带来的影响

1. 首先对底层存储HDFS来说，HDFS本身就不适合存储大量小文件，小文件过多会导致namenode元数据特别大, 占用太多内存，严重影响HDFS的性能
2. 对 hive 来说，在进行查询时，每个小文件都会当成一个块，启动一个Map任务来完成，而一个Map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的Map数量是受限的。

#### 3.3 如何解决

a. 使用 hive 自带的 concatenate 命令，自动合并小文件

```
#对于非分区表
alter table demo concatenate;

#对于分区表
alter table demo partition(day="school") concatenate;
```

>  注意： 
> 1、concatenate 命令只支持 RCFILE 和 ORC 文件类型。 
> 2、使用concatenate命令合并小文件时不能指定合并后的文件数量，但可以多次执行该命令。 
> 3、当多次使用concatenate后文件数量不在变化，这个跟参数 mapreduce.input.fileinputformat.split.minsize=256mb 的设置有关，可设定每个文件的最小size。

b. 调整参数减少Map数量

- **设置map输入合并小文件的相关参数**：

```
#执行Map前进行小文件合并
#CombineHiveInputFormat底层是 Hadoop的 CombineFileInputFormat 方法
#此方法是在mapper中将多个文件合成一个split作为输入
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; -- 默认

#每个Map最大输入大小(这个值决定了合并后文件的数量)
set mapred.max.split.size=256000000;   -- 256M

#一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)
set mapred.min.split.size.per.node=100000000;  -- 100M

#一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并)
set mapred.min.split.size.per.rack=100000000;  -- 100M
```

- **设置map输出和reduce输出进行合并的相关参数**:

```
#设置map端输出进行合并，默认为true
set hive.merge.mapfiles = true;

#设置reduce端输出进行合并，默认为false
set hive.merge.mapredfiles = true;

#设置合并文件的大小
set hive.merge.size.per.task = 256*1000*1000;   -- 256M

#当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge
set hive.merge.smallfiles.avgsize=16000000;   -- 16M 
```

- **启用压缩**

```
# hive的查询结果输出是否进行压缩
set hive.exec.compress.output=true;

# MapReduce Job的结果输出是否使用压缩
set mapreduce.output.fileoutputformat.compress=true;
```

c. **减少Reduce的数量**

```
#reduce 的个数决定了输出的文件的个数，所以可以调整reduce的个数控制hive表的文件数量，
#hive中的分区函数 distribute by 正好是控制MR中partition分区的，
#然后通过设置reduce的数量，结合分区函数让数据均衡的进入每个reduce即可。

#设置reduce的数量有两种方式，第一种是直接设置reduce个数
set mapreduce.job.reduces=10;

#第二种是设置每个reduce的大小，Hive会根据数据总大小猜测确定一个reduce个数
set hive.exec.reducers.bytes.per.reducer=5120000000; -- 默认是1G，设置为5G

#执行以下语句，将数据均衡的分配到reduce中
set mapreduce.job.reduces=10;
insert overwrite table A partition(dt)
select * from B
distribute by rand();

解释：如设置reduce数量为10，则使用 rand()， 随机生成一个数 x % 10 ，
这样数据就会随机进入 reduce 中，防止出现有的文件过大或过小
```

e. 使用hadoop的archive将小文件归档

Hadoop Archive简称HAR，是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样在减少namenode内存使用的同时，仍然允许对文件进行透明的访问

```
#用来控制归档是否可用
set hive.archive.enabled=true;
#通知Hive在创建归档时是否可以设置父目录
set hive.archive.har.parentdir.settable=true;
#控制需要归档文件的大小
set har.partfile.size=1099511627776;

#使用以下命令进行归档
ALTER TABLE A ARCHIVE PARTITION(dt='2020-12-24', hr='12');

#对已归档的分区恢复为原文件
ALTER TABLE A UNARCHIVE PARTITION(dt='2020-12-24', hr='12');
```

新集群最好在入库的时候就设好文件格式，直接以ORC格式，并启用LZO压缩

这样小文件过多可以使用hive的concatenate快速合并

---

### 4、hdfs小文件过多的情况处理

#### 4.1 为什么hdfs小文件过多会算是一个问题

每个文件均**按块存储**，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为**大量的小文件会耗尽NameNode中的大部分内存**。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。每个块的大小可以通过配置参数（`dfs.blocksize`）来规定，默认的大小`128M`。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB

**100个1k文件块，与100个100M的文件块，占用NN内存大小一样**

1）小文件是如何产生的？

- 动态分区插入数据，产生大量的小文件，从而导致 map 数量剧增；
- reduce 数量越多，小文件也越多，reduce 的个数和输出文件个数一致；
- 数据源本身就是大量的小文件；

2）文件块大小设置

![示意图](http://img.wqkenqing.ren/typora_img/eb7e341b695443c1a49483fc7d1608ed.png) 

所以对于块大小的设置既不能太大，也不能太小，**太大会使得传输时间加长**，程序在处理这块数据时会变得非常慢，如果文件块的大小**太小的话会增加每一个块的寻址时间**。所以文件块的大小设置取决于磁盘的传输速率。

#### 4.2 HDFS小文件问题处理方案

[处理方案](https://cloud.tencent.com/developer/article/1779976)


title:  数据采集服务
date:  2021年 4月28日
tags: [flume,beats,采集]
password: 7FKBKZrTTTPG2LnC

---
 <!--more-->

 # 数据采集服务

> 针对项目和产品化的构思,数据中心这个载体中的基本组成部件中,数据采集模块是必不可少的,在以往的经历中,数据采集主要是依托于某一两个组件,运行的某些采集服务,比较功能和内容单薄,这次对数据采集服务进行一次升级,封装

可能涉及到的组件和内容

1. flume
2. beats
3. python
4. java

## flume采集
Flume版本:1.9.0

> 因为结合业务来做,原始采集的这个第一环节，尽量封装度高一些。所以这里对数据源，一是关注采集源种类，二是关注采集协议

### Source

demo

```
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444
##这个参数可以设置对接数据的编码格式。
a1.sources.r1.encoding= gbk 

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

```

flume-ng agent --conf conf --conf-file test.conf --name a1 -Dflume.root.logger=INFO,console
flume-ng agent --conf conf --conf-file conf/gbkfile.conf --name a1 -Dflume.root.logger=DEBUG,console
flume-ng agent --conf conf --conf-file conf/gbkkafka.conf --name a1 -Dflume.root.logger=DEBUG,console


Q: tcp协议时,发送中文出现乱码

A:思路,修改flume的conf 编码协议
经过实际操作，通过具体的参数配置解决了这一问题
```
a1.sources.r1.encoding = gbk

```

采集服务封装思路
1. 封装几种类型的采集服务
2. 统一管理

功能方面
1. 任务的便捷配置&提交
2. 稳定
2. 失败自启
3. 异常通报

---

### 如何实现flume任务的便捷配置与提交

1. 添加key

相关任务语句
```
flume-ng agent --conf conf --conf-file conf/gbkkafka2.conf --name a1 -Dflume.root.logger=DEBUG,console
```

tips:flume采集任务的配置可以动态更改,即直接修改配置文件,而不用再重启服务,服务会监测配置文件,有更新则会自动重新更新。

经过调试,最后实现了直接给kafka数据加key的方案.实现思路是通过拦截器,通过拦截器实现添加key
1. flume有UUID的拦截器方式加Key
2. 通过static类型的拦截器方式加key 

参考配置如下:
**uuid**:

```
agent1.sources.nginxlogSource.interceptors = UUIDi1
agent1.sources.nginxlogSource.interceptors.UUIDi1.type=org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder
agent1.sources.nginxlogSource.interceptors.UUIDi1.headerName=key
agent1.sources.nginxlogSource.interceptors.UUIDi1.preserveExisting=false

```

**static**:

```
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = org.apache.flume.interceptor.StaticInterceptor$Builder
# a1.sources.r1.interceptors.i1.headerName = key
a1.sources.r1.interceptors.i1.key = key
a1.sources.r1.interceptors.i1.value = datacenter

## preserveExisting同名key不覆盖
a1.sources.r1.interceptors.i1.preserveExisting = false 

```

AutoMappingKeyInterceptor:

```
##拦截器
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.flume.interceptor.AutoMappingKeyInterceptor$Builder
a1.sources.r1.interceptors.i1.key= key
```



### flume自定义插件开发与部署

新建 flume-customdevolep 项目。

**自定义开发拦截器**

* AutoMappingKeyInterceptor



|  组件名   | 类型  |作用|备注|
|  ----  | ----  |----|----|
|AutoMappingKeyInterceptor|拦截器|自动映射kafka topic key|通过a1.sources.r1.interceptors.i1.key= key 来控制具体按照传入数据的哪个key来进行映射成topic message的key|



### 模拟不同的协议向采集服务发送数据,进行测试

#### tcp协议发送数据






